{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SCLSZFI4wIq"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRlgobfM27CF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import faiss\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S31hVJ-x27CG"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dotdZMsN27CH"
      },
      "outputs": [],
      "source": [
        "llm_choice = \"gpt\"\n",
        "openai.api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "client = openai.Client(api_key=userdata.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Set environment variable to prevent runtime issues\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
        "\n",
        "# Load tokenizer and model for embedding\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
        "\n",
        "# gpu for embed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARfJU-qb27CI"
      },
      "source": [
        "### Load knowledge data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UJL_uZo27CI"
      },
      "outputs": [],
      "source": [
        "# Load knowledge database\n",
        "rag_db = pd.read_csv(\"./dataset/kd.csv\")\n",
        "rag_data = rag_db[['ID', 'Content']].dropna()\n",
        "\n",
        "# Load knowledge database\n",
        "rag_data['Content'] = rag_data['Content'].str.lower()\n",
        "documents = rag_data['Content'].tolist()\n",
        "doc_ids = rag_data['ID'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gYaYgf27CI"
      },
      "source": [
        "### embed knowledge text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY0cN1J927CI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def embed(documents, batch_size=20):\n",
        "    \"\"\"Generate embeddings in batches to prevent memory overflow.\"\"\"\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        print(f\"embedding {i} / {len(documents)}\")\n",
        "        batch = documents[i:i+batch_size]\n",
        "        # Tokenize the batch with dynamic padding\n",
        "        inputs = tokenizer(batch, padding=\"longest\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        # move input to GPU\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "        # Disable gradient calculation for inference\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        #\n",
        "        embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
        "        all_embeddings.append(embeddings)\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "# Define the path for saving/loading embeddings\n",
        "embeddings_path = './dataset/doc_embeddings.npy'\n",
        "if os.path.exists(embeddings_path):\n",
        "    print(\"Loading embeddings from file...\")\n",
        "    # Load embeddings using memory mapping to avoid loading the entire file into RAM\n",
        "    doc_embeddings = np.load(embeddings_path, mmap_mode='r')\n",
        "else:\n",
        "    print(\"Generating embeddings in batches...\")\n",
        "    doc_embeddings = embed(documents)\n",
        "    np.save(embeddings_path, doc_embeddings)  # Save embeddings to disk\n",
        "\n",
        "# Build the FAISS index using L2 distance\n",
        "dimension = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(doc_embeddings)\n",
        "print(\"FAISS indexing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaqxNZZB27CJ"
      },
      "source": [
        "### Batch Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v0J84pui27CJ"
      },
      "outputs": [],
      "source": [
        "# Retrieve and validate documents\n",
        "def retrieve_documents(queries, k=3):\n",
        "    \"\"\"\n",
        "    Efficiently retrieves relevant documents for a batch of queries.\n",
        "\n",
        "    Args:\n",
        "        queries (list of str): A list of query strings.\n",
        "        k (int): The number of documents to retrieve per query.\n",
        "\n",
        "    Returns:\n",
        "        list of dict: A list where each element corresponds to a query and contains\n",
        "                      the keys 'ids' and 'contents' for the retrieved documents.\n",
        "    \"\"\"\n",
        "    # Embed all queries at once to avoid redundant computations\n",
        "    query_embeddings = embed([q.strip().lower() for q in queries])\n",
        "\n",
        "    # Perform batch search in the index\n",
        "    distances, indices = index.search(np.array(query_embeddings), k)\n",
        "\n",
        "    # Retrieve and return documents and their IDs for each query\n",
        "    results = []\n",
        "    for query_indices in indices:\n",
        "        result = {\n",
        "            \"ids\": [doc_ids[i] for i in query_indices],\n",
        "            \"contents\": [documents[i] for i in query_indices]\n",
        "        }\n",
        "        results.append(result)\n",
        "    return results\n",
        "\n",
        "# Roughly truncates the input text to a maximum number of characters.\n",
        "def truncate_text(text, max_chars=4096):\n",
        "    return text[:max_chars] if len(text) > max_chars else text\n",
        "\n",
        "\n",
        "def generate_responses(queries, contexts):\n",
        "    \"\"\"\n",
        "    Efficiently generates responses for a batch of queries.\n",
        "\n",
        "    Args:\n",
        "        queries (list of str): A list of query strings.\n",
        "        contexts (list of str): A list of context strings corresponding to each query.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of generated responses.\n",
        "    \"\"\"\n",
        "    messages_batch = [\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": (\n",
        "                \"You are recognized as a Kubernetes and NGINX ingress expert. Before providing an answer, validate the provided context for \"\n",
        "                \"errors, deprecated features, or potential conflicts. Always adhere to the latest Kubernetes and NGINX standards. \"\n",
        "                \"Identify and clearly explain any assumptions made based on the context, and provide necessary corrections or enhancements.\"\n",
        "            )},\n",
        "            {\"role\": \"user\", \"content\": (\n",
        "                f\"Given the following detailed context and choose what you think fit information for question:\\n{context}\\nCan you provide a validated and comprehensive response to this query:\\n{query}\\n\"\n",
        "                \"Your response should:\\n\"\n",
        "                \"1. Include YAML configurations with accurate and effective annotations tailored to address the query.\\n\"\n",
        "                \"2. Explain the rationale behind each configuration and validate them against the provided context and current best practices.\\n\"\n",
        "                \"3. Highlight and discuss any potential issues or critical assumptions that could affect the implementation.\\n\"\n",
        "                \"4. Offer detailed debugging steps and troubleshooting advice to verify and refine the solution.\"\n",
        "            )}\n",
        "        ] for query, context in zip(queries, contexts)\n",
        "    ]\n",
        "\n",
        "    responses = []\n",
        "    for messages in messages_batch:\n",
        "        try:\n",
        "            # Fix for openai 1.0.0+\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=messages,\n",
        "                max_tokens=4090,\n",
        "                temperature=0\n",
        "            )\n",
        "            responses.append(response.choices[0].message.content)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            responses.append(\"Error occurred.\")\n",
        "\n",
        "    return responses\n",
        "\n",
        "\n",
        "def process_questions(file_path, batch_size=40):\n",
        "    # Load CSV file into a Pandas DataFrame\n",
        "    df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
        "\n",
        "    # Ensure necessary columns exist\n",
        "    context_col_name = f\"{llm_choice}_Top_3_Contexts\"\n",
        "    response_col_name = f\"{llm_choice}_Generated_Response\"\n",
        "    context_id_col_name = f\"{llm_choice}_Context_IDs\"\n",
        "\n",
        "    if context_col_name not in df.columns:\n",
        "        df[context_col_name] = \"\"  # Initialize with empty strings\n",
        "    if response_col_name not in df.columns:\n",
        "        df[response_col_name] = \"\"  # Initialize with empty strings\n",
        "    if context_id_col_name not in df.columns:\n",
        "        df[context_id_col_name] = \"\"  # Initialize with empty strings\n",
        "\n",
        "    # Get list of unanswered questions\n",
        "    unanswered_mask = df[f\"{llm_choice}_Generated_Response\"] == \"\"\n",
        "    unanswered_df = df[unanswered_mask]\n",
        "\n",
        "    # Process questions in batches\n",
        "    for start in range(0, len(unanswered_df), batch_size):\n",
        "        batch = unanswered_df.iloc[start : start + batch_size]\n",
        "\n",
        "        queries = batch[\"Question Body\"].tolist()\n",
        "\n",
        "        # Retrieve context in batch (each result is a dict with 'ids' and 'contents')\n",
        "        results = retrieve_documents(queries, k=3)\n",
        "        # For each result, truncate each document's content to 1000 tokens and join them\n",
        "        contexts = [\n",
        "            \" \".join([truncate_text(doc) for doc in result[\"contents\"]])\n",
        "            for result in results\n",
        "        ]\n",
        "        # Create context_id strings (concatenated document IDs)\n",
        "        context_ids = [\", \".join([str(doc_id) for doc_id in result['ids']]) for result in results]\n",
        "\n",
        "        # Generate responses in batch\n",
        "        responses = generate_responses(queries, contexts)\n",
        "\n",
        "        # Update the DataFrame with responses\n",
        "        df.loc[batch.index, response_col_name] = responses\n",
        "        df.loc[batch.index, context_col_name] = contexts\n",
        "        df.loc[batch.index, context_id_col_name] = context_ids\n",
        "        print(f\"Processed {start + len(batch)} / {len(unanswered_df)} questions\")\n",
        "\n",
        "    # Save the updated CSV file with responses\n",
        "    df.to_csv(file_path, index=False, encoding=\"utf-8\")\n",
        "    print(f\"All questions processed and saved back to {file_path}\")\n",
        "\n",
        "# Run batch processing\n",
        "file_path = \"./dataset/test.csv\"  # Replace with your CSV file path\n",
        "process_questions(file_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
