Question ID,Question Title,Question Body,Question Tags,Answer ID,Answer Score,Answer Body
64125048,"get error ""unknown field ""servicename"" in io.k8s.api.networking.v1.ingressbackend"" when switch from v1beta1 to v1 in kubernetes ingress","i had the below yaml for my ingress and it worked (and continues to work):
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: test-ingress
  namespace: test-layer
annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
    - host: mylocalhost.com
      http:
        paths:
          - path: /
            backend:
              servicename: test-app
              serviceport: 5000

however, it tells me that it's deprecated and i should change to using networking.k8s.io/v1. when i do that (see below) it throws an error.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: test-ingress
  namespace: test-layer
annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
    - host: mylocalhost.com
      http:
        paths:
          - path: /
            backend:
              servicename: test-app
              serviceport: 5000

error
error: error validating &quot;test-ingress.yaml&quot;: 
  error validating data: [validationerror(ingress.spec.rules[0].http.paths[0].backend): 
    unknown field &quot;servicename&quot; in io.k8s.api.networking.v1.ingressbackend, 
    validationerror(ingress.spec.rules[0].http.paths[0].backend): 
      unknown field &quot;serviceport&quot; in io.k8s.api.networking.v1.ingressbackend]; 
      if you choose to ignore these errors, turn validation off with --validate=false

other than changing the api version, i made no other changes.
kubectl version returns:
client version: version.info{major:&quot;1&quot;, minor:&quot;19&quot;, gitversion:&quot;v1.19.0&quot;, gitcommit:&quot;e19964183377d0ec2052d1f1fa930c4d7575bd50&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2020-08-26t14:30:33z&quot;, goversion:&quot;go1.15&quot;, compiler:&quot;gc&quot;, platform:&quot;windows/amd64&quot;}

server version: version.info{major:&quot;1&quot;, minor:&quot;19&quot;, gitversion:&quot;v1.19.0&quot;, gitcommit:&quot;e19964183377d0ec2052d1f1fa930c4d7575bd50&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2020-08-26t14:23:04z&quot;, goversion:&quot;go1.15&quot;, compiler:&quot;gc&quot;, platform:&quot;linux/amd64&quot;}

",<kubernetes><kubectl><kubernetes-ingress><minikube>,64126069,299,"i think that this pr contains the change you're asking about.
`ingress` and `ingressclass` resources have graduated to `networking.k8s.io/v1`. ingress and ingressclass types in the `extensions/v1beta1` and `networking.k8s.io/v1beta1` api versions are deprecated and will no longer be served in 1.22+. persisted objects can be accessed via the `networking.k8s.io/v1` api. notable changes in v1 ingress objects (v1beta1 field names are unchanged):
* `spec.backend` -&gt; `spec.defaultbackend`
* `servicename` -&gt; `service.name`
* `serviceport` -&gt; `service.port.name` (for string values)
* `serviceport` -&gt; `service.port.number` (for numeric values)
* `pathtype` no longer has a default value in v1; &quot;exact&quot;, &quot;prefix&quot;, or &quot;implementationspecific&quot; must be specified
other ingress api updates:
* backends can now be resource or service backends
* `path` is no longer required to be a valid regular expression

if you look in the 1.19 ingress doc, it looks like the new syntax would be:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: minimal-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /testpath
        pathtype: prefix
        backend:
          service:
            name: test
            port:
              number: 80

i unfortunately don't have a 1.19 cluster to test myself, but i think this is what you're running into.
"
49918313,413 error with kubernetes and nginx ingress controller,"i'm trying to change the client_max_body_size value, so my nginx ingress will not return the http 413 content too large error (as seen in the logs).
i've tested a few solutions.
here is my config map:
kind: configmap
apiversion: v1
data:
  proxy-connect-timeout: &quot;15&quot;
  proxy-read-timeout: &quot;600&quot;
  proxy-send-timeout: &quot;600&quot;
  proxy-body-size: &quot;8m&quot;
  hsts-include-subdomains: &quot;false&quot;
  body-size: &quot;64m&quot;
  server-name-hash-bucket-size: &quot;256&quot;
  client-max-body-size: &quot;50m&quot;
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app: ingress-nginx

these changes had no effect at all: in nginx controller's log i can see the information about reloading the config map, but the values in nginx.conf are the same:
$ cat /etc/nginx/nginx.conf | grep client_max                                                                                                       
                            client_max_body_size                    &quot;8m&quot;;
                            client_max_body_size                    &quot;1m&quot;;
                            client_max_body_size                    &quot;1m&quot;;
                            client_max_body_size                    &quot;1m&quot;;
                            client_max_body_size                    &quot;1m&quot;;
                            client_max_body_size                    &quot;1m&quot;;
                            client_max_body_size                    &quot;1m&quot;;
                            client_max_body_size                    &quot;1m&quot;;
                            client_max_body_size                    &quot;1m&quot;;
                            client_max_body_size                    &quot;1m&quot;;
                            client_max_body_size                    &quot;1m&quot;;
                            client_max_body_size                    &quot;1m&quot;;
                            client_max_body_size                    &quot;1m&quot;;

my nginx-controller config uses this image:
quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.13.0
how can i force nginx to change this setting? i need to change it globally, for all my ingresses.
",<kubernetes><nginx><kubernetes-ingress><nginx-ingress><ingress-controller>,49918432,119,"you can use the annotation nginx.ingress.kubernetes.io/proxy-body-size to set the max-body-size option right in your ingress object instead of changing a base configmap.

here is the example of usage:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-app
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: ""50m""
...

"
42170380,how to add users to kubernetes (kubectl)?,"i've created a kubernetes cluster on aws with kops and can successfully administer it via kubectl from my local machine.

i can view the current config with kubectl config view as well as directly access the stored state at ~/.kube/config, such as:

apiversion: v1
clusters:
- cluster:
    certificate-authority-data: redacted
    server: https://api.{cluster_name}
  name: {cluster_name}
contexts:
- context:
    cluster: {cluster_name}
    user: {cluster_name}
  name: {cluster_name}
current-context: {cluster_name}
kind: config
preferences: {}
users:
- name: {cluster_name}
  user:
    client-certificate-data: redacted
    client-key-data: redacted
    password: redacted
    username: admin
- name: {cluster_name}-basic-auth
  user:
    password: redacted
    username: admin


i need to enable other users to also administer. this user guide describes how to define these on another users machine, but doesn't describe how to actually create the user's credentials within the cluster itself. how do you do this?

also, is it safe to just share the cluster.certificate-authority-data?
",<kubernetes><kubectl><kops>,42186135,111,"for a full overview on authentication, refer to the official kubernetes docs on authentication and authorization
for users, ideally you use an identity provider for kubernetes (openid connect).
if you are on gke / acs you integrate with respective identity and access management frameworks
if you self-host kubernetes (which is the case when you use kops), you may use coreos/dex to integrate with ldap / oauth2 identity providers - a good reference is this detailed 2 part sso for kubernetes article.
kops (1.10+) now has built-in authentication support which eases the integration with aws iam as identity provider if you're on aws.
for dex there are a few open source cli clients as follows:

nordstrom/kubelogin
pusher/k8s-auth-example

if you are looking for a quick and easy (not most secure and easy to manage in the long run) way to get started, you may abuse serviceaccounts - with 2 options for specialised policies to control access. (see below)
note since 1.6  role based access control is strongly recommended! this answer does not cover rbac setup
edit: great, but outdated (2017-2018), guide by bitnami on user setup with rbac is also available.
steps to enable service account access are (depending on if your cluster configuration includes rbac or abac policies, these accounts may have full admin rights!):
edit: here is a bash script to automate service account creation - see below steps

create service account for user alice
kubectl create sa alice


get related secret
secret=$(kubectl get sa alice -o json | jq -r .secrets[].name)


get ca.crt from secret (using osx base64 with -d flag for decode)
kubectl get secret $secret -o json | jq -r '.data[&quot;ca.crt&quot;]' | base64 -d &gt; ca.crt


get service account token from secret
user_token=$(kubectl get secret $secret -o json | jq -r '.data[&quot;token&quot;]' | base64 -d)


get information from your kubectl config (current-context, server..)
# get current context
c=$(kubectl config current-context)

# get cluster name of context
name=$(kubectl config get-contexts $c | awk '{print $3}' | tail -n 1)

# get endpoint of current context 
endpoint=$(kubectl config view -o jsonpath=&quot;{.clusters[?(@.name == \&quot;$name\&quot;)].cluster.server}&quot;)


on a fresh machine, follow these steps (given the ca.cert and $endpoint information retrieved above:

install kubectl
 brew install kubectl


set cluster (run in directory where ca.crt is stored)
 kubectl config set-cluster cluster-staging \
   --embed-certs=true \
   --server=$endpoint \
   --certificate-authority=./ca.crt


set user credentials
 kubectl config set-credentials alice-staging --token=$user_token


define the combination of alice user with the staging cluster
 kubectl config set-context alice-staging \
   --cluster=cluster-staging \
   --user=alice-staging \
   --namespace=alice


switch current-context to alice-staging for the user
 kubectl config use-context alice-staging





to control user access with policies (using abac), you need to create a policy file (for example):
{
  &quot;apiversion&quot;: &quot;abac.authorization.kubernetes.io/v1beta1&quot;,
  &quot;kind&quot;: &quot;policy&quot;,
  &quot;spec&quot;: {
    &quot;user&quot;: &quot;system:serviceaccount:default:alice&quot;,
    &quot;namespace&quot;: &quot;default&quot;,
    &quot;resource&quot;: &quot;*&quot;,
    &quot;readonly&quot;: true
  }
}

provision this policy.json on every master node and add --authorization-mode=abac --authorization-policy-file=/path/to/policy.json flags to api servers
this would allow alice (through her service account) read only rights to all resources in default namespace only.
"
72256006,service account secret is not listed. how to fix it?,"i have used kubectl create serviceaccount sa1 to create service account. then i used kubectl get serviceaccount sa1 -oyaml command to get service account info. but it returns as below.
apiversion: v1
kind: serviceaccount
metadata:
  creationtimestamp: &quot;2022-05-16t08:03:50z&quot;
  name: sa1
  namespace: default
  resourceversion: &quot;19651&quot;
  uid: fdddacba-be9d-4e77-a849-95ca243781cc

i need to get,
secrets:
- name: &lt;secret&gt;

part. but it doesn't return secrets. how to fix it?
",<kubernetes><kubectl>,72258300,101,"in kubernetes 1.24, serviceaccount token secrets are no longer automatically generated.  see &quot;urgent upgrade notes&quot; in the 1.24 changelog file:

the legacyserviceaccounttokennoautogeneration feature gate is beta, and enabled by default. when enabled, secret api objects containing service account tokens are no longer auto-generated for every serviceaccount. use the tokenrequest api to acquire service account tokens, or if a non-expiring token is required, create a secret api object for the token controller to populate with a service account token by following this guide. (#108309, @zshihang)

this means, in kubernetes 1.24, you need to manually create the secret; the token key in the data field will be automatically set for you.
apiversion: v1
kind: secret
metadata:
  name: sa1-token
  annotations:
    kubernetes.io/service-account.name: sa1
type: kubernetes.io/service-account-token

since you're manually creating the secret, you know its name: and don't need to look it up in the serviceaccount object.
this approach should work fine in earlier versions of kubernetes too.
"
56003777,how to pass environment variable in kubectl deployment?,"i am setting up the kubernetes setup for django webapp.

i am passing environment variable while creating deployment as below

kubectl create -f deployment.yml -l key1=value1 


i am getting error as below

error: no objects passed to create


able to create the deployment successfully, if i remove the env variable -l key1=value1 while creating deployment.

deployment.yaml as below

#deployment
apiversion: extensions/v1beta1
kind: deployment
metadata: 
 labels: 
   service: sigma-service
 name: $key1


what will be the reason for causing the above error while creating deployment?
",<kubernetes><kubectl>,56009991,97,"i used envsubst (https://www.gnu.org/software/gettext/manual/html_node/envsubst-invocation.html) for this. create a deployment.yaml

apiversion: apps/v1
kind: deployment
metadata:
  name: $name
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerport: 80


then:

export name=my-test-nginx
envsubst &lt; deployment.yaml | kubectl apply -f -


not sure what os are you using to run this. on macos, envsubst installed like:

brew install gettext
brew link --force gettext 

"
55213545,helm range without leaving global scope,"i need to loop through a list of instances and create 1 stateful set for every instance. however, inside range i then limit myself to the scope of that loop. i need to access some global values in my statefulset.

i've solved it by just putting all global objects i need in an env variable but... this very seems hacky.

what is the correct way to loop through ranges while still being able to reference global objects?

example of my loop

{{- $values := .values -}}
{{- $release := .release -}}

{{- range .values.nodes }}

apiversion: apps/v1
kind: statefulset
metadata:
  name: {{ $release.name }} &lt;-- global scope
  labels:
    .
    .
    .    
        env:
          - name: ip_address
            value: {{ .ip_address }} &lt;-- from range scope
    .
    .
    .
{{- end }}


example of values

# global
image:
  repository: ..ecr.....

# instances
nodes:

  - node1:
    name: node-1
    ip: 1.1.1.1
  - node2:
    name: node-2
    ip: 1.1.1.1

",<kubernetes><kubernetes-helm>,55257763,90,"when entering a loop block you lose your global context when using .. you can access the global context by using $. instead.
as written in the helm docs -

there is one variable that is always global - $ - this variable will always point to the root context. this can be very useful when you are looping in a range and need to know the chart's release name.

in your example, using this would look something like:
{{- range .values.nodes }}
apiversion: apps/v1
kind: statefulset
metadata:
  name: {{ $.release.name }}
  labels:
    .
    .
    .    
        env:
          - name: ip_address
            value: {{ .ip_address }}
    .
    .
    .
{{- end }}

"
59844622,ingress configuration for k8s in different namespaces,"i need to configure ingress nginx on azure k8s, and my question is if is possible to have ingress configured in one namespace et. ingress-nginx and some serivces in other namespace eg. resources?
my files looks like so:

# ingress-nginx.yaml
apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
spec:
  replicas: 3
  selector:
    matchlabels:
      app: ingress-nginx
  template:
    metadata:
      labels:
        app: ingress-nginx
      annotations:
        prometheus.io/port: '10254'
        prometheus.io/scrape: 'true' 
    spec:
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.12.0
          args:
            - /nginx-ingress-controller
            - --default-backend-service=$(pod_namespace)/default-http-backend
            - --configmap=$(pod_namespace)/nginx-configuration
            - --tcp-services-configmap=$(pod_namespace)/tcp-services
            - --udp-services-configmap=$(pod_namespace)/udp-services
            - --annotations-prefix=nginx.ingress.kubernetes.io
            - --publish-service=$(pod_namespace)/ingress-nginx
          env:
            - name: pod_name
              valuefrom:
                fieldref:
                  fieldpath: metadata.name
            - name: pod_namespace
              valuefrom:
                fieldref:
                  fieldpath: metadata.namespace
          ports:
          - name: http
            containerport: 80
          - name: https
            containerport: 443
          livenessprobe:
            failurethreshold: 3
            httpget:
              path: /healthz
              port: 10254
              scheme: http
            initialdelayseconds: 10
            periodseconds: 10
            successthreshold: 1
            timeoutseconds: 1
          readinessprobe:
            failurethreshold: 3
            httpget:
              path: /healthz
              port: 10254
              scheme: http
            periodseconds: 10
            successthreshold: 1
            timeoutseconds: 1


# configmap.yaml
kind: configmap
apiversion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app: ingress-nginx
---
kind: configmap
apiversion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
---
kind: configmap
apiversion: v1
metadata:
  name: udp-services
  namespace: ingress-nginx
---
# default-backend.yaml
apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: default-http-backend
  labels:
    app: default-http-backend
  namespace: ingress-nginx
spec:
  replicas: 1
  selector:
    matchlabels:
      app: default-http-backend
  template:
    metadata:
      labels:
        app: default-http-backend
    spec:
      terminationgraceperiodseconds: 60
      containers:
      - name: default-http-backend
        # any image is permissible as long as:
        # 1. it serves a 404 page at /
        # 2. it serves 200 on a /healthz endpoint
        image: gcr.io/google_containers/defaultbackend:1.4
        livenessprobe:
          httpget:
            path: /healthz
            port: 8080
            scheme: http
          initialdelayseconds: 30
          timeoutseconds: 5
        ports:
        - containerport: 8080
        resources:
          limits:
            cpu: 10m
            memory: 20mi
          requests:
            cpu: 10m
            memory: 20mi
---
apiversion: v1
kind: service
metadata:
  name: default-http-backend
  namespace: ingress-nginx
  labels:
    app: default-http-backend
spec:
  ports:
  - port: 80
    targetport: 8080
  selector:
    app: default-http-backend



kind: service
apiversion: v1
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app: ingress-nginx
spec:
  externaltrafficpolicy: local
  type: loadbalancer
  selector:
    app: ingress-nginx
  ports:
  - name: http
    port: 80
    targetport: http
  - name: https
    port: 443
    targetport: https


        # app-ingress.yaml
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: app-ingress
  namespace: ingress-nginx
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
    - hosts:
      - api-sand.fake.com
  rules:
  - host: api-sand.fake.com
    http:
      paths:
      - backend:
          servicename: api-sand
          serviceport: 80
        path: /



and then i have some app running in the resources namespace, and problem is that i am getting the following error 

error obtaining service endpoints: error getting service resources/api-sand from the cache: service resources/api-sand was not found



if i deploy api-sand in the same namespace where ingress is then this service works fine.
",<kubernetes><kubernetes-ingress><nginx-ingress>,59845018,80,"instead of creating the ingress app-ingress in ingress-nginx namespace you should create it in the namespace where you have the service api-sandand the pod.
alternatively there is way to achieve ingress in one namespace and service in another namespace via externalname.checkout kubernetes cross namespace ingress network
here is an example referred from here.
kind: service
apiversion: v1
metadata:
  name: my-service
spec:
  type: externalname
  externalname: test-service.namespacename.svc.cluster.local


apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: example-ingress
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        backend:
          servicename: my-service
          serviceport: 80

"
53429486,kubernetes - how to define configmap built using a file in a yaml?,"at present i am creating a configmap from the file config.json by executing:

kubectl create configmap jksconfig --from-file=config.json


i would want the configmap to be created as part of the deployment and tried to do this:

apiversion: v1
kind: configmap
metadata:
  name: jksconfig
data:
  config.json: |-
    {{ .files.get ""config.json"" | indent 4 }}


but doesn't seem to work. what should be going into configmap.yaml so that the same configmap is created? 

---update---

when i do a helm install dry run:

# source: mychartv2/templates/jks-configmap.yaml
apiversion: v1
kind: configmap
metadata:
  name: jksconfig
data:
  config.json: |


note: i am using minikube as my kubernetes cluster
",<kubernetes><minikube><kubernetes-helm><configmap>,53447306,69,"your config.json file should be inside your mychart/ directory, not inside  mychart/templates
chart template guide
configmap.yaml
apiversion: v1
kind: configmap
metadata:
  name: {{ .release.name }}-configmap
data:
  config.json: |-
{{ .files.get &quot;config.json&quot; | indent 4}}

config.json
{
    &quot;val&quot;: &quot;key&quot;
}

helm install --dry-run --debug mychart
[debug] created tunnel using local port: '52091'     
                                                     
[debug] server: &quot;127.0.0.1:52091&quot;                    
                                                     
...           
                                                     
name:   dining-saola                                 
revision: 1                                          
released: fri nov 23 15:06:17 2018                   
chart: mychart-0.1.0                                 
user-supplied values:                                
{}                                                   
                                                     
...
                                                     
---                                                  
# source: mychart/templates/configmap.yaml           
apiversion: v1                                       
kind: configmap                                      
metadata:                                            
  name: dining-saola-configmap                       
data:                                                
  config.json: |-                                    
    {                                                
        &quot;val&quot;: &quot;key&quot;                                 
    }     

edit:

but i want it the values in the config.json file to be taken from values.yaml. is that possible?

configmap.yaml
apiversion: v1
kind: configmap
metadata:
  name: {{ .release.name }}-configmap
data:
  config.json: |-
    {
{{- range $key, $val := .values.json }}
{{ $key | quote | indent 6}}: {{ $val | quote }}
{{- end}}
    }

values.yaml
json:
  key1: val1
  key2: val2
  key3: val3

helm install --dry-run --debug mychart
# source: mychart/templates/configmap.yaml
apiversion: v1
kind: configmap
metadata:
  name: mangy-hare-configmap
data:
  config.json: |-
    {
      &quot;key1&quot;: &quot;val1&quot;
      &quot;key2&quot;: &quot;val2&quot;
      &quot;key3&quot;: &quot;val3&quot;
    }

"
37555281,create kubernetes pod with volume using kubectl run,"i understand that you can create a pod with deployment/job using kubectl run.  but is it possible to create one with a volume attached to it?  i tried running this command:

kubectl run -i --rm --tty ubuntu --overrides='{ ""apiversion"":""batch/v1"", ""spec"": {""containers"": {""image"": ""ubuntu:14.04"", ""volumemounts"": {""mountpath"": ""/home/store"", ""name"":""store""}}, ""volumes"":{""name"":""store"", ""emptydir"":{}}}}' --image=ubuntu:14.04 --restart=never -- bash


but the volume does not appear in the interactive bash.

is there a better way to create a pod with volume that you can attach to?
",<kubernetes><kubectl>,37621761,61,"your json override is specified incorrectly. unfortunately kubectl run just ignores fields it doesn't understand.

kubectl run -i --rm --tty ubuntu --overrides='
{
  ""apiversion"": ""batch/v1"",
  ""spec"": {
    ""template"": {
      ""spec"": {
        ""containers"": [
          {
            ""name"": ""ubuntu"",
            ""image"": ""ubuntu:14.04"",
            ""args"": [
              ""bash""
            ],
            ""stdin"": true,
            ""stdinonce"": true,
            ""tty"": true,
            ""volumemounts"": [{
              ""mountpath"": ""/home/store"",
              ""name"": ""store""
            }]
          }
        ],
        ""volumes"": [{
          ""name"":""store"",
          ""emptydir"":{}
        }]
      }
    }
  }
}
'  --image=ubuntu:14.04 --restart=never -- bash


to debug this issue i ran the command you specified, and then in another terminal ran:

kubectl get job ubuntu -o json


from there you can see that the actual job structure differs from your json override (you were missing the nested template/spec, and volumes, volumemounts, and containers need to be arrays).
"
47021469,how to set google_application_credentials on gke running through kubernetes,"with the help of kubernetes i am running daily jobs on gke, on a daily basis based on cron configured in kubernetes a new container spins up and try to insert some data into bigquery.

the setup that we have is we have 2 different projects in gcp in one project we maintain the data in bigquery in other project we have all the gke running so when gke has to interact with different project resource my guess is i have to set an environment variable with name  google_application_credentials which points to a service account json file, but since every day kubernetes is spinning up a new container i am not sure how and where i should set this variable.

thanks in advance!

note: this file is parsed as a golang template by the drone-gke plugin.

---
apiversion: v1
kind: secret
metadata:
  name: my-data-service-account-credentials
type: opaque
data:
  sa_json: ""bas64jsonserviceaccount""
---
apiversion: v1
kind: pod
metadata:
  name: adtech-ads-apidata-el-adunit-pod
spec:
  containers:
  - name: adtech-ads-apidata-el-adunit-container
    volumemounts:
    - name: service-account-credentials-volume
     mountpath: ""/etc/gcp""
     readonly: true
  volumes:
  - name: service-account-credentials-volume
    secret:
      secretname: my-data-service-account-credentials
      items:
      - key: sa_json
        path: sa_credentials.json




this is our cron jobs for loading the adunit data

apiversion: batch/v2alpha1
kind: cronjob
metadata:
  name: adtech-ads-apidata-el-adunit
spec:
  schedule: ""*/5 * * * *""
  suspend: false
  concurrencypolicy: replace
  successfuljobshistorylimit: 10
  failedjobshistorylimit: 10
  jobtemplate:
    spec:
      template:
        spec:
          containers:
          - name: adtech-ads-apidata-el-adunit-container
            image: {{.image}}
            args:
            - -cp
            - opt/nyt/dfpdataingestion-1.0-jar-with-dependencies.jar
            - com.nyt.cron.adunitjob
            env:
              - name: env_app_name
                value: ""{{.env_app_name}}""
              - name: env_app_context_name
                value: ""{{.env_app_context_name}}""
              - name: env_google_projectid
                value: ""{{.env_google_projectid}}""
              - name: env_google_datasetid
                value: ""{{.env_google_datasetid}}""
              - name: env_reporting_datasetid
                value: ""{{.env_reporting_datasetid}}""
              - name: env_adbridge_datasetid
                value: ""{{.env_adbridge_datasetid}}""
              - name: env_salesforce_datasetid
                value: ""{{.env_salesforce_datasetid}}""
              - name: env_cloud_platform_url
                value: ""{{.env_cloud_platform_url}}""
              - name: env_smtp_host
                value: ""{{.env_smtp_host}}""
              - name: env_to_email
                value: ""{{.env_to_email}}""
              - name: env_from_email
                value: ""{{.env_from_email}}""
              - name: env_aws_username
                value: ""{{.env_aws_username}}""
              - name: env_client_id
                value: ""{{.env_client_id}}""
              - name: env_refresh_token
                value: ""{{.env_refresh_token}}""
              - name: env_network_code
                value: ""{{.env_network_code}}""
              - name: env_application_name
                value: ""{{.env_application_name}}""
              - name: env_salesforce_username
                value: ""{{.env_salesforce_username}}""
              - name: env_salesforce_url
                value: ""{{.env_salesforce_url}}""
              - name: google_application_credentials
                value: ""/etc/gcp/sa_credentials.json""
              - name: env_cloud_sql_url
                valuefrom:
                  secretkeyref:
                    name: secrets
                    key: cloud_sql_url
              - name: env_aws_password
                valuefrom:
                  secretkeyref:
                    name: secrets
                    key: aws_password
              - name: env_client_secret
                valuefrom:
                  secretkeyref:
                    name: secrets
                    key: dfp_client_secret
              - name: env_salesforce_password
                valuefrom:
                  secretkeyref:
                    name: secrets
                    key: salesforce_password


          restartpolicy: onfailure



",<kubernetes><google-kubernetes-engine>,47023291,60,"so, if your gke project is project my-gke, and the project containing the services/things your gke containers need access to is project my-data, one approach is to:


create a service account in the my-data project. give it whatever gcp roles/permissions are needed (ex. roles/bigquery.
dataviewer if you have some bigquery tables that your my-gke gke containers need to read).


create a service account key for that service account. when you do this in the console following https://cloud.google.com/iam/docs/creating-managing-service-account-keys, you should automatically download a .json file containing the sa credentials.

create a kubernetes secret resource for those service account credentials. it might look something like this:

apiversion: v1
kind: secret
metadata:
  name: my-data-service-account-credentials
type: opaque
data:
  sa_json: &lt;contents of running 'base64 the-downloaded-sa-credentials.json'&gt;

mount the credentials in the container that needs access:

[...]
spec:
  containers:
  - name: my-container
    volumemounts:
    - name: service-account-credentials-volume
      mountpath: /etc/gcp
      readonly: true
[...]
  volumes:
  - name: service-account-credentials-volume
    secret:
      secretname: my-data-service-account-credentials
      items:
      - key: sa_json
        path: sa_credentials.json

set the google_application_credentials environment variable in the container to point to the path of the mounted credentials:

[...]
spec:
  containers:
  - name: my-container
    env:
    - name: google_application_credentials
      value: /etc/gcp/sa_credentials.json



with that, any official gcp clients (ex. the gcp python client, gcp java client, gcloud cli, etc. should respect the google_application_credentials env var and, when making api requests, automatically use the credentials of the my-data service account that you created and mounted the credentials .json file for.
"
58075103,error: error installing: the server could not find the requested resource helm kubernetes,"what i did:
i installed helm with

curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash
helm init --history-max 200



getting an error:

$helm_home has been configured at /root/.helm.
error: error installing: the server could not find the requested resource



what does that error mean?
how should i install helm and tiller?


ubuntu version: 18.04
kubernetes version: 1.16
helm version: 

helm version
client: &amp;version.version{semver:""v2.14.3"", gitcommit:""0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085"", gittreestate:""clean""}
error: could not find tiller


update:
i tried @shawndodo's answer but still tiller not installed

helm init --service-account tiller --override spec.selector.matchlabels.'name'='tiller',spec.selector.matchlabels.'app'='helm'
--output yaml | sed 's@apiversion: extensions/v1beta1@apiversion: apps/v1@' | kubectl apply -f -


update 2:
helm init --history-max 200 working in kubernetes version 1.15
",<kubernetes><kubernetes-helm>,58079963,54,"i met the same problem, then i found this reply on here.

helm init --service-account tiller --override spec.selector.matchlabels.'name'='tiller',spec.selector.matchlabels.'app'='helm' --output yaml | sed 's@apiversion: extensions/v1beta1@apiversion: apps/v1@' | kubectl apply -f -


it works for me. you can see the detail in this issue.
"
67520866,"no matches for kind ""cronjob"" in version ""batch/v1""","i use kubernetes which v1.19.7, when i run the cronjob sample
apiversion: batch/v1
kind: cronjob
metadata:
  name: express-learn-cronjob
spec:
  schedule: &quot;*/1 * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          containers:
            - name: hello
              image: busybox
              command:
                - /bin/sh
                - -c
                - date; echo hello from the kubernetes cluster
          restartpolicy: onfailure

get  unable to recognize &quot;app-cronjob.yml&quot;: no matches for kind &quot;cronjob&quot; in version &quot;batch/v1&quot;
i can get the batch info by run kubectl api-versions | grep batch
batch/v1
batch/v1beta1

is there anything i missed? how can i fix it?
",<kubernetes><kubernetes-cronjob>,67521713,52,"for kubernetes version 1.19.x you need to use batch/v1beta1 as apiversion for your cronjob.
that is documented in the doc version 1-19:
https://v1-19.docs.kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/
it is stable only on k8s version 1.21.
"
30538210,how to mimic '--volumes-from' in kubernetes,"i'm looking for a pattern that allows to share volumes between two containers running on the same pod in kubernetes.

my use case is:
i have a ruby on rails application running inside a docker container.
the docker image contains static assets in /app/&lt;app-name&gt;/public directory, and i need to access those assets from the nginx container running alongside in the same pod.

in 'vanilla' docker i would have used --volumes-from flag to share this directory:

docker run --name app -v /app/&lt;app-dir&gt;/public &lt;app-image&gt;
docker run --volumes-from app nginx


after reading this doc: https://github.com/googlecloudplatform/kubernetes/blob/master/docs/volumes.md
i tried this (only relevant entries presented):

spec:
  containers:
    - image: &lt;app-image&gt;
      name: &lt;app-name&gt;
      volumemounts:
        - mountpath: /app/&lt;app-name&gt;/public
          name: assets
    - image: nginx
      name: nginx
      volumemounts:
        - mountpath: /var/www/html
          name: assets
          readonly: true
    volumes:
      - name: assets
        hostpath:
          path: /tmp/assets


but:


even though /tmp/assets on the node exists, it's empty
/app/&lt;app-name&gt;/public inside the app container is also empty


as a workaround i'm gonna try to populate the shared directory when the application container is up (simply cp /app/&lt;app-name&gt;/public/* to shared directory), but i really dislike this idea.

question: how to mimic --volumes-from in kubernetes, or if there is no direct counterpart, how can i share files from one container to other running in the same pod ?

apiversion: v1beta3

client version: version.info{major:""0"", minor:""17"", gitversion:""v0.17.0"", gitcommit:""82f8bdac06ddfacf493a9ed0fedc85f5ea62ebd5"", gittreestate:""clean""}
server version: version.info{major:""0"", minor:""17"", gitversion:""v0.17.0"", gitcommit:""82f8bdac06ddfacf493a9ed0fedc85f5ea62ebd5"", gittreestate:""clean""}

",<docker><kubernetes><google-kubernetes-engine>,33118902,51,"[update-2016-8] in latest kubernetes release, you can use a very nice feature named init-container to replace the poststart part in my answer below, which will make sure the container order.

apiversion: v1
kind: pod
metadata:
  name: javaweb-2
spec:
  initcontainers:
  - name: war
    image: resouer/sample:v2
    command: [""cp"", ""/sample.war"", ""/app""]
    volumemounts:
    - mountpath: /app
      name: app-volume
  containers:
  - name: tomcat
    image: resouer/mytomcat:7.0
    command: [""sh"",""-c"",""/root/apache-tomcat-7.0.42-v2/bin/start.sh""]
    volumemounts:
    - mountpath: /root/apache-tomcat-7.0.42-v2/webapps
      name: app-volume
    ports:
    - containerport: 8080
      hostport: 8001
  volumes:
  - name: app-volume
    emptydir: {}


note: initcontainer is still a beta feature so the work version of this yaml is actually like: http://kubernetes.io/docs/user-guide/production-pods/#handling-initialization, please notice the pod.beta.kubernetes.io/init-containers part.

---original answer begin---

actually, you can. you need to use container life cycle handler to control what files/dirs you want to share with other containers. like:

---
apiversion: v1
kind: pod
metadata:
    name: server
spec:
    restartpolicy: onfailure
    containers:
    - image: resouer/sample:v2
      name: war
      lifecycle:
        poststart:
          exec:
            command:
              - ""cp""
              - ""/sample.war""
              - ""/app""
      volumemounts:
      - mountpath: /app
        name: hostv1 
    - name: peer
      image: busybox
      command: [""tail"", ""-f"", ""/dev/null""]
      volumemounts:
      - name: hostv2
        mountpath: /app/sample.war
    volumes:
    - name: hostv1
      hostpath:
          path: /tmp
    - name: hostv2
      hostpath:
          path: /tmp/sample.war


please check my gist for more details:

https://gist.github.com/resouer/378bcdaef1d9601ed6aa

and of course you can use emptydir. thus, war container can share its /sample.war to peer container without mess peer's /app directory.

if we can tolerate /app been overridden, it will be much simpler:

---
apiversion: v1
kind: pod
metadata:
  name: javaweb-2
spec:
  restartpolicy: onfailure
  containers:
  - image: resouer/sample:v2
    name: war
    lifecycle:
      poststart:
        exec:
          command:
            - ""cp""
            - ""/sample.war""
            - ""/app""
    volumemounts:
    - mountpath: /app
      name: app-volume
  - image: resouer/mytomcat:7.0
    name: tomcat
    command: [""sh"",""-c"",""/root/apache-tomcat-7.0.42-v2/bin/start.sh""]
    volumemounts:
    - mountpath: /root/apache-tomcat-7.0.42-v2/webapps
      name: app-volume
    ports:
    - containerport: 8080
      hostport: 8001 
  volumes:
  - name: app-volume
    emptydir: {}

"
66080909,"logs complaining ""extensions/v1beta1 ingress is deprecated""","i'm adding an ingress as follows:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: cheddar
spec:
  rules:
  - host: cheddar.213.215.191.78.nip.io
    http:
      paths:
      - backend:
          service:
            name: cheddar
            port:
              number: 80
        path: /
        pathtype: implementationspecific

but the logs complain:
w0205 15:14:07.482439       1 warnings.go:67] extensions/v1beta1 ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 ingress
time=&quot;2021-02-05t15:14:07z&quot; level=info msg=&quot;updated ingress status&quot; namespace=default ingress=cheddar
w0205 15:18:19.104225       1 warnings.go:67] networking.k8s.io/v1beta1 ingressclass is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 ingressclasslist

why? what's the correct yaml to use?
i'm currently on microk8s 1.20
",<kubernetes><kubernetes-ingress><microk8s>,66207860,47,"i have analyzed you issue and came to the following conclusions:

the ingress will work and these warnings you see are just to inform you about the available api versioning. you don't have to worry about this. i've seen the same warnings:


@microk8s:~$ kubectl describe ing
warning: extensions/v1beta1 ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 ingress


as for the &quot;why&quot; this is happening even when you use apiversion: networking.k8s.io/v1, i have found the following explanation:


this is working as expected. when you create an ingress object, it can
be read via any version (the server handles converting into the
requested version). kubectl get ingress is an ambiguous request,
since it does not indicate what version is desired to be read.
when an ambiguous request is made, kubectl searches the discovery docs
returned by the server to find the first group/version that contains
the specified resource.
for compatibility reasons, extensions/v1beta1 has historically been
preferred over all other api versions. now that ingress is the only
resource remaining in that group, and is deprecated and has a ga
replacement, 1.20 will drop it in priority so that kubectl get ingress would read from networking.k8s.io/v1, but a 1.19 server
will still follow the historical priority.
if you want to read a specific version, you can qualify the get
request (like kubectl get ingresses.v1.networking.k8s.io ...) or can
pass in a manifest file to request the same version specified in the
file (kubectl get -f ing.yaml -o yaml)

long story short: despite the fact of using the proper apiversion, the deprecated one is still being seen as the the default one and thus generating the warning you experience.
i also see that changes are still being made recently so i assume that it is still being worked on.
"
52991038,how to create a servicemonitor for prometheus-operator?,"recently, prometheus-operator has been promoted to stable helm chart (https://github.com/helm/charts/tree/master/stable/prometheus-operator). 

i'd like to understand how to add a custom application to monitoring by prometheus-operator in a k8s cluster. an example for say gitlab runner which by default provides metrics on 9252 would be appreciated (https://docs.gitlab.com/runner/monitoring/#configuration-of-the-metrics-http-server).

i have a rudimentary yaml that obviously doesn't work but also not provides any feedback on what isn't working:

apiversion: monitoring.coreos.com/v1
kind: servicemonitor
metadata:
  name: gitlab-monitor
  # change this to the namespace the prometheus instance is running in
  namespace: default
  labels:
    app: gitlab-runner-gitlab-runner
    release: prometheus
spec:
  selector:
    matchlabels:
      app: gitlab-runner-gitlab-runner
  namespaceselector:
    # matchnames:
    # - default
    any: true
  endpoints:
  - port: http-metrics
    interval: 15s


this is the prometheus configuration:

&gt; kubectl get prometheus -o yaml

...
servicemonitornamespaceselector: {}
servicemonitorselector:
  matchlabels:
    release: prometheus
...


so the selectors should match. by ""not working"" i mean that the endpoints do not appear in the prometheus ui.
",<kubernetes><coreos><kubernetes-helm>,53013006,44,"thanks to peter who showed me that it idea in principle wasn't entirely incorrect i've found the missing link. as a servicemonitor does monitor services (haha), i missed the part of creating a service which isn't part of the gitlab helm chart. finally this yaml did the trick for me and the metrics appear in prometheus:

# service targeting gitlab instances
apiversion: v1
kind: service
metadata:
  name: gitlab-metrics
  labels:
    app: gitlab-runner-gitlab-runner
spec:
  ports:
  - name: metrics # expose metrics port
    port: 9252 # defined in gitlab chart
    targetport: metrics
    protocol: tcp
  selector:
    app: gitlab-runner-gitlab-runner # target gitlab pods
---
apiversion: monitoring.coreos.com/v1
kind: servicemonitor
metadata:
  name: gitlab-metrics-servicemonitor
  # change this to the namespace the prometheus instance is running in
  # namespace: default
  labels:
    app: gitlab-runner-gitlab-runner
    release: prometheus
spec:
  selector:
    matchlabels:
      app: gitlab-runner-gitlab-runner # target gitlab service
  endpoints:
  - port: metrics
    interval: 15s


nice to know: the metrics targetport is defined in the gitlab runner chart.
"
69517855,"microk8s dashboard using nginx-ingress via http not working (error: `no matches for kind ""ingress"" in version ""extensions/v1beta1""`)","i have microk8s v1.22.2 running on ubuntu 20.04.3 lts.
output from /etc/hosts:
127.0.0.1 localhost
127.0.1.1 main

excerpt from microk8s status:
addons:
  enabled:
    dashboard            # the kubernetes dashboard
    ha-cluster           # configure high availability on the current node
    ingress              # ingress controller for external access
    metrics-server       # k8s metrics server for api access to service metrics

i checked for the running dashboard (kubectl get all --all-namespaces):
namespace     name                                             ready   status    restarts   age
kube-system   pod/calico-node-2jltr                            1/1     running   0          23m
kube-system   pod/calico-kube-controllers-f744bf684-d77hv      1/1     running   0          23m
kube-system   pod/metrics-server-85df567dd8-jd6gj              1/1     running   0          22m
kube-system   pod/kubernetes-dashboard-59699458b-pb5jb         1/1     running   0          21m
kube-system   pod/dashboard-metrics-scraper-58d4977855-94nsp   1/1     running   0          21m
ingress       pod/nginx-ingress-microk8s-controller-qf5pm      1/1     running   0          21m

namespace     name                                type        cluster-ip       external-ip   port(s)    age
default       service/kubernetes                  clusterip   10.152.183.1     &lt;none&gt;        443/tcp    23m
kube-system   service/metrics-server              clusterip   10.152.183.81    &lt;none&gt;        443/tcp    22m
kube-system   service/kubernetes-dashboard        clusterip   10.152.183.103   &lt;none&gt;        443/tcp    22m
kube-system   service/dashboard-metrics-scraper   clusterip   10.152.183.197   &lt;none&gt;        8000/tcp   22m

namespace     name                                               desired   current   ready   up-to-date   available   node selector            age
kube-system   daemonset.apps/calico-node                         1         1         1       1            1           kubernetes.io/os=linux   23m
ingress       daemonset.apps/nginx-ingress-microk8s-controller   1         1         1       1            1           &lt;none&gt;                   22m

namespace     name                                        ready   up-to-date   available   age
kube-system   deployment.apps/calico-kube-controllers     1/1     1            1           23m
kube-system   deployment.apps/metrics-server              1/1     1            1           22m
kube-system   deployment.apps/kubernetes-dashboard        1/1     1            1           22m
kube-system   deployment.apps/dashboard-metrics-scraper   1/1     1            1           22m

namespace     name                                                   desired   current   ready   age
kube-system   replicaset.apps/calico-kube-controllers-69d7f794d9     0         0         0       23m
kube-system   replicaset.apps/calico-kube-controllers-f744bf684      1         1         1       23m
kube-system   replicaset.apps/metrics-server-85df567dd8              1         1         1       22m
kube-system   replicaset.apps/kubernetes-dashboard-59699458b         1         1         1       21m
kube-system   replicaset.apps/dashboard-metrics-scraper-58d4977855   1         1         1       21m

i want to expose the microk8s dashboard within my local network to access it through http://main/dashboard/
to do so, i did the following nano ingress.yaml:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: public
    nginx.ingress.kubernetes.io/backend-protocol: &quot;https&quot;
  name: dashboard
  namespace: kube-system
spec:
  rules:
  - host: main
    http:
      paths:
      - backend:
          servicename: kubernetes-dashboard
          serviceport: 443
        path: /

enabling the ingress-config through kubectl apply -f ingress.yaml gave the following error:
error: unable to recognize &quot;ingress.yaml&quot;: no matches for kind &quot;ingress&quot; in version &quot;extensions/v1beta1&quot;

help would be much appreciated, thanks!
update:
@harsh-manvar pointed out a mismatch in the config version. i have rewritten ingress.yaml to a very stripped down version:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: dashboard
  namespace: kube-system
spec:
  rules:
  - http:
      paths:
      - path: /dashboard
        pathtype: prefix
        backend:
          service:
            name: kubernetes-dashboard
            port:
              number: 443

applying this works. also, the ingress rule gets created.
namespace     name        class    hosts   address     ports   age
kube-system   dashboard   public   *       127.0.0.1   80      11m

however, when i access the dashboard through http://&lt;ip-of-kubernetes-master&gt;/dashboard, i get a 400 error.
log from the ingress controller:
192.168.0.123 - - [10/oct/2021:21:38:47 +0000] &quot;get /dashboard http/1.1&quot; 400 54 &quot;-&quot; &quot;mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/94.0.4606.71 safari/537.36&quot; 466 0.002 [kube-system-kubernetes-dashboard-443] [] 10.1.76.3:8443 48 0.000 400 ca0946230759edfbaaf9d94f3d5c959a

does the dashboard also need to be exposed using the microk8s proxy? i thought the ingress controller would take care of this, or did i misunderstand this?
",<http><kubernetes><kubernetes-ingress><nginx-ingress><microk8s>,69527326,42,"to fix the error error: unable to recognize &quot;ingress.yaml&quot;: no matches for kind &quot;ingress&quot; in version &quot;extensions/v1beta1 you need to set apiversion to the  networking.k8s.io/v1. from the kubernetes v1.16 article about deprecated apis:


networkpolicy in the  extensions/v1beta1  api version is no longer served
-   migrate to use the  networking.k8s.io/v1  api version, available since v1.8. existing persisted data can be retrieved/updated via the new version.


now moving to the second issue. you need to add a few annotations and make few changes in your ingress definition to make dashboard properly exposed on the microk8s cluster:

add nginx.ingress.kubernetes.io/rewrite-target: /$2 annotation
add nginx.ingress.kubernetes.io/configuration-snippet: | rewrite ^(/dashboard)$ $1/ redirect; annotation
change path: /dashboard to path: /dashboard(/|$)(.*)

we need them to properly forward the request to the backend pods - good explanation in this article:

note: the &quot;nginx.ingress.kubernetes.io/rewrite-target&quot; annotation rewrites the url before forwarding the request to the backend pods. in /dashboard(/|$)(.*) for path, (.*) stores the dynamic url that's generated while accessing the kubernetes dashboard. the &quot;nginx.ingress.kubernetes.io/rewrite-target&quot; annotation replaces the captured data in the url before forwarding the request to the kubernetes-dashboard service. the &quot;nginx.ingress.kubernetes.io/configuration-snippet&quot; annotation rewrites the url to add a trailing slash (&quot;/&quot;) only if alb-url/dashboard is accessed.

also we need another two changes:

add nginx.ingress.kubernetes.io/backend-protocol: &quot;https&quot; annotation to tell nginx ingress to communicate with dashboard service using https
add kubernetes.io/ingress.class: public annotation to use nginx ingress created by microk8s ingress plugin

after implementing everything above, the final yaml file looks like this:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/configuration-snippet: |
      rewrite ^(/dashboard)$ $1/ redirect;
    nginx.ingress.kubernetes.io/backend-protocol: &quot;https&quot;
    kubernetes.io/ingress.class: public
  name: dashboard
  namespace: kube-system
spec:
  rules:
  - http:
      paths:
      - path: /dashboard(/|$)(.*)
        pathtype: prefix
        backend:
          service:
            name: kubernetes-dashboard
            port:
              number: 443

it should work fine. no need to run microk8s proxy command.
"
66236346,kubernetes apiversion: networking.k8s.io/v1 issue with 'ingress',"wanted your guidance on an issue while executing a kubernetes yaml file.
my kubectl version is as follows:
    client version: version.info{major:&quot;1&quot;, minor:&quot;20&quot;, gitversion:&quot;v1.20.0&quot;, gitcommit:&quot;af46c47ce925f4c4ad5cc8d1fca46c7b77d13b38&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2020-12-08t17:59:43z&quot;, goversion:&quot;go1.15.5&quot;, compiler:&quot;gc&quot;, platform:&quot;windows/amd64&quot;}
    server version: version.info{major:&quot;1&quot;, minor:&quot;18&quot;, gitversion:&quot;v1.18.14&quot;, gitcommit:&quot;89182bdd065fbcaffefec691908a739d161efc03&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2020-12-18t12:02:35z&quot;, goversion:&quot;go1.13.15&quot;, compiler:&quot;gc&quot;, platform:&quot;linux/amd64&quot;}

this is the latest version downloaded from the kubernetes site
https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-windows
the yaml has
apiversion: networking.k8s.io/v1
kind: ingress
and the error on running the yaml is
    no matches for kind &quot;ingress&quot; in version &quot;networking.k8s.io/v1&quot;

kubernetes issue https://github.com/kubernetes/kubernetes/issues/90077 mentions that
  networking.k8s.io/v1beta1 == 1.14 to 1.18
  networking.k8s.io/v1 = 1.19+

so i guess it should be working right?
i have changed the api version to
apiversion: extensions/v1beta1 or
apiversion: networking.k8s.io/v1beta1

but fail in another section of the yaml
backend:
  service:
    name: {{ template &quot;fullname&quot; $ }}-srv
     port:
       number: 80

with the error
error validating data: validationerror(ingress.spec.rules[0].http.paths[0].backend): unknown field &quot;service&quot; in io.k8s.api.extensions.v1beta1.ingressbackend
i am informed that the same yaml works on macos with the same kubectl version (i do not have access to verify that though). but any thoughts on where i could be going wrong?
thanks,
prabal
",<kubernetes><kubernetes-ingress>,70855124,42,"i would like to add that according to the k8 deprecation guide, the networking.k8s.io/v1beta1 api versions of ingress is no longer served as of v1.22.
changes include:

the backend servicename field is renamed to service.name
numeric backend serviceport fields are renamed to service.port.number
string backend serviceport fields are renamed to service.port.name
pathtype is now required for each specified path. options are prefix, exact, and implementationspecific.

meaning we need to make the following changes to go from this:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: stackoverflw
  namespace: stacker
spec:
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          servicename: stacker
          serviceport: 80

to this (example):
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: stackoverflw
  namespace: stacker
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: stacker
            port:
              number: 80

"
50130797,kubernetes basic authentication with traefik,"i am trying to configure basic authentication on a nginx example with traefik as ingress controller.

i just create the secret ""mypasswd"" on the kubernetes secrets.

this is the ingress i am using:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: nginxingress
  annotations:
    ingress.kubernetes.io/auth-type: basic
    ingress.kubernetes.io/auth-realm: traefik
    ingress.kubernetes.io/auth-secret: mypasswd
spec:
  rules:
  - host: nginx.mycompany.com
    http:
      paths:
      - path: /
        backend:
          servicename: nginxservice
          serviceport: 80


i check in the traefik dashboard and it appear, if i access to nginx.mycompany.com i can check the nginx webpage, but without the basic authentication.

this is my nginx deployment:

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerport: 80


nginx service:

apiversion: v1
kind: service
metadata:
  labels:
    name: nginxservice
  name: nginxservice
spec:
  ports:
    # the port that this service should serve on.
    - port: 80
  # label keys and values that must match in order to receive traffic for this service.
  selector:
    app: nginx
  type: clusterip

",<kubernetes><traefik><kubernetes-ingress>,50138106,38,"it is popular to use basic authentication. in reference to kubernetes documentation, you should be  able to protect access to traefik using the following steps :

create authentication file using htpasswd tool. you'll be asked for a password for the user:


htpasswd -c ./auth 


now use  kubectl  to create a secret in the  monitoring  namespace using the file created by  htpasswd.


kubectl create secret generic mysecret --from-file auth
--namespace=monitoring


enable basic authentication by attaching annotations to ingress object:


ingress.kubernetes.io/auth-type: &quot;basic&quot;
ingress.kubernetes.io/auth-secret: &quot;mysecret&quot;

so, full example config of basic authentication can looks like:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: prometheus-dashboard
  namespace: monitoring
  annotations:
    kubernetes.io/ingress.class: traefik
    ingress.kubernetes.io/auth-type: &quot;basic&quot;
    ingress.kubernetes.io/auth-secret: &quot;mysecret&quot;
spec:
  rules:
  - host: dashboard.prometheus.example.com
    http:
      paths:
      - backend:
          servicename: prometheus
          serviceport: 9090


you can apply the example as following:


kubectl create -f prometheus-ingress.yaml -n monitoring

this should work without any issues.
"
48023475,add random string on kubernetes pod deployment name,"i have a template that is basically an utility container for running kubectl inside a pod.

what i want to do, is to be able to have multiple deployments of that same template, with different names, as in ""utilitypod-randomid"".

is there a way to do that, via kubectl and some shell scripting, or something similar?

the current template looks like this:

apiversion: v1
kind: pod
metadata:
  name: utilitypod
  namespace: blah-dev
labels:
  purpose: utility-pod
spec:
  containers:
  - name: utilitypod
  image: blahblah/utilitypod:latest
  command: [ ""/bin/bash"", ""-c"", ""--"" ]
  args: [ ""while true; do sleep 28800; done;"" ]
  env: 
  - name: kubernetes_service_host
    value: ""api.dev.blah.internal""
  - name: kubernetes_service_port
    value: ""443""

",<kubernetes><kubectl>,48023670,37,"you can replace name with generatename, which adds a random suffix. your template will look like this:
apiversion: v1
kind: pod
metadata:
  generatename: utilitypod-
  namespace: blah-dev
  labels:
    purpose: utility-pod
spec:
  containers:
    - name: utilitypod
      image: blahblah/utilitypod:latest
      command: [ &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
      args: [ &quot;while true; do sleep 28800; done;&quot; ]
      env:
        - name: kubernetes_service_host
          value: &quot;api.dev.blah.internal&quot;
        - name: kubernetes_service_port
          value: &quot;443&quot;

mind you, this will only work with kubectl create -f template.yaml, not apply, as apply looks for a resource by its name and tries to compare their definitions, but this template doesn't contain a specific name.
"
55955646,required value: must specify a volume type when statically provisioning pv,"trying to statically provision a pv with gcp ssd storage. errors out with the following message:

the persistentvolume ""monitoring"" is invalid: spec: required value: must specify a volume type


steps to reproduce:

$ cat storage.yaml
apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  name: ssd
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
reclaimpolicy: retain


$ kubectl apply -f storage.yaml
storageclass.storage.k8s.io/ssd created


$ cat pv.yaml
apiversion: v1
kind: persistentvolume
metadata:
  name: monitoring
spec:
  storageclassname: ssd
  capacity:
    storage: 50gi
  persistentvolumereclaimpolicy: retain
  accessmodes:
    - readwriteonce


$ kubectl apply -f pv.yaml
the persistentvolume ""monitoring"" is invalid: spec: required value: must specify a volume type


kubernetes version:

client version: version.info{major:""1"", minor:""14"", gitversion:""v1.14.1"", gitcommit:""b7394102d6ef778017f2ca4046abbaa23b88c290"", gittreestate:""clean"", builddate:""2019-04-08t17:11:31z"", goversion:""go1.12.1"", compiler:""gc"", platform:""linux/amd64""}
server version: version.info{major:""1"", minor:""12+"", gitversion:""v1.12.6-gke.10"", gitcommit:""aaf0906400b5fc1d858ce0566a571e4f3ed06b9f"", gittreestate:""clean"", builddate:""2019-03-30t19:30:48z"", goversion:""go1.10.8b4"", compiler:""gc"", platform:""linux/amd64""}

",<kubernetes><google-kubernetes-engine>,55964096,36,"if using a provisioner, you usually don't create the pv on your own. just create a pvc requiring that created storage class and gke will provide the pv with the requested storage size and kind for you:

kind: persistentvolumeclaim
apiversion: v1
metadata:
  name: my-retain-ssd-storage
  namespace: default
spec:
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 50gi
  storageclassname: ssd

"
59258223,how to resolve pod hostnames from other pods?,"
i have 2 pods running on 2 nodes, each pod runs in different node.
these nodes are on the same subnet and can tcp/udp/icmp themselves.


these pods got some hostnames, ie:


drill-staging-75cddd789-kbzsq
drill-staging-75cddd789-amsrj


from pod drill-staging-75cddd789-kbzsq i cannot resolve host name for drill-staging-75cddd789-amsrj and vice versa. resolving self pod's name works.

i tried setting various dnspolicies:


clusterfirst: no luck
default: no luck
clusterfirstwithhostnet: no luck and it event couldn't resolve hostname of it's own node
none: not tried (i don't think it's a good way)


apiversion: apps/v1
kind: deployment
metadata:
  name: {{ include ""app.name"" . }}
  namespace: {{ .values.global.namespace }}
spec:
  replicas: 2
  selector:
    matchlabels:
      app: {{ include ""app.name"" . }}
  template:
    metadata:
      labels:
        app: {{ include ""app.name"" . }}
    spec:
      containers:
      - name: {{ include ""app.name"" . }}
        image: ...
        resources:
          ...
        ports:
          ...
        imagepullpolicy: always
      restartpolicy: always

",<network-programming><kubernetes><kube-dns><kubernetes-networking>,59262628,34,"normally, only services get dns names, not pods. so, by default, you can't refer to another pod directly by a domain name, only by its ip address.

pods get dns names only under certain condidtions that include a headless service, as explained in the documentation. in particular, the conditions are:


the pods have a hostname field
the pods have a subdomain field
there is a headless service (in the same namespace) that selects the pods
the name of the headless service equals the subdomain field of the pods


in this case, each pod gets a fully-qualified domain name of the following form:

my-hostname.my-subdomain.default.svc.cluster.local


where my-hostname is the hostname field of the pod and my-subdomain is the subdomain field of the pod.


  note: the dns name is created for the ""hostname"" of the pod and not the ""name"" of the pod.


you can test this with the following setup:

apiversion: v1
kind: service
metadata:
  name: my-subdomain
spec:
  selector:
    name: my-test
  clusterip: none
---
apiversion: v1
kind: pod
metadata:
  name: my-pod-1
  labels:
    name: my-test
spec:
  hostname: my-hostname-1
  subdomain: my-subdomain
  containers:
  - image: weibeld/ubuntu-networking
    command: [sleep, ""3600""]
    name: ubuntu-networking
---
apiversion: v1
kind: pod
metadata:
  name: my-pod-2
  labels:
    name: my-test
spec:
  hostname: my-hostname-2
  subdomain: my-subdomain
  containers:
  - image: weibeld/ubuntu-networking
    command: [sleep, ""3600""]
    name: ubuntu-networking


after applying this, you can exec into one of the pods:

kubectl exec -ti my-pod-1 bash


and you should be able to resolve the fully-qualifed domain names of the two pods:

host my-hostname-1.my-subdomain.default.svc.cluster.local
host my-hostname-2.my-subdomain.default.svc.cluster.local


since you're making the requests from the same namespace as the target pods, you can abbreviate the domain name to:

host my-hostname-1.my-subdomain
host my-hostname-2.my-subdomain

"
54904069,how to schedule a cronjob which executes a kubectl command?,"how to schedule a cronjob which executes a kubectl command?

i would like to run the following kubectl command every 5 minutes:

kubectl patch deployment runners -p '{""spec"":{""template"":{""spec"":{""containers"":[{""name"":""jp-runner"",""env"":[{""name"":""start_time"",""value"":""'$(date +%s)'""}]}]}}}}' -n jp-test


for this, i have created a cronjob as below:

apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: hello
spec:
  schedule: ""*/5 * * * *""
  jobtemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - kubectl patch deployment runners -p '{""spec"":{""template"":{""spec"":{""containers"":[{""name"":""jp-runner"",""env"":[{""name"":""start_time"",""value"":""'$(date +%s)'""}]}]}}}}' -n jp-test
          restartpolicy: onfailure


but it is failing to start the container, showing the message : 

back-off restarting failed container


and with the error code 127:

state:          terminated
      reason:       error
      exit code:    127


from what i checked, the error code 127 says that the command doesn't exist. how could i run the kubectl command then as a cron job ? am i missing something?

note: i had posted a similar question ( scheduled restart of kubernetes pod without downtime ) , but that was more of having the main deployment itself as a cronjob, here i'm trying to run a kubectl command (which does the restart) using a cronjob - so i thought it would be better to post separately

kubectl describe cronjob hello -n jp-test:

name:                       hello
namespace:                  jp-test
labels:                     &lt;none&gt;
annotations:                kubectl.kubernetes.io/last-applied-configuration={""apiversion"":""batch/v1beta1"",""kind"":""cronjob"",""metadata"":{""annotations"":{},""name"":""hello"",""namespace"":""jp-test""},""spec"":{""jobtemplate"":{""spec"":{""templ...
schedule:                   */5 * * * *
concurrency policy:         allow
suspend:                    false
starting deadline seconds:  &lt;unset&gt;
selector:                   &lt;unset&gt;
parallelism:                &lt;unset&gt;
completions:                &lt;unset&gt;
pod template:
  labels:  &lt;none&gt;
  containers:
   hello:
    image:      busybox
    port:       &lt;none&gt;
    host port:  &lt;none&gt;
    args:
      /bin/sh
      -c
      kubectl patch deployment runners -p '{""spec"":{""template"":{""spec"":{""containers"":[{""name"":""jp-runner"",""env"":[{""name"":""start_time"",""value"":""'$(date +%s)'""}]}]}}}}' -n jp-test
    environment:     &lt;none&gt;
    mounts:          &lt;none&gt;
  volumes:           &lt;none&gt;
last schedule time:  wed, 27 feb 2019 14:10:00 +0100
active jobs:         hello-1551273000
events:
  type    reason            age   from                message
  ----    ------            ----  ----                -------
  normal  successfulcreate  6m    cronjob-controller  created job hello-1551272700
  normal  successfulcreate  1m    cronjob-controller  created job hello-1551273000
  normal  sawcompletedjob   16s   cronjob-controller  saw completed job: hello-1551272700


kubectl describe job hello -v=5 -n jp-test

name:           hello-1551276000
namespace:      jp-test
selector:       controller-uid=fa009d78-3a97-11e9-ae31-ac1f6b1a0950
labels:         controller-uid=fa009d78-3a97-11e9-ae31-ac1f6b1a0950
                job-name=hello-1551276000
annotations:    &lt;none&gt;
controlled by:  cronjob/hello
parallelism:    1
completions:    1
start time:     wed, 27 feb 2019 15:00:02 +0100
pods statuses:  0 running / 0 succeeded / 0 failed
pod template:
  labels:  controller-uid=fa009d78-3a97-11e9-ae31-ac1f6b1a0950
           job-name=hello-1551276000
  containers:
   hello:
    image:      busybox
    port:       &lt;none&gt;
    host port:  &lt;none&gt;
    args:
      /bin/sh
      -c
      kubectl patch deployment runners -p '{""spec"":{""template"":{""spec"":{""containers"":[{""name"":""jp-runner"",""env"":[{""name"":""start_time"",""value"":""'$(date +%s)'""}]}]}}}}' -n jp-test
    environment:  &lt;none&gt;
    mounts:       &lt;none&gt;
  volumes:        &lt;none&gt;
events:
  type     reason                age              from            message
  ----     ------                ----             ----            -------
  normal   successfulcreate      7m               job-controller  created pod: hello-1551276000-lz4dp
  normal   successfuldelete      1m               job-controller  deleted pod: hello-1551276000-lz4dp
  warning  backofflimitexceeded  1m (x2 over 1m)  job-controller  job has reached the specified backoff limit

name:           hello-1551276300
namespace:      jp-test
selector:       controller-uid=ad52e87a-3a98-11e9-ae31-ac1f6b1a0950
labels:         controller-uid=ad52e87a-3a98-11e9-ae31-ac1f6b1a0950
                job-name=hello-1551276300
annotations:    &lt;none&gt;
controlled by:  cronjob/hello
parallelism:    1
completions:    1
start time:     wed, 27 feb 2019 15:05:02 +0100
pods statuses:  1 running / 0 succeeded / 0 failed
pod template:
  labels:  controller-uid=ad52e87a-3a98-11e9-ae31-ac1f6b1a0950
           job-name=hello-1551276300
  containers:
   hello:
    image:      busybox
    port:       &lt;none&gt;
    host port:  &lt;none&gt;
    args:
      /bin/sh
      -c
      kubectl patch deployment runners -p '{""spec"":{""template"":{""spec"":{""containers"":[{""name"":""jp-runner"",""env"":[{""name"":""start_time"",""value"":""'$(date +%s)'""}]}]}}}}' -n jp-test
    environment:  &lt;none&gt;
    mounts:       &lt;none&gt;
  volumes:        &lt;none&gt;
events:
  type    reason            age   from            message
  ----    ------            ----  ----            -------
  normal  successfulcreate  2m    job-controller  created pod: hello-1551276300-8d5df

",<kubernetes><busybox><kubernetes-cronjob>,54908449,30,"long story short busybox doesn' have kubectl installed.

you can check it yourself using kubectl run -i --tty busybox --image=busybox -- sh which will run a busybox pod as interactive shell.

i would recommend using bitnami/kubectl:latest.

also keep in mind that you will need to set proper rbac, as you will get error from server (forbidden): services is forbidden

you could use something like this:

kind: role
apiversion: rbac.authorization.k8s.io/v1
metadata:
  namespace: jp-test
  name: jp-runner
rules:
- apigroups:
  - extensions
  - apps
  resources:
  - deployments
  verbs:
  - 'patch'

---
kind: rolebinding
apiversion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: jp-runner
  namespace: jp-test
subjects:
- kind: serviceaccount
  name: sa-jp-runner
  namespace: jp-test
roleref:
  kind: role
  name: jp-runner
  apigroup: """"

---
apiversion: v1
kind: serviceaccount
metadata:
  name: sa-jp-runner
  namespace: jp-test

---
apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: hello
spec:
  schedule: ""*/5 * * * *""
  jobtemplate:
    spec:
      template:
        spec:
          serviceaccountname: sa-jp-runner
          containers:
          - name: hello
            image: bitnami/kubectl:latest
            command:
            - /bin/sh
            - -c
            - kubectl patch deployment runners -p '{""spec"":{""template"":{""spec"":{""containers"":[{""name"":""jp-runner"",""env"":[{""name"":""start_time"",""value"":""'$(date +%s)'""}]}]}}}}' -n jp-test
          restartpolicy: onfailure

"
46734784,kubernetes nginx ingress controller not picking up tls certificates,"i setup a new kubernetes cluster on gke using the nginx-ingress controller. tls is not working, it's using the fake certificates.

there is a lot of configuration detail so i made a repo - https://github.com/jobevers/test_ssl_ingress

in short the steps were


create a new cluster without gke's load balancer
create a tls secret with my key and cert
create an nginx-ingress deployment / pod
create an ingress controller


the nginx-ingress config comes from https://zihao.me/post/cheap-out-google-container-engine-load-balancer/ (and looks very similar to a lot of the examples in the ingress-nginx repo).

my ingress.yaml is nearly identical to the example one

when i run curl, i get 

$ curl -kv https://35.196.134.52
[...]
*    common name: kubernetes ingress controller fake certificate (does not match '35.196.134.52')
[...]
*    issuer: o=acme co,cn=kubernetes ingress controller fake certificate
[...]


which shows that i'm still using the default certificates.

how am i supposed to get it using mine?



ingress definition

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: test-ssl-ingress
  annotations:
    kubernetes.io/ingress.class: ""nginx""
spec:
  tls:
    - secretname: tls-secret
  rules:
  - http:
      paths:
      - path: /
        backend:
          servicename: demo-echo-service
          serviceport: 80




creating the secret:

kubectl create secret tls tls-secret --key tls/privkey.pem --cert tls/fullchain.pem




debugging further, the certificate is being found and exist on the server:

$ kubectl -n kube-system exec -it $(kubectl -n kube-system get pods | grep ingress | head -1 | cut -f 1 -d "" "") -- ls -1 /ingress-controller/ssl/
default-fake-certificate-full-chain.pem
default-fake-certificate.pem
default-tls-secret-full-chain.pem
default-tls-secret.pem


and, from the log, i see

kubectl -n kube-system log -f $(kubectl -n kube-system get pods | grep ingress | head -1 | cut -f 1 -d "" "")
[...]
i1013 17:21:45.423998       6 queue.go:111] syncing default/test-ssl-ingress
i1013 17:21:45.424009       6 backend_ssl.go:40] starting syncing of secret default/tls-secret
i1013 17:21:45.424135       6 ssl.go:60] creating temp file /ingress-controller/ssl/default-tls-secret.pem236555242 for keypair: default-tls-secret.pem
i1013 17:21:45.424946       6 ssl.go:118] parsing ssl certificate extensions
i1013 17:21:45.743635       6 backend_ssl.go:102] found 'tls.crt' and 'tls.key', configuring default/tls-secret as a tls secret (cn: [...])
[...]


but, looking at the nginx.conf, its still using the fake certs:

$ kubectl -n kube-system exec -it $(kubectl -n kube-system get pods | grep ingress | head -1 | cut -f 1 -d "" "") -- cat /etc/nginx/nginx.conf | grep ssl_cert
        ssl_certificate                         /ingress-controller/ssl/default-fake-certificate.pem;
        ssl_certificate_key                     /ingress-controller/ssl/default-fake-certificate.pem;

",<ssl><nginx><kubernetes><google-kubernetes-engine>,46737039,30,"turns out that the ingress definition needs to look like:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: test-ssl-ingress
  annotations:
    kubernetes.io/ingress.class: ""nginx""
spec:
  tls:
    - hosts:
      - app.example.com
      secretname: tls-secret
  rules:
    - host: app.example.com
      http:
        paths:
        - path: /
          backend:
            servicename: demo-echo-service
            serviceport: 80


the host entry under rules needs to match one of the hosts entries under tls.
"
54884735,how to use configmap configuration with helm nginx ingress controller - kubernetes,"i've found a documentation about how to configure your nginx ingress controller using configmap: https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/

unfortunately i've no idea and couldn't find it anywhere how to load that configmap from my ingress controller.

my ingress controller:

helm install --name ingress --namespace ingress-nginx --set rbac.create=true,controller.kind=daemonset,controller.service.type=clusterip,controller.hostnetwork=true stable/nginx-ingress


my config map:

kind: configmap
apiversion: v1
metadata:
  name: ingress-configmap
data:
  proxy-read-timeout: ""86400s""
  client-max-body-size: ""2g""
  use-http2: ""false""


my ingress:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: ""https""
spec:
  tls:
    - hosts:
        - my.endpoint.net
      secretname: ingress-tls
  rules:
    - host: my.endpoint.net
      http:
        paths:
          - path: /
            backend:
              servicename: web
              serviceport: 443
          - path: /api
            backend:
              servicename: api
              serviceport: 443


how do i make my ingress to load the configuration from the configmap? 
",<kubernetes><kubernetes-helm><kubernetes-ingress><nginx-ingress>,54888611,28,"i've managed to display what yaml gets executed by helm using the: --dry-run --debug options at the end of helm install command. then i've noticed that there controller is executed with the: --configmap={namespace-where-the-nginx-ingress-is-deployed}/{name-of-the-helm-chart}-nginx-ingress-controller.
in order to load your configmap you need to override it with your own (check out the namespace).

kind: configmap
apiversion: v1
metadata:
  name: {name-of-the-helm-chart}-nginx-ingress-controller
  namespace: {namespace-where-the-nginx-ingress-is-deployed}
data:
  proxy-read-timeout: ""86400""
  proxy-body-size: ""2g""
  use-http2: ""false""


the list of config properties can be found here.
"
61355744,how do i make sure my cronjob job does not retry on failure?,"i have a kubernetes cronjob that runs on gke and runs cucumber jvm tests. in case a step fails due to assertion failure, some resource being unavailable, etc., cucumber rightly throws an exception which leads the cronjob job to fail and the kubernetes pod's status changes to error. this leads to creation of a new pod that tries to run the same cucumber tests again, which fails again and retries again.

i don't want any of these retries to happen. if a cronjob job fails, i want it to remain in the failed status and not retry at all. based on this, i have already tried setting backofflimit: 0 in combination with restartpolicy: never in combination with concurrencypolicy: forbid, but it still retries by creating new pods and running the tests again. 

what am i missing? here's my kube manifest for the cronjob:

apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: quality-apatha
  namespace: default
  labels:
    app: quality-apatha
spec:
  schedule: ""*/1 * * * *""
  concurrencypolicy: forbid
  jobtemplate:
    spec:
      backofflimit: 0
      template:
        spec:
          containers:
            - name: quality-apatha
              image: foo-image-path
              imagepullpolicy: ""always""
              resources:
                limits:
                  cpu: 500m
                  memory: 512mi
              env:
                - name: foo
                  value: bar
              volumemounts:
                - name: foo
                  mountpath: bar
              args:
                - java
                - -cp
                - qe_java.job.jar:qe_java-1.0-snapshot-tests.jar
                - org.junit.runner.junitcore
                - com.liveramp.qe_java.runcucumbertest
          restartpolicy: never
          volumes:
            - name: foo
              secret:
                secretname: bar


is there any other kubernetes kind i can use to stop the retrying?

thank you!
",<kubernetes><google-kubernetes-engine><cucumber-jvm><kubernetes-pod><kubernetes-cronjob>,61368328,25,"to make things as simple as possible i tested it using this example from the official kubernetes documentation, applying to it minor modifications to illustrate what really happens in different scenarios.

i can confirm that when backofflimit is set to 0 and restartpolicy to never everything works exactly as expected and there are no retries. note that every single run of your job which in your example is scheduled to run at intervals of 60 seconds (schedule: ""*/1 * * * *"") is not considerd a retry.

let's take a closer look at the following example (base yaml avialable here):

apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: hello
spec:
  schedule: ""*/1 * * * *""
  jobtemplate:
    spec:
      backofflimit: 0
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - non-existing-command
          restartpolicy: never


it spawns new cron job every 60 seconds according to the schedule, no matter if it fails or runs successfully. in this particular example it is configured to fail as we are trying to run non-existing-command.

you can check what's happening by running:

$ kubectl get pods
name                     ready   status              restarts   age
hello-1587558720-pgqq9   0/1     error               0          61s
hello-1587558780-gpzxl   0/1     containercreating   0          1s


as you can see there are no retries. although the first pod failed, the new one is spawned exactly 60 seconds later according to our specification. i'd like to emphasize it again. this is not a retry.

on the other hand when we modify the above example and set backofflimit: 3, we can observe the retries. as you can see, now new pods are created much more often than every 60 seconds. this are retries.

$ kubectl get pods
name                     ready   status   restarts   age
hello-1587565260-7db6j   0/1     error    0          106s
hello-1587565260-tcqhv   0/1     error    0          104s
hello-1587565260-vnbcl   0/1     error    0          94s
hello-1587565320-7nc6z   0/1     error    0          44s
hello-1587565320-l4p8r   0/1     error    0          14s
hello-1587565320-mjnb6   0/1     error    0          46s
hello-1587565320-wqbm2   0/1     error    0          34s


what we can see above are 3 retries (pod creation attempts), related with hello-1587565260 job and 4 retries (including the orignal 1st try not counted in backofflimit: 3) related with hello-1587565320 job.

as you can see the jobs themselves are still run according to the schedule, at 60 second intervals:

kubectl get jobs
name               completions   duration   age
hello-1587565260   0/1           2m12s      2m12s
hello-1587565320   0/1           72s        72s
hello-1587565380   0/1           11s        11s


however due to our backofflimit set this time to 3, every time the pod responsible for running the job fails, 3 additional retries occur.

i hope this helped to dispel any possible confusions about running cronjobs in kubernetes.

if you are rather interested in running something just once, not at regular intervals, take a look at simple job instead of cronjob.

also consider changing your cron configuration if you still want to run this particular job on regular basis but let's say once in 24 h, not every minute.
"
45720084,how to make two kubernetes services talk to each other?,"currently, i have working k8s api pods in a k8s service that connects to a k8s redis service, with k8s pods of it's own. the problem is, i am using nodeport meaning both are exposed to the public. i only want the api accessable to the public. the issue is that if i make the redis service not public, the api can't see it. is there a way to connect two services without exposing one to the public?

this is my api service yaml:  

apiversion: v1
kind: service
metadata:
   name: app-api-svc
spec:
   selector:
     app: app-api
     tier: api
   ports:
     - protocol: tcp
       port: 5000
       nodeport: 30400
   type: nodeport


and this is my redis service yaml:

apiversion: v1
kind: service
metadata:
   name: app-api-redis-svc
spec:
   selector:
     app: app-api-redis
     tier: celery_broker
   ports:
     - protocol: tcp
       port: 6379
       nodeport: 30537
   type: nodeport

",<kubernetes><cluster-computing><kubectl>,45722773,24,"first, configure the redis service as a clusterip service. it will be private, visible only for other services. this is could be done removing the line with the option type.

apiversion: v1
kind: service
metadata:
   name: app-api-redis-svc
spec:
   selector:
     app: app-api-redis
     tier: celery_broker
   ports:
     - protocol: tcp
       port: 6379
       targetport: [the port exposed by the redis pod]


finally, when you configure the api to reach redis, the address should be app-api-redis-svc:6379

and that's all. i have a lot of services communicating each other in this way. if this doesn't work for you, let me know in the comments.
"
54663845,apply hpa for statefulset in kubernetes?,"i am trying to setup hpa for my statefulset(for elasticsearch) in kubernetes environment. i am planning to scale the statefulset using the cpu utilization. i have created the metric server from https://github.com/stefanprodan/k8s-prom-hpa/tree/master/metrics-server.

and my hpa yaml for statefulset is as folows:

apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  name: dz-es-cluster
spec:
  scaletargetref:
    apiversion: extensions/v1beta1
    kind: statefulset
    name: dz-es-cluster
  minreplicas: 2
  maxreplicas: 3
  metrics:
  - type: resource
    resource:
      name: cpu
      targetaverageutilization: 80


but getting output in hpa as follows:

conditions:
  type         status  reason          message
  ----         ------  ------          -------
  abletoscale  false   failedgetscale  the hpa controller was unable to get the target's current scale: the server could not find the requested resource
events:
  type     reason          age                from                       message
  ----     ------          ----               ----                       -------
  warning  failedgetscale  1m (x71 over 36m)  horizontal-pod-autoscaler  the server could not find the requested resource


someone please help me..
",<kubernetes><autoscaling><kubernetes-statefulset>,54664782,24,"the support for autoscaling the statefulsets using hpa is added in kubernetes 1.9, so your version doesn't has support for it. after kubernetes 1.9, you can autoscale your statefulsets using:

apiversion: autoscaling/v1
kind: horizontalpodautoscaler
metadata:
  name: your_hpa_name
spec:
  maxreplicas: 3
  minreplicas: 1
  scaletargetref:
    apiversion: apps/v1
    kind: statefulset
    name: your_stateful_set_name
  targetcpuutilizationpercentage: 80


please refer the following link for more information:


  https://github.com/kubernetes/kubernetes/issues/44033

"
64781320,ingress controller name for the ingress class,"i am setting up my ingress controller, ingress class and ingress to expose a service outside the cluster. this is fresh cluster setup.
i have setup the nginx-ingress controller using
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.41.0/deploy/static/provider/baremetal/deploy.yaml
the next step based on my understanding is to create the ingress class https://v1-18.docs.kubernetes.io/docs/concepts/services-networking/ingress/#ingress-class
apiversion: networking.k8s.io/v1beta1
kind: ingressclass
metadata:
  name: external-lb
spec:
  controller: example.com/ingress-controller
  parameters:
    apigroup: k8s.example.com/v1alpha
    kind: ingressparameters
    name: external-lb

how did they get the name of the controller example.com/ingress-controller?
",<kubernetes><kubernetes-ingress>,64912413,23,"i have run multiple scenarios with ingressclass, ingress and nginx ingress controller.
scenario 1

ingressclass with custom name
nginx ingress controller with default --ingress-class value which is nginx
ingress using ingressclassname same as ingressclass name

output: response 404
scenario 2

ingressclass with custom name
nginx ingress controller with owningress-class ingress-test
ingress using ingressclassname same as ingressclass name

output: response 404
scenario 3

ingressclass with test name
nginx ingress controller --ingress-class with value test
ingress using test in ingressclassname

output: proper response
senario 4

ingressclass with nginx name
nginx ingress controller --ingress-class with value nginx
ingress using nginx in ingressclassname

output: proper response
conclusion
first of all, please keep in mind that there are 3 types of nginx. open source nginx ingress controller, you are probably using it. nginx incorporaton (nginx inc) and nginx incorporaton plus.
in one of the scenarios, when i have used spec.controller: nginx.org/ingress-controller with nginx ingress controller with argument --ingress-class=nginx, in nginx ingress controller pod you will see entry which is pointing to k8s.io/ingress-nginx.
to reproduce this behavior, you will need to deploy ingressclass with specific controller and then deploy nginx.
apiversion: networking.k8s.io/v1beta1
kind: ingressclass
metadata:
  name: nginx
spec:
  controller: nginx.org/ingress-controller

after deploying nginx ingress controller, controller pod will be in crashloopbackoff state. in logs you will find entry:
e1118 15:42:19.008911       8 main.go:134] invalid ingressclass (spec.controller) value &quot;nginx.org/ingress-controller&quot;. should be &quot;k8s.io/ingress-nginx&quot;

it works only when ingressclass name is set to nginx.
i would say that nginx.org/ingress-controller is for nginx incorporated and k8s.io/ingress-nginx for open source nginx ingress.
if custom value is used for --ingress-class argument in the controller deployment manifest, presence or absence of ingressclass object with the same name doesn't made any difference in, how the cluster works, if only you keep ingress spec.ingressclass value  the same with controller argument. moreover, if it's present, ingressclass spec.controller can have any value that match the required pattern &quot;domain like&quot; and that didn't affect ingress workflow behavior on my cluster at all.
in addition, ingress works fine if i put the correct value of the ingress-class either to spec.ingressclass property or to metadata.annotation.kubernetes.io/ingress.class accordingly. it gives an error like the following if you try to put both values to the same ingres object:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: test-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  ingressclassname: nginx

the ingress &quot;test-ingress&quot; is invalid: annotations.kubernetes.io/ingress.class: invalid value: &quot;nginx&quot;: can not be set when the class field is also set
please keep in mind it was tested only for nginx ingress controlle. if you would like to use ingressclass with other ingress controllers like traefik or ambasador, you would check their release notes.
"
54436623,why labels are mentioned three times in a single deployment,"i've gone over the following docomentation page: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/

the example deployment yaml is as follows:

apiversion: apps/v1
kind: deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerport: 80


we can see here three different times where the label app: nginx is mentioned.

why do we need each of them? i had a hard time understanding it from the official documentation.
",<kubernetes><kubernetes-deployment>,54438715,21,"the first label is for deployment itself, it gives label for that particular deployment. lets say you want to delete that deployment then you run following command:

kubectl delete deployment -l app=nginx


this will delete the entire deployment.

the second label is selector: matchlabels which tells the resources(service etc) to match the pod according to label. so lets say if you want to create the service which has all the pods having labels of app=nginx then you provide following definition:

apiversion: v1
kind: service
metadata:
  name: nginx
spec:
  type: loadbalancer
  ports:
    - port:  80
  selector:
    app: nginx


the above service will look for the matchlabels and bind pods which have label app: nginx assigned to them

the third label is podtemplate labels, the template is actually podtemplate. it describe the pod that it is launched. so lets say you have two replica deployment and k8s will launch 2 pods with the label specified in template: metadata: labels. this is subtle but important difference, so you can have the different labels for deployment and pods generated by that deployment.
"
65817334,kubernetes deployment mounts secret as a folder instead of a file,"i am having a config file as a secret in kubernetes and i want to mount it into a specific location inside the container. the problem is that the volume that is created inside the container is a folder instead of a file with the content of the secrets in it. any way to fix it?
my deployment looks like this:
kind: deployment
apiversion: apps/v1
metadata:
  name: jetty
  namespace: default
spec:
  replicas: 1
  selector:
    matchlabels:
      app: jetty
  template:
    metadata:
      labels:
        app: jetty
    spec:
      containers:
        - name: jetty
          image: quay.io/user/jetty
          ports:
            - containerport: 8080
          volumemounts:
          - name: config-properties
            mountpath: &quot;/opt/jetty/config.properties&quot;
            subpath: config.properties
          - name: secrets-properties
            mountpath: &quot;/opt/jetty/secrets.properties&quot;
          - name: doc-path
            mountpath: /mnt/storage/
          resources:
            limits:
              cpu: '1000m'
              memory: '3000mi'
            requests:
              cpu: '750m'
              memory: '2500mi'
      volumes:
      - name: config-properties
        configmap:
          name: jetty-config-properties
      - name: secrets-properties
        secret: 
          secretname: jetty-secrets
      - name: doc-path
        persistentvolumeclaim:
          claimname: jetty-docs-pvc
      imagepullsecrets:
      - name: rcc-quay

",<kubernetes><volumes><kubernetes-secrets>,65824065,21,"secrets vs configmaps
secrets let you store and manage sensitive information (e.g. passwords, private keys) and configmaps are used for non-sensitive configuration data.
as you can see in the secrets and configmaps documentation:

a secret is an object that contains a small amount of sensitive data such as a password, a token, or a key.


a configmap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable.

mounting secret as a file
it is possible to create secret and pass it as a file or multiple files to pods.
i've create simple example for you to illustrate how it works.
below you can see sample secret manifest file and deployment that uses this secret:
note: i used subpath with secrets and it works as expected.
---
apiversion: v1
kind: secret
metadata:
  name: my-secret
data:
  secret.file1: |
    c2vjcmv0rmlsztek
  secret.file2: |
    c2vjcmv0rmlsztik
---
apiversion: apps/v1
kind: deployment
metadata:
...
    spec:
      containers:
      - image: nginx
        name: nginx
        volumemounts:
        - name: secrets-files
          mountpath: &quot;/mnt/secret.file1&quot;  # &quot;secret.file1&quot; file will be created in &quot;/mnt&quot; directory
          subpath: secret.file1
        - name: secrets-files
          mountpath: &quot;/mnt/secret.file2&quot;  # &quot;secret.file2&quot; file will be created in &quot;/mnt&quot; directory
          subpath: secret.file2
      volumes:
        - name: secrets-files
          secret:
            secretname: my-secret # name of the secret
            

note: secret should be created before deployment.
after creating secret and deployment, we can see how it works:
$ kubectl get secret,deploy,pod
name                         type                                  data   age
secret/my-secret             opaque                                2      76s

name                    ready   up-to-date   available   age
deployment.apps/nginx   1/1     1            1           76s

name                         ready   status    restarts   age
pod/nginx-7c67965687-ph7b8   1/1     running   0          76s

$ kubectl exec nginx-7c67965687-ph7b8 -- ls /mnt
secret.file1
secret.file2
$ kubectl exec nginx-7c67965687-ph7b8 -- cat /mnt/secret.file1
secretfile1
$ kubectl exec nginx-7c67965687-ph7b8 -- cat /mnt/secret.file2
secretfile2


projected volume
i think a better way to achieve your goal is to use projected volume.

a projected volume maps several existing volume sources into the same directory.

in the projected volume documentation you can find detailed explanation but additionally i created an example that might help you understand how it works.
using projected volume i mounted secret.file1, secret.file2 from secret and config.file1 from configmap as files into the pod.
---
apiversion: v1
kind: secret
metadata:
  name: my-secret
data:
  secret.file1: |
    c2vjcmv0rmlsztek
  secret.file2: |
    c2vjcmv0rmlsztik
---
apiversion: v1
kind: configmap
metadata:
  name: my-config
data:
  config.file1: |
    configfile1  
---
apiversion: v1
kind: pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    volumemounts:
    - name: all-in-one
      mountpath: &quot;/config-volume&quot;
      readonly: true
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: my-secret
          items:
            - key: secret.file1
              path: secret-dir1/secret.file1
            - key: secret.file2
              path: secret-dir2/secret.file2
      - configmap:
          name: my-config
          items:
            - key: config.file1
              path: config-dir1/config.file1

we can check how it works:
$ kubectl exec nginx -- ls /config-volume
config-dir1
secret-dir1
secret-dir2    
$ kubectl exec nginx -- cat /config-volume/config-dir1/config.file1
configfile1
$ kubectl exec nginx -- cat /config-volume/secret-dir1/secret.file1
secretfile1
$ kubectl exec nginx -- cat /config-volume/secret-dir2/secret.file2
secretfile2

if this response doesn't answer your question, please provide more details about your secret and what exactly you want to achieve.
"
60412448,alb ingress - redirect traffic from http to https not working,"i am trying to route all http traffic to https. i have a alb ingress resource and following the guide here https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/tasks/ssl_redirect/#how-it-works  but its not working. when i try to access http://www.myhost.in it stays with http but does not redirect to https

below is my ingress resource file

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: eks-learning-ingress
  namespace: production
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/certificate-arn: arn878ef678df
    alb.ingress.kubernetes.io/listen-ports: '[{""http"": 80}, {""https"":443}]'
    alb.ingress.kubernetes.io/actions.ssl-redirect: '{""type"": ""redirect"", ""redirectconfig"": { ""protocol"": ""https"", ""port"": ""443"", ""statuscode"": ""http_301""}}'
  labels:
    app: eks-learning-ingress
spec:
  rules:
  - host: www.myhost.in
    http:
      paths:
        - path: /*
          backend:
            servicename: eks-learning-service
            serviceport: 80


any help in this would be really great, thanks.
",<amazon-web-services><kubernetes><kubernetes-ingress>,60413136,20,"for anyone stumbling on this post. i was missing adding this as my http paths. have in mind this needs to be the first specified path.
        - path: /*
          backend:
            servicename: ssl-redirect
            serviceport: use-annotation
 

once i added this redirection started working.
so the final config in question should look like this:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: eks-learning-ingress
  namespace: production
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/certificate-arn: arn878ef678df
    alb.ingress.kubernetes.io/listen-ports: '[{&quot;http&quot;: 80}, {&quot;https&quot;:443}]'
    alb.ingress.kubernetes.io/actions.ssl-redirect: '{&quot;type&quot;: &quot;redirect&quot;, &quot;redirectconfig&quot;: { &quot;protocol&quot;: &quot;https&quot;, &quot;port&quot;: &quot;443&quot;, &quot;statuscode&quot;: &quot;http_301&quot;}}'
  labels:
    app: eks-learning-ingress
spec:
  rules:
  - host: www.myhost.in
    http:
      paths:
        - path: /*
          backend:
            servicename: ssl-redirect
            serviceport: use-annotation
        - path: /*
          backend:
            servicename: eks-learning-service
            serviceport: 80

"
54717135,are multiple imagepullsecrets allowed and used by kubernetes to pull an image from a private registry?,"i have a private registry (gitlab) where my docker images are stored.
for deployment a secret is created that allows gke to access the registry. the secret is called deploy-secret. 
the secret's login information expires after short time in the registry.  

i additionally created a second, permanent secret that allows access to the docker registry, named permanent-secret.

is it possible to specify the pod with two secrets? for example:

apiversion: v1
kind: pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: &lt;your-private-image&gt;
  imagepullsecrets:
  - name: deploy-secret
  - name: permanent-secret


will kubernetes, when trying to re-pull the image later, recognize that the first secret does not work (does not allow authentication to the private registry) and then fallback successfully to the second secret?
",<kubernetes><google-kubernetes-engine><kubernetes-secrets>,54717484,20,"surprisingly this works! i just tried this on my cluster. i added a fake registry credentials secret, with the wrong values. i put both secrets in my yaml like you did (below) and the pods got created and container is created and running successfully:

apiversion: apps/v1
kind: deployment
metadata:
  name: test
  labels:
    app: test
spec:
  replicas: 1
  selector:
    matchlabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      nodeselector:
      containers:
      - image: gitlab.myapp.com/my-image:tag
        name: test
        ports:
        - containerport: 80
      imagepullsecrets:
      - name: regcred-test
      - name: regcred



the regcred secret has the correct values and the regcred-test is just a bunch of gibberish. so we can see that it ignores the incorrect secret.
"
48894816,prevent inter-namespace communication in kubernetes,"i am new to kubernetes networking.

we have separated a kubernetes cluster into a set of namespaces (e.g. namespace-a, namespace-b). every namespace has a set of kubernetes pods. every pod has a service that is available at my-svc.namespace-x.svc.cluster.local.

now, we want to prevent pods of namespace namespace-a to talk with services or pods that are part of namespace-b and vice versa. communication within a namespace should be unrestricted. 

this is what i found as an example in the network policies documentation:
https://kubernetes.io/docs/concepts/services-networking/network-policies/#the-networkpolicy-resource

apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: default-deny
spec:
  podselector: {}
  policytypes:
  - ingress
  - egress


as far as i understand, this prevents network communication completely, for all pods across a namespace. 


how can i allow all network traffic, but only within a particular namespace?
do i need a networking plugin, such as calico, flannel or weave? which one should i choose?

",<network-programming><kubernetes><kubernetes-networkpolicy>,48907777,20,"
  do i need a networking plugin, such as calico, flannel or weave?


no matter what you need a networking plugin, but not all plugins support the networkpolicy api object. according to the declare network policy walkthrough, the following is a (probably non-exhaustive) list of plugins that do support networkpolicy:


calico
cilium
kube-router
romana
weave net


without a plugin that supports networkpolicy, creating the resource would have no effect.


  which one should i choose?


as for which one you should choose, stackoverflow is not the place for soliciting that kind of advice. what i can recommend is reading the overview/features documentation for the various options available. maybe try one or two different plugins in a local development cluster to get a feel for how difficult or easy they are to install, maintain, and update.


  how can i allow all network traffic, but only within a particular namespace?


given your example setup, i think the following networkpolicy resources would address your need:

for pods in namespace-a, only allow ingress from namspace-a pods, denying ingress from any other source. egress is unrestricted:

apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: test-network-policy
  namespace: namespace-a
spec:
  policytypes:
  - ingress
  podselector: {}
  ingress:
  - from:
    - namespaceselector:
        matchlabels:
          name: namespace-a


for pods in namespace-b, only allow ingress from namspace-b pods, denying ingress from any other source. egress is unrestricted:

apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: test-network-policy
  namespace: namespace-b
spec:
  policytypes:
  - ingress
  podselector: {}
  ingress:
  - from:
    - namespaceselector:
        matchlabels:
          name: namespace-b


note that this assumes you have set the name: namespace-a and name: namespace-b labels on your namespaces, similar to this:

apiversion: v1
kind: namespace
metadata:
  name: namespace-a
  labels:
    name: namespace-a
    other: labelname


i only point this out to avoid confusing you with regard to the fact that the labels i showed above happen to match up with your hypothetical namespace names. the labels can be arbitrary and potentially inclusive of mulitple namespaces -- for example you might have namespace-a and namespace-c both with a label called other: labelname which would allow you to select multiple namespaces using a single namespaceselector in your networkpolicy resource.
"
65266223,how to set pvc with statefulset in kubernetes?,"on gke, i set a statefulset resource as
---
apiversion: apps/v1
kind: statefulset
metadata:
  name: redis
spec:
  servicename: &quot;redis&quot;
  selector:
    matchlabels:
      app: redis
  updatestrategy:
    type: rollingupdate
  replicas: 3
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis
        resources:
          limits:
            memory: 2gi
        ports:
          - containerport: 6379
        volumemounts:
          - name: redis-data
            mountpath: /usr/share/redis
      volumes:
        - name: redis-data
          persistentvolumeclaim:
            claimname: redis-data-pvc

want to use pvc so created this one. (this step was did before the statefulset deployment)
---
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: redis-data-pvc
spec:
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 10gi

when check the resource in kubernetes
kubectl get pvc
name             status   volume                                     capacity   access modes   storageclass   age
redis-data-pvc   bound    pvc-6163d1f8-fb3d-44ac-a91f-edef1452b3b9   10gi       rwo            standard       132m

the default storage class is standard.
kubectl get storageclass
name                 provisioner
standard (default)   kubernetes.io/gce-pd

but when check the statafulset's deployment status. it always wrong.
# describe its pod details
...
events:
  type     reason                  age                from                     message
  ----     ------                  ----               ----                     -------
  warning  failedscheduling        22s                default-scheduler        persistentvolumeclaim &quot;redis-data-pvc&quot; not found
  warning  failedscheduling        17s (x2 over 20s)  default-scheduler        pod has unbound immediate persistentvolumeclaims (repeated 2 times)
  normal   created                 2s (x2 over 3s)    kubelet                  created container redis
  normal   started                 2s (x2 over 3s)    kubelet                  started container redis
  warning  backoff                 0s (x2 over 1s)    kubelet                  back-off restarting failed container

why can't it find the redis-data-pvc name?
",<kubernetes><google-kubernetes-engine><persistent-volumes><kubernetes-statefulset><kubernetes-pvc>,65269844,17,"what you have done, should work. make sure that the persistentvolumeclaim and the statefulset is located in the same namespace.

thats said, this is an easier solution, and that let you easier scale up to more replicas:
when using statefulset and persistentvolumeclaim, use the volumeclaimtemplates: field in the statefulset instead.
the volumeclaimtemplates: will be used to create unique pvcs for each replica, and they have unique naming ending with e.g. -0 where the number is an ordinal used for the replicas in a statefulset.
so instead, use a satefuleset manifest like this:
apiversion: apps/v1
kind: statefulset
metadata:
  name: redis
spec:
  servicename: &quot;redis&quot;
  selector:
    matchlabels:
      app: redis
  updatestrategy:
    type: rollingupdate
  replicas: 3
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis
        resources:
          limits:
            memory: 2gi
        ports:
          - containerport: 6379
        volumemounts:
          - name: redis-data
            mountpath: /usr/share/redis
  volumeclaimtemplates:                     // this will be used to create pvc
  - metadata:
      name: redis-data
    spec:
      accessmodes: [ &quot;readwriteonce&quot; ]
      resources:
        requests:
          storage: 10gi

"
47991638,check if files/dirs/ used in templates exists,"given the following json:

    apiversion: v1
    kind: configmap
    metadata:
    name: {{ template ""something.server.fullname"" . }}
    data:
    {{ (.files.glob ""dashboards/*.json"").asconfig | indent 2 }}
    {{ (.files.glob ""datasources/*.json"").asconfig | indent 2 }}


how can i check if the folder exists and is not empty?

currently, if the folder is missing or doesn't have any files, helm install will abort with this message:

error: yaml parse error on domething/charts/grafana/templates/dashboards-configmap.yaml: error converting yaml to json: yaml: line 6821: could not find expected ':'

",<kubernetes><kubernetes-helm>,52484022,17,"you can pull your globs out to variables, and then move everything within if blocks, e.g.:

{{- $globdash := .files.glob ""dashboards/*.json"" }}
{{ if $globdash }}
{{- $globdata := .files.glob ""datasources/*.json"" }}
{{ if $globdata }}
apiversion: v1
kind: configmap
metadata:
name: {{ template ""something.server.fullname"" . }}
data:
{{ ($globdash).asconfig | indent 2 }}
{{ ($globdata).asconfig | indent 2 }}
{{ end }}
{{ end }}

"
51843992,embeding conf files into helm chart,"im new at helm. im building a splunk helm chart with numerous conf files. i currently  use something like this in a configmap ..
apiversion: v1
kind: configmap
metadata:
  name: splunk-master-configmap
data:
  indexes.conf: |
    # global settings
    # inheritable by all indexes: no hot/warm bucket can exceed 1 tb.
    # individual indexes can override this setting.
    homepath.maxdatasizemb = 1000000

but i would prefer to have the conf files in a seperate folder e.g. configs/helloworld.conf and have come accross &quot;tpl&quot; but am struggling to understand how to implement it. - can anyone advise best practices. on a side note splunk has orders of presidences &gt;&gt; so there may be many indexes.conf files used in various locations. does anyone have any thoughts on how best to implement this?!??!
cheers.
",<kubernetes><kubernetes-helm><splunk><configmap>,51847086,17,"if the content of the files is static then you could create a files directory in your chart at the same level as the templates directory (not inside it) and reference them like:

kind: configmap
metadata:
  name: splunk-master-configmap
data:
  {{ (.files.glob ""files/indexes.conf"").asconfig | indent 2 }}
  {{ (.files.glob ""files/otherfile.conf"").asconfig | indent 2 }}
# ... and so on


where this would break down is if you want to be able to reference the values of variables inside the files so that the content is controlled from the values.yaml. if you want to expose each value individually then there's an example in the helm documentation using range. but i think a good fit or your case is what the stable/mysql chart does. it has a configmap that takes values as strings:

{{- if .values.configurationfiles }}
apiversion: v1
kind: configmap
metadata:
  name: {{ template ""mysql.fullname"" . }}-configuration
data:
{{- range $key, $val := .values.configurationfiles }}
  {{ $key }}: |-
{{ $val | indent 4}}
{{- end }}
{{- end -}}


and the values.yaml allows both the files and their content to be set and overridden by the user of the chart:

# custom mysql configuration files used to override default mysql settings
configurationfiles:
#  mysql.cnf: |-
#    [mysqld]
#    skip-name-resolve
#    ssl-ca=/ssl/ca.pem
#    ssl-cert=/ssl/server-cert.pem
#    ssl-key=/ssl/server-key.pem


it comments out that content and leaves it to the user of the chart to set but you could have defaults in the values.yaml.

you would only need tpl if you needed further flexibility. the stable/keycloak chart lets the user of the chart create their own configmap and point it into the keycloak deployment via tpl. but i think your case is probably closest to the mysql one. 

edit: the tpl function can also be used to take the content of files loaded with files.get and effectively make that content part of the template - see how do i load multiple templated config files into a helm chart? if you're interested in this
"
71692891,argocd & traefik 2.x: how to configure argocd-server deployment to run with tls disabled (where to put --insecure flag),"we have a setup with traefik as the ingress controller / crd and argocd. we installed argocd into our eks setup as described in the argo getting stared guide:
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

now as the docs state the ingressroute object to configure traefik correctly looks like this:
apiversion: traefik.containo.us/v1alpha1
kind: ingressroute
metadata:
  name: argocd-server
  namespace: argocd
spec:
  entrypoints:
    - websecure
  routes:
    - kind: rule
      match: host(`argocd.tekton-argocd.de`)
      priority: 10
      services:
        - name: argocd-server
          port: 80
    - kind: rule
      match: host(`argocd.tekton-argocd.de`) &amp;&amp; headers(`content-type`, `application/grpc`)
      priority: 11
      services:
        - name: argocd-server
          port: 80
          scheme: h2c
  tls:
    certresolver: default
    

right now there's a bug in the docs - so be sure to remove the options: {} in order to let traefik accept the configuration.
traefik shows everything is fine in the dashboard:

but if we try to access the argocd dashboard at https://argocd.tekton-argocd.de we get multiple http 307 redirects and can't access the dashboard in the end. you can see the redirects inside the developer tools:

searching for a solution we already found this issue where the problem is described:

the problem is that by default argo-cd handles tls termination itself
and always redirects http requests to https. combine that with an
ingress controller that also handles tls termination and always
communicates with the backend service with http and you get argo-cd's
server always responding with a redirects to https.

also the solution is sketched:

so one of the solutions would be to disable https on argo-cd, which
you can do by using the --insecure flag on argocd-server.

but how can we configure the argocd-server deployment to add the --insecure flag to the argocd-server command - as it is also stated inside the argocd docs?
",<kubernetes><kubernetes-ingress><traefik><kustomize><argocd>,71692892,16,"0. why a declarative argocd setup with kustomize is a great way to configure custom parameters
there are multiple options on how to configure argocd. a great way is to use a declarative approach, which should be the default kubernetes-style. skimming the argocd docs there's a additional configuration section where the possible flags of the configmap argocd-cmd-params-cm can be found. the flags are described in argocd-cmd-params-cm.yaml. one of them is the flag server.insecure
## server properties
# run server without tls
server.insecure: &quot;false&quot;

the argocd-server deployment which ships with https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml will use this parameter, if it is defined in the argocd-cmd-params-cm configmap.
in order to declaratively configure the argocd configuration, the argocd docs have a great section on how to do that with kustomize. in fact the argocd team itself uses this approach to deploy their own argocd instances - a live deployment is available here https://cd.apps.argoproj.io/ and the configuration used can be found on github.
adopting this to our use case, we need to switch our argocd installation from simply using kubectl apply -f to a kustomize-based installation. the argocd docs also have a section on how to do this. here are the brief steps:
1. create a argocd/installation directory with a new file kustomization.yaml
we slightly enhance the kustomization.yaml proposed in the docs:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization

resources:
  - https://raw.githubusercontent.com/argoproj/argo-cd/v2.3.3/manifests/install.yaml

## changes to config maps
patches:
  - path: argocd-cmd-params-cm-patch.yml

namespace: argocd

since the docs state

it is recommended to include the manifest as a remote resource and
apply additional customizations using kustomize patches.

we use the patchesstrategicmerge configuration key, which contains another new file we need to create called argocd-cmd-params-cm-patch.yml.
2. create a new file argocd-cmd-params-cm-patch.yml
this new file only contains the configuration we want to change inside the configmap argocd-cmd-params-cm:
apiversion: v1
kind: configmap
metadata:
  name: argocd-cmd-params-cm
data:
  server.insecure: &quot;true&quot;

3. install argocd using the kustomization files &amp; kubectl apply -k
there's a separate kustomize cli one can install e.g. via brew install kustomize. but as kustomize is build into kubectl we only have to use kubectl apply -k and point that to our newly created argocd/installation directory like this. we just also need to make sure that the argocd namespace is created:
kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -    
kubectl apply -k argocd/installation

this will install argocd and configure the argocd-server deployment to use the --insecure flag as needed to stop argo from handling the tls termination itself and giving that responsibility to traefik. now accessing https://argocd.tekton-argocd.de should open the argocd dashboard as expected:

"
58561682,minikube with ingress example not working,"i'm trying to get an ingress controller working in minikube and am following the steps in the k8s documentation here, but am seeing a different result in that the ip address for the ingress controller is different than that for minikube (the example seems to indicate they should be the same):

$ kubectl get ingress
name              hosts              address     ports   age
example-ingress   hello-world.info   10.0.2.15   80      12m

$ minikube ip
192.168.99.101


when i try to connect to the minikube ip address (using the address directly vs. adding it to my local hosts file), i'm getting a ""not found"" response from nginx:

$ curl http://`minikube ip`/
&lt;html&gt;
    &lt;head&gt;&lt;title&gt;404 not found&lt;/title&gt;&lt;/head&gt;
    &lt;body&gt;
        &lt;center&gt;&lt;h1&gt;404 not found&lt;/h1&gt;&lt;/center&gt;
        &lt;hr&gt;&lt;center&gt;openresty/1.15.8.1&lt;/center&gt;
    &lt;/body&gt;
&lt;/html&gt;


when i try to connect to the ip address associated with the ingress controller, it just hangs.

should i expect the addresses to be the same as the k8s doc indicates?

some additional information:

$ kubectl get nodes -o wide
name       status   roles    age     version   internal-ip   external-ip   os-image              kernel-version   container-runtime
minikube   ready    master   2d23h   v1.16.0   10.0.2.15     &lt;none&gt;        buildroot 2018.05.3   4.15.0           docker://18.9.9

$ kubectl get ingresses example-ingress -o yaml
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {""apiversion"":""networking.k8s.io/v1beta1"",""kind"":""ingress"",""metadata"":{""annotations"":{""nginx.ingress.kubernetes.io/rewrite-target"":""/$1""},""name"":""example-ingress"",""namespace"":""default""},""spec"":{""rules"":[{""host"":""hello-world.info"",""http"":{""paths"":[{""backend"":{""servicename"":""web"",""serviceport"":8080},""path"":""/""}]}}]}}
    nginx.ingress.kubernetes.io/rewrite-target: /$1
  creationtimestamp: ""2019-10-28t15:36:57z""
  generation: 1
  name: example-ingress
  namespace: default
  resourceversion: ""25609""
  selflink: /apis/extensions/v1beta1/namespaces/default/ingresses/example-ingress
  uid: 5e96c378-fbb1-4e8f-9738-3693cbce7d9b
spec:
  rules:
  - host: hello-world.info
    http:
      paths:
      - backend:
          servicename: web
          serviceport: 8080
        path: /
status:
  loadbalancer:
    ingress:
    - ip: 10.0.2.15

",<kubernetes><minikube><kubernetes-ingress>,58604993,16,"i've reproduced your scenario in a linux environment (on gcp) and i also have different ips:
user@bf:~$ minikube ip
192.168.39.144

user@bf:~$ kubectl get ingresses
name              hosts   address           ports   age
example-ingress   *       192.168.122.173   80      30m

your problem is not related to the fact you have different ips. the guide instructs us to create an ingress with the following rule:
spec:
  rules:
  - host: hello-world.info

this rule is telling the ingress service that a dns record with hello-world.info name is expected.
if you follow the guide a bit further, it instructs you to create an entry on your hosts file pointing to your ingress ip or minikube ip.

note: if you are running minikube locally, use minikube ip to get the external ip. the ip address displayed within the ingress list
will be the internal ip.
source: set up ingress on minikube with the nginx ingress controller

(if you want to curl the ip instead of dns name, you need to remove the host rule from your ingress)
it should look like this:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
     paths:
     - path: /
       backend:
         servicename: web
         serviceport: 8080

apply your changes:
user@bf:~$ kubectl apply -f example-ingress.yaml

and curl the ip using -lk options to surpass problems related to secure connections.
user@bf:~$ curl -lk 192.168.39.144
hello, world!
version: 1.0.0
hostname: web-9bbd7b488-l5gc9

"
70075203,how to use volume gp3 in storage class on eks?,"i'm converting volume gp2 to volume gp3 for eks but getting this error.
failed to provision volume with storageclass &quot;gp3&quot;: invalid aws volumetype &quot;gp3&quot;
this is my config.
storageclass
apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
  name: gp3
parameters:
  fstype: ext4
  type: gp3
provisioner: kubernetes.io/aws-ebs
reclaimpolicy: retain
allowvolumeexpansion: true
volumebindingmode: waitforfirstconsumer   

pvc
apiversion: v1
kind: persistentvolumeclaim
metadata:
  labels:
    app: test-pvc
  name: test-pvc
  namespace: default
spec:
  accessmodes:
  - readwriteonce
  resources:
    requests:
      storage: 1gi
  storageclassname: gp3   

when i type kubectl describe pvc/test. this is response:
name:          test-pvc
namespace:     default
storageclass:  gp3
status:        pending
volume:        
labels:        app=test-pvc
annotations:   volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/aws-ebs
finalizers:    [kubernetes.io/pvc-protection]
capacity:      
access modes:  
volumemode:    filesystem
used by:       &lt;none&gt;
events:
  type     reason              age                  from                         message
  ----     ------              ----                 ----                         -------
  warning  provisioningfailed  58s (x9 over 4m35s)  persistentvolume-controller  failed to provision volume with storageclass &quot;gp3&quot;: invalid aws volumetype &quot;gp3&quot;   

i'm using kubernetes version 1.18.
can someone help me. thanks!
",<amazon-web-services><kubernetes><cloud><devops><amazon-eks>,70076942,16,"i found the solution to use volume gp3 in storage class on eks.

first, you need to install amazon ebs csi driver with offical instruction here.
the next, you need to create the storage class ebs-sc after amazon ebs csi driver is installed, example:


cat &lt;&lt; eof | kubectl apply -f -
apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  name: ebs-sc
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
reclaimpolicy: retain
volumebindingmode: waitforfirstconsumer
eof

so, you can use volume gp3 in storage class on eks.
you can check by deploying resources:
cat &lt;&lt; eof | kubectl apply -f -
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: ebs-gp3-claim
spec:
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 1gi
  storageclassname: ebs-sc
---
apiversion: v1
kind: pod
metadata:
  name: app-gp3-in-tree
spec:
  containers:
  - name: app
    image: nginx
    volumemounts:
    - name: persistent-storage
      mountpath: /usr/share/nginx/html
  volumes:
  - name: persistent-storage
    persistentvolumeclaim:
      claimname: ebs-gp3-claim
eof

detailed documentation on migrating amazon eks clusters from gp2 to gp3 ebs volumes: https://aws.amazon.com/vi/blogs/containers/migrating-amazon-eks-clusters-from-gp2-to-gp3-ebs-volumes/
references: persistent storage in eks failing to provision volume
"
55940828,what is the best way to setup proxy pass in an nginx ingress object for kubernetes,"currently i am trying to migrate a site that was living on an apache load balanced server to my k8s cluster. however the application was set up strangely with a proxypass and proxyreversepass like so:

proxypass /something http://example.com/something
proxypassreverse /something http://example.com/something


and i would like to mimic this in an nginx ingress

first i tried using the rewrite-target annotation however that does not keep the location header which is necessary to get the application running again. 

then i tried to get the proxy-redirect-to/from annotation in place inside a specific location block like so:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: gpg-app-ingress
  annotations:
    nginx.ingress.kubernetes.io/proxy-redirect-from: http://originalapp.com/something
    nginx.ingress.kubernetes.io/proxy-redirect-to: http://example.com/something
spec:
  rules:
  - host: example.com
    http:
      paths:
        - path: /something
          backend:
            servicename: example-com
            serviceport: 80


i would like to be able to instead use a custom proxy_pass variable but it doesn't seem like its possible.

what would be the best way to mimic this proxy pass?
",<nginx><kubernetes><proxy><kubernetes-ingress><nginx-ingress>,56148241,16,"firstly you can use custom configuration for your nginx ingress controller, documentation can be found here

also, if you just want to use nginx ingress controller as a reverse proxy, each ingress rule already creates proxy_pass directive to relevant upstream/backend service. 

and if paths are same with your rule and backend service, then you don't have to specify rewrite rule, only just path for backend service. but if paths
are different, then take consider using nginx.ingress.kubernetes.io/rewrite-target annotation, otherwise you will get 404 backend error

so to redirect request from which is coming to frontend http://example.com/something to backend example-com/something, your ingress rule should be similar to below

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: gpg-app-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
   #nginx.ingress.kubernetes.io/rewrite-target: /different-path
spec:
  rules:
  - host: example.com
    http:
      paths:
        - path: /something
          backend:
            servicename: example-com
            serviceport: 80


for more explanation about annotations, check nginx ingress annotations

also, consider checking logs of nginx-ingress-controller pod via if something wrong

kubectl logs nginx-ingress-controller-xxxxx


hope it helps!
"
57623894,how access mongodb in kubernetes from outside the cluster,"i deployed mongodb in a kubernetes cluster with this helm chart : https://github.com/helm/charts/tree/master/stable/mongodb. all is right. i can connect to mongo from within a replicatset container or from outside the cluster with a port-forward, or with a nodeport service. but i can't connect via an ingress.

when the ingress is deployed, i can curl mongodb and have this famous message : ""it looks like you are trying to access mongodb over http on the native driver port."". but i can't connect with a mongo client, the connection stucks and i can see in mongodb logs that i never reach mongo.

does someone have any information about accessing mongodb via an ingress object ? maybe it's a protocol problem ?

the ingress manifests :

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: {{ template ""mongodb.fullname"" . }}
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
  rules:
  - host: {{ .values.ingress.hostname }}
    http:
      paths:
        - path: /
          backend:
            servicename: ""{{ template ""mongodb.fullname"" $ }}""
            serviceport: mongodb
  tls:
  - hosts:
    - {{ .values.ingress.hostname }}
    secretname: secret


thank you very much !
",<mongodb><kubernetes><portforwarding><kubernetes-ingress>,57624252,16,"ingress controllers are designed for http connections, as the error hinted, the ingress is not the way to access mongodb.

none of the information in an ingress definition makes much sense for a plain tcp connection, host names and http url paths don't apply to plain tcp connections. 

some ingress controllers (like nginx-ingress) can support plain tcp load balancers but not via an ingress definition. they use custom config maps. 

use a service with type: loadbalancer if your hosting environment supports it or type: nodeport if not. there is an example in the stable mongodb helm chart and it's associated values. 

apiversion: v1
kind: service
metadata:
  name: {{ template ""mongodb.fullname"" . }}
  labels:
    app: {{ template ""mongodb.name"" . }}
spec:
  type: loadbalancer
  ports:
  - name: mongodb
    port: 27017
    targetport: mongodb
  - name: metrics
    port: 9216
    targetport: metrics

"
60840186,multiple resources using single helm template,"we had been using single ingress per application(public) by default but with the recent requirement we need to expose (private) endpoint as well for some of the apps. that means we had a single template that looks like this:

templates/ingress.yaml

{{- if .values.ingress.enabled -}}
{{- $fullname := include ""app.fullname"" . -}}
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: {{ $fullname }}
  labels:
{{ include ""app.labels"" . | indent 4 }}
  {{- with .values.ingress.annotations }}
  annotations:
    {{- toyaml . | nindent 4 }}
  {{- end }}
spec:
{{- if .values.ingress.tls }}
  tls:
  {{- range .values.ingress.tls }}
    - hosts:
      {{- range .hosts }}
        - {{ . | quote }}
      {{- end }}
      secretname: {{ .secretname }}
  {{- end }}
{{- end }}
  rules:
  {{- range .values.ingress.hosts }}
    - host: {{ .host | quote }}
      http:
        paths:
        {{- range .paths }}
          - path: {{ . }}
            backend:
              servicename: {{ $fullname }}
              serviceport: http
        {{- end }}
  {{- end }}
{{- end }}


templates/cert.yaml

{{- if .values.ingress.tls -}}
apiversion: certmanager.k8s.io/v1alpha1
kind: certificate
metadata:
  name: {{ .values.ingress.name }}
  namespace: {{ .values.ingress.namespace }}
spec:
{{- range .values.ingress.tls }}
  secretname: {{ .secretname }}
  duration: 24h
  renewbefore: 12h
  issuerref:
    name: {{ .issuerref.name }}
    kind: {{ .issuerref.kind }}
  dnsnames: 
    {{- range .hosts }}
        - {{ . | quote }}
    {{- end }}
{{- end -}}
{{- end -}}


and the values.yaml looks like this:

ingress:
  enabled: true
  name: apps-ingress
  namespace: app1-namespace
  annotations:
    kubernetes.io/ingress.class: hybrid-external
    nginx.ingress.kubernetes.io/backend-protocol: ""http""
  hosts:
    - host: apps.test.cluster
      paths:
        - /
  tls:
    - secretname: app1-tls
      issuerref: 
        name: vault-issuer
        kind: clusterissuer
      hosts:
        - ""apps.test.cluster""


so, to accomodate the new setup. i have added the below block on values.yaml file.

ingress-private:
  enabled: true
  name: apps-ingress-private
  namespace: app1-namespace
  annotations:
    kubernetes.io/ingress.class: hybrid-internal
    nginx.ingress.kubernetes.io/backend-protocol: ""http""
  hosts:
    - host: apps.internal.test.cluster
      paths:
        - /
  tls:
    - secretname: app1-tls
      issuerref: 
        name: vault-issuer
        kind: clusterissuer
      hosts:
        - ""apps.internal.test.cluster""


and duplicated both templates i.e templates/ingress-private.yaml and templates/certs-private.yaml, and is working fine but my question here is - is there a way using a single template for each ingress and certs and create conditional resource?

as i mentioned above, some apps need internal ingress and some don't. what i want to do is; make public ingress/certs as default and private as optional. i have been using {{- if .values.ingress.enabled -}} option to validate if ingress is required but in 2 different files.

also, in values.yaml file, rather than having 2 different block is there a way to use the list if multiple resources are required?
",<kubernetes><kubernetes-helm><amazon-eks>,60847873,15,"there are a couple of ways to approach this problem.
the way you have it now, with one file per resource but some duplication of logic, is a reasonably common pattern.  it's very clear exactly what resources are being created, and there's less logic involved.  the go templating language is a little bit specialized, so this can be more approachable to other people working on your project.
if you do want to combine things together there are a couple of options.  as @matt notes in their comment, you can put multiple kubernetes resources in the same file so long as they're separated by the yaml --- document separator.
{{/* ... arbitrary templating logic ... */}}
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: {{ $fullname }}
...
{{/* ... more logic ... */}}
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: {{ $fullname }}-private
...

the only thing that matters here is that the output of the template is a valid multi-document yaml file.  you can use the helm template command to see what comes out without actually sending it to the cluster.
this approach pairs well with having a list of configuration rules in your yaml file
ingresses:
  - name: apps-ingress
    annotations:
      kubernetes.io/ingress.class: hybrid-external
      nginx.ingress.kubernetes.io/backend-protocol: &quot;http&quot;
  - name: apps-ingress-private
    annotations:
      kubernetes.io/ingress.class: hybrid-internal
      nginx.ingress.kubernetes.io/backend-protocol: &quot;http&quot;

you can use the go template range construct to loop over all of these.  note that this borrows the . special variable, so if you do refer to arbitrary other things in .values you need to save away the current value of it.
{{- $top := . -}}
{{- range $ingress := .values.ingresses -}}
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: {{ $ingress.name }}
  annotations: {{- $ingress.annotations.toyaml | nindent 4 }}
...
{{ end }}

"
60426241,"error: upgrade failed: failed to replace object: service ""api"" is invalid: spec.clusterip: invalid value: """": field is immutable","when doing helm upgrade ... --force i'm getting this below error 

error: upgrade failed: failed to replace object: service ""api"" is invalid: spec.clusterip: invalid value: """": field is immutable


and this is how my service file looks like: (not passing clusterip anywhere )

apiversion: v1
kind: service
metadata:
  name: {{ .chart.name }}
  namespace: {{ .release.namespace }}
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: ""https""
    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: ""*""
  labels:
    app: {{ .chart.name }}-service
    kubernetes.io/name: {{ .chart.name | quote }}
    dns: route53
    chart: ""{{ .chart.name }}-{{ .chart.version }}""
    release: ""{{ .release.name }}""
spec:
  selector:
    app: {{ .chart.name }}
  type: loadbalancer
  ports:
  - port: 443
    name: https
    targetport: http-port
    protocol: tcp


helm version: 3.0.1

kubectl version: 1.13.1 [tried with the 1.17.1 as well]

server: 1.14

note: previously i was using some old version (of server, kubectl, helm) at that time i did not face this kind of issue. 
i can see lots of similar issues in github regarding this, but unable to find any working solution for me.

few of the similar issues:

https://github.com/kubernetes/kubernetes/issues/25241

https://github.com/helm/charts/pull/13646 [for nginx chart]
",<kubernetes><kubernetes-helm><kubectl>,60543378,15,"i've made some tests with helm and got the same issue when trying to change the service type from nodeport/clusterip to loadbalancer.

this is how i've reproduced your issue:

kubernetes 1.15.3 (gke)
helm 3.1.1

helm chart used for test: stable/nginx-ingress

how i reproduced:


get and decompress the file:


helm fetch stable/nginx-ingress  
tar xzvf nginx-ingress-1.33.0.tgz  



modify service type from type: loadbalancer to type: nodeport in the values.yaml file (line 271):


sed -i '271s/loadbalancer/nodeport/' values.yaml



install the chart:


helm install nginx-ingress ./



check service type, must be nodeport:


kubectl get svc -l app=nginx-ingress,component=controller

name                       type       cluster-ip   external-ip   port(s)                      age
nginx-ingress-controller   nodeport   10.0.3.137   &lt;none&gt;        80:30117/tcp,443:30003/tcp   1m



now modify the service type again to loadbalancer in the values.yaml:


sed -i '271s/nodeport/loadbalancer/' values.yaml



finally, try to upgrade the chart using --force flag:


helm upgrade nginx-ingress ./ --force


and then:

error: upgrade failed: failed to replace object: service ""nginx-ingress-controller"" is invalid: spec.clusterip: invalid value: """": field is immutable


explanation

digging around i found this in helm source code:

// if --force is applied, attempt to replace the existing resource with the new object.
    if force {
        obj, err = helper.replace(target.namespace, target.name, true, target.object)
        if err != nil {
            return errors.wrap(err, ""failed to replace object"")
        }
        c.log(""replaced %q with kind %s for kind %s\n"", target.name, currentobj.getobjectkind().groupversionkind().kind, kind)
    } else {
        // send patch to server
        obj, err = helper.patch(target.namespace, target.name, patchtype, patch, nil)
        if err != nil {
            return errors.wrapf(err, ""cannot patch %q with kind %s"", target.name, kind)
        }
    }


analyzing the code above helm will use similar to kubectl replace api request (instead of kubectl replace --force as we could expect)... when the helm --force flag is set.

if not, then helm will use kubectl patch api request to make the upgrade.

let's check if it make sense:

poc using kubectl


create a simple service as nodeport:


kubectl apply -f - &lt;&lt;eof
apiversion: v1
kind: service
metadata:
 labels:
   app: test-svc
 name: test-svc
spec:
 selector:
   app: test-app
 ports:
 - port: 80
   protocol: tcp
   targetport: 80
 type: nodeport
eof


make the service was created:

kubectl get svc -l app=test-svc

name       type       cluster-ip   external-ip   port(s)        age
test-svc   nodeport   10.0.7.37    &lt;none&gt;        80:31523/tcp   25


now lets try to use kubectl replace to upgrade the service to loadbalancer, like helm upgrade --force:

kubectl replace -f - &lt;&lt;eof
apiversion: v1
kind: service
metadata:
 labels:
   app: test-svc
 name: test-svc
spec:
 selector:
   app: test-app
 ports:
 - port: 80
   protocol: tcp
   targetport: 80
 type: loadbalancer
eof


this shows the error:

the service ""test-svc"" is invalid: spec.clusterip: invalid value: """": field is immutable


now, lets use kubectl patch to change the nodeport to loadbalancer, simulating the helm upgrade command without --force flag:

here is the kubectl patch documentation, if want to see how to use.

kubectl patch svc test-svc -p '{""spec"":{""type"":""loadbalancer""}}'


then you see:
service/test-svc patched

workaround

you should to use helm upgrade without --force, it will work.

if you really need to use --force to recreate some resources, like pods to get the latest configmap update, for example, then i suggest you first manually change the service specs before helm upgrade.

if you are trying to change the service type you could do it exporting the service yaml, changing the type and apply it again (because i experienced this behavior only when i tried to apply the same template from the first time):

kubectl get svc test-svc -o yaml | sed 's/nodeport/loadbalancer/g' | kubectl replace --force -f -


the output:

service ""test-svc"" deleted
service/test-svc replaced


now, if you try to use helm upgrade --force and doesn't have any change to do in the service, it will work and will recreate your pods and others resources.

i hope that helps you!
"
60655653,use multiple contexts with same user-name in kubectl config,"i want to use multiple clusters with my kubectl so i either put everything into one config or add one config file per cluster to the kubeconfig env variable. that's all fine.

my problem is now, that i've users with the same user-name for each cluster but they use different client-key-data for each cluster (context) but somehow the context uses that user-name so it's not clear which user belongs to which cluster.

better give an example:

cluster 1:

apiversion: v1
kind: config
clusters:
- cluster:
    server: https://10.11.12.13:8888
  name: team-cluster
contexts:
- context:
    cluster: team-cluster
    user: kubernetes-admin
  name: kubernetes-admin@team-cluster
users:
- name: kubernetes-admin
  user:
    client-certificate-data: xxyyyzzz
    client-key-data: xxxyyyzzz


cluster 2:

apiversion: v1
kind: config
clusters:
- cluster:
    server: https://10.11.12.14:8888
  name: dev-cluster
contexts:
- context:
    cluster: dev-cluster
    user: kubernetes-admin
  name: kubernetes-admin@dev-cluster
users:
- name: kubernetes-admin
  user:
    client-certificate-data: aabbcc
    client-key-data: aabbcc


as you see, in both cluster there's a user with name kubernetes-admin but from the context it's not clear which of those. maybe there's another way to give it a unique identifier that is used by the context.

maybe the solution is obvious but i've not found any example for such a case. thanks for any help.
",<kubernetes><kubectl>,65825922,15,"i had same issue with my config and found out that name in users is not username used to log in - its just name used to identify user section in config. in your case only cert key is used to know who you are. so you can use:
users:
- name: kubernetes-admin-1
  user:
    client-certificate-data: aabbcc
    client-key-data: aabbcc
- name: kubernetes-admin-2
  user:
    client-certificate-data: xxyyyzzz
    client-key-data: xxxyyyzzz

and refer to that in context just by key:
contexts:
- context:
    cluster: dev-cluster
    user: kubernetes-admin-1

full config:
apiversion: v1
kind: config
clusters:
- cluster:
    server: https://10.11.12.13:8888
  name: team-cluster
- cluster:
    server: https://10.11.12.14:8888
  name: dev-cluster
contexts:
- context:
    cluster: team-cluster
    user: kubernetes-admin-1
  name: kubernetes-admin@team-cluster
- context:
    cluster: dev-cluster
    user: kubernetes-admin-2
  name: kubernetes-admin@dev-cluster
users:
- name: kubernetes-admin-1
  user:
    client-certificate-data: xxyyyzzz
    client-key-data: xxxyyyzzz
- name: kubernetes-admin-2
  user:
    client-certificate-data: aabbcc
    client-key-data: aabbcc

for auth methods that require username it's used something like this:
users:
- name: kubernetes-admin-with-password
  user:
    username: kubernetes-admin
    password: mysecretpass

using more than one kubeconfig is not much comfortable - you need to specify them for each command. you can have as much contexts and users if you want in one config and select right context (and save selected context as default).
"
59872478,templating external files in helm,"i want to use application.yaml file to be passed as a config map.

so i have written this.

 apiversion: v1
 kind: configmap
 metadata:
  name: conf
data:
{{ (.files.glob ""foo/*"").asconfig | indent 2 }}


my application.yaml is present in foo folder and 
contains a service name which i need it to be dynamically populated via helm interpolation.

foo:
  service:
    name: {{.release.name}}-service


when i dry run , i am getting this

apiversion: v1
kind: configmap
metadata:
  name: conf
data:
  application.yaml: ""ei:\r\n  service:\r\n    name: {{.release.name}}-service""


but i want name: {{.release.name}}-service to contain actual helm release name.

is it possible to do templating for external files using helm , if yes then how to do it ?
i have gone through https://v2-14-0.helm.sh/docs/chart_template_guide/#accessing-files-inside-templates
i didn't find something which solves my use case.
 i can also copy the content to config map yaml and can do interpolation but i don't want to do it. i want application.yml to be in a separate file, so that, it will be simple to deal with config changes..
",<kubernetes><kubernetes-helm>,59877268,15,"helm includes a tpl function that can be used to expand an arbitrary string as a go template.  in your case the output of ...asconfig is a string that you can feed into the template engine.

apiversion: v1
kind: configmap
metadata:
  name: {{ .release.name }}-conf
data:
{{ tpl (.files.glob ""foo/*"").asconfig . | indent 2 }}


once you do that you can invoke arbitrary template code from within the config file.  for example, it's common enough to have a defined template that produces the name prefix of the current chart as configured, and so your config file could instead specify

foo:
  service:
    name: {{ template ""mychart.name"" . }}-service

"
63412552,why readwriteonce is working on different nodes?,"our platform which runs on k8s has different components. we need to share the storage between two of these components (comp-a and comp-b) but by mistake, we defined the pv and pvc for that as readwriteonce and even when those two components were running on different nodes everything was working and we were able to read and write to the storage from both components.
based on the k8s docs the readwriteonce can be mounted to one node and we have to use readwritemany:

readwriteonce -- the volume can be mounted as read-write by a single node
readonlymany -- the volume can be mounted read-only by many nodes
readwritemany -- the volume can be mounted as read-write by many nodes&quot;

so i am wondering why everything was working fine while it shouldn't?
more info:
we use nfs for storage and we are not using dynamic provisioning and below is how we defined our pv and pvc (we use helm):
- apiversion: v1
  kind: persistentvolume
  metadata:
    name: gstreamer-{{ .release.namespace }}
  spec:
    capacity:
      storage: 10gi
    accessmodes:
      - readwriteonce
    persistentvolumereclaimpolicy: recycle
    mountoptions:
      - hard
      - nfsvers=4.1
    nfs:
      server: {{ .values.global.nfsserver }}
      path: /var/nfs/general/gstreamer-{{ .release.namespace }}

- apiversion: v1
  kind: persistentvolumeclaim
  metadata:
    name: gstreamer-claim
    namespace: {{ .release.namespace }}
  spec:
    volumename: gstreamer-{{ .release.namespace }}
    accessmodes:
      - readwriteonce
    resources:
      requests:
        storage: 10gi

update
the output of some kubectl commands:
$ kubectl get -n 149 pvc
name              status   volume                                     capacity   access modes   storageclass   age
gstreamer-claim   bound    gstreamer-149                              10gi       rwo                           177d


$ kubectl get -n 149 pv
name                                       capacity   access modes   reclaim policy   status   claim                                       storageclass   reason   age
gstreamer-149                              10gi       rwo            recycle          bound    149/gstreamer-claim                                                 177d


i think somehow it takes care of it because the only thing the pods need to do is connecting to that ip.
",<kubernetes><kubernetes-pod><persistent-volumes><kubernetes-pvc>,63524679,15,"it's quite misleading concept regarding accessmode, especially in nfs.
in kubernetes persistent volume docs it's mentioned that nfs supports all types of access. rwo, rxx and rwx.
however accessmode is something like matching criteria, same as storage size. it's described better in openshift access mode documentation

a persistentvolume can be mounted on a host in any way supported by the resource provider. providers have different capabilities and each pvs access modes are set to the specific modes supported by that particular volume. for example, nfs can support multiple read-write clients, but a specific nfs pv might be exported on the server as read-only. each pv gets its own set of access modes describing that specific pvs capabilities.


claims are matched to volumes with similar access modes. the only two matching criteria are access modes and size. a claims access modes represent a request. therefore, you might be granted more, but never less. for example, if a claim requests rwo, but the only volume available is an nfs pv (rwo+rox+rwx), the claim would then match nfs because it supports rwo.


direct matches are always attempted first. the volumes modes must match or contain more modes than you requested. the size must be greater than or equal to what is expected. if two types of volumes, such as nfs and iscsi, have the same set of access modes, either of them can match a claim with those modes. there is no ordering between types of volumes and no way to choose one type over another.


all volumes with the same modes are grouped, and then sorted by size, smallest to largest. the binder gets the group with matching modes and iterates over each, in size order, until one size matches.

in the next paragraph:

a volumes accessmodes are descriptors of the volumes capabilities. they are not enforced constraints. the storage provider is responsible for runtime errors resulting from invalid use of the resource.


for example, nfs offers readwriteonce access mode. you must mark the claims as read-only if you want to use the volumes rox capability. errors in the provider show up at runtime as mount errors.

another example is that you can choose a few accessmodes as it is not constraint but a matching criteria.
$ cat &lt;&lt;eof | kubectl create -f -
&gt; apiversion: v1
&gt; kind: persistentvolumeclaim
&gt; metadata:
&gt;   name: exmaple-pvc
&gt; spec:
&gt;   accessmodes:
&gt;     - readonlymany
&gt;     - readwritemany
&gt;     - readwriteonce
&gt;   resources:
&gt;     requests:
&gt;       storage: 1gi
&gt; eof

or as per gke example:
$ cat &lt;&lt;eof | kubectl create -f -
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: exmaple-pvc-rwo-rom
spec:
  accessmodes:
    - readonlymany
    - readwriteonce
  resources:
    requests:
      storage: 1gi
eof               
persistentvolumeclaim/exmaple-pvc-rwo-rom created

pvc output
$ kubectl get pvc
name                  status    volume                                     capacity   access modes   storageclass   age
exmaple-pvc           pending                                                                        standard       2m18s
exmaple-pvc-rwo-rom   bound     pvc-d704d346-42b3-4090-af96-aebeee3053f5   1gi        rwo,rox        standard       6s
persistentvolumeclaim/exmaple-pvc created

exmaple-pvc is in pending state as default gke gcepersistentdisk its not supporting rreadwritemany.
warning  provisioningfailed  10s (x5 over 69s)  persistentvolume-controller  failed to provision volume with storageclass &quot;standard&quot;: invalid accessmodes [readonlymany readwritemany readwr
iteonce]: only accessmodes [readwriteonce readonlymany] are supported

however second pvc exmaple-pvc-rwo-rom were created and you can see it have 2 access mode rwo, rox.
in short accessmode is more like requirement for pvc/pv to bind. if nfs which is providing all access modes binds with rwo it fulfill requirement, however it will work as rwm as nfs providing that capability.
hope it answered cleared a bit.
in addition you can check other stackoverflow threads regarding accessmode
"
46389817,kubernetes share volume between containers inside a deployment,"before posting this question i followed this answer how to mimic &#39;--volumes-from&#39; in kubernetes but it didn't work for me.

i have 2 containers:


node: its image contains all the files related to the app ( inside /var/www ) 
nginx: it needs to access the files inside the node image (especially the /clientbuild folder where i have all the assets)


what is inside the node image:

$ docker run node ls -l
&gt; clientbuild/
&gt; package.json
&gt; ...


a part of the nginx.prod.conf:

location ~* \.(jpeg|jpg|gif|png|ico|css|js|gz|map|json)$ {
  include /etc/nginx/mime.types;
  root /usr/local/nginx/html/clientbuild/;
}


and the the deployment setup:

kind: deployment
apiversion: extensions/v1beta1
metadata:
  name: pwa-app-production
  labels:
    app: myapp
spec:
  replicas: 1
  template:
    metadata:
      name: app
      labels:
        app: myapp
        env: production
    spec:
      containers:
      - name: nginx
        image: nginx
        command: [nginx, -c, /nginx.prod.conf, -g, 'daemon off;']
        resources:
          limits:
            memory: ""500mi""
            cpu: ""100m""
        imagepullpolicy: always
        volumemounts:
          - mountpath: /usr/local/nginx/html
            name: pwa-disk
            readonly: true
        ports:
        - name: nginx
          containerport: 80
      initcontainers:
      - name: node
        image: node
        command: [npm, start]
        resources:
          limits:
            memory: ""500mi""
            cpu: ""100m""
        imagepullpolicy: always
        volumemounts:
          - mountpath: /var/www
            name: pwa-disk
        ports:
        - name: app
          containerport: 3000
        - name: api
          containerport: 3001
      volumes:
        - name: pwa-disk
          emptydir: {}


i first attempt to put both images in the same containers key, but i got: /var/www/package.json not found on npm start

then i moved it inside the initcontainers but now i only have a notice that it failed, but it does not tell me why. view logs does not show any details too.

notice that when i remove volume part, the npm start works.


",<kubernetes><kubectl>,46402618,15,"i assume your assets are already packaged inside the image at /var/www. if you mount an emptydir volume at that path, then everything there gets overriden with the content of the emptydir volume - which initially is nothing. that means all your assets are deleted through that mount - which is why your node server is most likely failing. 

what you want to do is mount the emptydir volume at some other path, say /data. then you override your node containers cmd with cp -r /var/www/* /data to copy the assets into yourpwa-disk volume. now, you can mount this volume into your nginx container. 

i think there is a misunderstanding on how initcontainers work. they are meant to terminate. they run before any other container is started - no other container inside your pod is started until your initcontainers have successfully terminated. so most likely you do not want to start your node server as an initcontainer. i guess your node server is not supposed to terminate, in which case your nginx container will never start up. instead, you might want to declare your node server together with your nginx inside the containers section. additionally, you also add your node container with an overridden cmd (cp -r /var/www/* /data) to the initcontainers section, to copy the assets to a volume. the whole thing might look sth like that: 

kind: deployment
apiversion: extensions/v1beta1
metadata:
  name: pwa-app-production
  labels:
    app: myapp
spec:
  replicas: 1
  template:
    metadata:
      name: app
      labels:
        app: myapp
        env: production
    spec:
      containers:
      - name: nginx
        image: nginx
        command: [nginx, -c, /nginx.prod.conf, -g, 'daemon off;']
        resources:
          limits:
            memory: ""500mi""
            cpu: ""100m""
        imagepullpolicy: always
        volumemounts:
          - mountpath: /usr/local/nginx/html
            name: pwa-disk
            readonly: true
        ports:
        - name: nginx
          containerport: 80
      - name: node
        image: node
        command: [npm, start]
        resources:
          limits:
            memory: ""500mi""
            cpu: ""100m""
        imagepullpolicy: always
        ports:
        - name: app
          containerport: 3000
        - name: api
          containerport: 3001

      initcontainers:
      - name: assets
        image: node
        command: [bash, -c]
        args: [""cp -r /var/www/* /data""]
        imagepullpolicy: always
        volumemounts:
          - mountpath: /data
            name: pwa-disk
      volumes:
        - name: pwa-disk
          emptydir: {}

"
66223566,adding public ip to nginx ingress controller with metallb,"i have three nodes in my cluster who are behind a firewall i do not control. this firewall has a public ip connected to it and can forward traffic to my kubernetes node. it has port 80 and 443 opened to my node.
initially, i used the public ip in the metallb config like this:
apiversion: v1
kind: configmap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 186.xx.xx.xx-186.xx.xx.xx


but after reading this answer of another question i'm guessing it is invalid since the ip used by metallb needs to be on the same subnet as the nodes? and they are all using private ips.
when i tested locally a http server listening on port 80 and ran it on the actual node (not in the cluster) then i was able get a response on the public ip from outside the network.
so my question is:
how do i make metallb or nginx ingress controller listen on port 80 and 443 for incoming request?
when using curl 186.xx.xx.xx:80 on of the nodes in the cluster then i'm receiving a response from the nginx ingress controller. but not when doing it outside of the node.
",<kubernetes><kubernetes-ingress><nginx-ingress><metallb>,66302342,15,"answering the question:

how can i create a setup with kubernetes cluster and separate firewall to allow users to connect to my nginx ingress controller which is exposing my application.

assuming the setup is basing on kubernetes cluster provisioned in the internal network and there is a firewall between the cluster and the &quot;internet&quot;, following points should be addressed (there could be some derivatives which i will address):

metallb provisioned on kubernetes cluster (assuming it's a bare metal self-managed solution)
nginx ingress controller with modified service
port-forwarding set on the firewall


service of type loadbalancer in the most part (there are some exclusions) is a resource that requires a cloud provider to assign an external ip address for your service.

a side note!
more reference can be found here:

kubernetes.io: docs: tasks: access application cluster: create external load balancer


for solutions that are on premise based, there is a tool called metallb:

kubernetes does not offer an implementation of network load-balancers (services of type loadbalancer) for bare metal clusters. the implementations of network lb that kubernetes does ship with are all glue code that calls out to various iaas platforms (gcp, aws, azure). if youre not running on a supported iaas platform (gcp, aws, azure), loadbalancers will remain in the pending state indefinitely when created.
bare metal cluster operators are left with two lesser tools to bring user traffic into their clusters, nodeport and externalips services. both of these options have significant downsides for production use, which makes bare metal clusters second class citizens in the kubernetes ecosystem.
metallb aims to redress this imbalance by offering a network lb implementation that integrates with standard network equipment, so that external services on bare metal clusters also just work as much as possible.
metallb.universe.tf

following the guide on the installation/configuration of metallb, there will be a configuration for a single internal ip address that the firewall will send the traffic to:
apiversion: v1
kind: configmap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: single-ip # &lt;-- here
      protocol: layer2
      addresses:
      - 10.0.0.100/32 # &lt;-- here

this ip address will be associated with the service of type loadbalancer of nginx ingress controller.

the changes required with the nginx ingress manifest (service part):

raw.githubusercontent.com: kubernetes: ingress nginx: controller: ... : deploy.yaml

# source: ingress-nginx/templates/controller-service.yaml
apiversion: v1
kind: service
metadata:
  annotations:
    metallb.universe.tf/address-pool: single-ip # &lt;-- important
  labels:
    # &lt;-- ommited --&gt; 
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  type: loadbalancer
  externaltrafficpolicy: local
  ports:
    - name: http
      port: 80
      protocol: tcp
      targetport: http
    - name: https
      port: 443
      protocol: tcp
      targetport: https
  selector:
    # &lt;-- ommited --&gt; 

above changes in the yaml manifest will ensure that the address that was configured in a metallb configmap will be used with the service.

a side note!
you can omit the metallb and use the service of type nodeport but this carries some disadvantages.


the last part is to set the port-forwarding on the firewall. the rule should be following:

firewall_ip:80 -&gt; single_ip:80
firewall_ip:443 -&gt; single_ip:443

after that you should be able to communicate with your nginx ingress controller by:

$ curl firewall_ip:80


additional resources:

kubernetes.io: docs: concepts: services networking: service

"
50916801,kubernetes - pod which encapsulates db is crashing,"i am experiencing issues when i try to deploy my django application to kubernetes cluster. more specifically, when i try to deploy postgresql.

here is what my .yml deployment file looks like:

apiversion: v1
kind: service
metadata:
  name: postgres-service
spec:
  selector:
    app: postgres-container
    tier: backend
  ports:
    - protocol: tcp
      port: 5432
      targetport: 5432
  type: clusterip
---
apiversion: v1
kind: persistentvolume
metadata:
  name: postgres-pv
  labels:
      type: local
spec:
  accessmodes:
    - readwriteonce
  capacity:
    storage: 2gi
  hostpath:
    path: /tmp/data/persistent-volume-1 #u okviru cvora n
  persistentvolumereclaimpolicy: retain
---
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: postgres-pv-claim
  labels:
    type: local
spec:
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 2gi
---
apiversion: apps/v1beta2
kind: deployment
metadata:
  name: postgres
spec:
  replicas: 1
  selector:
    matchlabels:
      app: postgres-container
      tier: backend
  template:
    metadata:
      labels:
        app: postgres-container
        tier: backend
    spec:
      containers:
        - name: postgres-container
          image: postgres:9.6.6
          env:
            - name: postgres_user
              valuefrom:
                secretkeyref:
                  name: postgres-credentials
                  key: user

            - name: postgres_password
              valuefrom:
                secretkeyref:
                  name: postgres-credentials
                  key: password

            - name: postgres_db
              value: agent_technologies_db
          ports:
            - containerport: 5432
          volumemounts:
            - name: postgres-volume-mount
              mountpath: /var/lib/postgresql/data/db-files

      volumes:
        - name: postgres-volume-mount
          persistentvolumeclaim:
            claimname: postgres-pv-claim
        - name: postgres-credentials
          secret:
            secretname: postgres-credentials


here is what i get when i run kubectl get pods command :

name                                             ready     status             restarts   age
agent-technologies-deployment-7c7c6676ff-8p49r   1/1       running            0          2m
agent-technologies-deployment-7c7c6676ff-dht5h   1/1       running            0          2m
agent-technologies-deployment-7c7c6676ff-gn8lp   1/1       running            0          2m
agent-technologies-deployment-7c7c6676ff-n9qql   1/1       running            0          2m
postgres-8676b745bf-8f7jv                        0/1       crashloopbackoff   4          3m


and here is what i get when i try to inspect what is going on with postgresql deployment by using kubectl logs $pod_name:

initdb: directory ""/var/lib/postgresql/data"" exists but is not empty
if you want to create a new database system, either remove or empty
the directory ""/var/lib/postgresql/data"" or run initdb
with an argument other than ""/var/lib/postgresql/data"".
the files belonging to this database system will be owned by user ""postgres"".
this user must also own the server process.

the database cluster will be initialized with locale ""en_us.utf8"".
the default database encoding has accordingly been set to ""utf8"".
the default text search configuration will be set to ""english"".

data page checksums are disabled.


note: i am using google cloud as a provider.
",<kubernetes><google-cloud-platform><google-kubernetes-engine>,50917972,15,"you can't have your db in /var/lib/postgres/data/whatever.
change that path by /var/lib/postgres/whatever and it will work.

17.2.1. use of secondary file systems
many installations create their database clusters on file systems (volumes) other than the machine's &quot;root&quot; volume. if you choose to do this, it is not advisable to try to use the secondary volume's topmost directory (mount point) as the data directory. best practice is to create a directory within the mount-point directory that is owned by the postgresql user, and then create the data directory within that. this avoids permissions problems, particularly for operations such as pg_upgrade, and it also ensures clean failures if the secondary volume is taken offline.

and, by the way, i had to create a secret, as it is not in the post:
apiversion: v1
kind: secret
metadata:
  name: postgres-credentials
type: opaque
data:
  user: cg9zdgdyzxm=            #postgres
  password: cgfzc3dvcmq=        #password

note that the username needs to be &quot;postgres&quot;. i don't know if you are covering this...
"
71008589,kustomize overlays when using a shared configmap,"i have an environment made of pods that address their target environment based on an environment variable called conf_env that could be test, stage or prod.
the application running inside the pod has the same source code across environments, the configuration file is picked according to the conf_env environment variable.
i'v encapsulated this conf_env in *.properties files just because i may have to add more environment variables later, but i make sure that each property file contains the expected conf_env e.g.:

test.properites has conf_env=test,
prod.properties has conf_env=prod, and so on...

i struggle to make this work with kustomize overlays, because i want to define a configmap as a shared resource across all the pods within the same overlay e.g. test (each pod in their own directory, along other stuff when needed).
so the idea is:

base/ (shared) with the definition of the namespace, the configmap (and potentially other shared resources
base/pod1/ with the definition of pod1 picking from the shared configmap (this defaults to test, but in principle it could be different)

then the overlays:

overlay/test that patches the base with conf_env=test (e.g. for overlay/test/pod1/ and so on)
overlay/prod/ that patches the base with conf_env=prod (e.g. for overlay/prod/pod1/ and so on)

each directory with their own kustomize.yaml.
the above doesn't work because when going into e.g. overlay/test/pod1/ and i invoke the command kubectl kustomize . to check the output yaml, then i get all sorts of errors depending on how i defined the lists for the yaml keys bases: or resources:.
i am trying to share the configmap across the entire conf_env environment in an attempt to minimize the boilerplate yaml by leveraging the patching-pattern with kustomize.
the kubernetes / kustomize yaml directory structure works like this:
 base
    configuration.yaml # i am trying to share this!
    kustomization.yaml
    my_namespace.yaml # i am trying to share this!
    my-scheduleset-etl-misc
       kustomization.yaml
       my_scheduleset_etl_misc.yaml
    my-scheduleset-etl-reporting
       kustomization.yaml
       my_scheduleset_etl_reporting.yaml
    test.properties # i am trying to share this!
 overlay
     test
         kustomization.yaml # here i want tell &quot;go and pick up the shared resources in the base dir&quot;
         my-scheduleset-etl-misc
            kustomization.yaml
            test.properties # i've tried to share this one level above, but also to add this inside the &quot;leaf&quot; level for a given pod
         my-scheduleset-etl-reporting
             kustomization.yaml

the command kubectl with kustomize:

sometimes complains that the shared namespace does not exist:

error: merging from generator &amp;{0xc001d99530 {  map[] map[]} {{ my-schedule-set-props merge {[conf_env=test] [] [] } &lt;nil&gt;}}}: 
id resid.resid{gvk:resid.gvk{group:&quot;&quot;, version:&quot;v1&quot;, kind:&quot;configmap&quot;, isclusterscoped:false}, name:&quot;my-schedule-set-props&quot;, namespace:&quot;&quot;} 
does not exist; cannot merge or replace


sometimes doesn't allow to have shared resources inside an overlay:

error: loading kv pairs: env source files: [../test.properties]: 
security; file '/my/path/to/yaml/overlay/test/test.properties' 
is not in or below '/my/path/to/yaml/overlay/test/my-scheduleset-etl-misc'


sometimes doesn't allow cycles when i am trying to have multiple bases - the shared resources and the original pod definition:

error: accumulating resources: accumulation err='accumulating resources from '../': 
'/my/path/to/yaml/overlay/test' must resolve to a file': 
cycle detected: candidate root '/my/path/to/yaml/overlay/test' 
contains visited root '/my/path/to/yaml/overlay/test/my-scheduleset-etl-misc'

the overlay kustomization.yaml files inside the pod dirs have:
bases:
  - ../ # tried with/without this to share the configmap
  - ../../../base/my-scheduleset-etl-misc/

the kustomization.yaml at the root of the overlay has:
bases:
  - ../../base

the kustomization.yaml at the base dir contains this configuration for the configmap:
# https://gist.github.com/hermanbanken/3d0f232ffd86236c9f1f198c9452aad9
configmapgenerator:
  - name: my-schedule-set-props
    namespace: my-ss-schedules
    envs:
      - test.properties

vars:
  - name: conf_env
    objref:
      kind: configmap
      name: my-schedule-set-props
      apiversion: v1
    fieldref:
      fieldpath: data.conf_env

configurations:
  - configuration.yaml

with configuration.yaml containing:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization

varreference:
- path: spec/confenv/value
  kind: pod

how do i do this?
how do i make sure that i minimise the amount of yaml by sharing all the configmap stuff and the pods definitions as much as i can?
",<kubernetes><environment-variables><kubectl><configmap><kustomize>,71008925,14,"if i understand your goal correctly, i think you may be grossly over-complicating things. i think you want a common properties file defined in your base, but you want to override specific properties in your overlays. here's one way of doing that.
in base, i have:
$ cd base
$ tree
.
 example.properties
 kustomization.yaml
 pod1
     kustomization.yaml
     pod.yaml

where example.properties contains:
some_other_var=somevalue
conf_env=test

and kustomization.yaml contains:
resources:
  - pod1

configmapgenerator:
  - name: example-props
    envs:
      - example.properties

i have two overlays defined, test and prod:
$ cd ../overlays
$ tree
.
 prod
  example.properties
  kustomization.yaml
 test
     kustomization.yaml

test/kustomization.yaml looks like this:
resources:
- ../../base

it's just importing the base without any changes, since the value of conf_env from the base directory is test.
prod/kustomization.yaml looks like this:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
resources:
- ../../base

configmapgenerator:
  - name: example-props
    behavior: merge
    envs:
      - example.properties

and prod/example.properties looks like:
conf_env=prod

if i run kustomize build overlays/test, i get as output:
apiversion: v1
data:
  conf_env: test
  some_other_var: somevalue
kind: configmap
metadata:
  name: example-props-7245222b9b
---
apiversion: v1
kind: pod
metadata:
  name: example
spec:
  containers:
  - command:
    - sleep
    - 1800
    envfrom:
    - configmapref:
        name: example-props-7245222b9b
    image: docker.io/alpine
    name: alpine

if i run kustomize build overlays/prod, i get:
apiversion: v1
data:
  conf_env: prod
  some_other_var: somevalue
kind: configmap
metadata:
  name: example-props-h4b5tc869g
---
apiversion: v1
kind: pod
metadata:
  name: example
spec:
  containers:
  - command:
    - sleep
    - 1800
    envfrom:
    - configmapref:
        name: example-props-h4b5tc869g
    image: docker.io/alpine
    name: alpine

that is, everything looks as you would expect given the configuration in base, but we have provided a new value for conf_env.
you can find all these files here.
"
59423084,kubernetes - use secrets on pre-install job,"on my helm chart i have a job with the pre-install hook where i need to use a property from my secrets. however when i try to install my helm chart i get the following error on my pre-install job:


  error: secret ""secretsfilename"" not found


secrets aren't created before the pods execution? what's the problem here? how can i solve this?

notes: 


i want to use secrets to have the properties encrypted. i don't want to use the decrypted value directly on my pod;
i already read helm install in certain order but i still not understanding the reason of this error;
i already tried to use ""helm.sh/hook"": pre-install,post-delete and ""helm.sh/hook-weight"": ""1"" on secrets, and ""helm.sh/hook-weight"": ""2"" on my pod but the problem remains.


my pre-install job:

apiversion: batch/v1
kind: job
metadata:
  name: ""mypodname""
  annotations:
    ""helm.sh/hook"": pre-install
    ""helm.sh/hook-delete-policy"": before-hook-creation,hook-succeeded
  #some more code
spec:
  template:
    #some more code
    spec:
      dnspolicy: {{ .values.specpolicy.dnspolicy }}
      restartpolicy: {{ .values.specpolicy.restartpolicy }}
      volumes:
        - name: {{ .values.volume.name }}
          persistentvolumeclaim:
            claimname: {{ .values.volume.claimname }}
      securitycontext:
        {{- toyaml .values.securitycontext | nindent 8 }}
      containers:
        - name: ""mycontainername""
          #some more code
          env:
            - name: secret_to_use
              valuefrom:
                secretkeyref:
                  name: secretsfilename
                  key: prop_from_screts
          #some more code


my secrets file:

apiversion: v1
kind: secret
metadata:
  name: ""secretsfilename""
  labels:
    app: ""myappname""
    #some more code
type: opaque
data:
    prop_from_screts: ehb0bw==

",<kubernetes><kubernetes-helm><kubernetes-secrets>,59425059,14,"while helm hooks are typically jobs, there's no requirement that they are, and helm doesn't do any analysis on the contents of a hook object to see what else it might depend on.  if you read through the installation sequence described there, it is (7) install things tagged as hooks, (8) wait for those to be ready, then (9) install everything else; it waits for the job to finish before it installs the secret it depends on.
the first answer, then, is that you also need to tag your secret as a hook for it to be installed during the pre-install phase, with a modified weight so that it gets installed before the main job (smaller weight numbers happen sooner):
apiversion: v1
kind: secret
annotations:
  &quot;helm.sh/hook&quot;: pre-install
  &quot;helm.sh/hook-weight&quot;: &quot;-5&quot;

the next question is when this secret gets deleted.  the documentation notes that helm uninstall won't delete hook resources; you need to add a separate helm.sh/hook-delete-policy annotation, or else it will stick around until the next time the hook is scheduled to be run.  this reads to me as saying that if you modify the secret (or the values that make it up) and upgrade (not delete and reinstall) the chart, the secret won't get updated.
i'd probably just create two copies of the secret, one that's useful at pre-install time and one that's useful for the primary chart lifecycle.  you could create a template to render the secret body and then call that twice:
{{- define &quot;secret.content&quot; -}}
type: opaque
data:
    prop_from_screts: ehb0bw==
{{- end -}}
---
apiversion: v1
kind: secret
metadata:
  name: &quot;secretsfilename&quot;
  labels:
    app: &quot;myappname&quot;
{{ include &quot;secret.content&quot; . }}
---
apiversion: v1
kind: secret
metadata:
  name: &quot;secretsfilename-preinst&quot;
  labels:
    app: &quot;myappname&quot;
  annotations:
    &quot;helm.sh/hook&quot;: pre-install
    &quot;helm.sh/hook-weight&quot;: &quot;-5&quot;
    &quot;helm.sh/hook-delete-policy&quot;: hook-succeeded
{{ include &quot;secret.content&quot; . }}

"
63162065,getting err_too_many_redirects on ingress with https web services,"i have a rancher cluster (v2.4.5) running on custom nodes with the following configuration:

external machine (example.com):

runs rancher server on port 8443;
runs nginx with (among other unrelated stuff) the following basic configuration:



user  nginx;
  
worker_processes 4;
worker_rlimit_nofile 40000;
  
error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;
  
events {
  worker_connections 8192;
}
 
http {
  upstream rancher_servers {
    least_conn;
    server &lt;my_node_ip&gt;:443 max_fails=3 fail_timeout=5s;
  }

  server {
    listen 443 ssl http2;
    server_name example.com service1.example.com service2.example.com;

    ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem;

    location / {
      proxy_set_header host $host;
      proxy_set_header x-forwarded-proto $scheme;
      proxy_set_header x-forwarded-port $server_port;
      proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;
      proxy_pass https://rancher_servers;
      proxy_http_version 1.1;
      proxy_set_header upgrade $http_upgrade;
      proxy_set_header connection $connection_upgrade;
      proxy_read_timeout 900s;
    }
  }
}


internal machine (my_node_ip):

runs rancher agent (etcd/control plane/worker)



firewall rules are ok, i can deploy minor web-apps with stuff running on port 80 only and get redirected automatically to https. an example of yaml i'm using to deploy stuff is the following:
---
apiversion: apps/v1
kind: deployment
metadata:
  name: www-deployment
  labels:
    app: www
spec:
  replicas: 1
  selector:
    matchlabels:
      app: www
  template:
    metadata:
      labels:
        app: www
    spec:
      containers:
        - name: www
          image: my-www-image

---
kind: service
apiversion: v1
metadata:
  name: www-service
spec:
  selector:
    app: www
  ports:
    - port: 80

---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: www-ingress
  annotations:
    ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: service1.example.com
    http:
      paths:
        - path: /
          backend:
            servicename: www-service
            serviceport: 80

the problem is when i try to deploy a service that runs on both ports 80 and 443 but, when requested on port 80, automatically redirects to port 443. when that's the case, if i specify the ingress like below (with port 443), i get a bad gateway response not from the host machine nginx. i can tell that because my host machine runs nginx/1.18.0 and the response comes from nginx/1.17.10.
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: www-ingress
  annotations:
    ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: service1.example.com
    http:
      paths:
        - path: /
          backend:
            servicename: www-service
            serviceport: 443

but then, if i change the configuration above to serviceport: 80 i keep getting err_too_many_redirects, because it enters an infinite loop of redirecting from anything to https://anything.
am i doing anything wrong here? how can i do a workaround to make these things work?
",<nginx><kubernetes><kubernetes-ingress><nginx-ingress><rancher>,63173789,14,"found it out. turns out that the only thing i needed to do was to tell the nginx-ingress-controller that i was expecting https connections. final yaml for exposing the service is the following:
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: www-ingress
  annotations:
    ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/backend-protocol: &quot;https&quot;
spec:
  rules:
    - host: service1.example.com
      http:
        paths:
          - path: /
            backend:
              servicename: www-service
              serviceport: 443

"
60903362,kubectl - how to read ingress hosts from config variables?,"i have a configmap with a variable for my domain:

apiversion: v1
kind: configmap
metadata:
  name: config
data:
  my_domain: mydomain.com


and my goal is to use the my_domain variable inside my ingress config

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: myingress
spec:
  tls:
    - hosts:
       - config.my_domain
      secretname: mytls
  rules:
   - host: config.my_domain
      http:
        paths:
          - backend:
              servicename: myservice
              serviceport: 3000



but obviously the config above is not valid. so how can this be achieved?
",<kubernetes><kubectl><kubernetes-ingress><nginx-ingress><configmap>,60905090,13,"the configmapref and secretmapref for the envfrom and valuefrom functions are only available for environment variables which means they cannot be used in this context. the desired functionality is not available in vanilla kubernetes as of 1.18.0. 

however, it can be done. helm and kustomize are probably the two best ways to accomplish this but it could also be done with sed or awk. helm is a templating engine for kubernetes manifests. meaning, you create generic manifests, template out the deltas between your desired manifests with the generic manifests by variables, and then provide a variables file. then, at runtime, the variables from your variables file are automatically injected into the template for you. 

another way to accomplish this is why kustomize. which is what i would personally recommend. kustomize is like helm in that it deals with producing customized manifests from generic ones, but it doesn't do so through templating. kustomize is unique in that it performs merge patches between yaml or json files at runtime. these patches are referred to as overlays so it is often referred to as an overlay engine to differentiate itself from traditional templating engines. reason being kustomize can be used with recursive directory trees of bases and overlays. which makes it much more scalable for environments where dozens, hundreds, or thousands of manifests might need to be generated from boilerplate generic examples. 

so how do we do this? well, with kustomize you would first define a kustomization.yml file. within you would define your resources. in this case, myingress:

apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
resources:
    - myingress.yml


so create a example directory and make a subdirectory called base inside it. create ./example/base/kustomization.yml and populate it with the kustomization above. now create a ./example/base/myingress.yml file and populate it with the example myingress file you gave above.

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: myingress
spec:
  tls:
    - hosts:
      - config.my_domain
      secretname: mytls
  rules:
     - host: config.my_domain
        http:
          paths:
            - backend:
                servicename: myservice
                serviceport: 3000


now we need to define our first overlay. we'll create two different domain configurations to provide an example of how overlays work. first create a ./example/overlays/domain-a directory and create a kustomization.yml file within it with the following contents:

apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
bases:
    - ../../../base/

patchesstrategicmerge:
    - ing_patch.yml

configmapgenerator:
    - name: config_a
      literals:
        - my_domain='domain_a'


at this point we have defined ing_patch.yml and config_a in this file. ing_patch.yml will serve as our ingress patch and config_a will serve as our configmap. however, in this case we'll be taking advantage of a kustomize feature known as a configmapgenerator rather than manually creating configmap files for single literal key:value pairs. 

now that we have done this, we have to actually make our first patch! since the deltas in your ingress are pretty small, it's not that hard. create ./example/overlays/domain_a/ing_patch.yml and populate it with:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: myingress
spec:
  tls:
    - hosts:
      - domain.a.com
  rules:
     - host: domain.a.com


perfect, you have created your first overlay. now you can use kubectl or kustomize to generate your resultant manifest to apply to the kubernetes api server. 


kubectl build:   kubectl kustomize ./example/overlays/domain_a 
kustomize build: kustomize build ./example/overlays/domain_a 


run one of the above build commands and review the stdout produced in your terminal. notice how it contains two files, myingress and config? and myingress contains the domain configuration present in your overlay's patch? 

so, at this point you're probably asking. why does kustomize exist if kubectl supports the features by default? well kustomize started as an external project initially and the kustomize binary is often running a newer release than the version available in kubectl. 

the next step is to create a second overlay. so go ahead and cp your first overlay over: cp -r ./example/overlays/domain_a ./example/overlays/domain_b.

now that you have done that, open up ./example/overlays/domain_b/ing_patch.yml up in a text editor and change the contents to look like so:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: myingress
spec:
  tls:
    - hosts:
      - domain.b.com
  rules:
     - host: domain.b.com


save the file and then build your two separate overlays:

kustomize build ./example/overlays/domain_a
kustomize build ./example/overlays/domain_b


notice how each generated stream of stdout varies based on the patch present in the overlay directory? you can continue to abstract this pattern by making your bases the overlays for other bases. or by making your overlays the bases for other overlays. doing so can allow you to scale this project in extremely powerful and efficient ways. apply them to your api server if you wish:

kubectl apply -k ./example/overlays/domain_a
kubectl apply -k ./example/overlays/domain_b


this is only the beginning of kustomize really. as you might have guessed after seeing the configmapgenerator field in the kustomization.yml file for each overlay, kustomize has a lot of features baked in. it can add labels to all of your resources, it can override their namespaces or container image information, etc. 

i hope this helps. let me know if you have any other questions. 
"
57979939,how to redirect http to https using a kubernetes ingress controller on amazon eks,"i have configured amazon certificate manager, alb ingress controller and a domain names for my application. i can access my application through port 80 and port 443 (all certificates works just fine). however i would like to redirect all coming traffic from http to https automatically so that people who typed the domain name by itself is redirected to https. i have followed this page and this onebut i cannot make it work

this is my ingress.yaml file:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: metabase
  namespace: bigdata
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-2:***:certificate/***
    alb.ingress.kubernetes.io/listen-ports: '[{""http"": 80}, {""https"":443}]'
    alb.ingress.kubernetes.io/actions.ssl-redirect: '{""type"": ""redirect"", ""redirectconfig"": { ""protocol"": ""https"", ""port"": ""443"", ""statuscode"": ""http_301""}}'
    alb.ingress.kubernetes.io/scheme: internet-facing

  labels:
    app: metabase
spec:
  rules:
    - http:
        paths:
          - path: /*
            backend:
              servicename: ssl-redirect
              serviceport: use-annotation
          - path: /*
            backend:
              servicename: metabase
              serviceport: 3000


this is my service:

apiversion: v1
kind: service
metadata:
  name: metabase
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-2:****:certificate/****
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: ""443""
  namespace: bigdata
  labels:
    app: metabase
spec:
  ports:
    - name: https
      protocol: tcp
      port: 443
      targetport: http-server
    - name: http
      protocol: tcp
      port: 80
      targetport: http-server
  selector:
    app: metabase
  type: loadbalancer


ad this is my deployment:

apiversion: apps/v1
kind: deployment
metadata:
  name: metabase-deployment
  namespace: bigdata
  labels:
    app: metabase
spec:
  replicas: 2
  selector:
    matchlabels:
      app: metabase
  template:
    metadata:
      labels:
        app: metabase
    spec:
      containers:
        - name: metabase
          image: metabase/metabase
          ports:
            - containerport: 3000
              name: http-server
          resources:
            limits:
              cpu: ""1""
              memory: ""2gi""


thanks for your support! :-)
",<kubernetes><kubernetes-ingress><amazon-eks>,58034777,13,"i was able to make it work!! 
basically i modified the ingress.yaml and service.yaml files

ingress.yaml looks like this:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: metabase
  namespace: bigdata
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-2:***:certificate/****
    alb.ingress.kubernetes.io/listen-ports: '[{""http"": 80}, {""https"":443}]'
    alb.ingress.kubernetes.io/actions.ssl-redirect: '{""type"": ""redirect"", ""redirectconfig"": { ""protocol"": ""https"", ""port"": ""443"", ""statuscode"": ""http_301""}}'
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/group: metabase # name of my app

  labels:
    app: metabase

spec:
  rules:
    - http:
        paths:
          - path: /*
            backend:
              servicename: ssl-redirect
              serviceport: use-annotation
          - path: /*
            backend:
              servicename: metabase
              serviceport: 443


and my service looks like this:

apiversion: v1
kind: service
metadata:
  name: metabase
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-2:***:certificate/***
  namespace: bigdata
  labels:
    app: metabase
spec:
  ports:
    - name: https
      protocol: tcp
      port: 443
      targetport: http-server
    - name: http
      protocol: tcp
      port: 80
      targetport: http-server
  selector:
    app: metabase
  type: loadbalancer

"
59976777,helm range get values outside of loop,"i was looking at the helm range example they have on their docs.

yaml

favorite:
  drink: coffee
  food: pizza
pizzatoppings:
  - mushrooms
  - cheese
  - peppers
  - onions


helm

apiversion: v1
kind: configmap
metadata:
  name: {{ .release.name }}-configmap
data:
  myvalue: ""hello world""
  {{- with .values.favorite }}
  drink: {{ .drink | default ""tea"" | quote }}
  food: {{ .food | upper | quote }}
  {{- end }}
  toppings: |-
    {{- range .values.pizzatoppings }}
    - {{ . | title | quote }}
    - {{ .values.favorite.drink }}
    {{- end }}


i updated it to have this line - {{ .values.favorite.drink }} but when i run helm template i get the error 

can't evaluate field values 


is there anyway to access the top level .values from within the range function and escape the loop?
",<kubernetes><kubernetes-helm>,59978477,13,"you can also use a global variable $ that points to the root context

apiversion: v1
kind: configmap
metadata:
  name: {{ .release.name }}-configmap
data:
  myvalue: ""hello world""
  {{- with .values.favorite }}
  drink: {{ .drink | default ""tea"" | quote }}
  food: {{ .food | upper | quote }}
  {{- end }}
  toppings: |-
    {{- range $.values.pizzatoppings }}
    - {{ . | title | quote }}
    - {{ $.values.favorite.drink }}
    {{- end }}

"
63289848,kubernetes nginx ingress error with configuration-snippet,"i have following ingress.yaml file
apiversion: extensions/v1beta1
kind: ingress
metadata:
    name: nginx-configuration-snippet
    annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /$2
        nginx.ingress.kubernetes.io/configuration-snippet: |
          location /base/path/v1/api/update {
              deny all;
              return 404;
            }
spec:
  rules:
    - http:
        paths:
          - path: /base/path(/|$)(.*)
            backend:
              servicename: myapi
              serviceport: 8080

but when i send a request to https:///base/path/v1/api/update it succeeds and i got following error in nginx ingress controller
error: exit status 1
2020/08/06 18:35:07 [emerg] 1734#1734: location &quot;/base/path/v1/api/update&quot; is outside location &quot;^/base/path(/|$)(.*)&quot; in /tmp/nginx-cfg008325631:2445
nginx: [emerg] location &quot;/base/path/v1/api/update&quot; is outside location &quot;^/base/path(/|$)(.*)&quot; in /tmp/nginx-cfg008325631:2445
nginx: configuration file /tmp/nginx-cfg008325631 test failed

can somebody help?
",<nginx><kubernetes><kubernetes-helm><kubernetes-ingress><nginx-ingress>,63292842,13,"the configuration-snippet is to add configs to locations.
if you want to add a custom location to the server context, you should use the server-snippet instead:

using the annotation nginx.ingress.kubernetes.io/server-snippet it is
possible to add custom configuration in the server configuration
block.

you also need to use some modifiers and regex to make it work (~* and ^).
the following config should work:
apiversion: extensions/v1beta1
kind: ingress
metadata:
    name: nginx-configuration-snippet
    annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /$2
        nginx.ingress.kubernetes.io/server-snippet: |
          location ~* &quot;^/base/path/v1/api/update&quot; {
              deny all;
              return 403;
            }
spec:
  rules:
    - http:
        paths:
          - path: /base/path(/|$)(.*)
            backend:
              servicename: myapi
              serviceport: 8080

the final nginx.config should end like this:
$ kubectl exec -n kube-system nginx-ingress-controller-6fc5bcc8c9-chkxf -- cat /etc/nginx/nginx.conf

[...]

location ~* &quot;^/base/path/v1/api/update&quot; {
            deny all;
            return 403;
        }
        
location ~* &quot;^/base/path(/|$)(.*)&quot; {
[...]           
}

"
47378592,how to bound a persistent volume claim with a gcepersistentdisk?,"i would like to bound persistentvolumeclaim with a gcepersistentdisk persistentvolume. below the steps i did for getting that:

1. creation of the gcepersistentdisk:

gcloud compute disks create --size=2gb --zone=us-east1-b gce-nfs-disk

2. definition the persistentvolume and the persistentvolumeclaim

# pv-pvc.yml
apiversion: v1
kind: persistentvolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 2gi
  accessmodes:
    - readwriteonce
  gcepersistentdisk:
    pdname: gce-nfs-disk
    fstype: ext4
---
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: nfs-pvc
  labels:
    app: test
spec:
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 2gi


after running kubectl apply -f pv-pvc.yml, the nfs-pvc is not bound with nfs-pv. in fact, below is the list of the persistentvolume and persistentvolumeclaim i have:

$ kubectl get pv
name                                       capacity   access modes   reclaim policy   status      claim             storageclass   reason    age
nfs-pv                                     2gi        rwo            retain           available                                              30s
pvc-16e4cdf2-cd3d-11e7-83ae-42010a8e0243   2gi        rwo            delete           bound       default/nfs-pvc   standard                 26s
$ kubectl get pvc
name      status    volume                                     capacity   access modes   storageclass   age
nfs-pvc   bound     pvc-16e4cdf2-cd3d-11e7-83ae-42010a8e0243   2gi        rwo            standard       59s


the obtained persistentvolume is a volume on the disk of the node i created on google container engine. 
so, have i missed something?

ps: the version of kubernetes

$ kubectl version
client version: version.info{major:""1"", minor:""8"", gitversion:""v1.8.3"", gitcommit:""f0efb3cb883751c5ffdbe6d515f3cb4fbe7b7acd"", gittreestate:""clean"", builddate:""2017-11-08t18:39:33z"", goversion:""go1.8.3"", compiler:""gc"", platform:""linux/amd64""}
server version: version.info{major:""1"", minor:""7+"", gitversion:""v1.7.8-gke.0"", gitcommit:""a7061d4b09b53ab4099e3b5ca3e80fb172e1b018"", gittreestate:""clean"", builddate:""2017-10-10t18:48:45z"", goversion:""go1.8.3"", compiler:""gc"", platform:""linux/amd64""}

",<kubernetes><google-kubernetes-engine><persistent-volumes><persistent-volume-claims><gce-persistent-disk>,47385182,13,"i found the solution.

below the new definitions of the pv and pvc:

apiversion: v1
kind: persistentvolume
metadata:
  name: nfs-pv
  labels:
    app: test  # the label has been added to make sure the bounding is working as expected
spec:
  capacity:
    storage: 2gi
  accessmodes:
    - readwriteonce
  gcepersistentdisk:
    pdname: gce-nfs-disk
    fstype: ext4
---
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: nfs-pvc
  labels:
    app: test
spec:
  accessmodes:
    - readwriteonce
  storageclassname: """" # the storageclassname has to be specified
  resources:
    requests:
      storage: 2gi
  selector:
    matchlabels:
      app: test


after these modifications, this is the bounding worked:

$ kubectl get pvc
name      status    volume    capacity   access modes   storageclass   age
nfs-pvc   bound     nfs-pv    2gi        rwo                           8s
$ kubectl get pv
name      capacity   access modes   reclaim policy   status    claim             storageclass   reason    age
nfs-pv    2gi        rwo            retain           bound     default/nfs-pvc                            22m


i hope it will help.
"
66317628,how to use dynamic/variable image tag in a kubernetes deployment?,"in our project, which also uses kustomize, our base deployment.yaml file looks like this:
apiversion: apps/v1
kind: deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:image_tag # &lt;------------------------------
        ports:
        - containerport: 80

then we use sed to replace image_tag with the version of the image we want to deploy.
is there a more sophisticated way to do this, rather than editing the text yaml file using sed?
",<kubernetes><google-kubernetes-engine><kustomize>,66364133,13,"there is a specific transformer for this called the images transformer.
you can keep your deployment as it is, with or without tag:
apiversion: apps/v1
kind: deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerport: 80

and then in your kustomization file:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization

resources:
  - deployment.yaml

images:
  - name: nginx
    newtag: mynewtag

do keep in mind that this will replace the tag of all the nginx images of all the resources included in your kustomization file. if you need to run multiple versions of nginx you can replace the image name in your deployment by a placeholder and have different entries in the transformer.
"
55498027,kubernetes nginx ingress removing part of url,"i'm deploying a simple app in kubernetes (on aks) which is sat behind an ingress using nginx, deployed using the nginx helm chart. i have a problem that for some reason nginx doesn't seem to be passing on the full url to the backend service. 

for example, my ingress is setup with the url of http://app.client.com and a path of /app1g going http://app.client.com/app1 works fine. however if i try to go to http://app.client.com/app1/service1 i just end up at http://app.client.com/app1, it seems to be stripping everything after the path.

my ingress looks  like this:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
  creationtimestamp: ""2019-04-03t12:44:22z""
  generation: 1
  labels:
    chart: app-1.1
    component: app
    hostname: app.client.com
    release: app
  name: app-ingress
  namespace: default
  resourceversion: ""1789269""
  selflink: /apis/extensions/v1beta1/namespaces/default/ingresses/app-ingress
  uid: 34bb1a1d-560e-11e9-bd46-9a03420914b9
spec:
  rules:
  - host: app.client.com
    http:
      paths:
      - backend:
          servicename: app-service
          serviceport: 8080
        path: /app1
  tls:
  - hosts:
    - app.client.com
    secretname: app-prod
status:
  loadbalancer:
    ingress:
    - {}


if i port forward to the service and hit that directly it works.
",<nginx><kubernetes><kubernetes-ingress>,55498682,12,"so i found the answer to this. it seems that as of nginx v0.22.0 you are required to use capture groups to capture any substrings in the request uri. prior to 0.22.0 using just  nginx.ingress.kubernetes.io/rewrite-target: / worked for any substring. now it does not. i needed to ammend my ingress to use this:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /$1
  creationtimestamp: ""2019-04-03t12:44:22z""
  generation: 1
  labels:
    chart: app-1.1
    component: app
    hostname: app.client.com
    release: app
  name: app-ingress
  namespace: default
  resourceversion: ""1789269""
  selflink: /apis/extensions/v1beta1/namespaces/default/ingresses/app-ingress
  uid: 34bb1a1d-560e-11e9-bd46-9a03420914b9
spec:
  rules:
  - host: app.client.com
    http:
      paths:
      - backend:
          servicename: app-service
          serviceport: 8080
        path: /app1/?(.*)
  tls:
  - hosts:
    - app.client.com
    secretname: app-prod
status:
  loadbalancer:
    ingress:
    - {}

"
48270971,how do i statically provision a volume for a statefulset?,"i am using google kubernetes engine and would like my statefulset to use my previously created disk my-app-disk instead of dynamically creating new persistent disk for each replica in the set. my config is as follows:

persistentvolume yaml:

apiversion: v1
kind: persistentvolume
metadata:
  name: my-app-data
  labels:
    app: my-app
spec:
  capacity:
    storage: 60gi
  accessmodes:
    - readwriteonce
  gcepersistentdisk:
    pdname: my-app-disk
    fstype: ext4


persistentvolumeclaim yaml:

apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: my-app-data
  labels:
    app: my-app
spec:
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 60gi
  selector:
    matchlabels:
      app: my-app


statefulset yaml:

apiversion: apps/v1beta2
kind: statefulset
metadata:
  name: my-stateful-set
spec:
  servicename: my-stateful-set
  replicas: 1
  selector:
    matchlabels:
      app: my-app
  updatestrategy:
    type: rollingupdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app
        image: gcr.io/projectname/my-app:0.0.3
        ports:
        - containerport: 80
          name: http
        volumemounts:
        - name: my-app-data
          mountpath: /data
  volumeclaimtemplates:
  - metadata:
      name: my-app-data
    spec:
      accessmodes: [ ""readwriteonce"" ]
      resources:
        requests:
          storage: 50gi

",<kubernetes><google-cloud-platform><google-kubernetes-engine>,48274965,12,"statefulset will create it's own persistentvolumeclaim for each pod so you don't have to create one yourself. a persistentvolume and a persistentvolumeclaim will bind exclusively one to one. your pvc is binding to your volume so any pvcs created by the statefulset can't bind to your volume so it won't be used.

in your case your persistentvolume and the statefulset below should do the trick. make sure to delete the persistentvolumeclaim you created so that it's not bound to your persistentvolume. also, make sure the storage class name is set properly below on your pv and in volumeclaimtemplates on your statefulset below or the pvc made by the statefulset may not bind to your volume.

persistentvolume.yaml:

apiversion: v1
kind: persistentvolume
metadata:
  name: my-app-data
  labels:
    app: my-app
spec:
  capacity:
    storage: 60gi
  storageclassname: standard
  accessmodes:
    - readwriteonce
  gcepersistentdisk:
    pdname: my-app-disk
    fstype: ext4


statefulset.yaml:

apiversion: apps/v1beta2
kind: statefulset
metadata:
  name: my-stateful-set
spec:
  servicename: my-stateful-set
  replicas: 1
  selector:
    matchlabels:
      app: my-app
  updatestrategy:
    type: rollingupdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app
        image: gcr.io/projectname/my-app:0.0.3
        ports:
        - containerport: 80
          name: http
        volumemounts:
        - name: my-app-data
          mountpath: /data
  volumeclaimtemplates:
  - metadata:
      name: my-app-data
    spec:
      selector:
        matchlabels:
          app: my-app
      storageclassname: standard
      accessmodes: [ ""readwriteonce"" ]
      resources:
        requests:
          storage: 50gi

"
49027234,k8s gce1.8.7 - pods is forbidden - unknown user system:serviceaccount:default:default,"i have a mongo database in the gce . (config see below)

when i deploy it to a 1.7.12-gke.1  everything works fine. which means the sidecar resolves the pods and links then

now when i deploy the same konfiguration to 1.8.7-gke.1 resultes in missing permissions to list pods see below.

i don't get the point what has changed . i assume i need to assign specific permissions to the user account is that right ?

what am i missing?

error log

message: 'pods is forbidden: user ""system:serviceaccount:default:default"" cannot list pods at the cluster scope: unknown user ""system:serviceaccount:default:default""',

mongo-sidecar | feb 28, 2018, 11:04:19 am | status: 'failure',
mongo-sidecar | feb 28, 2018, 11:04:19 am | metadata: {},
mongo-sidecar | feb 28, 2018, 11:04:19 am | apiversion: 'v1',
mongo-sidecar | feb 28, 2018, 11:04:19 am | { kind: 'status',
mongo-sidecar | feb 28, 2018, 11:04:19 am | message:
mongo-sidecar | feb 28, 2018, 11:04:19 am | error in workloop { [error: [object object]]
mongo-sidecar | feb 28, 2018, 11:04:14 am | statuscode: 403 }
mongo-sidecar | feb 28, 2018, 11:04:14 am | code: 403 },
mongo-sidecar | feb 28, 2018, 11:04:14 am | details: { kind: 'pods' },
mongo-sidecar | feb 28, 2018, 11:04:14 am | reason: 'forbidden',


config:

---
kind: storageclass
apiversion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
---
apiversion: v1
kind: service
metadata:
  name: mongo
  labels:
    name: mongo
spec:
  ports:
  - port: 27017
    targetport: 27017
  clusterip: none
  selector:
    role: mongo
---
apiversion: apps/v1beta1
kind: statefulset
metadata:
  name: mongo
spec:
  servicename: ""mongo""
  replicas: 3
  template:
    metadata:
      labels:
        role: mongo
        environment: test
    spec:
      terminationgraceperiodseconds: 10
      containers:
        - name: mongo
          image: mongo:3.4.9
          command:
            - mongod
            - ""--replset""
            - rs0
            - ""--smallfiles""
            - ""--noprealloc""
          ports:
            - containerport: 27017
          volumemounts:
            - name: mongo-persistent-storage
              mountpath: /data/db
        - name: mongo-sidecar
          image: cvallance/mongo-k8s-sidecar
          env:
            - name: mongo_sidecar_pod_labels
              value: ""role=mongo,environment=test""
  volumeclaimtemplates:
  - metadata:
      name: mongo-persistent-storage
      annotations:
        volume.beta.kubernetes.io/storage-class: ""fast""
    spec:
      accessmodes: [ ""readwriteonce"" ]
      resources:
        requests:

          storage: 5gi

",<kubernetes><google-kubernetes-engine><kubernetes-security><kubernetes-mongodb-sidecar>,49202261,12,"according to original solution: https://github.com/cvallance/mongo-k8s-sidecar/issues/75

you have to create role binding which will grant the default service account view permissions:

apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrolebinding
metadata:
  name: default-view
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: view
subjects:
  - kind: serviceaccount
    name: default
    namespace: default

"
51176283,how to create pod from rest api,"how can i create a pod using rest api ?  

i checked the kubernetes api documentation:
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#-strong-write-operations-strong--54  

they are writing that need to use post request:
post /api/v1/namespaces/{namespace}/pods  

i have this yaml of simple nginx pod:  

cat &gt; nginx-pod.yaml &lt;&lt;eof
apiversion: v1
kind: pod
metadata:
  name: nginx1
spec:
  containers:
  - name: nginx
    image: nginx:1.7.9
    ports:
    - containerport: 80
eof

",<kubernetes><kubernetes-pod><kubernetes-apiserver>,51176322,12,"need to translate the yaml file to json file:  

cat &gt; nginx-pod.json &lt;&lt;eof
{
    ""apiversion"": ""v1"",
    ""kind"": ""pod"",
    ""metadata"": {
        ""name"": ""nginx1""
    },
    ""spec"": {
        ""containers"": [
            {
                ""name"": ""nginx"",
                ""image"": ""nginx:1.7.9"",
                ""ports"": [
                    {
                        ""containerport"": 80
                    }
                ]
            }
        ]
    }
}
eof


use the curl command like that:  

curl -k -v -x post -h ""authorization: bearer &lt;jwt_token&gt;"" -h ""content-type: application/json"" https://127.0.0.1:6443/api/v1/namespaces/default/pods -d@nginx-pod.json  


of course, the token you are using should have permissions to create pod.  

if someone has a way to do it without converting to json, please share.    
"
71110829,unable to create a cronjob in k8s,"i am trying to create a cronjob , i have written a springboot application for this and have created a abc-dev.yml file for application configuration
error: unable to recognize &quot;src/java/k8s/abc-dev.yml&quot;: no matches for kind &quot;cronjob&quot; in version &quot;apps/v1&quot;
apiversion: apps/v1
kind: cronjob
metadata:
  name: abc-cron-job
spec:
  schedule: &quot;* * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          container:
          - name: abc-cron-job
            image: busybox
            imagepullpolicy: ifnotpresent
            command:
            - /bin/sh
            - -c
            - date; echo hello from the kubernetes cluster
          restartpolicy: onfailure            

",<kubernetes><kubernetes-cronjob>,71111525,11,"if you are running kubernetes 1.20 or lower, the correct apiversion value is:
apiversion: batch/v1beta1
if you are running kubernetes 1.21 or higher, its
apiversion: batch/v1
"
70286956,deploying a keycloak ha cluster to kubernetes | pods are not discovering each other,"i'm trying to deploy a ha keycloak cluster (2 nodes) on kubernetes (gke). so far the cluster nodes (pods) are failing to discover each other in all the cases as of what i deduced from the logs. where the pods initiate and the service is up but they fail to see other nodes.
components

postgresql db deployment with a clusterip service on the default port.
keycloak deployment of 2 nodes with the needed ports container ports 8080, 8443, a relevant clusterip, and a service of type loadbalancer to expose the service to the internet

logs snippet:
info  [org.infinispan.remoting.transport.jgroups.jgroupstransport] (msc service thread 1-4) ispn000078: starting jgroups channel ejb
info  [org.infinispan.remoting.transport.jgroups.jgroupstransport] (msc service thread 1-4) ispn000094: received new cluster view for channel ejb: [keycloak-567575d6f8-c5s42|0] (1) [keycloak-567575d6f8-c5s42]
info  [org.infinispan.remoting.transport.jgroups.jgroupstransport] (msc service thread 1-1) ispn000094: received new cluster view for channel ejb: [keycloak-567575d6f8-c5s42|0] (1) [keycloak-567575d6f8-c5s42]
info  [org.infinispan.remoting.transport.jgroups.jgroupstransport] (msc service thread 1-3) ispn000094: received new cluster view for channel ejb: [keycloak-567575d6f8-c5s42|0] (1) [keycloak-567575d6f8-c5s42]
info  [org.infinispan.remoting.transport.jgroups.jgroupstransport] (msc service thread 1-4) ispn000079: channel ejb local address is keycloak-567575d6f8-c5s42, physical addresses are [127.0.0.1:55200]
.
.
.
info  [org.jboss.as] (controller boot thread) wflysrv0025: keycloak 15.0.2 (wildfly core 15.0.1.final) started in 67547ms - started 692 of 978 services (686 services are lazy, passive or on-demand)
info  [org.jboss.as] (controller boot thread) wflysrv0060: http management interface listening on http://127.0.0.1:9990/management
info  [org.jboss.as] (controller boot thread) wflysrv0051: admin console listening on http://127.0.0.1:9990

and as we can see in the above logs the node sees itself as the only container/pod id
trying kube_ping protocol
i tried using the kubernetes.kube_ping protocol for discovery but it didn't work and the call to the kubernetes downward api. with a 403 authorization error in the logs (below is part of it):
server returned http response code: 403 for url: https://[server_ip]:443/api/v1/namespaces/default/pods

at this point, i was able to log in to the portal and do the changes but it was not yet an ha cluster since changes were not replicated and the session was not preserved, in other words, if i delete the pod that i was using i was redirected to the other with a new session (as if it was a separate node)
trying dns_ping protocol
when i tried dns_ping things were different i had no kubernetes downward api issues but i was not able to log in.
in detail, i was able to reach the login page normally, but when i enter my credentials and try logging in the page tries loading but gets me back to the login page with no logs in the pods in this regard.
below are some of the references i resorted to over the past couple of days:

https://github.com/keycloak/keycloak-containers/blob/main/server/readme.md#openshift-example-with-dnsdns_ping
https://github.com/keycloak/keycloak-containers/blob/main/server/readme.md#clustering
https://www.youtube.com/watch?v=g8lvir8kksa
https://www.keycloak.org/2019/05/keycloak-cluster-setup.html
https://www.keycloak.org/docs/latest/server_installation/#creating-a-keycloak-custom-resource-on-kubernetes

my yaml manifest files
postgresql deployment
apiversion: apps/v1
kind: deployment
metadata:
  name: postgres
spec:
  replicas: 1
  selector:
    matchlabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - name: postgres
          image: postgres:13
          imagepullpolicy: ifnotpresent
          ports:
          - containerport: 5432
          env:
            - name: postgres_password
              value: &quot;postgres&quot;
            - name: pgdata
              value: /var/lib/postgresql/data/pgdata
---
apiversion: v1
kind: service
metadata:
  name: postgres
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
    targetport: 5432

keycloak ha cluster deployment
apiversion: apps/v1
kind: deployment
metadata:
  name: keycloak
  labels:
    app: keycloak
spec:
  replicas: 2 
  strategy:
    type: rollingupdate
    rollingupdate:
      maxunavailable: 1
  selector:
    matchlabels:
      app: keycloak
  template:
    metadata:
      labels:
        app: keycloak
    spec:
      containers:
      - name: keycloak
        image: jboss/keycloak
        env:
            - name: keycloak_user 
              value: admin
            - name: keycloak_password 
              value: admin123
            - name: db_vendor
              value: postgres
            - name: db_addr
              value: &quot;postgres&quot; 
            - name: db_port
              value: &quot;5432&quot;
            - name: db_user
              value: &quot;postgres&quot;
            - name: db_password
              value: &quot;postgres&quot;
            - name: db_schema
              value: &quot;public&quot;
            - name: db_database
              value: &quot;keycloak&quot;
#            - name: jgroups_discovery_protocol
#              value: kubernetes.kube_ping
#            - name: jgroups_discovery_properties
#              value: dump_requests=true,port_range=0,namespace=default
#              value: port_range=0,dump_requests=true
            - name: jgroups_discovery_protocol
              value: dns.dns_ping
            - name: jgroups_discovery_properties
              value: &quot;dns_query=keycloak&quot;
            - name: cache_owners_count
              value: '2'
            - name: cache_owners_auth_sessions_count
              value: '2'
            - name: proxy_address_forwarding
              value: &quot;true&quot;
        ports:
            - name: http
              containerport: 8080
            - name: https
              containerport: 8443

---
apiversion: v1
kind: service
metadata:
  name: keycloak
  labels:
    app: keycloak
spec:
  type: clusterip
  ports:
    - name: http
      port: 80
      targetport: 8080
    - name: https
      port: 443
      targetport: 8443
  selector:
    app: keycloak
---
apiversion: v1
kind: service
metadata:
  name: keycloak-np
  labels:
    app: keycloak
spec:
  type: loadbalancer 
  ports:
    - name: http
      port: 80
      targetport: 8080
    - name: https
      port: 443
      targetport: 8443
  selector:
    app: keycloak

important note

i tried both protocols with and without the database setup.
the above yaml has all the discovery protocol combinations i tried each at a time (the ones commented)

",<kubernetes><google-kubernetes-engine><keycloak><high-availability><keycloak-services>,70291244,11,"for keycloak 17 and newer
by default, those versions use dns_ping as the discovery mechanism for jgroups (the underlying cluster mechanism) but you still need to activate it.
you'll need:

a headless service pointing to your keycloak pods (a headless service is just a normal service but with clusterip: none)
env kc_cache_stack=kubernetes (to activate the kubernetes jgroup configs) and java_opts_append=-djgroups.dns.query=&lt;name-of-headless-service&gt; (to tell it how to find the other keycloak pods).

that way, when starting up, jgroups will issue a dns query for  (example: keycloak-headless.my_namespace.svc.cluster.local) and the response will be the ip of all pods associated to the headless service.
jgroups will then contact every ip in communication port and stablish the cluster.

update 2022-08-01: this configuration below is for the legacy version of keycloak (or versions up to 16). from 17 on keycloak migrated to the quarkus distribution and the configuration is different, as above.
for keycloak up to 16
the way kube_ping works is similar to running kubectl get pods inside one keycloak pod to find the other keycloak pods' ips and then trying to connect to them one by one. however, keycloak does this by querying the kubernetes api directly instead of using kubectl.
to access the kubernetes api, keycloak needs credentials in the form of an access token. you can pass your token directly, but this is not very secure or convenient.
kubernetes has a built-in mechanism for injecting a token into a pod (or the software running inside that pod) to allow it to query the api. this is done by creating a service account, giving it the necessary permissions through a rolebinding, and setting that account in the pod configuration.
the token is then mounted as a file at a known location, which is hardcoded and expected by all kubernetes clients. when the client wants to call the api, it looks for the token at that location.
you can get a deeper look at the service account mechanism in the documentation.
in some situations, you may not have the necessary permissions to create rolebindings. in this case, you can ask an administrator to create the service account and rolebinding for you or pass your own user's token (if you have the necessary permissions) through the sa_token_file environment variable.
you can create the file using a secret or configmap, mount it to the pod, and set sa_token_file to the file location. note that this method is specific to jgroups library (used by keycloak) and the documentation is here.

if you do have permissions to create service accounts and rolebindings in the cluster:
an example (not tested):
export target_namespace=default

# convenient method to create a service account 
kubectl create serviceaccount keycloak-kubeping-service-account -n $target_namespace

# no convenient method to create role and rolebindings
# needed to explicitly define them.
cat &lt;&lt;eof | kubectl apply -f -
kind: role
apiversion: rbac.authorization.k8s.io/v1
metadata:
  name: keycloak-kubeping-pod-reader
rules:
- apigroups: [&quot;&quot;]
  resources: [&quot;pods&quot;]
  verbs: [&quot;get&quot;, &quot;list&quot;]

---

apiversion: rbac.authorization.k8s.io/v1beta1
kind: rolebinding
metadata:
  name: keycloak-kubeping-api-access
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: keycloak-kubeping-pod-reader
subjects:
- kind: serviceaccount
  name: keycloak-kubeping-service-account
  namespace: $target_namespace

eof

on the deployment, you set the serviceaccount:
apiversion: apps/v1
kind: deployment
metadata:
  name: keycloak
spec:
  template:
    spec:
      serviceaccount: keycloak-kubeping-service-account
      serviceaccountname: keycloak-kubeping-service-account
      containers:
      - name: keycloak
        image: jboss/keycloak
        env:
#          ...
            - name: jgroups_discovery_protocol
              value: kubernetes.kube_ping
            - name: jgroups_discovery_properties
              value: dump_requests=true
            - name: kubernetes_namespace
              valuefrom:
                fieldref:
                  apiversion: v1
                  fieldpath: metadata.namespace
#          ...

dump_requests=true will help you debug kubernetes requests. better to have it false in production. you can use namespace=&lt;yournamespace instead of kubernetes_namespace, but that's a handy way the pod has to autodetect the namespace it's running at.
please note that kube_ping will find all pods in the namespace, not only keycloak pods, and will try to connect to all of them. of course, if your other pods don't care about that, it's ok.
"
47700077,how can i generate external ip when creating an ingress that uses nginx controller in kubernetes,"apiversion: extensions/v1beta1
kind: ingress 
metadata:
  name: helloworld-rules
spec:
  rules:
  - host: helloworld-v1.example.com
http:
  paths:
  - path: /
    backend:
      servicename: helloworld-v1
      serviceport: 80
  - host: helloworld-v2.example.com
http:
  paths:
  - path: /
    backend:
           servicename: helloworld-v2
           serviceport: 80


i'm making kubernetes cluster and i will apply that cloudplatform isolated(not aws or google).
when creating an ingress for service i can choose host url but that is not exist anywhere(that address is not registrated something like dns server) so i can't access that url. visiting this ip just gives a 404.
how can i get or configure url  that can access external browser 
:(... 
",<kubernetes><kubernetes-go-client>,47704603,11,"it depends on how you configure your nginx controller.

you should have a service configured which is the entry point when accessing from outside see the docs https://kubernetes.io/docs/concepts/services-networking/ingress/#what-is-ingress.

so basically you have a service that points to the ingress controller and this will redirect the traffic to your pods based on ingress objects.

ingress -> services -> pods

since you don't run on aws or google you would have to use externalip or nodeport and configure the service accordingly

kind: service
apiversion: v1
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app: ingress-nginx
spec:
  selector:
    app: ingress-nginx
  ports:
  - name: http
    port: 80
    targetport: http
  - name: https
    port: 443
    targetport: http
  externalips:
  - 80.11.12.10


and dns needs to be managed with whatever you have for your domains in order to resolve, or for locally testing you can just edit your /etc/hostnames

basically in aws or google you just create a service with type: loadbalancer and point your dns records to the balancer address (cname for aws and the ip for google)
"
73006380,helm template is not using values.yaml file,"i can't seem to figure out whey my nginx template is not using the values file when i pass it with the helm template --debug nginx charts/transport-latency -f charts/transport-latency/values.yaml &gt; .ignore/nginx-out.yaml command.
output using --debug
install.go:178: [debug] original chart version: &quot;&quot;
install.go:195: [debug] chart path: /users/&lt;username&gt;/src/repos/helm_charts/charts/transport-latency

here is the structure of the chart:
charts/transport-latency
 chart.lock
 chart.yaml
 charts
    nginx-13.1.0.tgz
 templates
 values.yaml

and when i run the above mentioned command i just get the default chart without any modification that should be included in the values.yaml file.
here is what my values.yaml file looks like:
namespaceoverride: airflow
replicacount: 2
service:
  type: clusterip

if it helps here is my chart.yaml contents:
apiversion: v2
name: transport-latency
type: application
version: 1.0.0
appversion: &quot;1.0.0&quot;
dependencies:
- name: nginx
  version: 13.1.0
  repository: https://charts.bitnami.com/bitnami

",<kubernetes><kubernetes-helm>,73006449,11,"if you are referencing a dependent chart (in your case, the nginx chart), then you must nest values for that subchart in its own block with the name of the dependency.
so, since you named the dependency nginx as per your chart.yaml:
apiversion: v2
name: transport-latency
type: application
version: 1.0.0
appversion: &quot;1.0.0&quot;
dependencies:
- name: nginx
  version: 13.1.0
  repository: https://charts.bitnami.com/bitnami

you must therefore nest the values for that chart in a block labelled nginx
values.yaml
nginx:
  namespaceoverride: airflow
  replicacount: 2
  service:
    type: clusterip

using your values.yaml as it is:
namespaceoverride: airflow
replicacount: 2
service:
  type: clusterip

would only provide those to your &quot;root&quot; chart -- which is empty.
"
49856754,nginx-ingress: too many redirects when force-ssl is enabled,"i am setting up my first ingress in kubernetes using nginx-ingress. i set up the ingress-nginx load balancer service like so:

{
  ""kind"": ""service"",
  ""apiversion"": ""v1"",
  ""metadata"": {
    ""name"": ""ingress-nginx"",
    ""namespace"": ""..."",
    ""labels"": {
      ""k8s-addon"": ""ingress-nginx.addons.k8s.io""
    },
    ""annotations"": {     
      ""service.beta.kubernetes.io/aws-load-balancer-backend-protocol"": ""tcp"",
      ""service.beta.kubernetes.io/aws-load-balancer-proxy-protocol"": ""*"",
      ""service.beta.kubernetes.io/aws-load-balancer-ssl-cert"": ""arn...."",
      ""service.beta.kubernetes.io/aws-load-balancer-ssl-ports"": ""443""
    }
  },
  ""spec"": {
    ""ports"": [
      {
        ""name"": ""http"",
        ""protocol"": ""tcp"",
        ""port"": 80,
        ""targetport"": ""http"",
        ""nodeport"": 30591
      },
      {
        ""name"": ""https"",
        ""protocol"": ""tcp"",
        ""port"": 443,
        ""targetport"": ""http"",
        ""nodeport"": 32564
      }
    ],
    ""selector"": {
      ""app"": ""ingress-nginx""
    },
    ""clusterip"": ""..."",
    ""type"": ""loadbalancer"",
    ""sessionaffinity"": ""none"",
    ""externaltrafficpolicy"": ""cluster""
  },
  ""status"": {
    ""loadbalancer"": {
      ""ingress"": [
        {
          ""hostname"": ""blablala.elb.amazonaws.com""
        }
      ]
    }
  }
}


notice how the https port has its targetport property pointing to port 80 (http) in order to terminate ssl at the load balancer.

my ingress looks something like this:

apiversion: extensions/v1beta1
kind: ingress
metadata: 
  name: something
  namespace: ...
  annotations:
    ingress.kubernetes.io/ingress.class: ""nginx""
    ingress.kubernetes.io/force-ssl-redirect: ""true""
spec:
  rules:
    - host: www.exapmle.com
      http:
        paths:
         - path: /
           backend:
            servicename: some-service
            serviceport: 2100


now when i navigate to the url i get a too many redirects error. something that is confusing me is that when i add the following header ""x-forwarded-proto: https"" i get the expected response (curl https://www.example.com -v -h ""x-forwarded-proto: https""). 

any ideas how i can resolve the issue?

p.s. this works just fine with ingress.kubernetes.io/force-ssl-redirect: ""false"" and it doesn't seem that there are any extraneous redirects.
",<nginx><kubernetes><kops><kubernetes-ingress>,49857465,11,"that is a known issue with the annotation for ssl-redirection in combination with proxy-protocol and termination of ssl connections on elb.

question about it was published on github and here is a fix from that thread:


you should create a custom configmap for an nginx-ingress instead of using force-ssl-redirect annotation like the following:

apiversion: v1
kind: configmap
metadata:
  labels:
    app: ingress-nginx
  name: nginx-ingress-configuration
  namespace: &lt;ingress-namespace&gt;
data:
  ssl-redirect: ""false""
  hsts: ""true""
  server-tokens: ""false""
  http-snippet: |
    server {
      listen 8080 proxy_protocol;
      server_tokens off;
      return 301 https://$host$request_uri;
    }


that configuration will create an additional listener with a simple redirection to https.
then, apply that configmap to your ingress-controller, add nodeport 8080 to its container definition and to the service.
now, you can point the port 80 of your elb to port 8080 of the service. 


with that additional listener, it will work.
"
63275239,kubernetes nginx ingress server-snippet annotation not taking effect,"i have following ingress.yaml
apiversion: extensions/v1beta1
kind: ingress
metadata:
    name: nginx-configuration-snippet
    annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /$2
        nginx.ingress.kubernetes.io/server-snippet: |
          location /base/path/v1/api/update {
              deny all;
              return 404;
            }
spec:
  rules:
    - http:
        paths:
          - path: /base/path(/|$)(.*)
            backend:
              servicename: myapi
              serviceport: 8080

but when i send put request to /base/path/v1/api/update instead of getting 404 i am getting 500 which means that the path is reachable from ingress controller. can anybody help me identify why ?
i changed to configuration-snippet and the error i got is :
error: exit status 1
2020/08/06 18:35:07 [emerg] 1734#1734: location &quot;/base/path/v1/api/update&quot; is outside location &quot;^/base/path(/|$)(.*)&quot; in /tmp/nginx-cfg008325631:2445
nginx: [emerg] location &quot;/base/path/v1/api/update&quot; is outside location &quot;^/base/path(/|$)(.*)&quot; in /tmp/nginx-cfg008325631:2445
nginx: configuration file /tmp/nginx-cfg008325631 test failed

",<kubernetes><kubernetes-helm><kubernetes-ingress><nginx-ingress>,63421209,11,"adding my own answer :
the final config that worked for me was :
apiversion: extensions/v1beta1
kind: ingress
metadata:
    name: nginx-configuration-snippet
    annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /$2
        nginx.ingress.kubernetes.io/server-snippet: |
          location ~* &quot;^/base/path/v1/api/update&quot; {
              deny all;
              return 403;
            }
spec:
  rules:
    - http:
        paths:
          - path: /base/path(/|$)(.*)
            backend:
              servicename: myapi
              serviceport: 8080

"
62215795,pod has unbound immediate persistentvolumeclaims (repeated 3 times),"what is wrong with below.

# config for es data node
apiversion: v1
kind: configmap
metadata:
  namespace: infra
  name: elasticsearch-data-config
  labels:
    app: elasticsearch
    role: data
data:
  elasticsearch.yml: |-
    cluster.name: ${cluster_name}
    node.name: ${node_name}
    discovery.seed_hosts: ${node_list}
    cluster.initial_master_nodes: ${master_nodes}

    network.host: 0.0.0.0

    node:
      master: false
      data: true
      ingest: false

    xpack.security.enabled: true
    xpack.monitoring.collection.enabled: true
---
# service for es data node
apiversion: v1
kind: service
metadata:
  namespace: infra
  name: elasticsearch-data
  labels:
    app: elasticsearch
    role: data
spec:
  ports:
  - port: 9300
    name: transport
  selector:
    app: elasticsearch
    role: data
---
apiversion: apps/v1beta1
kind: statefulset
metadata:
  namespace: infra
  name: elasticsearch-data
  labels:
    app: elasticsearch
    role: data
spec:
  servicename: ""elasticsearch-data""
  replicas: 1
  template:
    metadata:
      labels:
        app: elasticsearch-data
        role: data
    spec:
      containers:
      - name: elasticsearch-data
        image: docker.elastic.co/elasticsearch/elasticsearch:7.3.0
        env:
        - name: cluster_name
          value: elasticsearch
        - name: node_name
          value: elasticsearch-data
        - name: node_list
          value: elasticsearch-master,elasticsearch-data,elasticsearch-client
        - name: master_nodes
          value: elasticsearch-master
        - name: ""es_java_opts""
          value: ""-xms300m -xmx300m""
        ports:
        - containerport: 9300
          name: transport
        volumemounts:
        - name: config
          mountpath: /usr/share/elasticsearch/config/elasticsearch.yml
          readonly: true
          subpath: elasticsearch.yml
        - name: elasticsearch-data-persistent-storage
          mountpath: /data/db
      volumes:
      - name: config
        configmap:
          name: elasticsearch-data-config
      initcontainers:
      - name: increase-vm-max-map
        image: busybox
        command: [""sysctl"", ""-w"", ""vm.max_map_count=262144""]
        securitycontext:
          privileged: true
  volumeclaimtemplates:
  - metadata:
      name: elasticsearch-data-persistent-storage
    spec:
      accessmodes: [ ""readwriteonce"" ]
      resources:
        requests:
          storage: 10gi


statefull output:

name:           elasticsearch-data-0
namespace:      infra
priority:       0
node:           &lt;none&gt;
labels:         app=elasticsearch-data
                controller-revision-hash=elasticsearch-data-76bdf989b6
                role=data
                statefulset.kubernetes.io/pod-name=elasticsearch-data-0
annotations:    &lt;none&gt;
status:         pending
ip:             
ips:            &lt;none&gt;
controlled by:  statefulset/elasticsearch-data
init containers:
  increase-vm-max-map:
    image:      busybox
    port:       &lt;none&gt;
    host port:  &lt;none&gt;
    command:
      sysctl
      -w
      vm.max_map_count=262144
    environment:  &lt;none&gt;
    mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-9nhmg (ro)
containers:
  elasticsearch-data:
    image:      docker.elastic.co/elasticsearch/elasticsearch:7.3.0
    port:       9300/tcp
    host port:  0/tcp
    environment:
      cluster_name:  elasticsearch
      node_name:     elasticsearch-data
      node_list:     elasticsearch-master,elasticsearch-data,elasticsearch-client
      master_nodes:  elasticsearch-master
      es_java_opts:  -xms300m -xmx300m
    mounts:
      /data/db from elasticsearch-data-persistent-storage (rw)
      /usr/share/elasticsearch/config/elasticsearch.yml from config (ro,path=""elasticsearch.yml"")
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-9nhmg (ro)
conditions:
  type           status
  podscheduled   false 
volumes:
  elasticsearch-data-persistent-storage:
    type:       persistentvolumeclaim (a reference to a persistentvolumeclaim in the same namespace)
    claimname:  elasticsearch-data-persistent-storage-elasticsearch-data-0
    readonly:   false
  config:
    type:      configmap (a volume populated by a configmap)
    name:      elasticsearch-data-config
    optional:  false
  default-token-9nhmg:
    type:        secret (a volume populated by a secret)
    secretname:  default-token-9nhmg
    optional:    false
qos class:       besteffort
node-selectors:  &lt;none&gt;
tolerations:     node.kubernetes.io/not-ready:noexecute for 300s
                 node.kubernetes.io/unreachable:noexecute for 300s
events:
  type     reason            age                  from               message
  ----     ------            ----                 ----               -------
  warning  failedscheduling  46s (x4 over 3m31s)  default-scheduler  pod has unbound immediate persistentvolumeclaims (repeated 3 times)


kubectl get sc

name                 provisioner            age
standard (default)   kubernetes.io/gce-pd   5d19h

kubectl get pv
no resources found in infra namespace.
kubectl get pvc
name                                                         status    volume   capacity   access modes   storageclass   age
elasticsearch-data-persistent-storage-elasticsearch-data-0   pending                                      gp2            8h

",<kubernetes><kubernetes-pod><kubernetes-pvc>,62217530,11,"it looks like there is some issue with your pvc.
name                                                         status    volume   capacity   access modes   storageclass   age
elasticsearch-data-persistent-storage-elasticsearch-data-0   pending                                      gp2            8h


as you can see your pv is also not created.i think there is an issue with your storage class.looks like gp2 storage class is not available in your cluster.
either run this yaml file if you are in aws eks
kind: storageclass
apiversion: storage.k8s.io/v1
metadata:
  name: gp2
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  fstype: ext4

or simply change the storage class to standard in  gcp gke
"
51613842,wildcard ssl certificate with subdomain redirect in kubernetes,"i've configured my kubernetes to use one wildcard ssl certificate to all my apps using cert-manager and letsencrypt, now the problem is that i can't configure subdomain redirects cause ingress is kinda ""stiff"". here's how i'm trying to achieve this:

apiversion: extensions/v1beta1
kind: ingress
metadata:
name: my-wildcard-ingress
  namespace: mynamespace
  annotations:
    kubernetes.io/ingress.class: nginx
    certmanager.k8s.io/cluster-issuer: letsencrypt-prod
    certmanager.k8s.io/acme-challenge-type: dns01
    certmanager.k8s.io/acme-dns01-provider: azuredns
    ingress.kubernetes.io/force-ssl-redirect: ""true""
    ingress.kubernetes.io/ssl-redirect: ""true""
spec:
  rules:
  - host: ""domain.com""
    http:
      paths:
      - path: /
        backend:
          servicename: some-service
          serviceport: 3000          
  - host: somesub.domain.com
    http:
      paths:
      - path: /
        backend:
          servicename: some-other-service
          serviceport: 80
  - host: othersub.domain.com
    http:
      paths:
      - path: /
        backend:
          servicename: one-more-service
          serviceport: 8080          
  - host: ""*.domain.com""
    http:
      paths:
      - path: /
        backend:
          servicename: default-service-to-all-other-non-mapped-subdomains
          serviceport: 8000          

  tls:
  - secretname: domain-com-tls
    hosts:         
     - ""*.domain.com.br""


the problem is that ingress ignores the declared subdomain redirects just because they're not listed in the ""tls:hosts"" section. and if i do put them there, it tries to issue the ssl certificate using the wildcard and the other subdomains as well in the same cert, which causes the issuer to refuse the order, saying the obvious: ""subdomain.domain.com and *.domain.com are redundant""

is there any other way that i can declare those redirects and force them to use my ssl wildcard certificate?
",<kubernetes><lets-encrypt><wildcard-subdomain><kubernetes-ingress><cert-manager>,52116111,11,"well, for anyone who's having this kind of trouble, i've managed to solve it (not the best solution, but it's a start). for this, i'll be using cert-manager and letsencrypt.

first, i've created a clusterissuer to issue for my certs with letsencrypt:

apiversion: certmanager.k8s.io/v1alpha1
kind: clusterissuer
metadata:      
  name: letsencrypt-prod-dns
spec:
  acme:
    dns01:
      providers:
      - azuredns:
          clientid: my_azure_client_id
          clientsecretsecretref:
            key: client-secret
            name: azure-secret
          hostedzonename: mydomain.com
          resourcegroupname: my_azure_resource_group_name
          subscriptionid: my_azure_subscription_id
          tenantid: my_azure_tenant_id
        name: azuredns
    email: somemail@mydomain.com
    privatekeysecretref:
      key: """"
      name: letsencrypt-prod-dns
    server: https://acme-v02.api.letsencrypt.org/directory


then i've created a fallback ingress to all my subdomains (this one will be the cert generator):

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    certmanager.k8s.io/acme-challenge-type: dns01
    certmanager.k8s.io/acme-dns01-provider: azuredns
    certmanager.k8s.io/cluster-issuer: letsencrypt-prod-dns
    ingress.kubernetes.io/force-ssl-redirect: ""true""
    ingress.kubernetes.io/ssl-redirect: ""true""    
    kubernetes.io/ingress.class: nginx    
  name: wildcard-ingress
  namespace: some-namespace  
spec:
  rules:
  - host: '*.mydomain.com'
    http:
      paths:
      - backend:
          servicename: some-default-service
          serviceport: 80
        path: /      
  tls:
  - hosts:
    - '*.mydomain.com'
    - mydomain.com
    secretname: wildcard-mydomain-com-tls


notice that i've declared at the tls section the wildcard and the absolute paths, so the cert will be valid for the urls without subdomains too.

at this point, any requests to your domain, will be redirected to ""some-default-service"" with ssl(cert-manager will issue for a new cert as soon as you create the fallback ingress. this can take a while once cert-manager dns01 issuer is not mature yet), great!!! 

but, what if you need to redirect some specific subdomain to another service? no problem (since they're running on the same namespace), all you have to do is to create a new ingress to your subdomain, pointing it to your existing wildcard-mydomain-com-tls cert secret:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    ingress.kubernetes.io/force-ssl-redirect: ""false""
    ingress.kubernetes.io/ssl-redirect: ""true""
    kubernetes.io/ingress.class: nginx
  name: somesubdomain-ingress
  namespace: some-namespace
spec:
  rules:
  - host: somesubdomain.mydomain.com
    http:
      paths:
      - backend:
          servicename: some-other-service
          serviceport: 8080
        path: /        
  tls:
  - hosts:
    - somesubdomain.mydomain.com
    secretname: wildcard-mydomain-com-tls


easy peasy lemon squeezy!!! now your somesubdomain.mydomain.com overrides your fallback rule and sends the user to another app. the only thing you should notice here is that the secret is valid only for ""some-namespace"" namespace, if you need to use this cert in another namespace, you could:


copy the secret from namespace ""some-namespace"" to ""other-namespace"". if you do this, remember that cert-manager will not renew this cert automatically for ""other-namespace"", so, you'd have to copy the secret again, every time your cert expires.
recreate the fallback ingress to every namespace you have, so you'd have a new cert for each of them. this approach is more ingress verbose, but, it's fully automatic.


i guess that's it. hope someone out there can benefit from this info.

cheers
"
65482653,nginx ingress controller rewrite-target annotation and rule to add a trailing slash to url,"i'm trying to deploy a static website to a kubernetes cluster which is using the official nginx ingress controller. the folder structure of the website looks somewhat like this:
/
 about
  index.html
 casestudy
  data-and-analytics
   index.html
  workflow-automation
      index.html
 contact
  index.html
 css
  font-awesome.min.css
  fonts
   slick.eot
   slick.svg
   slick.ttf
   slick.woff
  footer.css
  layout.css
...

my ingress definition looks like this:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: website-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    # nginx.ingress.kubernetes.io/rewrite-target: /$2
    cert-manager.io/cluster-issuer: letsencrypt
spec:
  tls:
    - hosts:
        - website.com
      secretname: website-tls
  rules:
    - host: website.com
      http:
        paths:
          - path: /
            backend:
              servicename: website-svc
              serviceport: 8080

this works fine for the most part, except that if i forget to put a trailing slash at the end of the url like https://website.com/about i get routed into an error page. i understand why this is happening - nginx is looking for a about.html file and is failing to find one. but i don't know how to fix this.
what i'd ideally like to do is that i want to add a trailing / to requests which don't have one. but i also want to not do this when the browser is requesting for a css file.
what redirect annotation and rule should i use for this?
thanks.
",<kubernetes><kubernetes-ingress><nginx-ingress>,65941913,11,"what ultimately worked for this situation is a snippet like this:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: website-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt
    nginx.ingress.kubernetes.io/rewrite-target: /
    # rewrite all urls not ending with a segment containing . or ? with a trailing slash
    # so basically we are rewriting all folder names with a trailing slash.
    nginx.ingress.kubernetes.io/configuration-snippet: |
      rewrite ^([^.?]*[^/])$ $1/ redirect;
spec:
  tls:
    - hosts:
        - website.com
      secretname: website-tls
  rules:
    - host: website.com
      http:
        paths:
          - path: /
            backend:
              servicename: website-svc
              serviceport: 8080

this will let us rewrite all urls ending with a segment containing no . (period - thus avoiding filenames) and ? (question mark - thus avoiding all query strings) with a trailing slash. this works for my case.
"
51024074,how can i iteratively create pods from list using helm?,"i'm trying to create a number of pods from a yaml loop in helm. if i run with --debug --dry-run the output matches my expectations, but when i actually deploy to to a cluster, only the last iteration of the loop is present.

some yaml for you: 

{{ if .values.componenttests }}
{{- range .values.componenttests }}
apiversion: v1
kind: pod
metadata:
  name: {{ . }}
  labels:
    app: {{ . }}
    chart: {{ $.chart.name }}-{{ $.chart.version | replace ""+"" ""_"" }}
    release: {{ $.release.name }}
    heritage: {{ $.release.service }}
spec:
{{ toyaml $.values.global.podspec | indent 2 }}
  restartpolicy: never
  containers:
  - name: {{ . }}
    ports:
      - containerport: 3000
    image: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/{{ . }}:latest
    imagepullpolicy: always
    command: [""sleep""]
    args: [""100d""]
    resources:
      requests:
        memory: 2000mi
        cpu: 500m
{{- end }}
{{ end }}


when i run  helm upgrade --install --set componenttests=""{a,b,c}"" --debug --dry-run

i get the following output: 

# source: &lt;path-to-file&gt;.yaml
apiversion: v1
kind: pod
metadata:
  name: a
  labels:
    app: a
    chart: integrationtests-0.0.1
    release: funny-ferret
    heritage: tiller
spec: 
  restartpolicy: never
  containers:
  - name: content-tests
    ports:
      - containerport: 3000
    image: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/a:latest
    imagepullpolicy: always
    command: [""sleep""]
    args: [""100d""]
    resources:
      requests:
        memory: 2000mi
        cpu: 500m
apiversion: v1
kind: pod
metadata:
  name: b
  labels:
    app: b
    chart: integrationtests-0.0.1
    release: funny-ferret
    heritage: tiller
spec:  
  restartpolicy: never
  containers:
  - name: b
    ports:
      - containerport: 3000
    image: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/b:latest
    imagepullpolicy: always
    command: [""sleep""]
    args: [""100d""]
    resources:
      requests:
        memory: 2000mi
        cpu: 500m
apiversion: v1
kind: pod
metadata:
  name: c
  labels:
    app: users-tests
    chart: integrationtests-0.0.1
    release: funny-ferret
    heritage: tiller
spec:
  restartpolicy: never
  containers:
  - name: c
    ports:
      - containerport: 3000
    image: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/c:latest
    imagepullpolicy: always
    command: [""sleep""]
    args: [""100d""]
    resources:
      requests:
        memory: 2000mi
        cpu: 500m
---


(some parts have been edited/removed due to sensitivity/irrelevance)

which looks to me like i it does what i want it to, namely create a pod for a another for b and a third for c.

however, when actually installing this into a cluster, i always only end up with the pod corresponding to the last element in the list. (in this case, c) it's almost as if they overwrite each other, but given that they have different names i don't think they should? even running with --debug but not --dry-run the output tells me i should have 3 pods, but using kubectl get pods i can see only one.

how can i iteratively create pods from a list using helm?
",<kubernetes><yaml><kubernetes-helm>,51025501,11,"found it! 

so apparently, helm uses --- as a separator between specifications of pods/services/whathaveyou.

specifying the same fields multiple times in a single chart is valid, it will use the last specified value for for any given field. to avoid overwriting values and instead have multiple pods created, simply add the separator at the end of the loop:

{{ if .values.componenttests }}
{{- range .values.componenttests }}
apiversion: v1
kind: pod
metadata:
  name: {{ . }}
  labels:
    app: {{ . }}
    chart: {{ $.chart.name }}-{{ $.chart.version | replace ""+"" ""_"" }}
    release: {{ $.release.name }}
    heritage: {{ $.release.service }}
spec:
{{ toyaml $.values.global.podspec | indent 2 }}
  restartpolicy: never
  containers:
  - name: {{ . }}
    ports:
      - containerport: 3000
    image: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/{{ . }}:latest
    imagepullpolicy: always
    command: [""sleep""]
    args: [""100d""]
    resources:
      requests:
        memory: 2000mi
        cpu: 500m
---
{{- end }}
{{ end }}

"
66875139,how to configure kube-prometheus-stack helm installation to scrape a kubernetes service?,"i have installed kube-prometheus-stack as a dependency in my helm chart on a local docker for mac kubernetes cluster v1.19.7. i can view the default prometheus targets provided by the kube-prometheus-stack.
i have a python flask service that provides metrics which i can view successfully in the kubernetes cluster using kubectl port forward.
however, i am unable to get these metrics displayed on the prometheus targets web interface.
the kube-prometheus-stack documentation states that prometheus.io/scrape does not support annotation-based discovery of services. instead the the reader is referred to the concept of servicemonitors and podmonitors.
so, i have configured my service as follows:
---
kind:                       service
apiversion:                 v1  
metadata:
  name:                     flask-api-service                    
  labels:
    app:                    flask-api-service
spec:
  ports:
    - protocol:             tcp 
      port:                 4444
      targetport:           4444
      name:                 web 
  selector:
    app:                    flask-api-service                    
    tier:                   backend 
  type:                     clusterip
---
apiversion:                 monitoring.coreos.com/v1
kind:                       servicemonitor
metadata:
  name:                     flask-api-service
spec:
  selector:
    matchlabels:
      app:                  flask-api-service
  endpoints:
  - port:                   web 

subsequently, i have setup a port forward to view the metrics:
kubectl port-forward prometheus-flaskapi-kube-prometheus-s-prometheus-0 9090

then visited prometheus web page at http://localhost:9090
when i select the status-&gt;targets menu option, my flask-api-service is not displayed.
i know that the service is up and running and i have checked that i can view the metrics for a pod for my flask-api-service using kubectl port-forward &lt;pod name&gt; 4444.
looking at a similar issue it looks as though there is a  configuration value servicemonitorselectorniluseshelmvalues that defaults to true. setting this to false makes the operator look outside its release labels in helm??
i tried adding this to the values.yml of my helm chart in addition to the extrascrapeconfigs configuration value. however, the flask-api-service still does not appear as an additional target on the prometheus web page when clicking the status-&gt;targets menu option.
prometheus:
  prometheusspec:
    servicemonitorselectorniluseshelmvalues: false
  extrascrapeconfigs: |
    - job_name: 'flaskapi'
    static_configs:
      - targets: ['flask-api-service:4444']

how do i get my flask-api-service recognised on the prometheus targets page at http://localhost:9090?
i am installing kube-prometheus-stack as a dependency via my helm chart with default values as shown below:
chart.yaml
apiversion: v2
appversion: &quot;0.0.1&quot;
description: a helm chart for flaskapi deployment
name: flaskapi
version: 0.0.1
dependencies:
- name: kube-prometheus-stack
  version: &quot;14.4.0&quot;
  repository: &quot;https://prometheus-community.github.io/helm-charts&quot;
- name: ingress-nginx
  version: &quot;3.25.0&quot;
  repository: &quot;https://kubernetes.github.io/ingress-nginx&quot;
- name: redis
  version: &quot;12.9.0&quot;
  repository: &quot;https://charts.bitnami.com/bitnami&quot;

values.yaml
docker_image_tag: dcs3spp/
hostname: flaskapi-service
redis_host: flaskapi-redis-master.default.svc.cluster.local 
redis_port: &quot;6379&quot;

prometheus:
  prometheusspec:
    servicemonitorselectorniluseshelmvalues: false
  extrascrapeconfigs: |
    - job_name: 'flaskapi'
    static_configs:
      - targets: ['flask-api-service:4444']

",<kubernetes><prometheus><kubernetes-helm><prometheus-operator>,66875766,11,"prometheus custom resource definition has a field called servicemonitorselector. prometheus only listens to those matched servicemonitor. in case of helm deployment it is your release name.
release: {{ $.release.name | quote }}

so adding this field in your servicemonitor  should solve the issue. then you servicemonitor manifest file will be:

apiversion:                 monitoring.coreos.com/v1
kind:                       servicemonitor
metadata:
  name:                     flask-api-service
  labels:
      release: &lt;your_helm_realese_name_&gt;
spec:
  selector:
    matchlabels:
      app:                  flask-api-service
  endpoints:
  - port:                   web 

"
54390577,"error validating data: [validationerror(pod): unknown field ""containers"" in io.k8s.api.core.v1.pod","i am trying to create some sample kubernetes pod file.

cat &lt;&lt; eof | kubectl create -f -
apiversion: v1
kind: pod
metadata:
name: nginx
spec:
containers:
- name: nginx
  image: nginx
eof


but on executing this i am getting below error.


  error: error validating ""pod.yaml"": error validating data:
  [validationerror(pod): unknown field ""containers"" in
  io.k8s.api.core.v1.pod, validationerror(pod): unknown field ""na me"" in
  io.k8s.api.core.v1.pod]; if you choose to ignore these errors, turn
  validation off with --validate=false

",<kubernetes><kubectl>,54390717,10,"i am not sure about the exact issue but it got resolved with proper space indentation

---
apiversion: v1
kind: pod
metadata:
 name: nginx
spec:
 containers:
   - name: nginx
     image: nginx


it worked for me now with proper spaces. mybad
"
69290796,persistent storage in eks failing to provision volume,"i followed the steps from aws knowledge base to create persistent storage: use persistent storage in amazon eks
unfortunately, persistentvolume(pv) wasn't created:
kubectl get pv
no resources found

when i checked the pvc logs, i'm getting the following  provisioning failed message:
storageclass.storage.k8s.io &quot;ebs-sc&quot; not found

failed to provision volume with storageclass &quot;ebs-sc&quot;: rpc error: code = deadlineexceeded desc = context deadline exceeded

i'm using kubernetes v1.21.2-eks-0389ca3

update:
the storageclass.yaml used in the example has provisioner set to ebs.csi.aws.com
kind: storageclass
apiversion: storage.k8s.io/v1
metadata:
  name: ebs-sc
provisioner: ebs.csi.aws.com
volumebindingmode: waitforfirstconsumer

when i updated it using @gohm'c answer, it created a pv.
apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  name: ebs-sc
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
reclaimpolicy: retain
volumebindingmode: waitforfirstconsumer

",<amazon-web-services><kubernetes><amazon-eks><persistent-volumes><kubernetes-pvc>,69293093,10,"storageclass.storage.k8s.io &quot;ebs-sc&quot; not found

failed to provision volume with storageclass &quot;ebs-sc&quot;

you need to create the storage class &quot;ebs-sc&quot; after ebs csi driver is installed, example:
cat &lt;&lt; eof | kubectl apply -f -
apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  name: ebs-sc
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
reclaimpolicy: retain
volumebindingmode: waitforfirstconsumer
eof

see here for more options.
"
57061680,is there a way to share existing data between containers in a pod?,"i have 2 containers in a pod.
1. webapp
2. nginx
i would like to share the data from webapp container /var/www/webapp/ with the nginx container. /var/www/html

/var/www/webapp ( folder structure )
   index.php
       

folder1
      service1.php
      
   subfolder1
          app.php
   
folder2
       service2.php  


the folder is mounted properly but all the files are missing.

apiversion: apps/v1
kind: deployment
    spec:
      volumes:
      - name: webapp-data
        persistentvolumeclaim:
          claimname: webapp-data
      containers:
      - name: webapp
        image: webapp
        imagepullpolicy: always 
        volumemounts:
        - name: webapp-data
          mountpath: /var/www/webapp/
       - name: nginx
        imagepullpolicy: always
        image: nginx
        volumemounts:
        - name: webapp-data
          mountpath: /var/www/html/


apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: webapp-data
spec:
  storageclassname: local
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 5gi


when mounting a volume under docker all the folders and files from within the container are available but not in k8s.
",<kubernetes><persistent-volumes><kubernetes-pvc>,57067062,10,"kubernetes will not automatically populate an empty volume with contents from an image.  (this is a difference from docker run.)  your application needs to figure out how to set up the shared-data directory itself, if it's empty.

for standard database containers this doesn't really matter since they typically start off with some sort of initdb type call which will create the required file structure.  similarly, if you're using a persistent volume as a cache or upload space, it doesn't matter.

for the use case you're describing where you want one container to just have a copy of files from the other, you don't really need a persistent volume.  i'd use an emptydir volume that can be shared between the two containers, and then an init container to copy data into the volume.  do not mount anything over the application content.

this would roughly look like (in reality use a deployment):

apiversion: v1
kind: pod
metadata:
  name: ...
spec:
  volumes:
    - name: webapp-data
      emptydir: {}
  initcontainers:
    - name: populate
      image: webapp
      volumemounts:
        - name: webapp-data
          mountpath: /data
      command: [cp, -a, /var/www/webapp, /data]
  containers:
    - name: webapp
      image: webapp
      # no volumemounts; default command
    - name: nginx
      image: nginx
      volumemounts:
        - name: webapp-data
          mountpath: /var/www/html


with this setup there's also not a hard requirement that the two containers run in the same pod; you could have one deployment that runs the back-end service, and a second deployment that runs nginx (starting up by copying data from the back-end image).

(the example in configure pod initialization in the kubernetes docs is very similar, but fetches the nginx content from an external site.)
"
48222871,i am trying to use gcs bucket as the volume in gke pod,"i am getting the error:


  error validating ""mysql.yaml"": error validating data:
  validationerror(deployment.spec.template.spec.volumes[0]): unknown
  field ""path"" in io.k8s.kubernetes.pkg.api.v1.volume; )


apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  replicas: 1
  selector:
    matchlabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
        - image: mysql:5.6
          name: mysql
          env:
            - name: mysql_root_password
              valuefrom:
                secretkeyref:
                  name: mysql
                  key: password
          ports:
            - containerport: 3306
              name: mysql
          volumemounts:
            - name: mapping-sandbox-test
              mountpath: /var/lib/mysql
      volumes:
        - name: mapping-sandbox-test
          path: gs://&lt;bucket-name&gt;

",<kubernetes><google-cloud-storage><docker-volume><google-kubernetes-engine>,48225221,10,"your deployment object looks correct using name and path as keys. you can see an example on how to mount a gcs bucket on kubernetes here

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: gcsfuse-test
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: gcsfuse-test
    spec:
      containers:
        - name: gcsfuse-test
          image: gcr.io/some-repo/gcs-fuse:latest
          securitycontext:
            privileged: true
            capabilities:
              add:
                - sys_admin
          lifecycle:
            poststart:
              exec:
                command: [""gcsfuse"", ""-o"", ""nonempty"", ""some-bucket"", ""/mnt/some-bucket""]
            prestop:
              exec:
                command: [""fusermount"", ""-u"", ""/mnt/some-bucket""]


this stack overflow question might help too.
"
59884269,expose port from container in a pod minikube kubernetes,"i'm new to k8s, i'll try minikube with 2 container running in a pod with this command:

kubectl apply -f deployment.yaml


and this deployment.yml:



apiversion: v1
kind: pod
metadata:
  name: site-home
spec:

  restartpolicy: never

  volumes:
  - name: v-site-home
    emptydir: {}

  containers:


  - name: site-web
    image: site-home:1.0.0
    ports:
    - containerport: 80
    volumemounts:
    - name: v-site-home
      mountpath: /usr/share/nginx/html/assets/quotalago
  
  - name: site-cron
    image: site-home-cron:1.0.0
    volumemounts:
    - name: v-site-home
      mountpath: /app/quotalago




i've a shared volume so if i understand i cannot use deployment but only pods (maybe stateful set?)

in any case i want to expose the port 80 from the container site-web in the pod site-home. 
in the official docs i see this for deployments:

kubectl expose deployment hello-node --type=loadbalancer --port=8080


but i cannot use for example:

kubectl expose pod site-web --type=loadbalancer --port=8080


any idea?
",<kubernetes><kubectl><minikube>,59886794,10,"
but i cannot use for example:
kubectl expose pod site-web --type=loadbalancer --port=8080


of course you can, however exposing a single pod via loadbalancer service doesn't make much sense. if you have a deployment which typically manages a set of pods between which the real load can be balanced, loadbalancer does its job. however you can still use it just for exposing a single pod.
note that your container exposes port 80, not 8080 (containerport: 80 in your container specification) so you need to specify it as target-port in your service. your kubectl expose command may look like this:
kubectl expose pod site-web --type=loadbalancer --port=8080 --target-port=80

if you provide only --port=8080 flag to your kubectl expose command it assumes that the target-port's value is the same as value of --port. you can easily check it by yourself looking at the service you've just created:
kubectl get svc site-web -o yaml

and you'll see something like this in spec.ports section:
- nodeport: 32576
    port: 8080
    protocol: tcp
    targetport: 8080

after exposing your pod (or deployment) properly i.e. using:
kubectl expose pod site-web --type=loadbalancer --port=8080 --target-port=80

you'll see something similar:
  - nodeport: 31181
    port: 8080
    protocol: tcp
    targetport: 80

after issuing kubectl get services you should see similar output:
name                            type           cluster-ip    external-ip      port(s)          age
site-web                              clusterip      &lt;cluster ip&gt;   &lt;external ip&gt;           8080:31188/tcp         4m42s

if then you go to http://&lt;external ip&gt;:8080 in your browser or run curl http://&lt;external ip&gt;:8080 you should see your website's frontend.
keep in mind that this solution makes sense and will be fully functional in cloud environment which is able to provide you with a real load balancer. note that if you declare such service type in minikube in fact it creates nodeport service as it is unable to provide you with a real load balancer. so your application will be available on your node's ( your minikube vm's ) ip address on randomly selected port in range 30000-32767 (in my example it's port 31181).
as to your question about the volume:

i've a shared volume so if i understand i cannot use deployment but
only pods (maybe stateful set?)

yes, if you want to use specifically emptydir volume, it cannot be shared between different pods (even if they were scheduled on the same node), it is shared only between containers within the same pod. if you want to use deployment you'll need to think about another storage solution such as persistentvolume.

edit
in the first moment i didn't notice the error in your command:
kubectl expose pod site-web --type=loadbalancer --port=8080

you're trying to expose non-existing pod as your pod's name is site-home, not site-web. site-web is a name of one of your containers (within your site-home pod). remember: we're exposing pod, not containers via service.

i change 80-&gt;8080 but i always come to error:kubectl expose pod site-home --type=loadbalancer --port=8080 return:error: couldn't retrieve selectors via --selector flag or introspection: the pod has no labels and cannot be exposed see 'kubectl expose -h' for help
and examples.

the key point here is: the pod has no labels and cannot be exposed
it looks like your pod doesn't have any labels defined which are required so that the service can select this particular pod (or set of similar pods which have the same label) from among other pods in your cluster. you need at least one label in your pod definition. adding simple label name: site-web under pod's metadata section should help. it may look like this in your pod definition:
apiversion: v1
kind: pod
metadata:
  name: site-home
  labels:
    name: site-web
spec:
...

now you may even provide this label as selector in your service however it should be handled automatically if you omit --selector flag:
kubectl expose pod site-home --type=loadbalancer --port=8080 --target-port=80 --selector=name=site-web

remember: in minikube, a real load balancer cannot be created and instead of loadbalancer nodeport type will be created. command kubectl get svc will tell you on which port (in range 30000-32767) your application will be available.

and `kubectl expose pod site-web --type=loadbalancer
--port=8080 return: error from server (notfound): pods &quot;site-web&quot; not found. site-home is the pod, site-web is the container with the port
exposed, what's the issue?

if you don't have a pod with name &quot;site-web&quot; you can expect such message. here you are simply trying to expose non-existing pod.

if i exposed a port from a container the port is automatically exposed also for the pod ?

yes, you have the port defined in container definition. your pods automatically expose all ports that are exposed by the containers within them.
"
51874503,kubernetes ingress network deny some paths,"i've a simple kubernetes ingress network.

i need deny the access some critical paths like /admin or etc.

my ingress network file shown as below.

 apiversion: extensions/v1beta1
 kind: ingress
 metadata:
 name: ingress-test
 spec:
   rules:
   - host: host.host.com
   http:
      paths:
        - path: /service-mapping
      backend:
         servicename: /service-mapping
         serviceport: 9042


how i can deny the custom path with kubernetes ingress network, with nginx annonations or another methods . 



i handle this issue with annotations shown as below . 

apiversion: extensions/v1beta1
kind: ingress
metadata:
   name: nginx-configuration-snippet
   annotations:
      nginx.ingress.kubernetes.io/configuration-snippet: |

     server_tokens off;
     location danger-path {
    deny all;
    return 403;
  }

spec:
  rules:
   - host: api.myhost.com
   http:
  paths:
  - backend:
      servicename: bookapi-2
      serviceport: 8080
    path: path 

",<nginx><kubernetes><kubernetes-ingress>,51894604,10,"ive faced the same issue and found the solution on github.
to achieve your goal, you need to create two ingresses first by default without any restriction:

apiversion: extensions/v1beta1
 kind: ingress
 metadata:
 name: ingress-test
 spec:
   rules:
   - host: host.host.com
   http:
      paths:
        - path: /service-mapping
      backend:
         servicename: /service-mapping
         serviceport: 9042


then, create a secret for auth as described in the doc:

creating the htpasswd

$ htpasswd -c auth foo
new password: &lt;bar&gt;
new password:
re-type new password:
adding password for user foo


creating the secret:

$ kubectl create secret generic basic-auth --from-file=auth
secret ""basic-auth"" created


second ingress with auth for paths which you need to restrict:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-with-auth
  annotations:
    # type of authentication
    nginx.ingress.kubernetes.io/auth-type: basic
    # name of the secret that contains the user/password definitions
    nginx.ingress.kubernetes.io/auth-secret: basic-auth
    # message to display with an appropiate context why the authentication is required
    nginx.ingress.kubernetes.io/auth-realm: ""authentication required - foo""
spec:
  rules:
  - host: host.host.com
    http:
      paths:
      - path: /admin
        backend:
          servicename: service_name
          serviceport: 80


according to sedooe answer, his solution may have some issues.
"
52146001,"kafka not able to connect with zookeeper with error ""timed out waiting for connection while in state: connecting""","i am trying to run my kafka and zookeeper in kubernetes pods. 

here is my zookeeper-service.yaml:

apiversion: v1
kind: service
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.1.0 (36652f6)
  creationtimestamp: null
  labels:
    io.kompose.service: zookeeper-svc
  name: zookeeper-svc
spec:
  ports:
  - name: ""2181""
    port: 2181
    targetport: 2181
  selector:
    io.kompose.service: zookeeper
status:
  loadbalancer: {}


below is zookeeper-deployment.yaml

apiversion: extensions/v1beta1
kind: deployment
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.1.0 (36652f6)
  creationtimestamp: null
  labels:
    io.kompose.service: zookeeper
  name: zookeeper
spec:
  replicas: 1
  strategy: {}
  template:
    metadata:
      creationtimestamp: null
      labels:
        io.kompose.service: zookeeper
    spec:
      containers:
      - image: wurstmeister/zookeeper
        name: zookeeper
        ports:
        - containerport: 2181
        resources: {}
      restartpolicy: always
status: {}


kafka-deployment.yaml is as below:

apiversion: extensions/v1beta1
kind: deployment
metadata:
  annotations:
    kompose.cmd: kompose convert -f docker-compose.yml
    kompose.version: 1.1.0 (36652f6)
  creationtimestamp: null
  labels:
    io.kompose.service: kafka
  name: kafka
spec:
  replicas: 1
  strategy: {}
  template:
    metadata:
      creationtimestamp: null
      labels:
        io.kompose.service: kafka
    spec:
      containers:
      - env:
        - name: kafka_advertised_host_name
          value: kafka
        - name: kafka_zookeeper_connect
          value: zookeeper:2181
        - name: kafka_port
          value: ""9092""
        - name: kafka_zookeeper_connect_timeout_ms
          value: ""60000""
        image: wurstmeister/kafka
        name: kafka
        ports:
        - containerport: 9092
        resources: {}
      restartpolicy: always
status: {}


i first start the zookeeper service and deployment. once the zookeeper is started and kubectl get pods shows it in running state, i start kafka deployment. kafka deployment starts failing and restarting again and again, due to restartpolicy as always. when i checked the logs from kafka docker, i found that it is not able to connect to zookeeper service and the connection timesout. here are the logs from kafka container.

[2018-09-03 07:06:06,670] error fatal error during kafkaserver startup. prepare to shutdown (kafka.server.kafkaserver)
kafka.zookeeper.zookeeperclienttimeoutexception: timed out waiting for connection while in state: connecting
atkafka.zookeeper.zookeeperclient$$anonfun$kafka$zookeeper$zookeeperclient$$ waituntilconnected$1.apply$mcv$sp(zookeeperclient.scala:230)
at kafka.zookeeper.zookeeperclient$$anonfun$kafka$zookeeper$zookeeperclient$$waituntilconnected$1.apply(zookeeperclient.scala:226)
at kafka.zookeeper.zookeeperclient$$anonfun$kafka$zookeeper$zookeeperclient$$waituntilconnected$1.apply(zookeeperclient.scala:226)
at kafka.utils.coreutils$.inlock(coreutils.scala:251)
at kafka.zookeeper.zookeeperclient.kafka$zookeeper$zookeeperclient$$waituntilconnected(zookeeperclient.scala:226)
at kafka.zookeeper.zookeeperclient.&lt;init&gt;(zookeeperclient.scala:95)
at kafka.zk.kafkazkclient$.apply(kafkazkclient.scala:1580)
at kafka.server.kafkaserver.kafka$server$kafkaserver$$createzkclient$1(kafkaserver.scala:348)
at kafka.server.kafkaserver.initzkclient(kafkaserver.scala:372)
at kafka.server.kafkaserver.startup(kafkaserver.scala:202)
at kafka.server.kafkaserverstartable.startup(kafkaserverstartable.scala:38)
at kafka.kafka$.main(kafka.scala:75)
at kafka.kafka.main(kafka.scala)
[2018-09-03 07:06:06,671] info shutting down (kafka.server.kafkaserver)
[2018-09-03 07:06:06,673] warn  (kafka.utils.coreutils$)
java.lang.nullpointerexception
atkafka.server.kafkaserver$$anonfun$shutdown$5.apply$mcv$sp(kafkaserver.scala:579)
at kafka.utils.coreutils$.swallow(coreutils.scala:86)
at kafka.server.kafkaserver.shutdown(kafkaserver.scala:579)
at kafka.server.kafkaserver.startup(kafkaserver.scala:329)
at kafka.server.kafkaserverstartable.startup(kafkaserverstartable.scala:38)
at kafka.kafka$.main(kafka.scala:75)
at kafka.kafka.main(kafka.scala)
[2018-09-03 07:06:06,676] info shut down completed 
(kafka.server.kafkaserver)
[2018-09-03 07:06:06,677] error exiting kafka. 
(kafka.server.kafkaserverstartable)
[2018-09-03 07:06:06,678] info shutting down 
(kafka.server.kafkaserver)


what could be the reason for this ? and solutions ? 

edit: logs from zookeeper pod:

2018-09-03 10:32:39,562 [myid:] - info  
[main:zookeeperservermain@96] - starting server
2018-09-03 10:32:39,567 [myid:] - info  [main:environment@100] - 
server environment:zookeeper.version=3.4.9-1757313, built on 
08/23/2016 06:50 gmt
2018-09-03 10:32:39,567 [myid:] - info  [main:environment@100] - 
server environment:host.name=zookeeper-7594d99b-sgm6p
2018-09-03 10:32:39,567 [myid:] - info  [main:environment@100] - 
server environment:java.version=1.7.0_65
2018-09-03 10:32:39,567 [myid:] - info  [main:environment@100] - 
server environment:java.vendor=oracle corporation
2018-09-03 10:32:39,567 [myid:] - info  [main:environment@100] - 
server environment:java.home=/usr/lib/jvm/java-7-openjdk-amd64/jre
2018-09-03 10:32:39,567 [myid:] - info  [main:environment@100] - 
server environment:java.class.path=/opt/zookeeper- 
3.4.9/bin/../build/classes:/opt/zookeeper- 
3.4.9/bin/../build/lib/*.jar:/opt/zookeeper-3.4.9/bin/../lib/slf4j- 
log4j12-1.6.1.jar:/opt/zookeeper-3.4.9/bin/../lib/slf4j-api-1.6. 
1.ja r:/opt/zookeeper-3.4.9/bin/../lib/netty- 
3.10.5.final.jar:/opt/zookeeper-3.4.9/bin/../lib/log4j- 
1.2.16.jar:/opt/zookeeper-3.4.9/bin/../lib/jline- 
0.9.94.jar:/opt/zookeeper-3.4.9/bin/../zookeeper- 
3.4.9.jar:/opt/zookeeper- 
3.4.9/bin/../src/java/lib/*.jar:/opt/zookeeper-3.4.9/bin/../conf:

2018-09-03 10:32:39,567 [myid:] - info  [main:environment@100] - 
server environment:java.io.tmpdir=/tmp
2018-09-03 10:32:39,569 [myid:] - info  [main:environment@100] - 
server environment:java.compiler=&lt;na&gt;
2018-09-03 10:32:39,569 [myid:] - info  [main:environment@100] - 
server environment:os.name=linux
2018-09-03 10:32:39,569 [myid:] - info  [main:environment@100] - 
server environment:os.arch=amd64 
2018-09-03 10:32:39,569 [myid:] - info  [main:environment@100] - 
server environment:os.version=4.15.0-20-generic
2018-09-03 10:32:39,569 [myid:] - info  [main:environment@100] -     
server environment:user.name=root
2018-09-03 10:32:39,569 [myid:] - info  [main:environment@100] - 
server environment:user.home=/root
2018-09-03 10:32:39,569 [myid:] - info  [main:environment@100] - 
server environment:user.dir=/opt/zookeeper-3.4.9
2018-09-03 10:32:39,570 [myid:] - info  [main:zookeeperserver@815] 
- 
ticktime set to 2000
2018-09-03 10:32:39,571 [myid:] - info  [main:zookeeperserver@824] 
- 
minsessiontimeout set to -1
2018-09-03 10:32:39,571 [myid:] - info  [main:zookeeperserver@833] 
- 
maxsessiontimeout set to -1
2018-09-03 10:32:39,578 [myid:] - info  
[main:nioservercnxnfactory@89] 
- binding to port 0.0.0.0/0.0.0.0:2181 


edit:
starting logs from kafka container:

excluding kafka_home from broker config
[configuring] 'advertised.host.name' in 
'/opt/kafka/config/server.properties'
[configuring] 'port' in '/opt/kafka/config/server.properties'
[configuring] 'broker.id' in '/opt/kafka/config/server.properties'
excluding kafka_version from broker config
[configuring] 'zookeeper.connect' in 
'/opt/kafka/config/server.properties'
[configuring] 'log.dirs' in '/opt/kafka/config/server.properties'
[configuring] 'zookeeper.connect.timeout.ms' in 
'/opt/kafka/config/server.properties'
 [2018-09-05 10:47:22,036] info registered 
kafka:type=kafka.log4jcontroller mbean 
(kafka.utils.log4jcontrollerregistration$) 
[2018-09-05 10:47:23,145] info starting (kafka.server.kafkaserver)
[2018-09-05 10:47:23,148] info connecting to zookeeper on 
zookeeper:2181 (kafka.server.kafkaserver)
[2018-09-05 10:47:23,288] info [zookeeperclient] initializing a new 
session to zookeeper:2181. (kafka.zookeeper.zookeeperclient)
[2018-09-05 10:47:23,300] info client 
environment:zookeeper.version=3.4.13- 
2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 
gmt (org.apache.zookeeper.zookeeper)
[2018-09-05 10:47:23,300] info client environment:host.name=kafka 
-757dc6c47b-zpzfz (org.apache.zookeeper.zookeeper)
[2018-09-05 10:47:23,300] info client 
environment:java.version=1.8.0_171 (org.apache.zookeeper.zookeeper)
[2018-09-05 10:47:23,301] info client 
environment:java.vendor=oracle corporation 
(org.apache.zookeeper.zookeeper)
[2018-09-05 10:47:23,301] info client 
environment:java.home=/usr/lib/jvm/java-1.8-openjdk/jre 
(org.apache.zookeeper.zookeeper)
[2018-09-05 10:47:23,301] info client 
environment:java.class.path=/opt/kafka/bin/../libs/activation- 
1.1.1.jar:/opt/kafka/bin/../libs/aopalliance-repackaged-2.5.0- 
b42.jar:/opt/kafka/bin/../libs/argparse4j- 
0.7.0.jar:/opt/kafka/bin/../libs/audience-annotations- 
0.5.0.jar:/opt/kafka/bin/../libs/commons-lang3- 
3.5.jar:/opt/kafka/bin/../libs/connect-api- 
2.0.0.jar:/opt/kafka/bin/../libs/connect-basic-auth-extension- 
2.0.0.jar:/opt/kafka/bin/../libs/connect-file- 
2.0.0.jar:/opt/kafka/bin/../libs/connect-json- 
2.0.0.jar:/opt/kafka/bin/../libs/connect-runtime- 
2.0.0.jar:/opt/kafka/bin/../libs/connect-transforms- 
2.0.0.jar:/opt/kafka/bin/../libs/guava- 
20.0.jar:/opt/kafka/bin/../libs/hk2-api-2.5.0- 
b42.jar:/opt/kafka/bin/../libs/hk2-locator-2.5.0- 
b42.jar:/opt/kafka/bin/../libs/hk2-utils-2.5.0- 
b42.jar:/opt/kafka/bin/../libs/jackson-annotations- 
2.9.6.jar:/opt/kafka/bin/../libs/jackson-core- 
2.9.6.jar:/opt/kafka/bin/../libs/jackson-databind- 
2.9.6.jar:/opt/kafka/bin/../libs/jackson-jaxrs-json-provider- 
2.9.6.jar:/opt/kafka/bin/../libs/jackson-module-jaxb-annotations- 
cr2.jar:/opt/kafka/bin/../libs/javax.annotation-api- 
1.2.jar:/opt/kafka/bin/../libs/javax.inject- 
1.jar:/opt/kafka/bin/../libs/javax.inject-2.5.0- 
b42.jar:/opt/kafka/bin/../libs/javax.servlet-api- 
3.1.0.jar:/opt/kafka/bin/../libs/javax.ws.rs-api- 
2.1.jar:/opt/kafka/bin/../libs/jaxb-api- 
2.3.0.jar:/opt/kafka/bin/../libs/jersey-client- 
2.27.jar:/opt/kafka/bin/../libs/jersey-common- 
2.27.jar:/opt/kafka/bin/../libs/jersey-container-servlet 
-2.27.jar:/opt/kafka/bin/../libs/jersey-container-servlet-core- 
2.27.jar:/opt/kafka/bin/../libs/jersey-hk2- 
2.27.jar:/opt/kafka/bin/../libs/jersey-media-jaxb- 
2.27.jar:/opt/kafka/bin/../libs/jersey-server 
-2.27.jar:/opt/kafka/bin/../libs/jetty-client 
-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-continuation- 
9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-http- 
9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-io- 
9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-security- 
9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-server- 
9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-servlet- 
9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-servlets- 
9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-util- 
9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jopt-simple- 
5.0.4.jar:/opt/kafka/bin/../libs/kafka-clients- 
2.0.0.jar:/opt/kafka/bin/../libs/kafka-log4j-appender- 
2.0.0.jar:/opt/kafka/bin/../libs/kafka-streams- 
2.0.0.jar:/opt/kafka/bin/../libs/kafka-streams-examples- 
2.0.0.jar:/opt/kafka/bin/../libs/kafka-streams-scala_2.11- 
2.0.0.jar:/opt/kafka/bin/../libs/kafka-streams-test-utils- 
2.0.0.jar:/opt/kafka/bin/../libs/kafka-tools- 
2.0.0.jar:/opt/kafka/bin/../libs/kafka_2.11-2.0.0- 
sources.jar:/opt/kafka/bin/../libs/kafka_2.11-2 
 .0.0.jar:/opt/kafka/bin/../libs/log4j 
1.2.17.jar:/opt/kafka/bin/../libs/lz4-java- 
1.4.1.jar:/opt/kafka/bin/../libs/maven-artifact- 
3.5.3.jar:/opt/kafka/bin/../libs/metrics-core- 
2.2.0.jar:/opt/kafka/bin/../libs/osgi-resource-locator- 
1.0.1.jar:/opt/kafka/bin/../libs/plexus-utils- 
3.1.0.jar:/opt/kafka/bin/../libs/reflections- 
0.9.11.jar:/opt/kafka/bin/../libs/rocksdbjni- 
5.7.3.jar:/opt/kafka/bin/../libs/scala-library- 
2.11.12.jar:/opt/kafka/bin/../libs/scala-logging_2.11- 
3.9.0.jar:/opt/kafka/bin/../libs/scala-reflect- 
2.11.12.jar:/opt/kafka/bin/../libs/slf4j-api- 
1.7.25.jar:/opt/kafka/bin/../libs/slf4j-log4j12- 
1.7.25.jar:/opt/kafka/bin/../libs/snappy-java- 
1.1.7.1.jar:/opt/kafka/bin/../libs/validation-api- 
1.1.0.final.jar:/opt/kafka/bin/../libs/zkclient- 
0.10.jar:/opt/kafka/bin/../libs/zookeeper-3.4.13.jar 
(org.apache.zookeeper.zookeeper) 


output for kubectl get svc -o wide is as follows:

name         type        cluster-ip      external-ip   port(s)    age       selector
kubernetes   clusterip   10.96.0.1       &lt;none&gt;        443/tcp    50m       &lt;none&gt;
zookeeper    clusterip   10.98.180.138   &lt;none&gt;        2181/tcp   48m       io.kompose.service=zookeeper


output from kubectl get pods -o wide:

name                       ready     status             restarts   age       ip           node
kafka-757dc6c47b-zpzfz     0/1       crashloopbackoff   15         1h        10.32.0.17   administrator-thinkpad-l480
zookeeper-7594d99b-784n9   1/1       running            0          1h        10.32.0.19   administrator-thinkpad-l480


edit:
output from kubectl describe pod kafka-757dc6c47b-zpzfz:

name:           kafka-757dc6c47b-zpzfz
namespace:      default
node:           administrator-thinkpad-l480/10.11.17.86
start time:     wed, 05 sep 2018 16:17:06 +0530
labels:         io.kompose.service=kafka
            pod-template-hash=3138727036
annotations:    &lt;none&gt;
status:         running
ip:             10.32.0.17
controlled by:  replicaset/kafka-757dc6c47b
containers:
  kafka:
   container id:docker://2bdc06d876ae23437c61f4e95539a67903cdb61e88fd9c68377b47c7705293a3
    image:          wurstmeister/kafka
    image id:       docker-pullable://wurstmeister/kafka@sha256:2e3ff64e70ea983530f590282f36991c0a1b105350510f53cc3d1a0279b83c28
    port:           9092/tcp
    state:          waiting
      reason:       crashloopbackoff
    last state:     terminated
      reason:       error
      exit code:    1
      started:      wed, 05 sep 2018 17:29:06 +0530
      finished:     wed, 05 sep 2018 17:29:14 +0530
    ready:          false
    restart count:  18
    environment:
      kafka_advertised_host_name:          kafka
      kafka_zookeeper_connect:             zookeeper:2181
      kafka_port:                          9092
      kafka_zookeeper_connect_timeout_ms:  160000
    mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-nhb9z (ro)
conditions:
  type              status
  initialized       true 
  ready             false 
  containersready   false 
  podscheduled      true 
volumes:
  default-token-nhb9z:
    type:        secret (a volume populated by a secret)
    secretname:  default-token-nhb9z
    optional:    false
qos class:       besteffort
node-selectors:  &lt;none&gt;
tolerations:     node.kubernetes.io/not-ready:noexecute for 300s
             node.kubernetes.io/unreachable:noexecute for 300s
events:
  type     reason   age                from                                  message
  ----     ------   ----               ----                                  -------
  warning  backoff  3m (x293 over 1h)  kubelet, administrator-thinkpad-l480  back-off restarting failed container

",<docker><kubernetes><apache-kafka><apache-zookeeper><google-kubernetes-engine>,52179517,10,"
  what could be the reason for this ? and solutions ?


the reason is hidden behind following log line:

info connecting to zookeeper on zookeeper:2181 (kafka.server.kafkaserver)


kafka is searching for zookeeper while it should search for your service name which is zookeeper-svc.

the solution is then straightforward: to either rename your zookeeper service from zookeeper-svc to zookeeper in your zookeeper-service.yaml like so:

apiversion: v1
kind: service
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.1.0 (36652f6)
  creationtimestamp: null
  labels:
    io.kompose.service: zookeeper
  name: zookeeper
spec:
  ports:
  - name: ""2181""
    port: 2181
    targetport: 2181
  selector:
    io.kompose.service: zookeeper
status:
  loadbalancer: {}


or, more clean, to keep service name as it is and reconfigure kafka to search for zookeeper-svc instead of zookeeper.

note: since your minimal example was enough to reproduce the issue (thank you, it is nice to see working minimal example!), although pod is up and running (not in error state) it still shows: java.io.ioexception: can't resolve address: kafka:9092 which is tied to another issue (service not covering kafka deployment) and is out of the scope of this question, just to let you know. 
"
64287430,permissions error trying to run prometheus on aws eks (fargate only) with efs,"i have an eks cluster that is fargate only. i really don't want to have to manage instances myself. i'd like to deploy prometheus to it - which requires a persistent volume. as of two months ago this should be possible with efs (managed nfs share)  i feel that i am almost there but i cannot figure out what the current issue is
what i have done:

set up an eks fargate cluster and a suitable fargate profile
set up an efs with an appropriate security group
installed the csi driver and validated the efs as per aws walkthough

all good so far
i set up the persistent volume claims (which i understand must be done statically) with:
kubectl apply -f pvc/

where
tree pvc/
pvc/
 two_pvc.yml
 ten_pvc.yml

and
cat pvc/*

apiversion: v1
kind: persistentvolume
metadata:
  name: efs-pv-two
spec:
  capacity:
    storage: 2gi
  volumemode: filesystem
  accessmodes:
    - readwriteonce
  persistentvolumereclaimpolicy: retain
  storageclassname: efs-sc
  csi:
    driver: efs.csi.aws.com
    volumehandle: fs-ec0e1234
apiversion: v1
kind: persistentvolume
metadata:
  name: efs-pv-ten
spec:
  capacity:
    storage: 8gi
  volumemode: filesystem
  accessmodes:
    - readwriteonce
  persistentvolumereclaimpolicy: retain
  storageclassname: efs-sc
  csi:
    driver: efs.csi.aws.com
    volumehandle: fs-ec0e1234

then
helm upgrade --install myrelease-helm-02 prometheus-community/prometheus \
    --namespace prometheus \
    --set alertmanager.persistentvolume.storageclass=&quot;efs-sc&quot;,server.persistentvolume.storageclass=&quot;efs-sc&quot;

what happens?
the prometheus alertmanager comes up fine with its pvc. so do the other pods for this deployment, but the prometheus server goes to crashloopbackoff with
invalid capacity 0 on filesystem

diagnostics
kubectl get pv -a
name                          capacity   access modes   reclaim policy   status     claim                                               storageclass   reason   age
efs-pv-ten                    8gi        rwo            retain           bound      prometheus/myrelease-helm-02-prometheus-server         efs-sc                  11m
efs-pv-two                    2gi        rwo            retain           bound      prometheus/myrelease-helm-02-prometheus-alertmanager   efs-sc                  11m

and
kubectl get pvc -a
namespace    name                                     status   volume       capacity   access modes   storageclass   age
prometheus   myrelease-helm-02-prometheus-alertmanager   bound    efs-pv-two   2gi        rwo            efs-sc         12m
prometheus   myrelease-helm-02-prometheus-server         bound    efs-pv-ten   8gi        rwo            efs-sc         12m

describe pod just shows 'error'
lastly, this (from a colleague):
level=info ts=2020-10-09t15:17:08.898z caller=main.go:346 msg=&quot;starting prometheus&quot; version=&quot;(version=2.21.0, branch=head, revision=e83ef207b6c2398919b69cd87d2693cfc2fb4127)&quot;
level=info ts=2020-10-09t15:17:08.898z caller=main.go:347 build_context=&quot;(go=go1.15.2, user=root@a4d9bea8479e, date=20200911-11:35:02)&quot;
level=info ts=2020-10-09t15:17:08.898z caller=main.go:348 host_details=&quot;(linux 4.14.193-149.317.amzn2.x86_64 #1 smp thu sep 3 19:04:44 utc 2020 x86_64 myrelease-helm-02-prometheus-server-85765f9895-vxrkn (none))&quot;
level=info ts=2020-10-09t15:17:08.898z caller=main.go:349 fd_limits=&quot;(soft=1024, hard=4096)&quot;
level=info ts=2020-10-09t15:17:08.898z caller=main.go:350 vm_limits=&quot;(soft=unlimited, hard=unlimited)&quot;
level=error ts=2020-10-09t15:17:08.901z caller=query_logger.go:87 component=activequerytracker msg=&quot;error opening query log file&quot; file=/data/queries.active err=&quot;open /data/queries.active: permission denied&quot;
panic: unable to create mmap-ed active query log
goroutine 1 [running]:
github.com/prometheus/prometheus/promql.newactivequerytracker(0x7fffeb6e85ee, 0x5, 0x14, 0x30ca080, 0xc000d43620, 0x30ca080)
    /app/promql/query_logger.go:117 +0x4cf
main.main()
    /app/cmd/prometheus/main.go:377 +0x510c

beyond the appearance of a permissions issue i am baffled - i know that the storage 'works' and is accessible - the other pod in the deployment seems happy with it - but not this one.
",<amazon-web-services><kubernetes><prometheus><amazon-eks><amazon-efs>,64309008,10,"working now - and writing up here for the common good. thanks to /u/emiikhaos on reddit for the suggestions where to look
problem:
efs shares are root:root only and prometheus forbids running pods as root.
solution:

create an efs access point for each pod requiring a persistent
volume to permit access for a specified user.
specify these access points for the persistent volumes
apply a suitable security context to run the pods as the matching user

method:
create 2x efs access points something like:
{
    &quot;name&quot;: &quot;prometheuserver&quot;,
    &quot;accesspointid&quot;: &quot;fsap-&lt;hex01&gt;&quot;,
    &quot;filesystemid&quot;: &quot;fs-ec0e1234&quot;,
    &quot;posixuser&quot;: {
        &quot;uid&quot;: 500,
        &quot;gid&quot;: 500,
        &quot;secondarygids&quot;: [
            2000
        ]
    },
    &quot;rootdirectory&quot;: {
        &quot;path&quot;: &quot;/prometheuserver&quot;,
        &quot;creationinfo&quot;: {
            &quot;owneruid&quot;: 500,
            &quot;ownergid&quot;: 500,
            &quot;permissions&quot;: &quot;0755&quot;
        }
    }
},
{
    &quot;name&quot;: &quot;prometheusalertmanager&quot;,
    &quot;accesspointid&quot;: &quot;fsap-&lt;hex02&gt;&quot;,
    &quot;filesystemid&quot;: &quot;fs-ec0e1234&quot;,
    &quot;posixuser&quot;: {
        &quot;uid&quot;: 501,
        &quot;gid&quot;: 501,
        &quot;secondarygids&quot;: [
            2000
        ]
    },
    &quot;rootdirectory&quot;: {
        &quot;path&quot;: &quot;/prometheusalertmanager&quot;,
        &quot;creationinfo&quot;: {
            &quot;owneruid&quot;: 501,
            &quot;ownergid&quot;: 501,
            &quot;permissions&quot;: &quot;0755&quot;
        }
    }
}

update my persistent volumes:
kubectl apply -f pvc/

to something like:
apiversion: v1
kind: persistentvolume
metadata:
  name: prometheusalertmanager
spec:
  capacity:
    storage: 2gi
  volumemode: filesystem
  accessmodes:
    - readwriteonce
  persistentvolumereclaimpolicy: retain
  storageclassname: efs-sc
  csi:
    driver: efs.csi.aws.com
    volumehandle: fs-ec0e1234::fsap-&lt;hex02&gt;
---    
apiversion: v1
kind: persistentvolume
metadata:
  name: prometheusserver
spec:
  capacity:
    storage: 8gi
  volumemode: filesystem
  accessmodes:
    - readwriteonce
  persistentvolumereclaimpolicy: retain
  storageclassname: efs-sc
  csi:
    driver: efs.csi.aws.com
    volumehandle: fs-ec0e1234::fsap-&lt;hex01&gt;

re-install prometheus as before:
helm upgrade --install myrelease-helm-02 prometheus-community/prometheus \
    --namespace prometheus \
    --set alertmanager.persistentvolume.storageclass=&quot;efs-sc&quot;,server.persistentvolume.storageclass=&quot;efs-sc&quot;

take an educated guess from
kubectl describe pod myrelease-helm-02-prometheus-server -n prometheus

and
kubectl describe pod myrelease-helm-02-prometheus-alert-manager -n prometheus

as to which container needed to be specified when setting the security context. then apply security context to run pods with appropriate uid:gid, e.g. with
kubectl apply -f setpermissions/

where
cat setpermissions/*

gives
apiversion: v1
kind: pod
metadata:
  name: myrelease-helm-02-prometheus-alertmanager
spec:
  securitycontext:
    runasuser: 501
    runasgroup: 501
    fsgroup: 501
  volumes:
    - name: prometheusalertmanager
  containers:
    - name: prometheusalertmanager
      image: jimmidyson/configmap-reload:v0.4.0
      securitycontext:
        runasuser: 501
        allowprivilegeescalation: false        
apiversion: v1
kind: pod
metadata:
  name: myrelease-helm-02-prometheus-server
spec:
  securitycontext:
    runasuser: 500
    runasgroup: 500
    fsgroup: 500
  volumes:
    - name: prometheusserver
  containers:
    - name: prometheusserver
      image: jimmidyson/configmap-reload:v0.4.0
      securitycontext:
        runasuser: 500
        allowprivilegeescalation: false

"
65979766,"ingress with nginx controller not working, address missing","i have a kubernetes cluster running on a 1 master, 2 worker setup ob linux servers. i have a haproxy forwarding my requests to nginx controllers. my complete setup is behind a corporate proxy. the dns entry is enabled in this corporate proxy.
requests will get to the nginx controller, but wont be forwarded to the service.
i installed the ingress controller as descibed by many tutorials with the files in https://github.com/kubernetes/ingress-nginx .
i'm new to stack overflow, so if i should give more specific information just let me know. i hope someone can help me with my issue, thank you in advance :d
my ingress with missing address:
name:             app-ingress
namespace:        default
address:
default backend:  default-http-backend:80 (&lt;none&gt;)
rules:
  host                       path  backends
  ----                       ----  --------
  test.kubetest.lff.bybn.de
                             /abc   app-service:80 (10.244.2.4:3000)
annotations:                 kubernetes.io/ingress.class: nginx
events:                      &lt;none&gt;

yaml files of deployment, service and ingress, ingressclass, configmap
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    run: app
  name: app-blue
spec:
  replicas: 1
  selector:
    matchlabels:
      run: app
      version: 0.0.1
  template:
    metadata:
      labels:
        run: app
        version: 0.0.1
    spec:
      containers:
      - name: app
        image: errm/versions:0.0.1
        ports:
        - containerport: 3000
----



apiversion: v1
kind: service
metadata:
  name: app-service
spec:
  selector:
    run: app
    version: 0.0.1
  ports:
  - name: http
    port: 80
    protocol: tcp
    targetport: 3000
---
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: app-ingress
  namespace: default
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: test.kubetest.lff.bybn.de
    http:
      paths:
      - path: /abc
        backend:
          servicename: app-service
          serviceport: 80
---

apiversion: networking.k8s.io/v1beta1
kind: ingressclass
metadata:
  name: nginx
  # annotations:
  #   ingressclass.kubernetes.io/is-default-class: &quot;true&quot;
spec:
  controller: nginx.org/ingress-controller
---

kind: configmap
apiversion: v1
metadata:
  name: nginx-config
  namespace: nginx-ingress
data:

curl from outside of the cluster and logs from controller pod
curl test.kubetest.lff.bybn.de/abc
% total    % received % xferd  average speed   time    time     time  current
                                 dload  upload   total   spent    left  speed
100    93    0    93    0     0      1      0 --:--:--  0:00:50 --:--:--    26&lt;html&gt;&lt;body&gt;&lt;h1&gt;504 gateway time-out&lt;/h1&gt;
the server didn't respond in time.
&lt;/body&gt;&lt;/html&gt;



e0131 19:44:11.949261       1 reflector.go:138] /home/runner/work/kubernetes-ingress/kubernetes-ingress/internal/k8s/controller.go:574: failed to watch *v1.policy: failed to list *v1.policy: the server could not find the requested resource (get policies.k8s.nginx.org)
e0131 19:45:06.894791       1 reflector.go:138] /home/runner/work/kubernetes-ingress/kubernetes-ingress/internal/k8s/controller.go:574: failed to watch *v1.policy: failed to list *v1.policy: the server could not find the requested resource (get policies.k8s.nginx.org)
e0131 19:45:48.532075       1 reflector.go:138] /home/runner/work/kubernetes-ingress/kubernetes-ingress/internal/k8s/controller.go:574: failed to watch *v1.policy: failed to list *v1.policy: the server could not find the requested resource (get policies.k8s.nginx.org)
10.48.25.57 - - [31/jan/2021:19:46:35 +0000] &quot;get /abc http/1.1&quot; 499 0 &quot;-&quot; &quot;curl/7.73.0&quot; &quot;-&quot;
e0131 19:46:37.902444       1 reflector.go:138] /home/runner/work/kubernetes-ingress/kubernetes-ingress/internal/k8s/controller.go:574: failed to watch *v1.policy: failed to list *v1.policy: the server could not find the requested resource (get policies.k8s.nginx.org)
e0131 19:47:15.346193       1 reflector.go:138] /home/runner/work/kubernetes-ingress/kubernetes-ingress/internal/k8s/controller.go:574: failed to watch *v1.policy: failed to list *v1.policy: the server could not find the requested resource (get policies.k8s.nginx.org)
e0131 19:47:48.536636       1 reflector.go:138] /home/runner/work/kubernetes-ingress/kubernetes-ingress/internal/k8s/controller.go:574: failed to watch *v1.policy: failed to list *v1.policy: the server could not find the requested resource (get policies.k8s.nginx.org)
e0131 19:48:21.890770       1 reflector.go:138] /home/runner/work/kubernetes-ingress/kubernetes-ingress/internal/k8s/controller.go:574: failed to watch *v1.policy: failed to list *v1.policy: the server could not find the requested resource (get policies.k8s.nginx.org)

",<nginx><kubernetes><kubernetes-ingress><nginx-ingress>,65980273,10,"looking at the ingress definition, i see that it misses the ingress class. either you defined an ingressclass annotated as the default class to use, or that may be the reason your ingress is not working, at the moment.
an ingress class is basically a category which specify who needs to serve and manage the ingress, this is necessary since in a cluster you can have more than one ingress controller, each one with its rules and configurations.
depending on the kubernetes version, the ingress class can be defined with an annotation on the ingress (before v1.18) such as:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: example-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  ...

or with a whole resource and then referred into the ingress as shown in the documentation (https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-class)
even in new versions of kubernetes, the old annotation may still be supported, depends on the controller.
if you are unsure on what ingress class you should use, that should be defined by the controller, you probably decided one when you installed it or you used the default one (which most of the times is nginx)
"
66485722,how to implement kubernetes horizontal pod autoscaling with scale up/down policies?,"kubernetes v1.19 in aws eks
i'm trying to implement horizontal pod autoscaling in my eks cluster, and am trying to mimic what we do now with ecs. with ecs, we do something similar to the following

scale up when cpu &gt;= 90% after 3 consecutive 1-min periods of sampling
scale down when cpu &lt;= 60% after 5 consecutive 1-min periods of sampling
scale up when memory &gt;= 85% after 3 consecutive 1-min periods of sampling
scale down when memory &lt;= 70% after 5 consecutive 1-min periods of sampling

i'm trying to use the horizontalpodautoscaler kind, and helm create gives me this template. (note i modified it to suit my needs, but the metrics stanza remains.)
{- if .values.autoscaling.enabled }}
apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  name: {{ include &quot;microservicechart.name&quot; . }}
  labels:
    {{- include &quot;microservicechart.name&quot; . | nindent 4 }}
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: {{ include &quot;microservicechart.name&quot; . }}
  minreplicas: {{ include &quot;microservicechart.minreplicas&quot; . }}
  maxreplicas: {{ include &quot;microservicechart.maxreplicas&quot; . }}
  metrics:
    {{- if .values.autoscaling.targetcpuutilizationpercentage }}
    - type: resource
      resource:
        name: cpu
        targetaverageutilization: {{ .values.autoscaling.targetcpuutilizationpercentage }}
    {{- end }}
    {{- if .values.autoscaling.targetmemoryutilizationpercentage }}
    - type: resource
      resource:
        name: memory
        targetaverageutilization: {{ .values.autoscaling.targetmemoryutilizationpercentage }}
    {{- end }}
{{- end }}

however, how do i fit the scale up/down information shown in horizontal pod autoscaling in the above template, to match the behavior that i want?
",<kubernetes><kubernetes-helm><amazon-eks><hpa>,66526685,10,"the horizontal pod autoscaler automatically scales the number of pods in a replication controller, deployment, replica set or stateful set based on observed metrics (like cpu or memory).
there is an official walkthrough focusing on hpa and it's scaling:

kubernetes.io: docs: tasks: run application: horizontal pod autoscale: walkthrough


the algorithm that scales the amount of replicas is the following:

desiredreplicas = ceil[currentreplicas * ( currentmetricvalue / desiredmetricvalue )]

an example (of already rendered) autoscaling can be implemented with a yaml manifest like below:
apiversion: autoscaling/v2beta2
kind: horizontalpodautoscaler
metadata:
  name: hpa-name
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: deployment-name
  minreplicas: 1
  maxreplicas: 10
  metrics:
  - type: resource
    resource:
      name: cpu
      target:
        type: utilization
        averageutilization: 75
  - type: resource
    resource:
      name: memory
      target:
        type: utilization
        averageutilization: 75


a side note!
hpa will use calculate both metrics and chose the one with bigger desiredreplicas!

addressing a comment i wrote under the question:

i think we misunderstood each other. it's perfectly okay to &quot;scale up when cpu &gt;= 90&quot; but due to logic behind the formula i don't think it will be possible to say &quot;scale down when cpu &lt;=70&quot;. according to the formula it would be something in the midst of: scale up when cpu &gt;= 90 and scale down when cpu =&lt; 45.

this example could be misleading and not 100% true in all scenarios. taking a look on following example:

hpa set to averageutilization of 75%.

quick calculations with some degree of approximation (default tolerance for hpa is 0.1):

2 replicas:

scale-up (by 1) should happen when: currentmetricvalue is &gt;=80%:

x = ceil[2 * (80/75)], x = ceil[2,1(3)], x = 3


scale-down (by 1) should happen when currentmetricvalue is &lt;=33%:

x = ceil[2 * (33/75)], x = ceil[0,88], x = 1




8 replicas:

scale-up (by 1) should happen when currentmetricvalue is &gt;=76%:

x = ceil[8 * (76/75)], x = ceil[8,10(6)], x = 9


scale-down (by 1) should happen when currentmetricvalue is &lt;=64%:

x = ceil[8 * (64/75)], x = ceil[6,82(6)], x = 7





following this example, having 8 replicas with their currentmetricvalue at 55 (desiredmetricvalue set to 75) should scale-down to 6 replicas.
more information that describes the decision making of hpa (for example why it's doesn't scale) can be found by running:

$ kubectl describe hpa hpa-name

name:                                                     nginx-scaler
namespace:                                                default
labels:                                                   &lt;none&gt;
annotations:                                              &lt;none&gt;
creationtimestamp:                                        sun, 07 mar 2021 22:48:58 +0100
reference:                                                deployment/nginx-scaling
metrics:                                                  ( current / target )
  resource memory on pods  (as a percentage of request):  5% (61903667200m) / 75%
  resource cpu on pods  (as a percentage of request):     79% (199m) / 75%
min replicas:                                             1
max replicas:                                             10
deployment pods:                                          5 current / 5 desired
conditions:
  type            status  reason              message
  ----            ------  ------              -------
  abletoscale     true    readyfornewscale    recommended size matches current size
  scalingactive   true    validmetricfound    the hpa was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  scalinglimited  false   desiredwithinrange  the desired count is within the acceptable range
events:
  type     reason                   age                   from                       message
  ----     ------                   ----                  ----                       -------
  warning  failedgetresourcemetric  4m48s (x4 over 5m3s)  horizontal-pod-autoscaler  did not receive metrics for any ready pods
  normal   successfulrescale        103s                  horizontal-pod-autoscaler  new size: 2; reason: cpu resource utilization (percentage of request) above target
  normal   successfulrescale        71s                   horizontal-pod-autoscaler  new size: 4; reason: cpu resource utilization (percentage of request) above target
  normal   successfulrescale        71s                   horizontal-pod-autoscaler  new size: 5; reason: cpu resource utilization (percentage of request) above target


hpa scaling procedures can be modified by the changes introduced in kubernetes version 1.18 and newer where the:

support for configurable scaling behavior
starting from v1.18 the v2beta2 api allows scaling behavior to be configured through the hpa behavior field. behaviors are specified separately for scaling up and down in scaleup or scaledown section under the behavior field. a stabilization window can be specified for both directions which prevents the flapping of the number of the replicas in the scaling target. similarly specifying scaling policies controls the rate of change of replicas while scaling.
kubernetes.io: docs: tasks: run application: horizontal pod autoscale: support for configurable scaling behavior

i'd reckon you could used newly introduced field like behavior and stabilizationwindowseconds to tune your workload to your specific needs.
i also do recommend reaching out to eks documentation for more reference, support for metrics and examples.
"
60324528,"""services is forbidden: user \""system:serviceaccount:tick:external-dns\"" cannot list resource \""services\"" in api group \""\"" at the cluster scope""","i've been following the walkthrough to create an aws alb ingress controller for my app which is also deployed at an eks cluster.  
everything seems okay , similar answers with the walkthrough but when it comes to the setting up of an external dns i  get the error :

kubectl logs -f $(kubectl get po | egrep -o 'external-dns[a-za-z0-9-]+')



  time=""2020-02-20t16:21:57z"" level=error msg=""services is forbidden:
  user \""system:serviceaccount:tick:external-dns\"" cannot list resource
  \""services\"" in api group \""\"" at the cluster scope""
  time=""2020-02-20t16:22:58z"" level=error msg=""services is forbidden:
  user \""system:serviceaccount:tick:external-dns\"" cannot list resource
  \""services\"" in api group \""\"" at the cluster scope""


every one minute .
i made sure that all the permissions are the needed ones so it should not be because of that. 

i tried the solutions from here , but nothing helped and i couldn't find any other solutions. 

what does this error practically means? what should i do to fix it?

update edit 
my external-dns configuration looks like:

apiversion: v1
kind: serviceaccount
metadata:
  name: external-dns
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::*my*account*id*:role/eksrole
---
apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrole
metadata:
  name: external-dns
rules:
- apigroups: [""""]
  resources: [""services""]
  verbs: [""get"",""watch"",""list""]
- apigroups: [""""]
  resources: [""pods""]
  verbs: [""get"",""watch"",""list""]
- apigroups: [""extensions""]
  resources: [""ingresses""]
  verbs: [""get"",""watch"",""list""]
- apigroups: [""""]
  resources: [""nodes""]
  verbs: [""list"",""watch""]
---
apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrolebinding
metadata:
  name: external-dns-viewer
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: external-dns
subjects:
- kind: serviceaccount
  name: external-dns
  namespace: tick
---
apiversion: apps/v1
kind: deployment
metadata:
  name: external-dns
spec:
  selector:
    matchlabels:
      app: external-dns
  strategy:
    type: recreate
  template:
    metadata:
      labels:
        app: external-dns
      annotations:
        iam.amazonaws.com/role: arn:aws:iam::*my*account*id*:role/eksrole
    spec:
      serviceaccountname: external-dns
      containers:
      - name: external-dns
        image: registry.opensource.zalan.do/teapot/external-dns:v0.5.9
        args:
        - --source=service
        - --source=ingress
        - --domain-filter=external-dns-test.my-org.com   #external-dns-test.my-org.com # will make externaldns see only the hosted zones matching provided domain, omit to process all available hosted zones
        - --provider=aws
        - --policy=upsert-only # would prevent externaldns from deleting any records, omit to enable full synchronization
        - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)
        - --registry=txt
        - --txt-owner-id=my-identifier
      securitycontext:
        fsgroup: 65534

",<kubernetes><kubernetes-ingress><aws-alb><external-dns>,60334649,9,"your error suggests that service account with name external-dns in tick namespace can't perform certain actions. in this case it is list services. to solve this you can apply the following:

apiversion: v1
kind: serviceaccount
metadata:
  name: external-dns
  namespace: tick
---
apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrole
metadata:
  name: external-dns-role
rules:
- apigroups: [""""]
  resources: [""services""]
  verbs: [""get"",""watch"",""list""]
- apigroups: [""""]
  resources: [""pods""]
  verbs: [""get"",""watch"",""list""]
- apigroups: [""extensions""]
  resources: [""ingresses""]
  verbs: [""get"",""watch"",""list""]
- apigroups: [""""]
  resources: [""nodes""]
  verbs: [""list"",""watch""]
---
apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrolebinding
metadata:
  name: external-dns-role-binding
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: external-dns-role
subjects:
- kind: serviceaccount
  name: external-dns
  namespace: tick


note, that first rule inside clusterrole is granting correct permissions to list services in """" apigroup, which is solving the error you have reported in your question.
"
64624877,cert-manager certificate creation stuck at created new certificaterequest resource,"i am using cert-manager v1.0.0 on gke, i tried to use the staging environment for acme and it worked fine but when shifting to production i can find the created certificate stuck at created new certificaterequest resource and nothing changes after that
i expect to see the creation of the certificate to be succeeded and change the status of the certificate from false to true as happens in staging
environment details::
kubernetes version (v1.18.9):
cloud-provider/provisioner (gke):
cert-manager version (v1.0.0):
install method (helm)
here is my clusterissuer yaml file
apiversion: cert-manager.io/v1

kind: clusterissuer

metadata:
  name: i-storage-ca-issuer-prod
  namespace: default
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: my_email_here
    privatekeysecretref:
      name: i-storage-ca-issuer-prod
    solvers:
    - http01:
        ingress:
          class: gce

and here is my ingress yaml file
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: i-storage-core
  namespace: i-storage
  annotations:
    kubernetes.io/ingress.global-static-ip-name: i-storage-core-ip
    cert-manager.io/cluster-issuer: i-storage-ca-issuer-prod
  labels:
    app: i-storage-core
spec:
  tls:
  - hosts:
    - i-storage.net
    secretname: i-storage-core-prod-cert
  rules:
  - host: i-storage.net
    http:
      paths:
      - path: /*
        backend:
          servicename: i-storage-core-service
          serviceport: 80

describe certificaterequest output
name:         i-storage-core-prod-cert-stb6l
namespace:    i-storage
labels:       app=i-storage-core
annotations:  cert-manager.io/certificate-name: i-storage-core-prod-cert
              cert-manager.io/certificate-revision: 1
              cert-manager.io/private-key-secret-name: i-storage-core-prod-cert-2pw26
api version:  cert-manager.io/v1
kind:         certificaterequest
metadata:
  creation timestamp:  2020-10-31t15:44:57z
  generate name:       i-storage-core-prod-cert-
  generation:          1
  managed fields:
    api version:  cert-manager.io/v1
    fields type:  fieldsv1
    fieldsv1:
      f:metadata:
        f:annotations:
          .:
          f:cert-manager.io/certificate-name:
          f:cert-manager.io/certificate-revision:
          f:cert-manager.io/private-key-secret-name:
        f:generatename:
        f:labels:
          .:
          f:app:
        f:ownerreferences:
          .:
          k:{&quot;uid&quot;:&quot;f3442651-3941-49af-81de-dcb937e8ba40&quot;}:
            .:
            f:apiversion:
            f:blockownerdeletion:
            f:controller:
            f:kind:
            f:name:
            f:uid:
      f:spec:
        .:
        f:issuerref:
          .:
          f:group:
          f:kind:
          f:name:
        f:request:
      f:status:
        .:
        f:conditions:
    manager:    controller
    operation:  update
    time:       2020-10-31t15:44:57z
  owner references:
    api version:           cert-manager.io/v1
    block owner deletion:  true
    controller:            true
    kind:                  certificate
    name:                  i-storage-core-prod-cert
    uid:                   f3442651-3941-49af-81de-dcb937e8ba40
  resource version:        18351251
  self link:               /apis/cert-manager.io/v1/namespaces/i-storage/certificaterequests/i-storage-core-prod-cert-stb6l
  uid:                     83412862-903f-4fff-a736-f170e840748e
spec:
  issuer ref:
    group:  cert-manager.io
    kind:   clusterissuer
    name:   i-storage-ca-issuer-prod
  request:  ls0tls1crudjtibdrvjusuzjq0fursbsrvfvrvnuls0tls0ktuljq2zuq0nbv1vdqvfbd0feq0nbu0l3rffzsktvwklodmnoqvffqkjrqurnz0vqqurdq0frb0nnz0vcqu5hcqovrdrvzlrhv0xfa01guzdsdvn1rmrlr0nnvjj4czrecg5pem1hbjjxslrutlbns2hhbgved0p2tkziatc5wwxhcmpycjhjndfhu1jut2u4uddus3avwxpbsutxsxppmlliehy5vza5bezdwwq4mtbymunsog5jb2nya3bgzlaxmzakzurlczz6sukwzw9ztw1urxq3cmrunk52dhhuz1zzvmlnai9vcxpxskz4nmlla0r6v1vhk3lncwtqm1zka1lyeapzufntnwzswxltdki4emdxb3pnnunjundra09ktu1arlnowhvxykpnznjvqmr2yw9nqwteymzysws0svriaxlyckv4udfbafdieghpbnddd2h5bxpgwmgzskzuzhhzefdtrdzjmmp3mzv1sxz1wwliwej4vtbcmg50k3fymvvwawwkskrlofdnctdjt3azwmtlt2fha0nbd0vbqwfbne1ewuddu3fhu0lim0rrrupeakvwtunjd0dbwurwujbsqkjfdwpeneloyvmxemrhoxlzv2rstg01bgrequxcz05wsfe4rujbtuncyuf3rffzsktvwklodmnoqvfftejrqurnz0vcckflmkhhseqxd3ddzvfqs1diu1n0sfkxmm1da1a1amq0rnfmzffyrg5xr3grk3fcwexgy0f4tvzhbvf2cstqk0gklzexqjhvdluydu9icgrhrktoak9andjsdjnnmvllrwk5ug5ns0rfdndcber0q0vsa0lhqzv4t1zencthevlmaapemui2l20vdejsdlhyns8zrdlyejjstwnrszrnstnvq3mxd0y0bmduq3jymehosdjeendhexi5d2qvy1v1clzlclloys9hzjcyaefccgqxsmkrr2hkagxzvdlgbtnvzvnuti9oykpvwmk4nkm1s1dtrw1dblnjv3dzwgnovw1vviskvhpgqmnhoehqouxsvfdjvvbsyvl0bfq2tehrujvluw1el2tjrtzdajlidtnxmg9owdz2uc9cq012swdatvzeugoyefvwy3lhumjad2ttwtq2mktnz25wut0kls0tls1ftkqgq0vsvelgsunbveugukvrvuvtvc0tls0tcg==
status:
  conditions:
    last transition time:  2020-10-31t15:44:57z
    message:               waiting on certificate issuance from order i-storage/i-storage-core-prod-cert-stb6l-177980933: &quot;pending&quot;
    reason:                pending
    status:                false
    type:                  ready
events:                    &lt;none&gt;

describe order output
name:         i-storage-core-prod-cert-stb6l-177980933
namespace:    i-storage
labels:       app=i-storage-core
annotations:  cert-manager.io/certificate-name: i-storage-core-prod-cert
              cert-manager.io/certificate-revision: 1
              cert-manager.io/private-key-secret-name: i-storage-core-prod-cert-2pw26
api version:  acme.cert-manager.io/v1
kind:         order
metadata:
  creation timestamp:  2020-10-31t15:44:57z
  generation:          1
  managed fields:
    api version:  acme.cert-manager.io/v1
    fields type:  fieldsv1
    fieldsv1:
      f:metadata:
        f:annotations:
          .:
          f:cert-manager.io/certificate-name:
          f:cert-manager.io/certificate-revision:
          f:cert-manager.io/private-key-secret-name:
        f:labels:
          .:
          f:app:
        f:ownerreferences:
          .:
          k:{&quot;uid&quot;:&quot;83412862-903f-4fff-a736-f170e840748e&quot;}:
            .:
            f:apiversion:
            f:blockownerdeletion:
            f:controller:
            f:kind:
            f:name:
            f:uid:
      f:spec:
        .:
        f:dnsnames:
        f:issuerref:
          .:
          f:group:
          f:kind:
          f:name:
        f:request:
      f:status:
        .:
        f:authorizations:
        f:finalizeurl:
        f:state:
        f:url:
    manager:    controller
    operation:  update
    time:       2020-10-31t15:44:57z
  owner references:
    api version:           cert-manager.io/v1
    block owner deletion:  true
    controller:            true
    kind:                  certificaterequest
    name:                  i-storage-core-prod-cert-stb6l
    uid:                   83412862-903f-4fff-a736-f170e840748e
  resource version:        18351252
  self link:               /apis/acme.cert-manager.io/v1/namespaces/i-storage/orders/i-storage-core-prod-cert-stb6l-177980933
  uid:                     92165d9c-e57e-4d6e-803d-5d28e8f3033a
spec:
  dns names:
    i-storage.net
  issuer ref:
    group:  cert-manager.io
    kind:   clusterissuer
    name:   i-storage-ca-issuer-prod
  request:  ls0tls1crudjtibdrvjusuzjq0fursbsrvfvrvnuls0tls0ktuljq2zuq0nbv1vdqvfbd0feq0nbu0l3rffzsktvwklodmnoqvffqkjrqurnz0vqqurdq0frb0nnz0vcqu5hcqovrdrvzlrhv0xfa01guzdsdvn1rmrlr0nnvjj4czrecg5pem1hbjjxslrutlbns2hhbgved0p2tkziatc5wwxhcmpycjhjndfhu1jut2u4uddus3avwxpbsutxsxppmlliehy5vza5bezdwwq4mtbymunsog5jb2nya3bgzlaxmzakzurlczz6sukwzw9ztw1urxq3cmrunk52dhhuz1zzvmlnai9vcxpxskz4nmlla0r6v1vhk3lncwtqm1zka1lyeapzufntnwzswxltdki4emdxb3pnnunjundra09ktu1arlnowhvxykpnznjvqmr2yw9nqwteymzysws0svriaxlyckv4udfbafdieghpbnddd2h5bxpgwmgzskzuzhhzefdtrdzjmmp3mzv1sxz1wwliwej4vtbcmg50k3fymvvwawwkskrlofdnctdjt3azwmtlt2fha0nbd0vbqwfbne1ewuddu3fhu0lim0rrrupeakvwtunjd0dbwurwujbsqkjfdwpeneloyvmxemrhoxlzv2rstg01bgrequxcz05wsfe4rujbtuncyuf3rffzsktvwklodmnoqvfftejrqurnz0vcckflmkhhseqxd3ddzvfqs1diu1n0sfkxmm1da1a1amq0rnfmzffyrg5xr3grk3fcwexgy0f4tvzhbvf2cstqk0gklzexqjhvdluydu9icgrhrktoak9andjsdjnnmvllrwk5ug5ns0rfdndcber0q0vsa0lhqzv4t1zencthevlmaapemui2l20vdejsdlhyns8zrdlyejjstwnrszrnstnvq3mxd0y0bmduq3jymehosdjeendhexi5d2qvy1v1clzlclloys9hzjcyaefccgqxsmkrr2hkagxzvdlgbtnvzvnuti9oykpvwmk4nkm1s1dtrw1dblnjv3dzwgnovw1vviskvhpgqmnhoehqouxsvfdjvvbsyvl0bfq2tehrujvluw1el2tjrtzdajlidtnxmg9owdz2uc9cq012swdatvzeugoyefvwy3lhumjad2ttwtq2mktnz25wut0kls0tls1ftkqgq0vsvelgsunbveugukvrvuvtvc0tls0tcg==
status:
  authorizations:
    challenges:
      token:        emtpmo_jt5ykitiwk_loul66xu_q38scnmf1o0lpgvs
      type:         http-01
      url:          https://acme-v02.api.letsencrypt.org/acme/chall-v3/8230128790/0ecdqa
      token:        emtpmo_jt5ykitiwk_loul66xu_q38scnmf1o0lpgvs
      type:         dns-01
      url:          https://acme-v02.api.letsencrypt.org/acme/chall-v3/8230128790/9chkyq
      token:        emtpmo_jt5ykitiwk_loul66xu_q38scnmf1o0lpgvs
      type:         tls-alpn-01
      url:          https://acme-v02.api.letsencrypt.org/acme/chall-v3/8230128790/barezw
    identifier:     i-storage.net
    initial state:  pending
    url:            https://acme-v02.api.letsencrypt.org/acme/authz-v3/8230128790
    wildcard:       false
  finalize url:     https://acme-v02.api.letsencrypt.org/acme/finalize/100748195/5939190036
  state:            pending
  url:              https://acme-v02.api.letsencrypt.org/acme/order/100748195/5939190036
events:             &lt;none&gt;

",<ssl><kubernetes><devops><kubernetes-ingress><cert-manager>,65809340,9,"list all certificates that you have:
kubectl get certificate --all-namespaces

try to figure out the problem using describe command:
kubectl describe certificate certificate_name -n your_namespace

the output of the above command contains the name of the associated certificate request. dig into more details using describe command once again:
kubectl describe certificaterequest certtificate_request_name -n your_namespace

you may also want to troubleshoot challenges with the following command:
kubectl describe challenges --all-namespaces

in my case, to make it work, i had to replace clusterissuer with just issuer for reasons explained in the comment.
here is my issuer manifest:
apiversion: cert-manager.io/v1
kind: issuer
metadata:
  name: cert-manager-staging
  namespace: your_namespace
spec:
  acme:
    # you must replace this email address with your own.
    # let's encrypt will use this to contact you about expiring
    # certificates, and issues related to your account.
    email: example@example.com
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    privatekeysecretref:
      # secret resource that will be used to store the account's private key.
      name: cert-manager-staging-private-key
    # add a single challenge solver, http01 using nginx
    solvers:
      - http01:
          ingress:
            class: nginx


here is my simple ingress manifest:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/issuer: cert-manager-staging
  name: your_name
  namespace: your_namespace
spec:
  tls:
    - hosts:
        - example.com
      secretname: example-com-staging-certificate
  rules:
    - host: example.com
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: example.com
                port:
                  number: 80

"
65236289,kubernetes create statefulset with image pull secret?,"for kubernetes deployment we can specify imagepullsecrets to allow it to pull docker images from our private registry. but as far as i can tell, statefulset doesn't support this?
how can i supply a pullsecret to my statefulset?
apiversion: apps/v1
kind: statefulset
metadata:
  name: redis
  namespace: {{ .values.namespace }}
  labels:
    app: redis
spec:
  replicas: 1
  selector:
    matchlabels:
      app: redis
  servicename: redis-service
  updatestrategy:
    type: rollingupdate
  template:
    metadata:
      labels:
        app: redis
    spec:
      terminationgraceperiodseconds: 10
      # imagepullsecrets not valid here for statefulset :-(
      containers:
        - image: {{ .values.image }}

",<kubernetes><kubernetes-helm>,65237772,9,"statefulset supports imagepullsecrets. you can check it as follows.
$ kubectl explain statefulset.spec.template.spec --api-version apps/v1
:
   imagepullsecrets &lt;[]object&gt;
     imagepullsecrets is an optional list of references to secrets in the same
     namespace to use for pulling any of the images used by this podspec. if
     specified, these secrets will be passed to individual puller
     implementations for them to use. for example, in the case of docker, only
     dockerconfig type secrets are honored. more info:
     https://kubernetes.io/docs/concepts/containers/images#specifying-imagepullsecrets-on-a-pod
:

for instance, you can try if the following sample statefulset can create in your cluster first.
$ kubectl create -f - &lt;&lt;eof
apiversion: apps/v1
kind: statefulset
metadata:
  name: web
spec:
  servicename: &quot;nginx&quot;
  replicas: 2
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      imagepullsecrets:
      - name: your-pull-secret-name
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerport: 80
          name: web
eof

$ kubectl get pod web-0 -o yaml | \
  grep -e '^[[:space:]]+imagepullsecrets:' -a1
  imagepullsecrets:
  - name: your-pull-secret-name

"
65045482,"in aws eks, how can i define ingress to use one alb for multiple subdomain urls, each with their own certificate?","i have multiple services that need to be exposed to the internet, but i'd like to use a single alb for them.
i am using the latest aws load balancer controller, and i've been reading the documentation here (https://kubernetes-sigs.github.io/aws-load-balancer-controller/guide/ingress/annotations/#traffic-routing), but i haven't found a clear explanation on how to achieve this.
here's the setup:
i have service-a.example.com -and- service-b.example.com. they each have their own certificates within amazon certificate manager.
within kubernetes, each has its own service object defined as follows (each unique):
apiversion: v1
kind: service
metadata:
  name: svc-a-service
  annotations:
    alb.ingress.kubernetes.io/healthcheck-protocol: http
    alb.ingress.kubernetes.io/healthcheck-port: traffic-port
    alb.ingress.kubernetes.io/healthy-threshold-count: '5'
    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'
    alb.ingress.kubernetes.io/healthcheck-path: /index.html
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '30'
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '5'
    alb.ingress.kubernetes.io/success-codes: '200'
    alb.ingress.kubernetes.io/tags: environment=test,app=servicea
spec:
  selector:
    app: service-a
  ports:
  - port: 80
    targetport: 80
  type: nodeport

and each service has it's own ingress object defined as follows (again, unique to each and with the correct certificates specified for each service):
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: svc-a-ingress
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/group.name: services
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/security-groups: sg-01234567898765432
    alb.ingress.kubernetes.io/ip-address-type: ipv4
    alb.ingress.kubernetes.io/listen-ports: '[{&quot;http&quot;: 80}, {&quot;https&quot;: 443}]'
    alb.ingress.kubernetes.io/actions.ssl-redirect: '{&quot;type&quot;: &quot;redirect&quot;, &quot;redirectconfig&quot;: { &quot;protocol&quot;: &quot;https&quot;, &quot;port&quot;: &quot;443&quot;, &quot;statuscode&quot;: &quot;http_301&quot;}}'
    alb.ingress.kubernetes.io/actions.response-503: &gt;
      {&quot;type&quot;:&quot;fixed-response&quot;,&quot;fixedresponseconfig&quot;:{&quot;contenttype&quot;:&quot;text/plain&quot;,&quot;statuscode&quot;:&quot;503&quot;,&quot;messagebody&quot;:&quot;unknown host&quot;}}
    alb.ingress.kubernetes.io/target-type: instance
    alb.ingress.kubernetes.io/load-balancer-attributes: routing.http2.enabled=true,idle_timeout.timeout_seconds=600
    alb.ingress.kubernetes.io/tags: environment=test
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-2:555555555555:certificate/33333333-2222-4444-aaaa-eeeeeeeeeeee
    alb.ingress.kubernetes.io/ssl-policy: elbsecuritypolicy-2016-08
spec:
  rules:
    - http:
        paths:
          - path: /*
            backend:
              servicename: ssl-redirect
              serviceport: use-annotation
          - path: /*
            backend:
              servicename: svc-a-service
              serviceport: 80
          - path: /*
            backend:
              servicename: response-503
              serviceport: use-annotation


the http to https redirection works as expected.
however -- there is no differentiation between my two apps for the load balancer to be able to know that traffic destined for service-a.example.com and service-b.example.com should be routed to two different target groups.
in the http:443 listener rules in the console, it shows:

if path is /* then forward to serviceatargetgroup
if path is /* then return fixed 503
if path is /* then forward to servicebtargetgroup
if path is /* then return fixed 503
if request otherwise not routed then return fixed 404

so the important question here is:
how should the ingress be defined to force traffic destined for service-a.example.com to serviceatargetgroup - and traffic destined for service-b.example.com to servicebtargetgroup?
and secondarily, i need the &quot;otherwise not routed&quot; to return a 503 instead of 404. i was expecting this to appear only once in the rules (be merged) - yet it is created for each ingress. how should my yaml be structured to achieve this?
",<amazon-web-services><kubernetes><kubernetes-ingress><amazon-eks>,65076576,9,"i eventually figured this out -- so for anyone else stumbling onto this post, here's how i resolved it:
the trick was not relying on merging between the ingress objects. yes, it can handle a certain degree of merging, but there's not really a one-to-one relationship between services as targetgroups and ingress as alb. so you have to be very cautious and aware of what's in each ingress object.
once i combined all of my ingress into a single object definition, i was able to get it working exactly as i wanted with the following yaml:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: svc-ingress
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/group.name: services
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/security-groups: sg-01234567898765432
    alb.ingress.kubernetes.io/ip-address-type: ipv4
    alb.ingress.kubernetes.io/listen-ports: '[{&quot;http&quot;: 80}, {&quot;https&quot;: 443}]'
    alb.ingress.kubernetes.io/actions.ssl-redirect: '{&quot;type&quot;: &quot;redirect&quot;, &quot;redirectconfig&quot;: { &quot;protocol&quot;: &quot;https&quot;, &quot;port&quot;: &quot;443&quot;, &quot;statuscode&quot;: &quot;http_301&quot;}}'
    alb.ingress.kubernetes.io/actions.response-503: &gt;
      {&quot;type&quot;:&quot;fixed-response&quot;,&quot;fixedresponseconfig&quot;:{&quot;contenttype&quot;:&quot;text/plain&quot;,&quot;statuscode&quot;:&quot;503&quot;,&quot;messagebody&quot;:&quot;unknown host&quot;}}
    alb.ingress.kubernetes.io/actions.svc-a-host: &gt;
      {&quot;type&quot;:&quot;forward&quot;,&quot;forwardconfig&quot;:{&quot;targetgroups&quot;:[{&quot;servicename&quot;:&quot;svc-a-service&quot;,&quot;serviceport&quot;:80,&quot;weight&quot;:100}]}}
    alb.ingress.kubernetes.io/conditions.svc-a-host: &gt;
      [{&quot;field&quot;:&quot;host-header&quot;,&quot;hostheaderconfig&quot;:{&quot;values&quot;:[&quot;svc-a.example.com&quot;]}}]
    alb.ingress.kubernetes.io/actions.svc-b-host: &gt;
      {&quot;type&quot;:&quot;forward&quot;,&quot;forwardconfig&quot;:{&quot;targetgroups&quot;:[{&quot;servicename&quot;:&quot;svc-b-service&quot;,&quot;serviceport&quot;:80,&quot;weight&quot;:100}]}}
    alb.ingress.kubernetes.io/conditions.svc-b-host: &gt;
      [{&quot;field&quot;:&quot;host-header&quot;,&quot;hostheaderconfig&quot;:{&quot;values&quot;:[&quot;svc-b.example.com&quot;]}}]
    alb.ingress.kubernetes.io/target-type: instance
    alb.ingress.kubernetes.io/load-balancer-attributes: routing.http2.enabled=true,idle_timeout.timeout_seconds=600
    alb.ingress.kubernetes.io/tags: environment=test
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-2:555555555555:certificate/33333333-2222-4444-aaaa-eeeeeeeeeeee,arn:aws:acm:us-east-2:555555555555:certificate/44444444-3333-5555-bbbb-ffffffffffff
    alb.ingress.kubernetes.io/ssl-policy: elbsecuritypolicy-2016-08
spec:
  backend:
    servicename: response-503
    serviceport: use-annotation
  rules:
    - http:
        paths:
          - backend:
              servicename: ssl-redirect
              serviceport: use-annotation
          - backend:
              servicename: svc-a-host
              serviceport: use-annotation
          - backend:
              servicename: svc-b-host
              serviceport: use-annotation

default action:
set by specifying the servicename and serviceport directly under spec:
spec:
  backend:
    servicename: response-503
    serviceport: use-annotation

routing:
because i'm using subdomains and paths won't work for me, i simply omitted the path and instead relied on hostname as a condition.
metadata:
  alb.ingress.kubernetes.io/actions.svc-a-host: &gt;
      {&quot;type&quot;:&quot;forward&quot;,&quot;forwardconfig&quot;:{&quot;targetgroups&quot;:[{&quot;servicename&quot;:&quot;svc-a-service&quot;,&quot;serviceport&quot;:80,&quot;weight&quot;:100}]}}
  alb.ingress.kubernetes.io/conditions.svc-a-host: &gt;
      [{&quot;field&quot;:&quot;host-header&quot;,&quot;hostheaderconfig&quot;:{&quot;values&quot;:[&quot;svc-a.example.com&quot;]}}]

end result:
the alb rules were configured precisely how i wanted them:

default action is a 503 fixed response
all http traffic is redirected to https
traffic is directed to targetgroups based on the host header

"
61415980,"https connection refused, using ingress-nginx","i've kubernetes installed on ubuntu 19.10.
i've setup ingress-nginx and can access my test service using http.
however, i get a ""connection refused"" when i try to access via https.

[edit] i'm trying to get https to terminate in the ingress and pass unencrypted traffic to my service the same way http does. i've implemented the below based on many examples i've seen but with little luck.

yaml

kind: service
apiversion: v1
metadata:
  name: messagemanager-service
  namespace: default
  labels:
    name: messagemanager-service
spec:
  type: nodeport
  selector:
    app: messagemanager
  ports:
  - port: 80
    protocol: tcp
    targetport: 8080
    nodeport: 31212
    name: http

  externalips:
    - 192.168.0.210


---
kind: deployment
#apiversion: extensions/v1beta1
apiversion: apps/v1
metadata:
  name: messagemanager
  labels:
        app: messagemanager
        version: v1
spec:
  replicas: 3
  selector:
      matchlabels:
        app: messagemanager
  template:
    metadata:
      labels:
        app: messagemanager
        version: v1
    spec:  
      containers:
      - name: messagemanager
        image: test/messagemanager:1.0
        imagepullpolicy: ifnotpresent
        ports:
        - containerport: 8080
          protocol: tcp
---

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: messagemanager-ingress
  annotations: 
    nginx.ingress.kubernetes.io/ssl-passthrough: false
    ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
    - secretname: tls-secret  
  rules:
  - http:
      paths:
        - path: /message
          backend:
            servicename: messagemanager-service
            serviceport: 8080



https test

curl -kl https://192.168.0.210/message -verbose
*   trying 192.168.0.210:443...
* tcp_nodelay set
* connect to 192.168.0.210 port 443 failed: connection refused
* failed to connect to 192.168.0.210 port 443: connection refused
* closing connection 0
curl: (7) failed to connect to 192.168.0.210 port 443: connection refused


http test

curl -kl http://192.168.0.210/message -verbose
*   trying 192.168.0.210:80...
* tcp_nodelay set
* connected to 192.168.0.210 (192.168.0.210) port 80 (#0)
&gt; get /message http/1.1
&gt; host: 192.168.0.210
&gt; user-agent: curl/7.65.3
&gt; accept: */*
&gt; referer: rbose
&gt;
* mark bundle as not supporting multiuse
&lt; http/1.1 200 ok
&lt; content-type: text/plain;charset=utf-8
&lt; date: fri, 24 apr 2020 18:44:07 gmt
&lt; connection: keep-alive
&lt; content-length: 50
&lt;
* connection #0 to host 192.168.0.210 left intact


$ kubectl -n ingress-nginx get svc
name                                 type           cluster-ip      external-ip   port(s)                      age
ingress-nginx-controller             loadbalancer   10.105.92.236   &lt;pending&gt;     80:31752/tcp,443:32035/tcp   2d
ingress-nginx-controller-admission   clusterip      10.100.223.87   &lt;none&gt;        443/tcp                      2d

$ kubectl get ingress -o wide
name                     class    hosts   address   ports     age
messagemanager-ingress   &lt;none&gt;   *                 80, 443   37m


key creation

openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj ""/cn=nginxsvc/o=nginxsvc""
kubectl create secret tls tls-secret --key tls.key --cert tls.crt


$ kubectl describe ingress
name:             messagemanager-ingress
namespace:        default
address:
default backend:  default-http-backend:80 (&lt;error: endpoints ""default-http-backend"" not found&gt;)
tls:
  tls-secret terminates
rules:
  host        path  backends
  ----        ----  --------
  *
              /message   messagemanager-service:8080 ()
annotations:  events:
  type        reason  age   from                      message
  ----        ------  ----  ----                      -------
  normal      create  107s  nginx-ingress-controller  ingress default/messagemanager-ingress


i was under the assumption that tls would terminate in the ingress and the request would be passed on to the service as http.
i had to add the external ips in the service to get http to work.
am i missing something similar for https?

any help and guidance is appreciated.

thanks

mark
",<ssl><kubernetes><https><kubernetes-ingress>,61456324,9,"i've reproduced your scenario in my lab and after a few changes in your ingress it's working as you described.
in my lab i used an nginx image that serves a default landing page on port 80 and with this ingress rule, it's possible to serve it on port 80 and 443.
kind: deployment
apiversion: apps/v1
metadata:
  name: nginx
  labels:
        app: nginx
        version: v1
spec:
  replicas: 3
  selector:
      matchlabels:
        app: nginx
  template:
    metadata:
      labels:
        app: nginx
        version: v1
    spec:  
      containers:
      - name: nginx
        image: nginx
        imagepullpolicy: ifnotpresent
        ports:
        - containerport: 80
          protocol: tcp
---
kind: service
apiversion: v1
metadata:
  name: nginx-service
  namespace: default
  labels:
    name: nginx-service
spec:
  type: nodeport
  selector:
    app: nginx
  ports:
  - port: 80
    protocol: tcp
    targetport: 80
    nodeport: 31000
    name: http          
---
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: nginx
  labels:
    app: nginx  
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /  
spec:
  tls:
    - secretname: tls-secret  
  rules:
  - http:
      paths:
      - path: /nginx
        backend:
          servicename: nginx-service
          serviceport: 80

the only difference between my ingress and yours is that i removed nginx.ingress.kubernetes.io/ssl-passthrough: false. in the documentation we can read:

note ssl passthrough is disabled by default

so there is no need for you to specify that.
i used the same secret as you:
$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/cn=nginxsvc/o=nginxsvc&quot;
$ kubectl create secret tls tls-secret --key tls.key --cert tls.crt

in your question i have the impression that you are trying to reach your ingress through the ip 192.168.0.210. this is your service ip and not your ingress ip.
if you are using cloud managed kubernetes you have to run the following command to find your ingress ip:
$ kubectl get ingresses nginx 
name    hosts   address        ports     age
nginx   *       34.89.108.48   80, 443   6m32s

if you are running on bare metal without any loadbalancer solution as metallb, you can see that your ingress-nginx service will be with external-ip on pending forever.
$ kubectl get service -n ingress-nginx 
name                                            type           cluster-ip       external-ip   port(s)                      age
ingress-nginx-1587980954-controller             loadbalancer   10.110.188.236   &lt;pending&gt;     80:31024/tcp,443:30039/tcp   23s

you can do the same thing as you did with your service and add an externalip manually:
 kubectl get service -n ingress-nginx 
name                                            type           cluster-ip       external-ip   port(s)                      age
ingress-nginx-1587980954-controller             loadbalancer   10.110.188.236   10.156.0.24   80:31024/tcp,443:30039/tcp   9m14s

after this change, your ingress will have the same ip as you defined in your ingress service:
$ kubectl get ingress nginx 
name    class    hosts   address       ports     age
nginx   &lt;none&gt;   *       10.156.0.24   80, 443   118s

$ curl -kl https://10.156.0.24/nginx --verbose
*   trying 10.156.0.24...
* tcp_nodelay set
* connected to 10.156.0.24 (10.156.0.24) port 443 (#0)
* alpn, offering h2
* alpn, offering http/1.1
* cipher selection: all:!export:!export40:!export56:!anull:!low:!rc4:@strength
* successfully set certificate verify locations:
*   cafile: /etc/ssl/certs/ca-certificates.crt
  capath: /etc/ssl/certs
* tlsv1.2 (out), tls header, certificate status (22):
* tlsv1.2 (out), tls handshake, client hello (1):
* tlsv1.2 (in), tls handshake, server hello (2):
* tlsv1.2 (in), tls handshake, certificate (11):
* tlsv1.2 (in), tls handshake, server key exchange (12):
* tlsv1.2 (in), tls handshake, server finished (14):
* tlsv1.2 (out), tls handshake, client key exchange (16):
* tlsv1.2 (out), tls change cipher, client hello (1):
* tlsv1.2 (out), tls handshake, finished (20):
* tlsv1.2 (in), tls change cipher, client hello (1):
* tlsv1.2 (in), tls handshake, finished (20):
* ssl connection using tlsv1.2 / ecdhe-rsa-aes128-gcm-sha256
* alpn, server accepted to use h2
* server certificate:
*  subject: o=acme co; cn=kubernetes ingress controller fake certificate
*  start date: apr 27 09:49:19 2020 gmt
*  expire date: apr 27 09:49:19 2021 gmt
*  issuer: o=acme co; cn=kubernetes ingress controller fake certificate
*  ssl certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* using http2, server supports multi-use
* connection state changed (http/2 confirmed)
* copying http/2 data in stream buffer to connection buffer after upgrade: len=0
* using stream id: 1 (easy handle 0x560cee14fe90)
&gt; get /nginx http/1.1
&gt; host: 10.156.0.24
&gt; user-agent: curl/7.52.1
&gt; accept: */*
&gt; 
* connection state changed (max_concurrent_streams updated)!
&lt; http/2 200 
&lt; server: nginx/1.17.10
&lt; date: mon, 27 apr 2020 10:01:29 gmt
&lt; content-type: text/html
&lt; content-length: 612
&lt; vary: accept-encoding
&lt; last-modified: tue, 14 apr 2020 14:19:26 gmt
&lt; etag: &quot;5e95c66e-264&quot;
&lt; accept-ranges: bytes
&lt; strict-transport-security: max-age=15724800; includesubdomains
&lt; 
&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: tahoma, verdana, arial, sans-serif;
    }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;welcome to nginx!&lt;/h1&gt;
&lt;p&gt;if you see this page, the nginx web server is successfully installed and
working. further configuration is required.&lt;/p&gt;

&lt;p&gt;for online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
* curl_http_done: called premature == 0
* connection #0 to host 10.156.0.24 left intact

edit:

there does not seem to be a way to manually set the &quot;external ips&quot; for
the ingress as can be done in the service. if you know of one please
let me know :-). looks like my best bet is to try metallb.

metallb would be the best option for production. if you are running it for lab only, you have the option to add your node public ip (the same you can get by running kubectl get nodes -o wide) and attach it to your nginx ingress controller.
adding your node ip to your nginx ingress controller
spec:
  externalips:
  - 192.168.0.210

create a file called  ingress-nginx-svc-patch.yaml  and paste the contents above.
next apply the changes with the following command:
kubectl patch service ingress-nginx-controller -n kube-system --patch &quot;$(cat ingress-nginx-svc-patch.yaml)&quot;

and as result:
$ kubectl get service -n kube-system ingress-nginx-controller
name                       type           cluster-ip    external-ip   port(s)                      age
ingress-nginx-controller   loadbalancer   10.97.0.243   192.168.0.210   80:31409/tcp,443:30341/tcp   39m

"
61541812,ingress nginx - how to serve assets to application,"i have an issue, i am deploying an application on [hostname]/product/console, but the .css .js files are being requested from [hostname]/product/static, hence they are not being loaded and i get 404.

i have tried nginx.ingress.kubernetes.io/rewrite-target: to no avail.

i also tried using: nginx.ingress.kubernetes.io/location-snippet: |
                 location = /product/console/ {
                   proxy_pass http://[hostname]/product/static/;
                 }

but the latter does not seem to be picked up by the nginx controller at all. this is my ingress.yaml

---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-resource
  annotations:
    kubernetes.io/ingress.class: ""nginx""
    nginx.ingress.kubernetes.io/enable-rewrite-log: ""true""
    # nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/location-snippet: |
      location = /product/console/ {
        proxy_pass http://[hostname]/product/static/;
        }
spec:
  rules:
    - host: {{.values.hostname}}
      http:
        paths:
        - path: /product/console
          backend:
            servicename: product-svc
            serviceport: prod ##25022
        - path: /product/
          backend:
            servicename: product-svc
            serviceport: prod #25022


--
can i ask for some pointers? i have been trying to google this out and tried some different variations, but i seem to be doing something wrong. thanks!
",<nginx><kubernetes><kubernetes-ingress><nginx-config><nginx-ingress>,61751019,9,"tl;dr

to diagnose the reason why you get error 404 you can check in nginx-ingress controller pod logs. you can do it with below command: 

kubectl logs -n ingress-nginx ingress_nginx_controller_pod_name

you should get output similar to this (depending on your use case): 

client_ip - - [12/may/2020:11:06:56 +0000] ""get / http/1.1"" 200 238 ""-"" ""redacted"" 430 0.003 [default-ubuntu-service-ubuntu-port] [] 10.48.0.13:8080 276 0.003 200 
client_ip - - [12/may/2020:11:06:56  +0000] ""get /assets/styles/style.css http/1.1"" 200 22 ""http://server_ip/"" ""redacted"" 348 0.002 [default-ubuntu-service-ubuntu-port] [] 10.48.0.13:8080 22 0.002 200 


with above logs you can check if the requests are handled properly by nginx-ingress controller and where they are sent. 

also you can check the kubernetes.github.io: ingress-nginx: ingress-path-matching. it's a document describing how ingress matches paths with regular expressions. 



you can experiment with ingress, by following below example: 


deploy nginx-ingress controller
create a pod and a service
run example application 
create an ingress resource 
test
rewrite example 


deploy nginx-ingress controller

you can deploy your nginx-ingress controller by following official documentation: 

kubernetes.github.io: ingress-nginx

create a pod and a service

below is an example definition of a pod and a service attached to it which will be used for testing purposes: 

apiversion: apps/v1
kind: deployment
metadata:
  name: ubuntu-deployment
spec:
  selector:
    matchlabels:
      app: ubuntu
  replicas: 1 
  template:
    metadata:
      labels:
        app: ubuntu
    spec:
      containers:
      - name: ubuntu
        image: ubuntu
        command:
        - sleep
        - ""infinity"" 
---
apiversion: v1
kind: service
metadata:
  name: ubuntu-service
spec:
  selector:
    app: ubuntu
  ports:
    - name: ubuntu-port
      port: 8080
      targetport: 8080
      nodeport: 30080
  type: nodeport 


example page

i created a basic index.html with one css to simulate the request process. you need to create this files inside of a pod (manually or copy them to pod). 

the file tree looks like this: 


index.html
assets/styles/style.css


index.html: 

&lt;!doctype html&gt;
&lt;html lang=""en""&gt;
&lt;head&gt;
  &lt;meta charset=""utf-8""&gt;
  &lt;meta name=""viewport"" content=""width=device-width, initial-scale=1.0""&gt;
  &lt;link rel=""stylesheet"" href=""assets/styles/style.css""&gt;
  &lt;title&gt;document&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;h1&gt;hi&lt;/h1&gt;
&lt;/body&gt;


please take a specific look on a line: 

  &lt;link rel=""stylesheet"" href=""assets/styles/style.css""&gt;


style.css:

h1 {
  color: red;
}


you can run above page with python: 


$ apt update &amp;&amp; apt install -y python3
$ python3 -m http.server 8080 where the index.html and assets folder is stored. 


create an ingress resource

below is an example ingress resource configured to use nginx-ingress controller: 

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: nginx-ingress-example
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: 
    http:
      paths:
      - path: /
        backend:
          servicename: ubuntu-service 
          serviceport: ubuntu-port


after applying above resource you can start to test. 

test

you can go to your browser and enter the external ip address associated with your ingress resource. 

as i said above you can check the logs of nginx-ingress controller pod to check how your controller is handling request.

if you run command mentioned earlier python3 -m http.server 8080 you will get logs too: 

$ python3 -m http.server 8080
serving http on 0.0.0.0 port 8080 (http://0.0.0.0:8080/) ...
10.48.0.16 - - [12/may/2020 11:06:56] ""get / http/1.1"" 200 -
10.48.0.16 - - [12/may/2020 11:06:56] ""get /assets/styles/style.css http/1.1"" 200 -


rewrite example

i've edited the ingress resource to show you an example of a path rewrite: 

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: nginx-ingress-example
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
  - host: 
    http:
      paths:
      - path: /product/(.*)
        backend:
          servicename: ubuntu-service 
          serviceport: ubuntu-port


changes were made to lines: 

    nginx.ingress.kubernetes.io/rewrite-target: /$1


and: 

      - path: /product/(.*)


steps: 


the browser sent: /product/
controller got /product/ and had it rewritten to /
pod got / from a controller. 


logs from thenginx-ingress controller: 

client_ip - - [12/may/2020:11:33:23 +0000] ""get /product/ http/1.1"" 200 228 ""-"" ""redacted"" 438 0.002 [default-ubuntu-service-ubuntu-port] [] 10.48.0.13:8080 276 0.001 200 fb0d95e7253335fc82cc84f70348683a
client_ip - - [12/may/2020:11:33:23 +0000] ""get /product/assets/styles/style.css http/1.1"" 200 22 ""http://server_ip/product/"" ""redacted"" 364 0.002 [default-ubuntu-service-ubuntu-port] [] 10.48.0.13:8080 22 0.002 200 


logs from the pod: 

10.48.0.16 - - [12/may/2020 11:33:23] ""get / http/1.1"" 200 -
10.48.0.16 - - [12/may/2020 11:33:23] ""get /assets/styles/style.css http/1.1"" 200 -


please let me know if you have any questions in that. 
"
76712848,use kustomize replacements to replace values in one base with values from another base?,"i'm updating some of my kubernetes configurations to use 'replacements' and 'resources' in kustomize as 'vars' and 'bases' have been deprecated.
previously, i used 'vars' in a base (/base/secrets/) like this:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization

secretgenerator:
- name: test_secret
  env: secret.env

vars:
- name : secret_value
  objref:
    kind: secret
    name: test_secret
    apiversion: v1
  fieldref:
    fieldpath: metadata.name


this base was used in various overlays for different services:
namespace: test-overlay

bases:
- ../../base/secrets/
- ../../base/service/

now, with 'resources' and 'replacements', my understanding is that it's not possible to replace values in /base/service/ from /base/secrets/ as before. i could apply the 'replacement' in the overlay itself and target the base i want to modify, but i would prefer to perform the operation from a base for reusability and ease of use.
here's what i'm trying to do:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization

secretgenerator:
- name: test_secret
  env: secret.env

replacements:
- source:
    name: test_secret
    kind: secret
  targets:
  - select:
      kind: deployment
      name: service
    fieldpaths:
    - spec.template.spec.&lt;field&gt;

in the 'replacements' directive, spec.template.spec.&lt;field&gt; is the field in the deployment resource that i'm trying to replace.
i'm using kustomize version v5.1.0.
how can i get 'replacements' to target other bases so that they can be used from any overlay? what's the best practice for this scenario?
i've attempted to apply the 'replacements' in the overlay itself and target the base i want to modify like this:
namespace: test-overlay

resources:
- ../../base/secrets/
- ../../base/service/

replacements:
- source:
    kind: secret
    name: test_secret
  targets:
  - select:
      kind: deployment
      name: service
    fieldpaths:
    - spec.template.spec.&lt;field&gt;

while this does apply the replacement to the service, it's not a satisfactory solution for me because i have multiple overlays that all need to use the same replacement for various deployments. i would prefer to define the replacement once in a base, rather than having to define it in each overlay.
edit: a more clear minimal reproducible example
/base
  /secrets
    kustomization.yaml
  /service
    deployment.yaml
    kustomization.yaml
/overlays
  /test-overlay
    kustomization.yaml

with the /secrets/ implemented as:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization

secretgenerator:
- name: test_secret
  env: secret.env

replacements:
- source:
    name: test_secret
    kind: secret
  targets:
  - select:
      kind: deployment
      name: service
    fieldpaths:
    - spec.template.spec.volumes.name

this would be the /service/:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
    
resources:
- deployment.yaml

---

apiversion: apps/v1
kind: deployment
metadata:
  name: service
spec:
  template:
    spec:
      volumes:
      - name: placeholder_value
        emptydir: {}

with /test-overlay/
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization

namespace: test-overlay

resources:
- ../../base/secrets/
- ../../base/service/

i've tried including the 'replacements' in the test-overlays kustomization file which did work but is less desirable then letting the replacement be defined in a base
",<kubernetes><kubernetes-secrets><configuration-management><kustomize>,76713665,9,"thanks for updating your question with an example.
the answer i've posted here is still the correct solution in terms of sharing a replacement configuration among multiple overlays, but there are some errors in your replacement syntax: you cannot target spec.template.spec.volumes.name, because volumes is a list and has no name attribute.
you can only target list elements with a [name=value] style selector, so:
replacements:
  - source:
      name: test_secret
      kind: secret
    targets:
      - select:
          kind: deployment
          name: service
        fieldpaths:
          - spec.template.spec.volumes.[name=placeholder_value].name


a kustomization.yaml can only apply transformations (labels, patches, replacements, etc) to resources that are emitted by that kustomization.yaml -- which means that if you want a transformation to affect all resources, it needs to be applied in the &quot;outermost&quot; kustomization.
this means that you can't place something in a &quot;base&quot; that will modify resources generated in your overlays.
but don't worry, there is a solution! components allow you to reuse kustomization fragments. if we move your replacement configuration into a component, we can get the behavior you want.
for example, here is a project with a base and two overlays:
.
 base
  deployment.yaml
  kustomization.yaml
 components
  replace-username-password
      kustomization.yaml
 overlay
     env1
      kustomization.yaml
     env2
         kustomization.yaml

base/deployment.yaml looks like this:
apiversion: apps/v1
kind: deployment
metadata:
  name: example
spec:
  replicas: 2
  template:
    spec:
      containers:
        - name: example
          image: docker.io/alpine:latest
          command:
            - sleep
            - inf
          env:
            - name: user_name
              value: update-via-replacement
            - name: user_password
              value: update-via-replacement

and base/kustomization.yaml looks like:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
commonlabels:
  app: replacement-example

resources:
  - deployment.yaml

secretgenerator:
  - name: example
    literals:
      - password=secret

configmapgenerator:
  - name: example
    literals:
      - username=alice

so the base directory results in a deployment, a secret, and a configmap. there are two overlays, env1 and env2. in both overlays i want to apply the same replacement configuration, so i put that into components/replace-username-password/kustomization.yaml:
apiversion: kustomize.config.k8s.io/v1alpha1
kind: component

replacements:
  - source:
      kind: configmap
      name: example
      fieldpath: data.username
    targets:
      - select:
          kind: deployment
          name: example
        fieldpaths:
          - spec.template.spec.containers.[name=example].env.[name=user_name].value
  - source:
      kind: secret
      name: example
      fieldpath: data.password
    targets:
      - select:
          kind: deployment
          name: example
        fieldpaths:
          - spec.template.spec.containers.[name=example].env.[name=user_password].value

now in overlays/env1/kustomization.yaml i can make use of this component:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
commonlabels:
  envname: env1

resources:
  - ../../base

components:
  - ../../components/replace-username-password

and the same in overlays/env2:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
commonlabels:
  envname: env2

resources:
  - ../../base

components:
  - ../../components/replace-username-password

"
50708551,helm charts nested loops,"trying to generate deployments for my helm charts by using this template

{{- range .values.services }}
apiversion: apps/v1beta1
kind: deployment
metadata:
  name: myapp-{{ . }}
spec:
  replicas: {{ .replicacount }}
  template:
    metadata:
      labels:
        app: myapp-{{ . }}
        chart: myapp-{{ $.values.cluster }}-{{ $.values.environment }}
    spec:
      containers:
      - name: myapp-{{ . }}
        image: {{ $.values.containerregistry }}/myapp-{{ . }}:latest
        ports:
        - containerport: {{ .targetport }}
        env:
  {{- with .environmentvariables }}
  {{ indent 10 }}
  {{- end }}
      imagepullsecrets:
       - name: myregistry
{{- end }}


for 2 of my services. in values.yaml i got

environment: dev

cluster: sandbox

ingress:
  enabled: true

containerregistry: myapp.io

services:
- backend:
    port: 80
    targetport: 8080
    replicacount: 1
    environmentvariables:
      - name: some_var
        value: ""hello""
- web:
    port: 80
    targetport: 8080
    replicacount: 1
    environmentvariables:
      - name: some_var
        value: ""hello""


... but the output is not being properly formatted

apiversion: apps/v1beta1
kind: deployment
metadata:
  name: myapp-map[backend:map[replicacount:1 targetport:8080 environmentvariables:[map[name:some_var value:hello] port:80]]


instead of

apiversion: apps/v1beta1
kind: deployment
metadata:
  name: myapp-web
(...)


and another config

apiversion: apps/v1beta1
kind: deployment
metadata:
  name: myapp-backend
(...)


what functions can i use or some different data structure? none of the references (i.e. .environmentvariables are working correctly)
",<templates><kubernetes><kubernetes-helm>,50719515,9,"i think you should reconsider the way the data is structured, this would work better:

  services:
  - name: backend
    settings:
      port: 80
      targetport: 8080
      replicacount: 1
      environmentvariables:
        - name: some_var
          value: ""hello""

  - name: web
    settings:
      port: 80
      targetport: 8080
      replicacount: 1
      environmentvariables:
        - name: some_var
          value: ""hello""


and your deployment to look like this:

{{- range .values.services }}
---
apiversion: apps/v1beta1
kind: deployment
metadata:
  name: myapp-{{ .name }}
spec:
  replicas: {{ .settings.replicacount }}
  template:
    metadata:
      labels:
        app: myapp-{{ .name }}
    spec:
      containers:
      - name: myapp-{{ .name }}
        image: {{ $.values.containerregistry }}/myapp-{{ .name }}:latest
        ports:
        - containerport: {{ .settings.targetport }}
        env:
  {{- with .settings.environmentvariables }}
  {{ toyaml . | trim | indent 6 }}
  {{- end }}
      imagepullsecrets:
       - name: myregistry
{{- end }}


would actually create two deployments, by adding the --- separator.
"
54135446,unable to access exposed port on kubernetes,"i have build a custom tcserver image exposing port 80 8080 and 8443. basically you have an apache and inside the configuration you have a proxy pass to forward it to the tcserver tomcat.

expose 80 8080 8443


after that i created a kubernetes yaml to build the pod exposing only port 80.

apiversion: v1
kind: pod
metadata:
  name: tcserver
  namespace: default
spec:
  containers:
  - name: tcserver
    image: tcserver-test:v1
    imagepullpolicy: ifnotpresent
    ports:
    - containerport: 80


and the service along with it.

apiversion: v1
kind: service
metadata:
  name: tcserver-svc
  labels:
    app: tcserver
spec:
  type: nodeport
  ports:
  - port: 80
    nodeport: 30080
  selector:
    app: tcserver


but the problem is that i'm unable to access it.
if i log to the pod (kubectl exec -it tcserver -- /bin/bash), i'm able to do a curl -k -v http://localhost and it will reply.

i believe i'm doing something wrong with the service, but i don't know what.
any help will be appreciated. 

svc change 
as suggested by sfgroups, i added the targetport: 80 to the svc, but still not working.

when i try to curl the ip, i get a no route to host

[root@testmaster tcserver]# curl -k -v http://172.30.62.162:30080/
* about to connect() to 172.30.62.162 port 30080 (#0)
*   trying 172.30.62.162...
* no route to host
* failed connect to 172.30.62.162:30080; no route to host
* closing connection 0
curl: (7) failed connect to 172.30.62.162:30080; no route to host


this is the describe from the svc:

[root@testmaster tcserver]# kubectl describe svc tcserver-svc
name:                   tcserver-svc
namespace:              default
labels:                 app=tcserver
annotations:            &lt;none&gt;
selector:               app=tcserver
type:                   nodeport
ip:                     172.30.62.162
port:                   &lt;unset&gt; 80/tcp
nodeport:               &lt;unset&gt; 30080/tcp
endpoints:              &lt;none&gt;
session affinity:       none
events:                 &lt;none&gt;

",<kubernetes><kubernetes-service>,54138796,9,"when you look at the kubectl describe service output, you'll see it's not actually attached to any pods:

endpoints:              &lt;none&gt;


that's because you say in the service spec that the service will attach to pods labeled with app: tcserver

spec:
  selector:
    app: tcserver


but, in the pod spec's metadata, you don't specify any labels at all

metadata:
  name: tcserver
  namespace: default
  # labels: {}


and so the fix here is to add to the pod spec the appropriate label

metadata:
  labels:
    app: tcserver


also note that it's a little unusual in practice to deploy a bare pod.  usually they're wrapped up in a higher-level controller, most often a deployment, that actually creates the pods.  the deployment spec has a template pod spec and it's the pod's labels that matter.

apiversion: apps/v1
kind: deployment
metadata:
  name: tcserver
  # labels here are useful, but the service doesn't look for them
spec:
  template:
    metadata:
      labels:
        # these labels are what the service cares about
        app: tcserver
    spec:
      containers: [...]

"
75034450,how to rewrite target for two paths in an ingress yaml,"i have an ingress.yaml with two paths; each to one of my microfrontends. however i'm really struggling to get the rewrite-target to work. mf1 loads correctly, but mf2 doesn't. i've done some research and know i need to use captured groups, but can't seem to properly implement this. how do i do that?
this is what my ingress looks like:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: http-ingress
  annotations:
    kubernetes.io/ingress.class: public
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: mf1
                port:
                  number: 80
          - path: /mf2
            pathtype: prefix
            backend:
              service:
                name: mf2
                port:
                  number: 80

",<kubernetes><kubernetes-ingress><nginx-ingress>,75035187,8,"you need to use a regular expression capture group in your path expression, and then reference the capture group in your .../rewrite-target annotation.
that might look like this:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: http-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: &quot;/$2&quot;
spec:
  rules:
    - http:
        paths:
          - path: /()(.*)
            pathtype: prefix
            backend:
              service:
                name: backend1
                port:
                  name: http
          - path: /mf2(/|$)(.*)
            pathtype: prefix
            backend:
              service:
                name: backend2
                port:
                  name: http

we need to ensure that for both rules, capture group $2 contains the desired path. for the first rule (path: /), we have an empty group $1 (because it's not necessary here), with the entire path captured in $2.
for the second rule, we match either /mf2 followed by either a /path... or as the end of the url (this ensures we don't erroneously match /mf2something). group $1 will contain the / (or nothing), and the path goes into $2.
in both cases, the rewritten path (/$2) will have what we want.
"
40743432,google container engine (kubernetes): websocket (socket.io) not working on multiple replicas,"i am new to google container engine (gke). when run on localhost it's working fine but when i deploy to production with gke i got websocket error.

my node app is develop with hapi.js and socket.io and my structure is shown in image below.

application architecture

i'm using glue to compose hapi server. below is my manifest.json

{
...
""connections"": [
    {
      ""host"": ""app"",
      ""address"": ""0.0.0.0"",
      ""port"": 8000,
      ""labels"": [""api""],
      ""routes"": {
        ""cors"": false,
        ""security"": {
          ""hsts"": false,
          ""xframe"": true,
          ""xss"": true,
          ""noopen"": true,
          ""nosniff"": true
        }
      },
      ""router"": {
        ""striptrailingslash"": true
      },
      ""load"": {
        ""maxheapusedbytes"": 1073741824,
        ""maxrssbytes"": 1610612736,
        ""maxeventloopdelay"": 5000
      }
    },
    {
      ""host"": ""app"",
      ""address"": ""0.0.0.0"",
      ""port"": 8099,
      ""labels"": [""web""],
      ""routes"": {
        ""cors"": true,
        ""security"": {
          ""hsts"": false,
          ""xframe"": true,
          ""xss"": true,
          ""noopen"": true,
          ""nosniff"": true
        }
      },
      ""router"": {
        ""striptrailingslash"": true
      },
      ""load"": {
        ""maxheapusedbytes"": 1073741824,
        ""maxrssbytes"": 1610612736,
        ""maxeventloopdelay"": 5000
      }
    },
    {
      ""host"": ""app"",
      ""address"": ""0.0.0.0"",
      ""port"": 8999,
      ""labels"": [""admin""],
      ""routes"": {
        ""cors"": true,
        ""security"": {
          ""hsts"": false,
          ""xframe"": true,
          ""xss"": true,
          ""noopen"": true,
          ""nosniff"": true
        }
      },
      ""router"": {
        ""striptrailingslash"": true
      },
      ""load"": {
        ""maxheapusedbytes"": 1073741824,
        ""maxrssbytes"": 1610612736,
        ""maxeventloopdelay"": 5000
      },
      ""state"": {
        ""ttl"": null,
        ""issecure"": false,
        ""ishttponly"": true,
        ""path"": null,
        ""domain"": null,
        ""encoding"": ""none"",
        ""clearinvalid"": false,
        ""strictheader"": true
      }
    }
  ],
...
}


and my nginx.conf

worker_processes                5; ## default: 1
worker_rlimit_nofile            8192;
error_log                       /dev/stdout info;

events {
  worker_connections            4096; ## default: 1024
}

http {
    access_log                  /dev/stdout;

    server {
        listen                  80          default_server;
        listen                  [::]:80     default_server;

        # redirect all http requests to https with a 301 moved permanently response.
        return                  301         https://$host$request_uri;
    }

    server {
        listen                  443         ssl default_server;
        listen                  [::]:443    ssl default_server;
        server_name             _;

        # configure ssl
        ssl_certificate         /etc/secret/ssl/myapp.com.csr;
        ssl_certificate_key     /etc/secret/ssl/myapp.com.key;
        include                 /etc/nginx/ssl-params.conf;
    }

    server {
        listen                  443         ssl;
        listen                  [::]:443    ssl;
        server_name             api.myapp.com;

        location / {
            proxy_pass          http://api_app/;
            proxy_set_header    host                $http_host;
            proxy_set_header    x-real-ip           $remote_addr;
            proxy_set_header    x-forwarded-for     $proxy_add_x_forwarded_for;

            # handle web socket connections
            proxy_http_version  1.1;
            proxy_set_header    upgrade     $http_upgrade;
            proxy_set_header    connection  ""upgrade"";
        }
    }

    server {
        listen                  443         ssl;
        listen                  [::]:443    ssl;
        server_name             myapp.com;

        location / {
            proxy_pass          http://web_app/;
            proxy_set_header    host                $http_host;
            proxy_set_header    x-real-ip           $remote_addr;
            proxy_set_header    x-forwarded-for     $proxy_add_x_forwarded_for;

            # handle web socket connections
            proxy_http_version  1.1;
            proxy_set_header    upgrade     $http_upgrade;
            proxy_set_header    connection  ""upgrade"";
        }
    }

    server {
        listen                  443         ssl;
        listen                  [::]:443    ssl;
        server_name             admin.myapp.com;

        location / {
            proxy_pass          http://admin_app/;
            proxy_set_header    host                $http_host;
            proxy_set_header    x-real-ip           $remote_addr;
            proxy_set_header    x-forwarded-for     $proxy_add_x_forwarded_for;

            # handle web socket connections
            proxy_http_version  1.1;
            proxy_set_header    upgrade     $http_upgrade;
            proxy_set_header    connection  ""upgrade"";
        }
    }

    # define your ""upstream"" servers - the
    # servers request will be sent to
    upstream api_app {
        server                  localhost:8000;
    }

    upstream web_app {
        server                  localhost:8099;
    }

    upstream admin_app {
        server                  localhost:8999;
    }
}


kubernetes service app-service.yaml

apiversion: v1
kind: service
metadata:
  name: app-nginx
  labels:
    app: app-nginx
spec:
  type: loadbalancer
  ports:
    # the port that this service should serve on.
    - port: 80
      targetport: 80
      protocol: tcp
      name: http
    - port: 443
      targetport: 443
      protocol: tcp
      name: https
  # label keys and values that must match in order to receive traffic for this service.
  selector:
    app: app-nginx


kubernetes deployment app-deployment.yaml

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: app-nginx
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: app-nginx
    spec:
      containers:
        - name: nginx
          image: us.gcr.io/myproject/nginx
          ports:
            - containerport: 80
              name: http
            - containerport: 443
              name: https
          volumemounts:
              # this name must match the volumes.name below.
            - name: ssl-secret
              readonly: true
              mountpath: /etc/secret/ssl
        - name: app
          image: us.gcr.io/myproject/bts-server
          ports:
            - containerport: 8000
              name: api
            - containerport: 8099
              name: web
            - containerport: 8999
              name: admin
          volumemounts:
              # this name must match the volumes.name below.
            - name: client-secret
              readonly: true
              mountpath: /etc/secret/client
            - name: admin-secret
              readonly: true
              mountpath: /etc/secret/admin
      volumes:
        - name: ssl-secret
          secret:
            secretname: ssl-key-secret
        - name: client-secret
          secret:
            secretname: client-key-secret
        - name: admin-secret
          secret:
            secretname: admin-key-secret


and i'm using cloudflare ssl full strict.

error get from browser console:

websocket connection to 'wss://api.myapp.com/socket.io/?eio=3&amp;transport=websocket&amp;sid=4ky-y9k7j0xotrbfaaaq' failed: websocket is closed before the connection is established.
https://api.myapp.com/socket.io/?eio=3&amp;transport=polling&amp;t=lybynd2&amp;sid=4ky-y9k7j0xotrbfaaaq failed to load resource: the server responded with a status of 400 ()
vm50:35 websocket connection to 'wss://api.myapp.com/socket.io/?eio=3&amp;transport=websocket&amp;sid=fscgx-ue7ohrsssqaaat' failed: error during websocket handshake: unexpected response code: 502wrappedwebsocket @ vm50:35ws.doopen @ socket.io.js:6605transport.open @ socket.io.js:4695socket.probe @ socket.io.js:3465socket.onopen @ socket.io.js:3486socket.onhandshake @ socket.io.js:3546socket.onpacket @ socket.io.js:3508(anonymous function) @ socket.io.js:3341emitter.emit @ socket.io.js:6102transport.onpacket @ socket.io.js:4760callback @ socket.io.js:4510(anonymous function) @ socket.io.js:5385exports.decodepayloadasbinary @ socket.io.js:5384exports.decodepayload @ socket.io.js:5152polling.ondata @ socket.io.js:4514(anonymous function) @ socket.io.js:4070emitter.emit @ socket.io.js:6102request.ondata @ socket.io.js:4231request.onload @ socket.io.js:4312xhr.onreadystatechange @ socket.io.js:4184
socket.io.js:4196 get https://api.myapp.com/socket.io/?eio=3&amp;transport=polling&amp;t=lybynpy&amp;sid=fscgx-ue7ohrsssqaaat 400 ()


and here is nginx's logs:

[22/nov/2016:12:10:19 +0000] ""get /socket.io/?eio=3&amp;transport=websocket&amp;sid=mgc--oncqbqi6nozaaax http/1.1"" 101 0 ""-"" ""mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/54.0.2840.99 safari/537.36""
10.8.0.1 - - [22/nov/2016:12:10:19 +0000] ""post /socket.io/?eio=3&amp;transport=polling&amp;t=lybyqbw&amp;sid=mgc--oncqbqi6nozaaax http/1.1"" 200 2 ""https://myapp.com/"" ""mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/54.0.2840.99 safari/537.36""
10.128.0.2 - - [22/nov/2016:12:10:20 +0000] ""get /socket.io/?eio=3&amp;transport=polling&amp;t=lybyqkp http/1.1"" 200 101 ""https://myapp.com/"" ""mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/54.0.2840.99 safari/537.36""
10.8.0.1 - - [22/nov/2016:12:10:21 +0000] ""get /socket.io/?eio=3&amp;transport=polling&amp;t=lybyqwo&amp;sid=c5nkust9feprsu2raaay http/1.1"" 200 24 ""https://myapp.com/"" ""mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/54.0.2840.99 safari/537.36""
2016/11/22 12:10:21 [error] 6#6: *157 connect() failed (111: connection refused) while connecting to upstream, client: 10.8.0.1, server: api.myapp.com, request: ""get /socket.io/?eio=3&amp;transport=polling&amp;t=lybyqan&amp;sid=c5nkust9feprsu2raaay http/1.1"", upstream: ""http://[::1]:8000/socket.io/?eio=3&amp;transport=polling&amp;t=lybyqan&amp;sid=c5nkust9feprsu2raaay"", host: ""api.myapp.com"", referrer: ""https://myapp.com/""
2016/11/22 12:10:21 [warn] 6#6: *157 upstream server temporarily disabled while connecting to upstream, client: 10.8.0.1, server: api.myapp.com, request: ""get /socket.io/?eio=3&amp;transport=polling&amp;t=lybyqan&amp;sid=c5nkust9feprsu2raaay http/1.1"", upstream: ""http://[::1]:8000/socket.io/?eio=3&amp;transport=polling&amp;t=lybyqan&amp;sid=c5nkust9feprsu2raaay"", host: ""api.myapp.com"", referrer: ""https://myapp.com/""
10.8.0.1 - - [22/nov/2016:12:10:22 +0000] ""get /socket.io/?eio=3&amp;transport=polling&amp;t=lybyqan&amp;sid=c5nkust9feprsu2raaay http/1.1"" 200 4 ""https://myapp.com/"" ""mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/54.0.2840.99 safari/537.36""


update

when i change replicas to 1 in app-deployment.yaml it's work. but i think it's not a good solution. i need 3 replicas. 

apiversion: extensions/v1beta1
    kind: deployment
    metadata:
      name: app-nginx
    spec:
      replicas: 1
      template:
        metadata:
          labels:
            app: app-nginx


how to make it work with 3 replicas?
",<node.js><nginx><socket.io><kubernetes><google-kubernetes-engine>,41499381,8,"after i update kubernetes service template to use sessionaffinity: clientip it works now. but just get some error when first press ctrl + f5 and on second press it's work fine.

error during websocket handshake: unexpected response code: 400


however, i still get data from server. so i think it's okay.

updated service template

apiversion: v1
kind: service
metadata:
  name: app-nginx
  labels:
    app: app-nginx
spec:
  sessionaffinity: clientip
  type: loadbalancer
  ports:
    # the port that this service should serve on.
    - port: 80
      targetport: 80
      protocol: tcp
      name: http
    - port: 443
      targetport: 443
      protocol: tcp
      name: https
  # label keys and values that must match in order
  # to receive traffic for this service.
  selector:
    app: app-nginx

"
72492756,kubernetes patch multiple resources not working,"i'm trying to apply the same job history limits to a number of cronjobs using a patch like the following, named kubejobhistorylimit.yml:
apiversion: batch/v1beta1
kind: cronjob
spec:
  successfuljobshistorylimit: 1
  failedjobshistorylimit: 1

my kustomization.yml looks like:
bases:
  - ../base
configmapgenerator:
- name: inductions-config
  env: config.properties
patches:
  - path: kubejobhistorylimit.yml
    target:
      kind: cronjob
patchesstrategicmerge:
  - job_specific_patch_1.yml
  - job_specific_patch_2.yml
  ...
resources:
  - secrets-uat.yml

and at some point in my ci pipeline i have:
kubectl --kubeconfig $kubeconfig apply --force -k ./

the kubectl version is 1.21.9.
the issue is that the job history limit values don't seem to be getting picked up. is there something wrong w/ the configuration or the version of k8s i'm using?
",<kubernetes><kubectl><patch>,72493326,8,"with kustomize 4.5.2, your patch as written doesn't apply; it fails with:
error: trouble configuring builtin patchtransformer with config: `
path: kubejobhistorylimit.yml
target:
  kind: cronjob
`: unable to parse sm or json patch from [apiversion: batch/v1
kind: cronjob
spec:
  successfuljobshistorylimit: 1
  failedjobshistorylimit: 1
]

this is because it's missing metadata.name, which is required, even if it's ignored when patching multiple objects. if i modify the patch to look like this:
apiversion: batch/v1
kind: cronjob
metadata:
  name: ignored
spec:
  successfuljobshistorylimit: 1
  failedjobshistorylimit: 1

it seems to work.
if i have base/cronjob1.yaml that looks like:
apiversion: batch/v1
kind: cronjob
metadata:
  name: cronjob1
spec:
  failedjobshistorylimit: 2
  successfuljobshistorylimit: 5
  jobtemplate:
    spec:
      template:
        spec:
          containers:
          - command:
            - sleep
            - 60
            image: docker.io/alpine:latest
            name: example
  schedule: 30 3 * * *

then using the above patch and a overlay/kustomization.yaml like this:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
resources:
- ../base
patches:
- path: kubejobhistorylimit.yml
  target:
    kind: cronjob

i see the following output from kustomize build overlay:
apiversion: batch/v1
kind: cronjob
metadata:
  name: cronjob2
spec:
  failedjobshistorylimit: 1
  jobtemplate:
    spec:
      template:
        spec:
          containers:
          - command:
            - sleep
            - 60
            image: docker.io/alpine:latest
            name: example
  schedule: 30 3 * * *
  successfuljobshistorylimit: 1

you can see the two attributes have been updated correctly.
"
58697950,kubernetes nginx ingress and socket.io connection issues,"i'm currently having issues with my react app chatting with a nodejs socket.io app. 

however, i have narrowed it down and believe it is an ingress misconfiguration. port-forwarding the socket.io nodejs pod and connecting with react via 127.0.0.1:3020 works fine.

socket.io deployment file

apiversion: apps/v1
kind: deployment
metadata:
  name: websockettest-deployment
spec:
  replicas: 1
  selector:
    matchlabels:
      component: websockettest
  template:
    metadata:
      labels:
        component: websockettest
    spec:
      containers:
        - name: websockettest
          image: websockettest
          imagepullpolicy: ifnotpresent
          ports:
            - containerport: 3020


socket io service config

apiversion: v1
kind: service
metadata:
  name: websockettest-cluster-ip-service
spec:
  type: clusterip
  selector:
    component: websockettest
  ports:
    - port: 3020
      targetport: 3020


ingress configuration

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-service
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.org/websocket-services: websockettest-cluster-ip-service
spec:
  rules:
    - http:
        paths:
          - path: /websockettest/?(.*)
            backend:
              servicename: websockettest-cluster-ip-service
              serviceport: 3020



nodejs socket.io

const http = require('http');
const express = require('express');
var app = express();
var server = http.createserver(app);
var io = require('socket.io')(server);
io.set(""transports"", [""websocket""]);

io.on('connection', function (socket) {
  console.log('connected socket!');

  socket.on('greet', function (data) {
    console.log(data);
    socket.emit('respond', { hello: 'hello' });
  });
  socket.on('disconnect', function () {
    console.log('socket disconnected');
  });
});

const port = process.env.port || 3020;
server.listen(port, () =&gt; {
  console.log(`server is up on port ${port}`);
});


react socket.io-client

// various attempts:
// websockettest
// websockettest-cluster-ip-service
// http://192.168.64.11:3020
// :3020/websockettest
// 127.0.0.1:3020 - port forwarding works

const socket = io('192.168.64.11/websockettest', {
  'reconnection': false, 
  transports: ['websocket']
});

",<node.js><reactjs><kubernetes><socket.io><kubernetes-ingress>,59206507,8,"socket.io has a specific path it uses for websocket communication. due to this, the kubernetes ingress needs to be configured to support ""/socket.io"". additionally, the node.js middleware also needs to have custom namespace for the socket endpoint.

ingress-ws-service.yaml

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-ws-service
  annotations:
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.org/websocket-services: ""websockettest-cluster-ip-service""

spec:
  rules:
    - http:
        paths:
          - path: /websockettest/.*
            backend:
              servicename: websockettest-cluster-ip-service
              serviceport: 3020
          - path: /socket.io/.*
            backend:
              servicename: websockettest-cluster-ip-service
              serviceport: 3020


nodejs socket.io

const http = require('http');
const express = require('express');
var app = express();
var server = http.createserver(app);
var io = require('socket.io')(server);

const nsp = io.of(""/websockettest"");

nsp.on('connection', function (socket) {
  console.log('connected socket!');

  socket.on('greet', function (data) {
    console.log(data);
    nsp.emit('respond', { hello: 'hello' });
  });
  socket.on('disconnect', function () {
    console.log('socket disconnected');
  });
});

const port = process.env.port || 3020;
server.listen(port, () =&gt; {
  console.log(`server is up on port ${port}`);
});


react client

const socket = io('/websockettest', {
  'reconnection': true, 
  transports: ['websocket']
});

"
55154756,how to loop different templates in helm for kubernetes?,"i want to deploy multiple deployments of pods with different images, ports, etc. but with very similar other properties. so i want to declare a single deployment.yaml file that looks something like this

{{- range .values.types }}
apiversion: extensions/v1beta1
kind: deployment
metadata:
...
{{- end }}



where my values.yaml is 

types:
  - foo
  - bar
  - baz


however, this only spins up a single kubernetes deployment when i helm install because everything is in one template file. any ideas on how to do this?
",<kubernetes><kubernetes-helm>,55159868,8,"kubernetes generally uses yaml syntax, and that allows multiple ""documents"" to be in a single physical file with a --- delimiter before each one.  helm in turn generally operates by applying the templating to produce a plain-text file and in effect feeding it to kubectl apply.

the upshot of this is that if you start each kubernetes object description with the --- start-of-document delimiter, it should work:

{{- range .values.types }}
---
apiversion: extensions/v1beta1
kind: deployment
metadata:
...
{{- end }}

"
67617808,helm: how to avoid recreating secrets on upgrade?,"i have something in a secret template like this:
apiversion: v1
kind: secret
metadata:
  # not relevant
type: opaque
data:
  password: {{ randalphanum 32 | b64enc | quote }}

now, when doing helm upgrade, the secret is recreated, but the pods using this aren't (they also shouldn't, this is ok).
this causes the pods to fail when they are restarted or upgraded as the new password now doesn't match the old one.
is it possible to skip re-creation of the secret when it exists, like, a {{- if not(exists thesecret) }} and how to do it?
",<kubernetes><kubernetes-helm>,67618633,8,"you can use the look up function in helm to check the if secret exist or not
https://helm.sh/docs/chart_template_guide/functions_and_pipelines/#using-the-lookup-function
function in helm chart goes like : https://github.com/sankalp-r/helm-charts-examples/blob/1081ab5a5af3a1c7924c826c5a2bed4c19889daf/sample_chart/templates/_helpers.tpl#l67
{{/*
example for function
*/}}
{{- define &quot;gen.secret&quot; -}}
{{- $secret := lookup &quot;v1&quot; &quot;secret&quot; .release.namespace &quot;test-secret&quot; -}}
{{- if $secret -}}
{{/*
   reusing value of secret if exist
*/}}
password: {{ $secret.data.password }}
{{- else -}}
{{/*
    add new data
*/}}
password: {{ randalphanum 32 | b64enc | quote }}
{{- end -}}
{{- end -}}

secret creation will be something like
example file : https://github.com/sankalp-r/helm-charts-examples/blob/main/sample_chart/templates/secret.yaml
apiversion: v1
kind: secret
metadata:
  name: &quot;test-secret&quot;
type: opaque
data:
{{- ( include &quot;gen.secret&quot; . ) | indent 2 -}}

chart example : https://github.com/sankalp-r/helm-charts-examples
{{- $secret := (lookup &quot;v1&quot; &quot;secret&quot; .release.namespace &quot;test-secret&quot; -}}
apiversion: v1
kind: secret
metadata:
  name: test-secret
type: opaque

# 2. if the secret exists, write it back
{{ if $secret -}}
data:
  password: {{ $secret.data.password }}

# 3. if it doesn't exist ... create new
{{ else -}}
stringdata:
  password: {{ randalphanum 32 | b64enc | quote }}
{{ end }}

"
54698875,gcloud kubernetes cluster with 1 insufficient cpu error,"i created a kubernetes cluster on google cloud using:

gcloud container clusters create my-app-cluster --num-nodes=1


then i deployed my 3 apps (backend, frontend and a scraper) and created a load balancer. i used the following configuration file:

apiversion: apps/v1
kind: deployment
metadata:
    name: my-app-deployment
    labels:
        app: my-app
spec:
    replicas: 1
    selector:
        matchlabels:
            app: my-app
    template:
        metadata:
            labels:
                app: my-app
        spec:
            containers:
              - name: my-app-server
                image: gcr.io/my-app/server
                ports:
                  - containerport: 8009
                envfrom:
                  - secretref:
                        name: my-app-production-secrets
              - name: my-app-scraper
                image: gcr.io/my-app/scraper
                ports:
                  - containerport: 8109
                envfrom:
                  - secretref:
                        name: my-app-production-secrets
              - name: my-app-frontend
                image: gcr.io/my-app/frontend
                ports:
                  - containerport: 80
                envfrom:
                  - secretref:
                        name: my-app-production-secrets

---

apiversion: v1
kind: service
metadata:
    name: my-app-lb-service
spec:
    type: loadbalancer
    selector:
        app: my-app
    ports:
      - name: my-app-server-port
        protocol: tcp
        port: 8009
        targetport: 8009
      - name: my-app-scraper-port
        protocol: tcp
        port: 8109
        targetport: 8109
      - name: my-app-frontend-port
        protocol: tcp
        port: 80
        targetport: 80


when typing kubectl get pods i get:

name                                   ready     status    restarts   age
my-app-deployment-6b49c9b5c4-5zxw2   0/3       pending   0          12h


when investigation i google cloud i see ""unschedulable"" state with ""insufficient cpu"" error on pod:



when going to nodes section under my cluster in the clusters page, i see 681 mcpu requested and 940 mcpu allocated:


what is wrong? why my pod doesn't start?
",<kubernetes><google-cloud-platform><gcloud><kubectl><google-kubernetes-engine>,54699582,8,"every container has a default cpu request (in gke ive noticed its 0.1 cpu or 100m). assuming these defaults you have three containers in that pod so youre requesting another 0.3 cpu.

the node has 0.68 cpu (680m) requested by other workloads and a total limit (allocatable) on that node of 0.94 cpu (940m).

if you want to see what workloads are reserving that 0.68 cpu, you need to inspect the pods on the node. in the page on gke where you see the resource allocations and limits per node, if you click the node it will take you to a page that provides this information.
in my case i can see 2 pods of kube-dns taking 0.26 cpu each, amongst others. these are system pods that are needed to operate the cluster correctly. what you see will also depend on what add-on services you have selected, for example: http load balancing (ingress), kubernetes dashboard and so on.

your pod would take cpu to 0.98 cpu for the node which is more than the 0.94 limit, which is why your pod cannot start.

note that the scheduling is based on the amount of cpu requested for each workload, not how much it actually uses, or the limit.

your options:


turn off any add-on service which is taking cpu resource that you don't need.
add more cpu resource to your cluster. to do that you will either need to change your node pool to use vms with more cpu, or increase the number of nodes in your existing pool. you can do this in gke console or via the gcloud command line.
make explicit requests in your containers for less cpu that will override the defaults.


apiversion: apps/v1
kind: deployment
...
        spec:
            containers:
              - name: my-app-server
                image: gcr.io/my-app/server
                ...
                resources:
                  requests:
                     cpu: ""50m""
              - name: my-app-scraper
                image: gcr.io/my-app/scraper
                ...
                resources:
                  requests:
                     cpu: ""50m""
              - name: my-app-frontend
                image: gcr.io/my-app/frontend
                ...
                resources:
                  requests:
                     cpu: ""50m""

"
69837573,restart a kubernetes job or pod with a different command,"i'm looking for a way to quickly run/restart a job/pod from the command line and override the command to be executed in the created container.
for context, i have a kubernetes job that gets executed as a part of our deploy process. sometimes that job crashes and i need to run certain commands inside the container the job creates to debug and fix the problem (subsequent jobs then succeed).
the way i have done this so far is:

copy the yaml of the job, save into a  file
clean up the yaml (delete kubernetes-managed fields)
change the command: field to tail -f /dev/null (so that the container stays alive)
kubectl apply -f job.yaml &amp;&amp; kubectl get all &amp;&amp; kubectl exec -ti pod/foobar bash
run commands inside the container
kubectl delete job/foobar when i am done

this is very tedious. i am looking for a way to do something like the following
kubectl restart job/foobar --command &quot;tail -f /dev/null&quot;

# or even better
kubectl run job/foobar --exec --interactive bash


i cannot use the run command to create a pod:
kubectl run --image xxx -ti

because the job i am trying to restart has certain volumemounts and other configuration i need to reuse. so i would need something like kubectl run --from-config job/foobar.

is there a way to achieve this or am i stuck with juggling the yaml definition file?

edit: the job yaml looks approx. like this:
apiversion: batch/v1
kind: job
metadata:
    name: database-migrations
    labels:
        app: myapp
        service: myapp-database-migrations
spec:
    backofflimit: 0
    template:
        metadata:
            labels:
                app: myapp
                service: myapp-database-migrations
        spec:
            restartpolicy: never
            containers:
                - name: migrations
                  image: registry.example.com/myapp:977b44c9
                  command:
                      - &quot;bash&quot;
                      - &quot;-c&quot;
                      - |
                          set -e -e
                          echo &quot;running database migrations...&quot;
                          do-migration-stuff-here
                          echo &quot;migrations finished at $(date)&quot;
                  imagepullpolicy: always
                  volumemounts:
                      -   mountpath: /home/example/myapp/app/config/conf.yml
                          name: myapp-config-volume
                          subpath: conf.yml
                      -   mountpath: /home/example/myapp/.env
                          name: myapp-config-volume
                          subpath: .env
            volumes:
                - name: myapp-config-volume
                  configmap:
                      name: myapp
            imagepullsecrets:
                -   name: k8s-pull-project

",<kubernetes><kubectl><kubernetes-jobs>,70112781,8,"the commands you suggested don't exist. take a look at this reference where you can find all available commands.
based on that documentation the task of the job is to create one or more pods and continue retrying execution them until the specified number of successfully terminated ones will be achieved. then the job tracks the successful completions. you cannot just update the job because these fields are not updatable. to do what's you want you should delete current job and create one once again.

i recommend you to keep all your configurations in files. if you have a problem with configuring job commands, practice says that you should modify these settings in yaml and apply to the cluster - if your deployment crashes - by storing the configuration in files, you have a backup.
if you are interested how to improve this task, you can try those 2 examples describe below:
firstly i've created several files:
example job  (job.yaml):
apiversion: batch/v1
kind: job
metadata:
  name: test1
spec:
  template:
    spec:
      containers:
      - name: test1
        image: busybox
        command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 300&quot;]
        volumemounts:
        - name: foo
          mountpath: &quot;/script/foo&quot;
      volumes:
      - name: foo
        configmap:
          name: my-conf
          defaultmode: 0755
      restartpolicy: onfailure

patch-file.yaml:
spec:
  template:
    spec:
      containers:
      - name: test1
        image: busybox
        command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo 'patching test' &amp;&amp; sleep 500&quot;]

and  configmap.yaml:
apiversion: v1
kind: configmap
metadata:
  name: my-conf
data:
  test: |
    #!/bin/sh
    echo &quot;skrypt test&quot;



if you want to automate this process you can use plugin


a plugin is a standalone executable file, whose name begins with kubectl-. to install a plugin, move its executable file to anywhere on your path.
there is no plugin installation or pre-loading required. plugin executables receive the inherited environment from the kubectl binary. a plugin determines which command path it wishes to implement based on its name.

here is the file that can replace your job

a plugin determines the command path that it will implement based on its filename.

kubectl-job:
#!/bin/bash
kubectl patch -f job.yaml -p &quot;$(cat patch-job.yaml)&quot; --dry-run=client -o yaml | kubectl replace --force -f - &amp;&amp; kubectl wait --for=condition=ready pod -l job-name=test1 &amp;&amp; kubectl exec -it $(kubectl get pod -l job-name=test1 --no-headers -o custom-columns=&quot;:metadata.name&quot;) -- /bin/sh

this command uses an additional file (patch-job.yaml, see this link) - within we can put our changes for job.
then you should change the permissions of this file and move it:
sudo chmod +x .kubectl-job
sudo mv ./kubectl-job /usr/local/bin

it's all done. right now you can use it.
$ kubectl job
job.batch &quot;test1&quot; deleted
job.batch/test1 replaced
pod/test1-bdxtm condition met
pod/test1-nh2pv condition met
/ #

as you can see job has been replaced (deleted and created).


you can also use single-line command, here is the example:

kubectl get job test1 -o json | jq &quot;del(.spec.selector)&quot; | jq &quot;del(.spec.template.metadata.labels)&quot; | kubectl patch -f - --patch '{&quot;spec&quot;:  {&quot;template&quot;:  {&quot;spec&quot;:  {&quot;containers&quot;: [{&quot;name&quot;: &quot;test1&quot;, &quot;image&quot;: &quot;busybox&quot;, &quot;command&quot;: [&quot;/bin/sh&quot;, &quot;-c&quot;,  &quot;sleep 200&quot;]}]}}}}' --dry-run=client -o yaml | kubectl replace --force -f -

with this command you can change your job entering parameters &quot;by hand&quot;. here is the output:
job.batch &quot;test1&quot; deleted
job.batch/test1 replaced

as you can see this solution works as well.
"
61185530,another nginx ingress rewrite-target problem,"i have one service and a single ingress resource with kubenetes nginx ingress controller. i want the /student path of my url to go to the root of the application and match any other url segments which follow the student. 

for example: http://example.com/student/ver should match the /ver route of my application. 

however, my ingress always hit the application with the /student url path prefixing the other url segments. if i call http://example.com/student/ver, my application is hit with the same url (student/ver).

my ingress:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
  name: ingress-resource
spec:
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          servicename: lesson-streaming
          serviceport: 80
        path: /student


i spent days with this and was not successful once. 



edit: 

the ingress is changed to the following - not my requests say http 404

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    ingress.kubernetes.io/rewrite-target: /$2
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
  name: ingress-resource
  namespace: default
spec:
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          servicename: lesson-streaming
          serviceport: 80
        path: /student(/|$)(.*)

",<node.js><nginx><kubernetes><kubernetes-ingress><nginx-ingress>,61226772,8,"you can follow the link to use the rewrite-target annotation correctly and keep the right key nginx.ingress.kubernetes.io/rewrite-target.

apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  name: rewrite
  namespace: default
spec:
  rules:
  - host: rewrite.bar.com
    http:
      paths:
      - backend:
          servicename: http-svc
          serviceport: 80
        path: /something(/|$)(.*)

"
63324514,how to set https as default on gke ingress-gce,"i currently have a working frontend and backend nodeports with an ingress service setup with gke's google-managed certificates.
however, my issue is that by default when a user goes to samplesite.com, it uses http as default. this means that the user needs to specifically type in the browser https://samplesite.com in order to get the https version of my website.
how do i properly disable http on gke ingress, or how do i redirect all my traffic to https? i understand that this can be forcefully done in my backend code as well but i want to separate concerns and handle this in my kubernetes setup.
here is my ingress.yaml file:
kind: service
apiversion: v1
metadata:
  name: frontend-node-service
  namespace: default
spec:
  type: nodeport
  selector:
    app: frontend
  ports:
  - port: 5000
    targetport: 80
    protocol: tcp
    name: http
---
kind: service
apiversion: v1
metadata:
  name: backend-node-service
  namespace: default
spec:
  type: nodeport
  selector:
    app: backend
  ports:
  - port: 8081
    targetport: 9229
    protocol: tcp
    name: http
---
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: samplesite-ingress-frontend
  namespace: default
  annotations:
    kubernetes.io/ingress.global-static-ip-name: &quot;samplesite-static-ip&quot;
    kubernetes.io/ingress.allow-http: &quot;false&quot;
    networking.gke.io/managed-certificates: samplesite-ssl
spec:
  backend:
    servicename: frontend-node-service
    serviceport: 5000
---
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: samplesite-ingress-backend
  namespace: default
  annotations:
    kubernetes.io/ingress.global-static-ip-name: &quot;samplesite-backend-ip&quot;
    kubernetes.io/ingress.allow-http: &quot;false&quot;
    networking.gke.io/managed-certificates: samplesite-api-ssl
spec:
  backend:
    servicename: backend-node-service
    serviceport: 8081

",<docker><kubernetes><google-cloud-platform><google-kubernetes-engine><kubernetes-ingress>,63344422,8,"currently gke ingress does not support out of the box http-&gt;https redirect.
there is an ongoing feature request for it here:

issuetracker.google.com: issues: redirect all http traffic to https when using the http(s) load balancer


there are some workarounds for it:

use different ingress controller like nginx-ingress.
create a http-&gt;https redirection in gcp cloud console.



how do i properly disable http on gke ingress, or how do i redirect all my traffic to https?

to disable http on gke you can use following annotation:

kubernetes.io/ingress.allow-http: &quot;false&quot;

this annotation will:

allow traffic only on port: 443 (https).
deny traffic on port: 80 (http) resulting in error code: 404.


focusing on previously mentioned workarounds:
use different ingress controller like nginx-ingress
one of the ways to have the http-&gt;https redirection is to use nginx-ingress. you can deploy it with official documentation:

kubernetes.github.io: ingress-nginx: deploy: gce-gke

this ingress controller will create a service of type loadbalancer which will be the entry point for your traffic. ingress objects will respond on loadbalancer ip. you can download the manifest from installation part and modify it to support the static ip you have requested in gcp. more reference can be found here:

stackoverflow.com: how to specify static ip address for kubernetes load balancer?

you will need to provide your own certificates or use tools like cert-manager to have https traffic as the annotation: networking.gke.io/managed-certificates will not work with nginx-ingress.
i used this yaml definition and without any other annotations i was always redirected to the https:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: nginx-ingress
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot; # important
spec:
  tls: # https part
  - secretname: ssl-certificate # self provided cert name
  rules:
  - host:
    http:
      paths:
      - path: /
        backend:
          servicename: hello-service
          serviceport: hello-port


create a http-&gt;https redirection in gcp cloud console.
there is also an option to manually create a redirection rule for your ingress resource. you will need to follow official documentation:

cloud.google.com: load balancing: docs: https: setting up http -&gt; https redirect

using the part of above documentation, you will need to create a http loadbalancer responding on the same ip as your ingress resource (reserved static ip) redirecting traffic to https.

disclaimer!
your ingress resource will need to have following annotation:

kubernetes.io/ingress.allow-http: &quot;false&quot;

lack there of will result in forbidding you to create a redirection mentioned above.

"
65703968,"kubernetes ingress: nginx, use-regex to match exact url path","i have a few pods that i am trying to match urls for their respective services.
please note that i need to use nginx.ingress.kubernetes.io/rewrite-target to solve this and not nginx.ingress.kubernetes.io/rewrite-target
my ingress config file looks like this. notice the /api/tile-server/ does not have any regex pattern
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: ingress-service
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    kubernetes.io/ingress.class: &quot;nginx&quot;
    cert-manager.io/cluster-issuer: &quot;letsencrypt-prod&quot;
  namespace: default
spec:
  tls:
    - hosts:
        - example.com
      secretname: tls-secret
  rules:
    - host: example.com
      http:
        paths:
          - path: /?(.*)
            backend:
              servicename: client
              serviceport: 80
          - path: /api/auth/?(.*)
            backend:
              servicename: auth
              serviceport: 8000
          - path: /api/data/?(.*)
            backend:
              servicename: data
              serviceport: 8001
          - path: /api/tile-server/
            backend:
              servicename: tile-server
              serviceport: 7800


client pod is a react app built inside nginx docker image working fine
nginx.conf looks like this (if it's helpful)

server {
    # listen on port 80
    listen 80;
    # where the root here
    root /usr/share/nginx/html;
    # what file to server as index
    index index.html index.htm;

    location / {
        # first attempt to serve request as file, then
        # as directory, then fall back to redirecting to index.html
        try_files $uri $uri/ /index.html;
    }

    # media: images, icons, video, audio, htc
    location ~* \.(?:jpg|jpeg|gif|png|ico|cur|gz|svg|svgz|mp4|ogg|ogv|webm|htc)$ {
        expires 1m;
        access_log off;
        add_header cache-control &quot;public&quot;;
    }

    # javascript and css files
    location ~* \.(?:css|js)$ {
        try_files $uri =404;
        expires 1y;
        access_log off;
        add_header cache-control &quot;public&quot;;
    }

    # any route containing a file extension (e.g. /devicesfile.js)
    location ~ ^.+\..+$ {
        try_files $uri =404;
    }
}


auth and data are flask api pods working fine
tile-server is also a flask pod but need not do any pattern matching. i need to match the exact /api/tile-server/ url

i have tried the following patterns but failed:

/api/tile-server/
/api/tile-server/?(.*)
/api/tile-server(/|$)?(.*)

i can confirm that the pods/services are running on their proper ports and i am able to access them through node ports but not through load balancer/domain.
what would be the right pattern to exactly match /api/tile-server/ url?
",<nginx><kubernetes><kubernetes-ingress>,65772589,8,"first solution - create separate ingress object for tile-server with rewrite-target annotation. this will work because ingress rules with the same host are merged together by ingress controller and separate ingress object allow for use of different annotations per object:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: tile-ingress-service
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/rewrite-target: &quot;/$2&quot;
    kubernetes.io/ingress.class: &quot;nginx&quot;
    cert-manager.io/cluster-issuer: &quot;letsencrypt-prod&quot;
  namespace: default
spec:
  tls:
    - hosts:
        - example.com
      secretname: tls-secret
  rules:
    - host: example.com
      http:
        paths:
          - path: /api/tile-server(/|$)(.*)
            backend:
              servicename: tile-server
              serviceport: 7800

second solution - rewrite current ingress to work with rewrite-path. some regex changes are necessary.
notice the non-capturing group notation: (?:&lt;regex&gt;). this allows to skip numbering for these groups since i need everything relevant to be in the first group in order for it to work, because rewrite-target: &quot;/$1&quot;.
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: ingress-service
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/rewrite-target: &quot;/$1&quot;
    kubernetes.io/ingress.class: &quot;nginx&quot;
    cert-manager.io/cluster-issuer: &quot;letsencrypt-prod&quot;
  namespace: default
spec:
  tls:
    - hosts:
        - example.com
      secretname: tls-secret
  rules:
    - host: example.com
      http:
        paths:
          - path: /(.*)
            backend:
              servicename: client
              serviceport: 80
          - path: /(api/auth(?:/|$).*)
            backend:
              servicename: auth
              serviceport: 8000
          - path: /(api/data(?:/|$).*)
            backend:
              servicename: data
              serviceport: 8001
          - path: /api/tile-server(?:/|$)(.*)
            backend:
              servicename: tile-server
              serviceport: 7800

here is how the rewrites will work for:

auth service (same applies to data service)

    /api/auth      ---&gt;  /api/auth
    /api/auth/     ---&gt;  /api/auth/
    /api/auth/xxx  ---&gt;  /api/auth/xxx


tile-server service:

    /api/tile-server      ---&gt;  /
    /api/tile-server/     ---&gt;  /
    /api/tile-server/xxx  ---&gt;  /xxx


client service

    /xxx  ---&gt;  /xxx

notice that the following paths will be forwarded to client service (where xxx is any alphanumerical string):
    /api/authxxx
    /api/dataxxx
    /api/tile-serverxxx

if you want them to be forwaded to other/matching services, add ? after (?:/|$) in path.
"
52616189,kubernetes nginx ingress http to https redirect via 301 instead of 308?,"we are running a couple of k8s clusters on azure aks.
the service (ghost blog) is behind the nginx ingress and secured with a cert from letsencrypt.  all of that works fine but the redirect behavior is what i am having trouble with.

the ingress correctly re-directs from http://whatever.com to
https://whatever.com the issue is that it does so using a 308
redirect which strips all post/page meta anytime a user shares a
page from the site.

the issue results in users who share any page of the site on most social properties receiving a 'preview link'  where the title of the page and the page meta preview do not work and are instead replaced with '308 permanent redirect' text  which looks like this:

from the ingress-nginx docs over here i can see that this is the intended behavior (ie. 308 redirect) what i believe is not intended is the interaction with social sharing services when those services attempt to create a page preview.
while the issue would be solved by facebook (or twitter, etc etc) pointing direct to the https site by default, i currently have no way to force those sites to look to https for the content that will be used to create the previews.
setting permanent re-direct code
i can also see that it looks like i should be able to set the redirect code to whatever i want it to be (i believe a 301 redirect will allow facebook et al. to correctly pull post/page snippet meta), docs on that found here.
the problem is that when i add the redirect-code annotation as specified:
nginx.ingress.kubernetes.io/permanent-redirect-code: &quot;301&quot;

i still get a 308 re-direct on my resources despite being able to see (from my kubectl proxy) that the redirect-code annotation correctly applied. for reference, my full list of annotations on my ingress looks like this:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ghost-ingress
  annotations:
    kubernetes.io/tls-acme: &quot;true&quot;
    nginx.ingress.kubernetes.io/permanent-redirect-code: &quot;301&quot;


to reiterate  my question is; what is the correct way to force a redirect to https via a custom error code (in my case 301)?

",<kubernetes><kubernetes-ingress><nginx-ingress>,52617528,8,"my guess is the tls redirect shadows the nginx.ingress.kubernetes.io/permanent-redirect-code annotation. 

you can actually change the configmap for your nginx-configuration so that the default redirect is 301. that's the configuration your nginx ingress controller uses for nginx itself. the configmap looks like this:

apiversion: v1
kind: configmap
metadata:
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
  name: nginx-configuration
  namespace: ingress-nginx
data:
  use-proxy-protocol: ""true""
  http-redirect-code: ""301""


you can find more about the configmap options here. note that if you change the configmap you'll have to restart your nginx-ingress-controller pod.

you can also shell into the nginx-ingress-controller pod and see the actual nginx  configs that the controller creates:

kubectl -n ingress-nginx exec -it nginx-ingress-controller-xxxxxxxxxx-xxxxx bash
www-data@nginx-ingress-controller-xxxxxxxxx-xxxxx:/etc/nginx$ cat /etc/nginx/nginx.conf

"
60744883,redis in kubernetes - connection refused,"i'm trying to deploy a redis pod to allow my staging app connecting to it (to avoid using redis managed service in staging env).

i'm using google cloud platform with gke, so managed k8s cluster...

however, when i try to make a connection (from another redis pod only to test) i receive this message :

could not connect to redis at redis-cip.redis.svc.cluster.local:6379: connection refused


the command that i use to make the test is this :

redis-cli -h redis-cip.redis.svc.cluster.local -p 6379 


the url is composed by :


redis-cip: is the service clusterip that i use to allow connection to my redis pod
redis : is the namespace where is hosted redis pod


moreover,i use, as is already written in some question on stackoverflow, this redis configuration :

protected-mode no
maxmemory 32mb
maxmemory-policy allkeys-lru


in attached the full k8s mapping :

configmap :

apiversion: v1
kind: configmap
metadata:
  name: redis-configmap
  namespace: redis
data:
  redis.conf: |
    protected-mode no
    maxmemory 32mb
    maxmemory-policy allkeys-lru


redis deploy :

apiversion: v1
kind: pod
metadata:
  name: redis
  namespace: redis
spec:
  containers:
    - name: redis
      image: redis
      command:
        - redis-server
        - /usr/local/etc/redis/redis.conf
      env:
        - name: master
          value: ""true""
      ports:
        - containerport: 6379
      volumemounts:
        - mountpath: /redis-master-data
          name: data
        - mountpath: /usr/local/etc/redis/
          name: redis-configmap
      resources:
        requests:
          memory: {{ .values.resources.requests.memory }}
          cpu: {{ .values.resources.requests.cpu }}
        limits:
          memory: {{ .values.resources.limits.memory }}
          cpu: {{ .values.resources.limits.cpu }}
  volumes:
    - name: data
      emptydir: {}
    - name: redis-configmap
      configmap:
        name: redis-configmap


clusterip service:

apiversion: v1
kind: service
metadata:
  name: redis-cip
  namespace: redis
  labels:
    app: redis
spec:
  ports:
  - port: 6379
    targetport: 6379
  selector:
    app: redis
  type: clusterip

",<kubernetes><redis><google-kubernetes-engine>,60750777,7,"the connection gets refused because there are no pods with label app:redis .
add labels to your pod that are identical to service spec.selector

apiversion: v1
kind: pod
metadata:
  name: redis
  namespace: redis
  labels:
     app:redis
spec:
  containers:
    - name: redis
      image: redis

"
42393569,kubernetes client-python creating a service error,"i am trying to create a new service for one of my deployments named node-js-deployment in gce hostes kubernetes cluster

i followed the documentation to create_namespaced_service

this is the service data: 

{
    ""kind"": ""service"",
    ""apiversion"": ""v1"",
    ""metadata"": {
        ""name"": ""node-js-service""
    },
    ""spec"": {
        ""selector"": {
            ""app"": ""node-js""
        },
        ""ports"": [
            {
                ""protocol"": ""tcp"",
                ""port"": 80,
                ""targetport"": 8000
            }
        ]
    }
}


this is the python function to create the service 

api_instance = kubernetes.client.corev1api()
namespace = 'default'  

body = kubernetes.client.v1service()  # v1serice

# creating meta data
metadata = kubernetes.client.v1objectmeta()
metadata.name = ""node-js-service""

# creating spec 
spec = kubernetes.client.v1servicespec()

# creating port object
ports = kubernetes.client.v1serviceport()
ports.protocol = 'tcp'
ports.target_port = 8000
ports.port = 80

spec.ports = ports
spec.selector = {""app"": ""node-js""}

body.spec = spec


try:
    api_response = api_instance.create_namespaced_service(namespace, body, pretty=pretty)
    pprint(api_response)
except apiexception as e:
    print(""exception when calling corev1api-&gt;create_namespaced_service: %s\n"" % e)


error: 

reason: bad request
http response headers: httpheaderdict({'content-type': 'application/json', 'date': 'tue, 21 feb 2017 03:54:55 gmt', 'content-length': '227'})
http response body: {""kind"":""status"",""apiversion"":""v1"",""metadata"":{},""status"":""failure"",""message"":""service in version \""v1\"" cannot be handled as a service: only encoded map or array can be decoded into a struct"",""reason"":""badrequest"",""code"":400}


but the service is being created if i am passing json. not sure what i am doing wrong. 

any help is greatly appreciated, thank you.
",<python-3.x><kubernetes><google-kubernetes-engine>,42402431,7,"from reading your code, it seems that you miss assigning the metadata to body.metadata. and you missed that the ports field of the v1servicespec is supposed to be a list, but you used a single v1serviceport so without testing i assume this should works:

api_instance = kubernetes.client.corev1api()
namespace = 'default'

body = kubernetes.client.v1service()  # v1serice

# creating meta data
metadata = kubernetes.client.v1objectmeta()
metadata.name = ""node-js-service""

body.metadata = metadata

# creating spec
spec = kubernetes.client.v1servicespec()

# creating port object
port = kubernetes.client.v1serviceport()
port.protocol = 'tcp'
port.target_port = 8000
port.port = 80

spec.ports = [ port ]
spec.selector = {""app"": ""node-js""}

body.spec = spec


the definition could also be loaded from json / yaml directly as shown in two of the examples within the offical repo - see exec.py create_deployment.py.

your solution could then look like:

api_instance = kubernetes.client.corev1api()
namespace = 'default'

manifest = {
    ""kind"": ""service"",
    ""apiversion"": ""v1"",
    ""metadata"": {
        ""name"": ""node-js-service""
    },
    ""spec"": {
        ""selector"": {
            ""app"": ""node-js""
        },
        ""ports"": [
            {
                ""protocol"": ""tcp"",
                ""port"": 80,
                ""targetport"": 8000
            }
        ]
    }
}

try:
    api_response = api_instance.create_namespaced_service(namespace, manifest, pretty='true')
    pprint(api_response)
except apiexception as e:
    print(""exception when calling corev1api-&gt;create_namespaced_endpoints: %s\n"" % e)

"
60378117,limit the number of pods per node,"i'm trying to limit the number of pods per each node from my cluster.
i managed to add a global limit per node from kubeadm init with config file:

apiversion: kubeadm.k8s.io/v1beta1
kind: clusterconfiguration
networking:
  podsubnet: &lt;subnet&gt;
---
apiversion: kubelet.config.k8s.io/v1beta1
kind: kubeletconfiguration
maxpods: 10


this is not quite well because the limit is applied even on master node (where multiple kube-system pods are running and the number of pods here may increase over 10).
i would like to keep the default value at init and change the value at join on each node.
i have found something:

apiversion: kubelet.config.k8s.io/v1beta1
kind: kubeletconfiguration
maxpods: 10
---
apiversion: kubeadm.k8s.io/v1beta1
kind: joinconfiguration
discovery:
  bootstraptoken:
    apiserverendpoint: ""&lt;api_endpoint&gt;""
    token: ""&lt;token&gt;""
    unsafeskipcaverification: true


but, even if no error/warning appears, it seems that the value of maxpods is ignored. i can create more than 10 pods for that specific node.
also kubectl get node &lt;node&gt; -o yaml returns status.capacity.pods with its default value (110).
how can i proceed in order to have this pods limit applied per each node?

i would like to mention that i have basic/limited knowledge related to kubernetes.

thank you!
",<kubernetes><kubectl><kubeadm>,60379205,7,"there is a config.yaml file at /var/lib/kubelet. this config file is generated from kubelet config map in kube-system namespace when you run kubeadm join.partial content of the file is as below.

apiversion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cachettl: 0s
    enabled: true
  x509:
    clientcafile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: webhook
  webhook:
    cacheauthorizedttl: 0s
    cacheunauthorizedttl: 0s
clusterdns:
- 10.96.0.10
maxpods: 10


you can change that file and add maxpods parameter and then restart kubelet on the node.

sudo systemctl restart kubelet 


currently in kubeadm join there is no way to pass a kubelet config file.
"
54992723,allow access to all resources on kubernetes cluster except get nodes,"team, i have below cluster role on kubernetes that allows access to everything but i wan't to restrict node level commands and allow all rest.
what to modify below?
basically, user should be able to run
kubectl get all --all-namespaces

but not nodes info should not display
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: cluster-admin-test
rules:
  - apigroups:
      - '*'
    resources:
      - '*'
    verbs:
      - '*'
  - nonresourceurls:
      - '*'
    verbs:
      - '*'

",<kubernetes><kubectl><rbac>,54999647,7,"rules are purely additive, means that you cannot restrict rules.

thus, you will need to list all accessible resources, but ""nodes"" with appropriate operations

for example:

apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: ""true""
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: cluster-admin
rules: 
- apigroups: [""""] 
  resources: [""pods"",""services"",""namespaces"",""deployments"",""jobs""] 
  verbs: [""get"", ""watch"", ""list""]


also, it is highly not recommended to change cluster-admin role.
it is worth to create a new role and assign users to it.
"
54350071,exclude specific hosts from ssl redirect in kubernetes nginx ingress,"i have an nginx ingress controller set up on my kubernetes cluster, which by default does an https redirect for any requests that it receives, so http://example.com is automatically forwarded on to https://example.com.

i now have a host that i need to serve over http and not https, essentially excluding it from the ssl redirect. what i have found is that i can disable the ssl redirect across the whole ingress, but not for a specific host. 

my ingress yaml:

apiversion: extensions/v1beta1
kind: ingress
metadata:
   name: ingress
annotations:
    kubernetes.io/ingress.class: nginx
spec:
  tls:
  - hosts:
    - mysslsite.co.uk
secretname: tls-secret

rules:
 - host: my-ssl-site.co.uk
   http:
    paths:
    - path: /
      backend:
        servicename: my-service
        serviceport: 80
 - host: my-non-ssl-site.co.uk
   http:
      paths:
      - path: /
        backend:
          servicename: my-other-service
          serviceport: 80


my config map:

apiversion: v1
kind: configmap
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-0.28.3
    component: controller
    heritage: tiller
    release: nginx-ingress
  name: undercooked-moth-nginx-ingress-controller
  namespace: default
data:
  proxy-buffer-size: ""512k""
  client-header-buffer-size: ""512k""
  proxy-body-size: ""100m""
  large-client-header-buffers: ""4 512k""
  http2-max-field-size: ""512k""
  http2-max-header-size: ""512k""
  fastcgi_buffers: ""16 16k"" 
  fastcgi_buffer_size: ""32k""


what i have tried:


attempt to turn off ssl redirect across the board and set a rule to redirect to the site requiring ssl to https by setting the annotation nginx.ingress.kubernetes.io/ssl-redirect: ""false"" and adding the following config snippet:

nginx.ingress.kubernetes.io/configuration-snippet: |
      if ($host = 'my-ssl-site.co.uk' ) {
        rewrite ^ https://my-ssl-site.co.uk$request_uri permanent;
      }


this does remove the https redirect but results in a too many redirects error for the site requiring ssl.
attempted to add rules in the configmap as per this answer to turn off ssl redirect and handle the conditional redirect in a server config snippet but this still resulted in an ssl redirect.
tried to add a second ingress controller so that one could have ssl redirect enabled and the other one could have it turned off. i created the controller but i think i also need to create a second nginx ingress and configure and label the apps that will be returned to each? this seems like overkill when all i want to do is exclude one service on the cluster from the ssl redirect.


is there anything obvious i am missing? it feels as though it shouldn't be this hard to add a simple rule to exclude one host from the ssl-redirect.
",<ssl><nginx><https><kubernetes><kubernetes-ingress>,54370053,7,"you can create two ingress objects, one for each site in the same namespace.

use annotation nginx.ingress.kubernetes.io/ssl-redirect: ""true"" for ssl site

use annotation nginx.ingress.kubernetes.io/ssl-redirect: ""false"" for non-ssl site

apiversion: extensions/v1beta1
kind: ingress
metadata:
   name: cmac-ingress
   namespace: ns1
   annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: ""true""
spec:
  tls:
  - hosts:
    - my-ssl-site.co.uk
    secretname: testsecret-tls
  rules:
  - host: my-ssl-site.co.uk
    http:
      paths:
      - path: /
        backend: 
          servicename: my-service
          serviceport: 80
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
   name: cmac-ingress1
   namespace: ns1
   annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
spec:
  tls:
  - hosts:
    - my-site.co.uk
    secretname: testsecret-tls
  rules:
  - host: my-site.co.uk
    http:
      paths:
      - path: /
        backend: 
          servicename: my-service
          serviceport: 80


here is the result from ingress-controller nginx.conf file:

    ## start server my-site.co.uk
    server {
            server_name my-site.co.uk ;

            listen 80;

            set $proxy_upstream_name ""-"";

            listen 443  ssl http2;

            # pem sha: ffa288482443e529d72a0984724f79d5267a2a22
            ssl_certificate                         /etc/ingress-controller/ssl/default-fake-certificate.pem;
            ssl_certificate_key                     /etc/ingress-controller/ssl/default-fake-certificate.pem;

            location / {

                    &lt;some lines skipped&gt;

                    if ($scheme = https) {
                            more_set_headers                        ""strict-transport-security: max-age=15724800; includesubdomains"";
                    }

                    &lt;some lines skipped&gt;

            }

    }       
    ## end server my-site.co.uk

    ## start server my-ssl-site.co.uk
    server {
            server_name my-ssl-site.co.uk ;

            listen 80;

            set $proxy_upstream_name ""-"";

            listen 443  ssl http2;

            # pem sha: ffa288482443e529d72a0984724f79d5267a2a22
            ssl_certificate                         /etc/ingress-controller/ssl/default-fake-certificate.pem;
            ssl_certificate_key                     /etc/ingress-controller/ssl/default-fake-certificate.pem;

            location / {


                    &lt;some lines skipped&gt;

                    if ($scheme = https) {
                            more_set_headers                        ""strict-transport-security: max-age=15724800; includesubdomains"";
                    }

                    # enforce ssl on server side
                    if ($redirect_to_https) {

                            return 308 https://$best_http_host$request_uri;

                    }

                    &lt;some lines skipped&gt;

            }

    }    
    ## end server my-ssl-site.co.uk


you can find additional redirection section in the ssl-enforced site definition:

# enforce ssl on server side
if ($redirect_to_https) {

        return 308 https://$best_http_host$request_uri;

}

"
64742390,ingress to redirect to external resource using alb,"i have some services running on the cluster, and the alb is working fine. but i have a cloudfront distribution, and i want to use the cluster as an entry point because of some internal factors. so, i am trying to add an ingress to redirect the requests to the cloudfront distribution based on the default rule or a named host, both will work.
i tried 2 different ways, but no dice:
creating an external name and the ingress:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: test-ingress
  namespace: default
annotations:
  kubernetes.io/ingress.class: alb
  alb.ingress.kubernetes.io/scheme: &quot;internet-facing&quot;
  alb.ingress.kubernetes.io/listen-ports: '[{&quot;http&quot;:80,&quot;https&quot;: 443}]'
  alb.ingress.kubernetes.io/certificate-arn: &lt;my-cert-arn&gt;
  alb.ingress.kubernetes.io/actions.ssl-redirect: '{&quot;type&quot;: &quot;redirect&quot;, &quot;redirectconfig&quot;: { 
  &quot;protocol&quot;: &quot;https&quot;, &quot;port&quot;: &quot;443&quot;, &quot;statuscode&quot;: &quot;http_301&quot;}}'
  alb.ingress.kubernetes.io/group.name: &lt;my-group-name&gt;
spec:
  rules:
    - host: test.my-host.net
      http:
        paths:
          - backend:
              servicename: test
              serviceport: use-annotation
            path: /
---
apiversion: v1
kind: service
metadata:
  name: test
spec:
  type: externalname
  externalname: test.my-host.net

i also tried to create the ingress with the redirect with the annotation on the alb ingress v2 just like the docs:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: test-ingress
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: &quot;internet-facing&quot;
    alb.ingress.kubernetes.io/listen-ports: '[{&quot;http&quot;:80,&quot;https&quot;: 443}]'
    alb.ingress.kubernetes.io/certificate-arn: &lt;my-cert-arn&gt;
    alb.ingress.kubernetes.io/actions.ssl-redirect: '{&quot;type&quot;: &quot;redirect&quot;, &quot;redirectconfig&quot;: { 
&quot;protocol&quot;: &quot;https&quot;, &quot;port&quot;: &quot;443&quot;, &quot;statuscode&quot;: &quot;http_301&quot;}}'
    alb.ingress.kubernetes.io/actions.redirect-to-eks: &gt;
      {&quot;type&quot;:&quot;redirect&quot;,&quot;redirectconfig&quot;:{&quot;host&quot;:&quot;my- 
 dist.cloudfront.net&quot;,&quot;path&quot;:&quot;/&quot;,&quot;port&quot;:&quot;443&quot;,
&quot;protocol&quot;:&quot;https&quot;,&quot;query&quot;:&quot;k=v&quot;,&quot;statuscode&quot;:&quot;http_302&quot;}}
    alb.ingress.kubernetes.io/group.name: &lt;my-group-name&gt;
spec:
  rules:
    - host: &lt;my-host-name&gt;
      http:
        paths:
          - backend:
              servicename: ssl-redirect
              serviceport: use-annotation

",<kubernetes><amazon-cloudfront><kubernetes-ingress><amazon-eks>,64791760,7,"the reason why it isn't working with externalname service type is because it has no endpoints. alb can set target only to instance or ip (documentation) so externalname isn't an option here.
you can create redirect with annotation but it is a bit tricky. first, you need at least two subnets with public access for alb.ingress.kubernetes.io/subnets annotation. subnets can be automatically discovered but i don't know if it can pick public ones from all assigned to eks cluster, so it's better to set explicitly. second, you need to use alb.ingress.kubernetes.io/actions.&lt;action name&gt; annotation (documentation), where &lt;action name&gt; must match servicename from ingress rules. third, you need to specify host in redirect config or it'll be host from ingress spec.
here is a working example:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: test-alb
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: &quot;internet-facing&quot;
    alb.ingress.kubernetes.io/subnets: &quot;public-subnet-id1,public-subnet-id2&quot;
    alb.ingress.kubernetes.io/listen-ports: '[{&quot;http&quot;:80,&quot;https&quot;: 443}]'
    alb.ingress.kubernetes.io/actions.redirect: '{&quot;type&quot;: &quot;redirect&quot;, &quot;redirectconfig&quot;: { &quot;protocol&quot;: &quot;https&quot;, &quot;host&quot;:&quot;example.com&quot;, &quot;port&quot;:&quot;443&quot;, &quot;statuscode&quot;: &quot;http_301&quot;}}'
spec:
  rules:
    - host: alb-test.host.name
      http:
        paths:
          - backend:
              servicename: redirect
              serviceport: use-annotation

your ssl-redirect annotation is almost correct, except that it redirects to &lt;my-host-name&gt;. i recommend you to use the web console (change region in the link) https://console.aws.amazon.com/ec2/v2/home?region=eu-central-1#loadbalancers:sort=loadbalancername to see generated redirect rules and debug other options.
there are other options to solve this, a dedicated deployment with nginx for example. or you can use nginx ingress controller, from my experience it is much easier to set redirect with it.
"
58453553,kubernetes ingress backend subpath,"is it possible in any way to redirect a hostpath to a subpath on the backend? similar how subpaths work for volumes.

the ingress would look like this:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: jupyter-notebook-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
    - host: jptrntbk.mydomain.com
      http:
        paths:
          - path: /
            backend:
              servicename: jupyter-notebook-service
              serviceport: 8888
              subpath: /lab


navigation to jptrntbk.mydomain.com would redirect to /lab on the backend and all other parentpaths are unavailable.
",<kubernetes><kubernetes-ingress>,58453885,7,"create an ingress rule with a app-root annotation:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/app-root: /app1
  name: approot
  namespace: default
spec:
  rules:
  - host: approot.bar.com
    http:
      paths:
      - backend:
          servicename: http-svc
          serviceport: 80
        path: /

check the rewrite is working
$ curl -i -k http://approot.bar.com/
http/1.1 302 moved temporarily
server: nginx/1.11.10
date: mon, 13 mar 2017 14:57:15 gmt
content-type: text/html
content-length: 162
location: http://approot.bar.com/app1
connection: keep-alive

or can you create an ingress rule with a rewrite annotation:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  name: rewrite
  namespace: default
spec:
  rules:
  - host: rewrite.bar.com
    http:
      paths:
      - backend:
          servicename: http-svc
          serviceport: 80
        path: /something(/|$)(.*)

in this ingress definition, any characters captured by (.*) will be assigned to the placeholder $2, which is then used as a parameter in the rewrite-target annotation.
for example, the ingress definition above will result in the following rewrites: - rewrite.bar.com/something rewrites to rewrite.bar.com/ - rewrite.bar.com/something/ rewrites to rewrite.bar.com/ - rewrite.bar.com/something/new rewrites to rewrite.bar.com/new
source: https://kubernetes.github.io/ingress-nginx/examples/rewrite/
"
71358674,kustomize: how to reference a value from a configmap in another resource/overlay?,"i have a couple of overlays (dev, stg, prod) pulling data from multiple bases where each base contains a single service so that each overlay can pick and choose what services it needs. i generate the manifests from the dev/stg/prod directories.
a simplified version of my kubernetes/kustomize directory structure looks like this:
 base
    serviceone
       kustomization.yaml
       service_one_config.yaml
    servicetwo
       kustomization.yaml
       service_two_config.yaml
    configmap
        kustomization.yaml
        config_map_constants.yaml
 overlays
     dev
        kustomization.yaml
        dev_patch.yaml
     stg
        kustomization.yaml
        stg_patch.yaml
     prod
         kustomization.yaml
         prod_patch.yaml

under base/configmap, config_map_constants.yaml file contains key/value pairs that are non-secrets:
apiversion: v1
kind: configmap
metadata:
  labels:
    app: myapp
  name: global-config-map
  namespace: mynamespace
data:
  aws_region: &quot;us-west&quot;
  env_id: &quot;1234&quot;

if an overlay just needs a default value, it should reference the key/value pair as is, and if it needs a custom value, i would use a patch to override the value.
kustomization.yaml from base/configmap looks like this and refers to configmap as a resource:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
resources:
  - config_map_constants.yaml

question: how do i reference &quot;aws_region&quot; in my overlays' yaml files so that i can retrieve the value?
for example, i want to be able to do something like this in base/serviceone/service_one_config.yaml:
apiversion: v1
kind: service
metadata:
  labels:
    app: myapp
    aws_region: ../configmap/${aws_region} #pseudo syntax
  name: service_one
spec:
  env_id: ../configmap/${env_id} #pseudo syntax

i am able to build the configmap and append it to my services but i am struggling to find how to reference its contents within other resources.
edit:
kustomize version: v4.5.2
",<kubernetes><kubectl><configmap><kustomize>,71523075,7,"you can try using https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/replacements/
for your scenario, if you want to reference the aws-region into your service labels. you need to create a replacement file.
replacements/region.yaml
source:
  kind: configmap
  fieldpath: data.aws-region
targets:
  - select:
      kind: service
      name: service_one
    fieldpaths:
      - metadata.labels.aws_region


and add it to your kustomization.yaml
replacements:
  - path: replacements/region.yaml

kustomize output should be similar to this
---
apiversion: v1
kind: service
metadata:
  labels:
    app: myapp
    aws_region: us-west-1
  name: service_one

"
71634186,cannot connect to kubernetes nodeport service,"i have a running pod that was created with the following pod-definition.yaml:
apiversion: v1
kind: pod
metadata:
    name: microservice-one-pod-name
    labels:
        app: microservice-one-app-label
        type: front-end
spec:
    containers:
    - name: microservice-one
      image: vismarkjuarez1994/microserviceone
      ports:
      - containerport: 2019

i then created a service using the following service-definition.yaml:
kind: service
apiversion: v1
metadata:
  name: microserviceone-service
spec:
  ports:
    - port: 30008
      targetport: 2019
      protocol: tcp
  selector:
    app: microservice-one-app-label
  type: nodeport

i then ran kubectl describe node minikube to find the node ip i should be connecting to -- which yielded:
addresses:
  internalip:  192.168.49.2
  hostname:    minikube

but i get no response when i run the following curl command:
curl 192.168.49.2:30008

the request also times out when i try to access 192.168.49.2:30008 from a browser.
the pod logs show that the container is up and running. why can't i access my service?
",<kubernetes><kubernetes-pod><kubernetes-service>,71634717,7,"the problem is that you are trying to access your service at the port parameter which is the internal port at which the service will be exposed, even when using nodeport type.
the parameter you were searching is called nodeport, which can optionally be specified together with port and targetport. quoting the documentation:

by default and for convenience, the kubernetes control plane will
allocate a port from a range (default: 30000-32767)

since you didn't specify the nodeport, one in the range was automatically picked up. you can check which one by:
kubectl get svc -owide

and then access your service externally at that port.
as an alternative, you can change your service definition to be something like:
kind: service
apiversion: v1
metadata:
  name: microserviceone-service
spec:
  ports:
    - port: 30008
      targetport: 2019
      nodeport: 30008
      protocol: tcp
  selector:
    app: microservice-one-app-label
  type: nodeport

but take in mind that you may need to delete your service and create it again in order to change the nodeport allocated.
"
69991507,how to trigger tekton pipeline from gitlab ci directly with predefined gitlab ci variables & tekton logs streamed into gitlab pipeline logs,"we have a aws eks running (setup using pulumi), where we installed tekton as described in the cloud native buildpacks tekton docs. the example project is available.
our tekton pipeline is configured like this (which is derived from the cloud native buildpacks tekton docs also):
apiversion: tekton.dev/v1beta1
kind: pipeline
metadata:
  name: buildpacks-test-pipeline
spec:
  params:
    - name: image
      type: string
      description: image url to push
    - name: source_url
      type: string
      description: a git repo url where the source code resides.
    - name: source_revision
      description: the branch, tag or sha to checkout.
      default: &quot;&quot;
  workspaces:
    - name: source-workspace # directory where application source is located. (required)
    - name: cache-workspace # directory where cache is stored (optional)
  tasks:
    - name: fetch-repository # this task fetches a repository from github, using the `git-clone` task you installed
      taskref:
        name: git-clone
      workspaces:
        - name: output
          workspace: source-workspace
      params:
        - name: url
          value: &quot;$(params.source_url)&quot;
        - name: revision
          value: &quot;$(params.source_revision)&quot;
        - name: subdirectory
          value: &quot;&quot;
        - name: deleteexisting
          value: &quot;true&quot;
    - name: buildpacks # this task uses the `buildpacks` task to build the application
      taskref:
        name: buildpacks
      runafter:
        - fetch-repository
      workspaces:
        - name: source
          workspace: source-workspace
        - name: cache
          workspace: cache-workspace
      params:
        - name: app_image
          value: &quot;$(params.image)&quot;
        - name: builder_image
          value: paketobuildpacks/builder:base # this is the builder we want the task to use (required)

we added source_url and source_revision as parameters already.
the question is: how can we trigger a tekton pipelinerun from gitlab ci (inside our .gitlab-ci.yml) adhering to the following requirements:

simplest possible approach
do not use the extra complexity introduced by tekton triggers (incl. commit-status-tracker) but still keep gitlab as the source of truth (e.g. see green/red pipeline runs on commits etc.)
report successfully run tekton pipelines as green gitlab ci pipelines &amp; failed tekton pipelines as red gitlab ci pipelines
preserve/stream the tekton pipeline logs into gitlab ci pipeline logs - both in case of errors or success inside the tekton pipelines
use gitlab ci predefined variables for a generic approach

",<kubernetes><gitlab><gitlab-ci><amazon-eks><tekton>,69991508,7,"tldr;
i created a fully comprehensible example project showing all necessary steps and running pipelines here: https://gitlab.com/jonashackt/microservice-api-spring-boot/ with the full .gitlab-ci.yml to directly trigger a tekton pipeline:
image: registry.gitlab.com/jonashackt/aws-kubectl-tkn:0.21.0

variables:
  aws_default_region: 'eu-central-1'

before_script:
  - mkdir ~/.kube
  - echo &quot;$ekskubeconfig&quot; &gt; ~/.kube/config
  - echo &quot;--- testdrive connection to cluster&quot;
  - kubectl get nodes

stages:
  - build

build-image:
  stage: build
  script:
    - echo &quot;--- create parameterized tekton pipelinerun yaml&quot;
    - tkn pipeline start buildpacks-test-pipeline
      --serviceaccount buildpacks-service-account-gitlab
      --workspace name=source-workspace,subpath=source,claimname=buildpacks-source-pvc
      --workspace name=cache-workspace,subpath=cache,claimname=buildpacks-source-pvc
      --param image=$ci_registry_image
      --param source_url=$ci_project_url
      --param source_revision=$ci_commit_ref_slug
      --dry-run
      --output yaml &gt; pipelinerun.yml

    - echo &quot;--- trigger pipelinerun in tekton / k8s&quot;
    - pipeline_run_name=$(kubectl create -f pipelinerun.yml --output=jsonpath='{.metadata.name}')

    - echo &quot;--- show tekton pipelinerun logs&quot;
    - tkn pipelinerun logs $pipeline_run_name --follow

    - echo &quot;--- check if tekton pipelinerun failed &amp; exit gitlab pipeline accordingly&quot;
    - kubectl get pipelineruns $pipeline_run_name --output=jsonpath='{.status.conditions[*].reason}' | grep failed &amp;&amp; exit 1 || exit 0

here are the brief steps you need to do:
1. choose a base image for your .gitlab-ci.yml providing aws cli, kubectl and tekton cli (tkn)
this is entirely up to you. i created an example project https://gitlab.com/jonashackt/aws-kubectl-tkn which provides an image, which is based on the official https://hub.docker.com/r/amazon/aws-cli image and is accessible via registry.gitlab.com/jonashackt/aws-kubectl-tkn:0.21.0.
2. ci/cd variables for aws cli &amp; kubernetes cluster access
inside your gitlab ci project (or better: inside the group, where your gitlab ci project resides in) you need to create aws_access_key_id, aws_secret_access_key as ci/cd variables holding the the aws cli credentials (beware to mask them while creating them in order to prevent them beeing printed into the gitlab ci logs). depending on your eks clusters (or other k8s clusters) config, you need to provide a kubeconfig to access your cluster. one way is to create a gitlab ci/cd variable like ekskubeconfig providing the necessary file (e.g. in the example project this is provided by pulumi with pulumi stack output kubeconfig &gt; kubeconfig). in this setup using pulumi there are no secret credentials inside the kubeconfig so the variable doesn't need to be masked. but be aware of possible credentials here and protect them accordingly if needed.

also define aws_default_region containing your eks cluster's region:
# as we need kubectl, aws &amp; tkn cli we use https://gitlab.com/jonashackt/aws-kubectl-tkn
image: registry.gitlab.com/jonashackt/aws-kubectl-tkn:0.21.0

variables:
  aws_default_region: 'eu-central-1'

3. use kubeconfig and testdrive cluster connection in before_script section
preparing things we need later inside other steps could be done inside the before_script section. so let's create the directory ~/.kube there and create the file ~/.kube/config from the contents of the variable ekskubeconfig. finally fire a kubectl get nodes to check if the cluster connection is working. our before_script section now looks like this:
before_script:
  - mkdir ~/.kube
  - echo &quot;$ekskubeconfig&quot; &gt; ~/.kube/config
  - echo &quot;--- testdrive connection to cluster&quot;
  - kubectl get nodes

4. pass parameters to tekton pipelinerun
passing parameters via kubectl isn't trivial - or even needs to be done using a templating engine like helm. but luckily the tekton cli has something for us: tkn pipeline start accepts parameters. so we can transform the cloud native buildpacks tekton pipelinerun yaml file into a tkn cli command like this:
tkn pipeline start buildpacks-test-pipeline \
    --serviceaccount buildpacks-service-account-gitlab \
    --workspace name=source-workspace,subpath=source,claimname=buildpacks-source-pvc \
    --workspace name=cache-workspace,subpath=cache,claimname=buildpacks-source-pvc \
    --param image=registry.gitlab.com/jonashackt/microservice-api-spring-boot \
    --param source_url=https://gitlab.com/jonashackt/microservice-api-spring-boot \
    --param source_revision=main \
    --timeout 240s \
    --showlog

now here are some points to consider. first the name buildpacks-test-pipeline right after the tkn pipeline start works as an equivalent to the yaml files spec: pipelineref: name: buildpacks-test-pipeline definition.
it will also work as a reference to the pipeline object defined inside the file pipeline.yml which starts with metadata: name: buildpacks-test-pipeline like:
apiversion: tekton.dev/v1beta1
kind: pipeline
metadata:
name: buildpacks-test-pipeline
...
second to define workspaces isn't trivial. luckily there's help. we can define a workspace in tkn cli like this: --workspace name=source-workspace,subpath=source,claimname=buildpacks-source-pvc.
third using the parameters as intended now becomes easy. simply use --param accordingly. we also use --showlog to directly stream the tekton logs into the commandline (or gitlab ci!) together with --timeout.
finally using gitlab ci predefined variables our .gitlab-ci.yml's build stage looks like this:
build-image:
  stage: build
  script:
    - echo &quot;--- run tekton pipeline&quot;
    - tkn pipeline start buildpacks-test-pipeline
      --serviceaccount buildpacks-service-account-gitlab
      --workspace name=source-workspace,subpath=source,claimname=buildpacks-source-pvc
      --workspace name=cache-workspace,subpath=cache,claimname=buildpacks-source-pvc
      --param image=$ci_registry_image
      --param source_url=$ci_project_url
      --param source_revision=$ci_commit_ref_slug
      --timeout 240s
      --showlog

5. solve the every gitlab ci pipeline is green problem
this could have been everything we need to do. but: right now every gitlab ci pipeline is green, regardless of the tekton pipeline's status.
therefore we remove --showlog and --timeout again, but add a --dry-run together with the --output yaml flags. without the --dry-run the tkn pipeline start command would create a pipelinerun object definition already, which we can't create then using kubectl anymore:
build-image:
  stage: build
  script:
    - echo &quot;--- create parameterized tekton pipelinerun yaml&quot;
    - tkn pipeline start buildpacks-test-pipeline
      --serviceaccount buildpacks-service-account-gitlab
      --workspace name=source-workspace,subpath=source,claimname=buildpacks-source-pvc
      --workspace name=cache-workspace,subpath=cache,claimname=buildpacks-source-pvc
      --param image=$ci_registry_image
      --param source_url=$ci_project_url
      --param source_revision=$ci_commit_ref_slug
      --dry-run
      --output yaml &gt; pipelinerun.yml

now that we removed --showlog and don't start an actual tekton pipeline using tkn cli, we need to create the pipeline run using:
- pipeline_run_name=$(kubectl create -f pipelinerun.yml --output=jsonpath='{.metadata.name}')

having the temporary variable pipeline_run_name available containing the exact pipeline run id, we can stream the tekton pipeline logs into our gitlab ci log again:
- tkn pipelinerun logs $pipeline_run_name --follow

finally we need to check for tekton pipeline run's status and exit our gitlab ci pipeline accordingly in order to prevent red tekton pipelines resulting in green gitlab ci pipelines. therefore let's check the status of the tekton pipeline run first. this can be achieved using --output=jsonpath='{.status.conditions[*].reason}' together with a kubectl get pipelineruns:
kubectl get pipelineruns $pipeline_run_name --output=jsonpath='{.status.conditions[*].reason}'

then we pipe the result into a grep which checks, if failed is inside the status.condiditons.reason field.
finally we use a bash onliner (which is &lt;expression to check true or false&gt; &amp;&amp; command when true || command when false) to issue the suitable exit command (see https://askubuntu.com/a/892605):
- kubectl get pipelineruns $pipeline_run_name --output=jsonpath='{.status.conditions[*].reason}' | grep failed &amp;&amp; exit 1 || exit 0

now every gitlab ci pipeline becomes green, when the tekton pipeline succeeded - and gets red when the tekton pipeline failed. the example project has some logs if you're interested. it's pretty cool to see the tekton logs inside the gitlab ci logs:

"
69878685,ingress-nginx not working when using ingressclassname instead of kubernetes.io/ingress.class in annotations,"i have a baremetal cluster deployed using kubespray with kubernetes 1.22.2, metallb, and ingress-nginx enabled. i am getting 404 not found when trying to access any service deployed via helm when setting ingressclassname: nginx. however, everything works fine if i don't use ingressclassname: nginx but kubernetes.io/ingress.class: nginx instead in the helm chart values.yaml. how can i get it to work using ingressclassname?
these are my kubespray settings for inventory/mycluster/group_vars/k8s_cluster/addons.yml
# nginx ingress controller deployment
ingress_nginx_enabled: true
ingress_nginx_host_network: false
ingress_publish_status_address: &quot;&quot;
ingress_nginx_nodeselector:
  kubernetes.io/os: &quot;linux&quot;
ingress_nginx_tolerations:
  - key: &quot;node-role.kubernetes.io/master&quot;
    operator: &quot;equal&quot;
    value: &quot;&quot;
    effect: &quot;noschedule&quot;
  - key: &quot;node-role.kubernetes.io/control-plane&quot;
    operator: &quot;equal&quot;
    value: &quot;&quot;
    effect: &quot;noschedule&quot;
ingress_nginx_namespace: &quot;ingress-nginx&quot;
ingress_nginx_insecure_port: 80
ingress_nginx_secure_port: 443
ingress_nginx_configmap:
  map-hash-bucket-size: &quot;128&quot;
  ssl-protocols: &quot;tlsv1.2 tlsv1.3&quot;
ingress_nginx_configmap_tcp_services:
  9000: &quot;default/example-go:8080&quot;
ingress_nginx_configmap_udp_services:
  53: &quot;kube-system/coredns:53&quot;
ingress_nginx_extra_args:
  - --default-ssl-certificate=default/mywildcard-tls
ingress_nginx_class: &quot;nginx&quot;

grafana helm values.yaml
ingress:
  enabled: true
  # for kubernetes &gt;= 1.18 you should specify the ingress-controller via the field ingressclassname
  # see https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
  ingressclassname: nginx
  # values can be templated
  annotations:
  #  kubernetes.io/ingress.class: nginx
  #  kubernetes.io/tls-acme: &quot;true&quot;
  labels: {}
  path: /

  # pathtype is only for k8s &gt;= 1.1=
  pathtype: prefix

  hosts:
    - grafana.mycluster.org
  tls:
   - secretname: mywildcard-tls
     hosts:
       - grafana.mycluster.org

kubectl describe pod grafana-679bbfd94-p2dd7
...
events:
  type     reason     age                from               message
  ----     ------     ----               ----               -------
  normal   scheduled  25m                default-scheduler  successfully assigned default/grafana-679bbfd94-p2dd7 to node1
  normal   pulled     25m                kubelet            container image &quot;grafana/grafana:8.2.2&quot; already present on machine
  normal   created    25m                kubelet            created container grafana
  normal   started    25m                kubelet            started container grafana
  warning  unhealthy  24m (x3 over 25m)  kubelet            readiness probe failed: get &quot;http://10.233.90.33:3000/api/health&quot;: dial tcp 10.233.90.33:3000: connect: connection refused

kubectl get svc
name         type           cluster-ip     external-ip   port(s)        age
grafana      loadbalancer   10.233.14.90   10.10.30.52   80:30285/tcp   55m
kubernetes   clusterip      10.233.0.1     &lt;none&gt;        443/tcp        9d

kubectl get ing (no node address assigned)
name      class   hosts                    address   ports     age
grafana   nginx   grafana.mycluster.org             80, 443   25m

kubectl describe ing grafana (no node address assigned)
name:             grafana
namespace:        default
address:
default backend:  default-http-backend:80 (&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;)
tls:
  mywildcard-tls terminates grafana.mycluster.org
rules:
  host                    path  backends
  ----                    ----  --------
  grafana.mycluster.org
                          /   grafana:80 (10.233.90.33:3000)
annotations:              meta.helm.sh/release-name: grafana
                          meta.helm.sh/release-namespace: default
events:                   &lt;none&gt;

kubectl get all --all-namespaces
namespace        name                                                              ready   status    restarts   age
default          pod/grafana-b988b9b6-pxccw                                        1/1     running   0          2m53s
default          pod/nfs-client-nfs-subdir-external-provisioner-68f44cd9f4-wjlpv   1/1     running   0          17h
ingress-nginx    pod/ingress-nginx-controller-6m2vt                                1/1     running   0          17h
ingress-nginx    pod/ingress-nginx-controller-xkgxl                                1/1     running   0          17h
kube-system      pod/calico-kube-controllers-684bcfdc59-kmsst                      1/1     running   0          17h
kube-system      pod/calico-node-dhlnt                                             1/1     running   0          17h
kube-system      pod/calico-node-r8ktz                                             1/1     running   0          17h
kube-system      pod/coredns-8474476ff8-9sbwh                                      1/1     running   0          17h
kube-system      pod/coredns-8474476ff8-fdgcb                                      1/1     running   0          17h
kube-system      pod/dns-autoscaler-5ffdc7f89d-vskvq                               1/1     running   0          17h
kube-system      pod/kube-apiserver-node1                                          1/1     running   0          17h
kube-system      pod/kube-controller-manager-node1                                 1/1     running   1          17h
kube-system      pod/kube-proxy-hbjz6                                              1/1     running   0          16h
kube-system      pod/kube-proxy-lfqzt                                              1/1     running   0          16h
kube-system      pod/kube-scheduler-node1                                          1/1     running   1          17h
kube-system      pod/kubernetes-dashboard-548847967d-qqngw                         1/1     running   0          17h
kube-system      pod/kubernetes-metrics-scraper-6d49f96c97-2h7hc                   1/1     running   0          17h
kube-system      pod/nginx-proxy-node2                                             1/1     running   0          17h
kube-system      pod/nodelocaldns-64cqs                                            1/1     running   0          17h
kube-system      pod/nodelocaldns-t5vv6                                            1/1     running   0          17h
kube-system      pod/registry-proxy-kljvw                                          1/1     running   0          17h
kube-system      pod/registry-proxy-nz4qk                                          1/1     running   0          17h
kube-system      pod/registry-xzh9d                                                1/1     running   0          17h
metallb-system   pod/controller-77c44876d-c92lb                                    1/1     running   0          17h
metallb-system   pod/speaker-fkjqp                                                 1/1     running   0          17h
metallb-system   pod/speaker-pqjgt                                                 1/1     running   0          17h

namespace     name                                type           cluster-ip      external-ip   port(s)                  age
default       service/grafana                     loadbalancer   10.233.1.104    10.10.30.52   80:31116/tcp             2m53s
default       service/kubernetes                  clusterip      10.233.0.1      &lt;none&gt;        443/tcp                  17h
kube-system   service/coredns                     clusterip      10.233.0.3      &lt;none&gt;        53/udp,53/tcp,9153/tcp   17h
kube-system   service/dashboard-metrics-scraper   clusterip      10.233.35.124   &lt;none&gt;        8000/tcp                 17h
kube-system   service/kubernetes-dashboard        clusterip      10.233.32.133   &lt;none&gt;        443/tcp                  17h
kube-system   service/registry                    clusterip      10.233.30.221   &lt;none&gt;        5000/tcp                 17h

namespace        name                                      desired   current   ready   up-to-date   available   node selector            age
ingress-nginx    daemonset.apps/ingress-nginx-controller   2         2         2       2            2           kubernetes.io/os=linux   17h
kube-system      daemonset.apps/calico-node                2         2         2       2            2           kubernetes.io/os=linux   17h
kube-system      daemonset.apps/kube-proxy                 2         2         2       2            2           kubernetes.io/os=linux   17h
kube-system      daemonset.apps/nodelocaldns               2         2         2       2            2           kubernetes.io/os=linux   17h
kube-system      daemonset.apps/registry-proxy             2         2         2       2            2           &lt;none&gt;                   17h
metallb-system   daemonset.apps/speaker                    2         2         2       2            2           kubernetes.io/os=linux   17h

namespace        name                                                         ready   up-to-date   available   age
default          deployment.apps/grafana                                      1/1     1            1           2m53s
default          deployment.apps/nfs-client-nfs-subdir-external-provisioner   1/1     1            1           17h
kube-system      deployment.apps/calico-kube-controllers                      1/1     1            1           17h
kube-system      deployment.apps/coredns                                      2/2     2            2           17h
kube-system      deployment.apps/dns-autoscaler                               1/1     1            1           17h
kube-system      deployment.apps/kubernetes-dashboard                         1/1     1            1           17h
kube-system      deployment.apps/kubernetes-metrics-scraper                   1/1     1            1           17h
metallb-system   deployment.apps/controller                                   1/1     1            1           17h

namespace        name                                                                    desired   current   ready   age
default          replicaset.apps/grafana-b988b9b6                                        1         1         1       2m53s
default          replicaset.apps/nfs-client-nfs-subdir-external-provisioner-68f44cd9f4   1         1         1       17h
kube-system      replicaset.apps/calico-kube-controllers-684bcfdc59                      1         1         1       17h
kube-system      replicaset.apps/coredns-8474476ff8                                      2         2         2       17h
kube-system      replicaset.apps/dns-autoscaler-5ffdc7f89d                               1         1         1       17h
kube-system      replicaset.apps/kubernetes-dashboard-548847967d                         1         1         1       17h
kube-system      replicaset.apps/kubernetes-metrics-scraper-6d49f96c97                   1         1         1       17h
kube-system      replicaset.apps/registry                                                1         1         1       17h
metallb-system   replicaset.apps/controller-77c44876d                                    1         1         1       17h

kubectl get ing grafana -o yaml
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    kubernetes.io/tls-acme: &quot;true&quot;
    meta.helm.sh/release-name: grafana
    meta.helm.sh/release-namespace: default
    nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
  creationtimestamp: &quot;2021-11-11t07:16:12z&quot;
  generation: 1
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 8.2.2
    helm.sh/chart: grafana-6.17.5
  name: grafana
  namespace: default
  resourceversion: &quot;3137&quot;
  uid: 6c34d3bd-9ab6-42fe-ac1b-7620a9566f62
spec:
  ingressclassname: nginx
  rules:
  - host: grafana.mycluster.org
    http:
      paths:
      - backend:
          service:
            name: ssl-redirect
            port:
              name: use-annotation
        path: /*
        pathtype: prefix
      - backend:
          service:
            name: grafana
            port:
              number: 80
        path: /
        pathtype: prefix
status:
  loadbalancer: {}

",<kubernetes><kubernetes-helm><nginx-ingress>,69975358,7,"
running kubectl get ingressclass returned 'no resources found'.

that's the main reason of your issue.
why?
when you are specifying ingressclassname: nginx in your grafana values.yaml file you are setting your ingress resource to use nginx ingress class which does not exist.
i replicated your issue using minikube, metallb and nginx ingress installed via modified deploy.yaml file with commented ingressclass resource + set nginx ingress controller name to nginx as in your example. the result was exactly the same - ingressclassname: nginx didn't work (no address), but annotation kubernetes.io/ingress.class: nginx worked.

(for the below solution i'm using controller pod name ingress-nginx-controller-86c865f5c4-qwl2b, but in your case it will be different - check it using kubectl get pods -n ingress-nginx command. also keep in mind it's kind of a workaround - usually ingressclass resource should be installed automatically with a whole installation of nginx ingress. i'm presenting this solution to understand why it's not worked for you before, and why it works with nginx ingress installed using helm)
in the logs of the ingress nginx controller i found (kubectl logs ingress-nginx-controller-86c865f5c4-qwl2b -n ingress-nginx):
&quot;ignoring ingress because of error while validating ingress class&quot; ingress=&quot;default/minimal-ingress&quot; error=&quot;no object matching key \&quot;nginx\&quot; in local store&quot;

so it's clearly shown that there is no matching key to nginx controller class - because there is no ingressclass resource which is the &quot;link&quot; between the nginx ingress controller and running ingress resource.
you can verify which name of controller class is bidden to controller by running kubectl get pod ingress-nginx-controller-86c865f5c4-qwl2b -n ingress-nginx -o yaml:
...
spec:
  containers:
  - args:
    - /nginx-ingress-controller
    - --publish-service=$(pod_namespace)/ingress-nginx-controller
    - --election-id=ingress-controller-leader
    - --controller-class=k8s.io/nginx
...

now i will create and apply following ingress class resource:
apiversion: networking.k8s.io/v1
kind: ingressclass
metadata:
  name: nginx
spec:
  controller: k8s.io/nginx

now in the logs i can see that it's properly configured:
i1115 12:13:42.410384       7 main.go:101] &quot;successfully validated configuration, accepting&quot; ingress=&quot;minimal-ingress/default&quot;
i1115 12:13:42.420408       7 store.go:371] &quot;found valid ingressclass&quot; ingress=&quot;default/minimal-ingress&quot; ingressclass=&quot;nginx&quot;
i1115 12:13:42.421487       7 event.go:282] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;default&quot;, name:&quot;minimal-ingress&quot;, uid:&quot;c708a672-a8dd-45d3-a2ec-f2e2881623ea&quot;, apiversion:&quot;networking.k8s.io/v1&quot;, resourceversion:&quot;454362&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'sync' scheduled for sync

i re-applied the ingress resource definition, i get ip address for ingress resource.

as i said before, instead of using this workaround, i'd suggest installing the nginx ingress resource using a solution that automatically installs ingressclass as well. as you have chosen helm chart, it has ingress class resource so the problem is gone. other possible ways to install are here.
"
57670083,helm - configmap - replace only the contents of file and not file names,"i have defined application.properties files in a config dir. config dir is on the below structure.

config
  application.properties
  application-test.properties
  application-dev.properties
  application-prod.properties


i have created a helm chart to create the configmap. helm chart is defined as below

apiversion: v1
kind: configmap
metadata:
  name: nginx-configmap
data:
  {{- (.files.glob ""config/*"").asconfig | nindent 2 }}


i see that the configmap is created. 

we are consuming the configmap via environment variables in a running container using the envfrom property. (this in my deployment yaml file)

spec:
   containers:
   - envfrom:
     - configmapref:
         name: nginx-configmap


i see that the values are stored as environment variables. however the variables are in lower cases. 

server.port=8080
server.name=localhost


since these are env variables, they have to be uppercase and . should be replaced with _. so, i have modifed my chart as below

data:
  {{- (.files.glob ""config/*"").asconfig | nindent 2 | upper | replace ""."" ""_"" }}



the generated configmap is as below

  application_properties: |
    server_port = 8080
    server_name = localhost


below is the env variables inside container

application_properties=server_port = 8080
server_name = localhost


my requirement is that only the contents of the file should be upper case and . should be replaced with _.  the filename should not be converted. the filename should be as is.

can this be achieved? 
",<kubernetes><kubernetes-helm>,57672586,7,"try this:

apiversion: v1
kind: configmap
metadata:
  name: nginx-configmap
data:
  {{ $root := . }}
  {{ range $path, $bytes := .files.glob ""config/*"" }}
  {{ base $path }}: '{{ $root.files.get $path | nindent 2 | upper | replace ""."" ""_""  }}'
  {{ end }}

"
59664244,how to generate a random string / password in kubernetes secrets,"for now, i deploy my application pods using static files and one of them is app-secrets.yaml with all secrets to deploy an application

---
apiversion: v1
kind: secret
metadata:
  name: app-secrets
type: opaque
data:
  root: xxxxxx
  user1: xxxxxx
  user2: xxxxxx


but this is not neither secure nor convenient (if i need another app instance, i have to create another file with human-generated password).

i'm looking to generate random passwords at application creation but i don't know if it's possible.
i've already looked to the topic secret and especially secretgenerator but this is not directly what i want as i understand it, because it does not create a random string but a random secret name like secret/app-secrets-ssdsdfmfh4k but i have to provide still the passwords.
",<kubernetes><kubernetes-secrets>,59666255,7,"you may want to use kubernetes-secret-generator. i've tested it and it's doing exactly what you need.
to accomplish it you have to have helm in your cluster and follow these instructions:
clone repository
$ git clone https://github.com/mittwald/kubernetes-secret-generator

create helm deployment
$ helm upgrade --install secret-generator ./deploy/chart

now you to use it, you just have to

add annotation secret-generator.v1.mittwald.de/autogenerate to any
kubernetes secret object .the value of the annotation can be a field
name (or comma separated list of field names) within the secret; the
secretgeneratorcontroller will pick up this annotation and add a field
[or fields] (password in the example below) to the secret with a
randomly generated string value. from here.

$ kubectl apply -f mysecret.yaml
apiversion: v1
kind: secret
metadata:
  name: mysecret
  annotations:
    secret-generator.v1.mittwald.de/autogenerate: password
data:
  username: ugxlyxnlqwnjzxb0cg==

after applying this secret you can take a look at it to check if the passward was generated as expected:
$ kubectl get secrets mysecret -o yaml
apiversion: v1
data:
  password: dnvktdbjz0tfs1bacmttmnbuc3d2yws2ylzsz0xptufkdstda3dwuq==
  username: ugxlyxnlqwnjzxb0cg==
kind: secret
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {&quot;apiversion&quot;:&quot;v1&quot;,&quot;data&quot;:{&quot;username&quot;:&quot;ugxlyxnlqwnjzxb0cg==&quot;},&quot;kind&quot;:&quot;secret&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;secret-generator.v1.mittwald.de/autogenerate&quot;:&quot;password&quot;},&quot;name&quot;:&quot;mysecret&quot;,&quot;namespace&quot;:&quot;default&quot;}}
    secret-generator.v1.mittwald.de/autogenerate: password
    secret-generator.v1.mittwald.de/autogenerate-generated-at: 2020-01-09 14:29:44.397648062
      +0000 utc m=+664.011602557
    secret-generator.v1.mittwald.de/secure: &quot;yes&quot;
  creationtimestamp: &quot;2020-01-09t14:29:44z&quot;
  name: mysecret
  namespace: default
  resourceversion: &quot;297425&quot;
  selflink: /api/v1/namespaces/default/secrets/mysecret
  uid: 7ae42d71-32ec-11ea-92b3-42010a800009
type: opaque

as we can see, the password was generated.
"
63749146,kubernetes - how do i mention hostpath in pvc?,"i need to make use of pvc to specify the specs of the pv and i also need to make sure it uses a custom local storage path in the pv.
i am unable to figure out how to mention the hostpath in a pvc?
this is the pvc config:
apiversion: v1 
kind: persistentvolumeclaim
metadata: 
  name: mongo-pvc 
spec: 
  accessmodes: 
    - readwriteonce
  resources: 
    requests: 
      storage: 1gi

and this is the mongodb deployment:
spec:
    replicas: 1
    selector:
        matchlabels:
            app: mongo
    template:
        metadata:
            labels: 
                app: mongo
        spec:
            volumes: 
                - name: mongo-volume 
                  persistentvolumeclaim: 
                    claimname: mongo-pvc 
            containers:
                - name: mongo
                  image: mongo
                  ports:
                    - containerport: 27017
                  volumemounts: 
                    - name: mongo-volume 
                      mountpath: /data/db 

how and where do i mention the hostpath to be mounted in here?
",<kubernetes><microservices><google-kubernetes-engine>,63749257,7,"doc says that you set hostpath when creating a pv (the step before creating pvc).
apiversion: v1
kind: persistentvolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageclassname: manual
  capacity:
    storage: 10gi
  accessmodes:
    - readwriteonce
  hostpath:
    path: &quot;/mnt/data&quot;

after you create the persistentvolumeclaim, the kubernetes control plane looks for a persistentvolume that satisfies the claim's requirements. if the control plane finds a suitable persistentvolume with the same storageclass, it binds the claim to the volume.
please see https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/
"
51751462,nginx ingress jenkins path rewrite configuration not working,"i have deployed jenkins on kubernetes and am trying to configure the nginx ingress for it. 

assume i want it to be available at https://myip/jenkins

this is my initial ingress configuration:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: jenkins-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/add-base-url: ""true""
spec:
  rules:
  - http:
      paths:
      - path: /jenkins
        backend:
          servicename: jenkins
          serviceport: 8080


with this when i access https://myip/jenkins i am redirected to http://myip/login?from=%2f.

when accessing https://myip/jenkins/login?from=%2f it stays on that page but none of the static resources are found since they are looked for at https://myip/static...
",<nginx><jenkins><kubernetes><kubernetes-ingress>,51756313,7,"this is how i solved it configuring the jenkins image context path without the need to use the ingress rewrite annotations:

kind: deployment
metadata:
  creationtimestamp: null
  labels:
    app: jenkins
  name: jenkins
spec:
  replicas: 1
  selector:
    matchlabels:
      app: jenkins
  strategy: {}
  template:
    metadata:
      creationtimestamp: null
      labels:
        app: jenkins
    spec:
      securitycontext:
        fsgroup: 2000
        runasuser: 1000
        runasnonroot: true
      volumes:
      - name: jenkins-storage
        persistentvolumeclaim:
          claimname: jenkins
      containers:
      - image: jenkins/jenkins:lts
        name: jenkins
        ports:
        - containerport: 8080
          name: ""http-server""
        - containerport: 50000
          name: ""jnlp""
        resources: {}
        env:
        - name: jenkins_opts
          value: --prefix=/jenkins
        volumemounts:
        - mountpath: ""/var/jenkins_home""
          name: jenkins-storage
status: {}


ingress:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: prfl-apps-devops-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
    nginx.ingress.kubernetes.io/add-base-url: ""true""
spec:
  rules:
  - http:
      paths:
      - path: /jenkins
        backend:
          servicename: jenkins
          serviceport: 8080

"
51598467,kubernetes rbac - forbidden attempt to grant extra privileges,"i'm using kubernetes v1.8.14 on custom built coreos cluster:

$ kubectl version --short 
client version: v1.10.5
server version: v1.8.14+coreos.0


when trying to create the following clusterrole:

$ cat clusterrole.yml 
---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
rules:
- apigroups:
  - """"
  resources:
  - endpoints
  - services
  - pods
  - namespaces
  verbs:
  - list
  - watch


i get the following error:

$ kubectl create -f clusterrole.yml 
error from server (forbidden): error when creating ""clusterrole.yml"": clusterroles.rbac.authorization.k8s.io ""system:coredns"" is forbidden: attempt to grant extra privileges: [policyrule{resources:[""endpoints""], apigroups:[""""], verbs:[""list""]} policyrule{resources:[""endpoints""], apigroups:[""""], verbs:[""watch""]} policyrule{resources:[""services""], apigroups:[""""], verbs:[""list""]} policyrule{resources:[""services""], apigroups:[""""], verbs:[""watch""]} policyrule{resources:[""pods""], apigroups:[""""], verbs:[""list""]} policyrule{resources:[""pods""], apigroups:[""""], verbs:[""watch""]} policyrule{resources:[""namespaces""], apigroups:[""""], verbs:[""list""]} policyrule{resources:[""namespaces""], apigroups:[""""], verbs:[""watch""]}] user=&amp;{cluster-admin  [system:authenticated] map[]} ownerrules=[policyrule{resources:[""selfsubjectaccessreviews""], apigroups:[""authorization.k8s.io""], verbs:[""create""]} policyrule{nonresourceurls:[""/api"" ""/api/*"" ""/apis"" ""/apis/*"" ""/healthz"" ""/swagger-2.0.0.pb-v1"" ""/swagger.json"" ""/swaggerapi"" ""/swaggerapi/*"" ""/version""], verbs:[""get""]}] ruleresolutionerrors=[]


as far as i can tell i'm connecting as cluster-admin, therefore should have sufficient permissions for what i'm trying to achieve. below are relevant cluster-admin config:

$ cat ~/.kube/config
apiversion: v1
kind: config
current-context: dev
preferences:
  colors: true

clusters:
- cluster:
    certificate-authority: cluster-ca.pem
    server: https://k8s.loc:4430
  name: dev

contexts:
- context:
    cluster: dev
    namespace: kube-system
    user: cluster-admin
  name: dev

users:
- name: cluster-admin
  user:
    client-certificate: cluster.pem
    client-key: cluster-key.pem


$ kubectl get clusterrole cluster-admin -o yaml
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: ""true""
  creationtimestamp: 2018-07-30t14:44:44z
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: cluster-admin
  resourceversion: ""1164791""
  selflink: /apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-admin
  uid: 196ffecc-9407-11e8-bd67-525400ac0b7d
rules:
- apigroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
- nonresourceurls:
  - '*'
  verbs:
  - '*'


$ kubectl get clusterrolebinding cluster-admin -o yaml
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: ""true""
  creationtimestamp: 2018-07-30t14:44:45z
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: cluster-admin
  resourceversion: ""1164832""
  selflink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/cluster-admin
  uid: 19e516a6-9407-11e8-bd67-525400ac0b7d
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: cluster-admin
subjects:
- apigroup: rbac.authorization.k8s.io
  kind: group
  name: system:masters


$ kubectl get serviceaccount cluster-admin -o yaml
apiversion: v1
kind: serviceaccount
metadata:
  creationtimestamp: 2018-07-30t13:32:13z
  name: cluster-admin
  namespace: kube-system
  resourceversion: ""1158783""
  selflink: /api/v1/namespaces/kube-system/serviceaccounts/cluster-admin
  uid: f809e079-93fc-11e8-8b85-525400546bcd
secrets:
- name: cluster-admin-token-t7s4c


i understand this is rbac problem, but have no idea how further debug this.

edit-1.

i tried the suggested, no joy unfortunately...

$ kubectl get clusterrolebinding cluster-admin-binding -o yaml
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  creationtimestamp: 2018-07-31t09:21:34z
  name: cluster-admin-binding
  resourceversion: ""1252260""
  selflink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/cluster-admin-binding
  uid: 1e1c0647-94a3-11e8-9f9b-525400ac0b7d
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: cluster-admin
subjects:
- kind: serviceaccount
  name: cluster-admin
  namespace: default


$ kubectl describe secret $(kubectl get secret | awk '/cluster-admin/{print $1}')
name:         cluster-admin-token-t7s4c
namespace:    kube-system
labels:       &lt;none&gt;
annotations:  kubernetes.io/service-account.name=cluster-admin
              kubernetes.io/service-account.uid=f809e079-93fc-11e8-8b85-525400546bcd

type:  kubernetes.io/service-account-token

data
====
ca.crt:     1785 bytes
namespace:  11 bytes
token:      eyjhbgcioijsuzi1niisinr5cci6ikpxvcj9.eyjpc3mioijrdwjlcm5ldgvzl3nlcnzpy2vhy2nvdw50iiwia3vizxjuzxrlcy5pby9zzxj2awnlywnjb3vudc9uyw1lc3bhy2uioijrdwjllxn5c3rlbsisimt1ymvybmv0zxmuaw8vc2vydmljzwfjy291bnqvc2vjcmv0lm5hbwuioijjbhvzdgvylwfkbwlulxrva2vulxq3czrjiiwia3vizxjuzxrlcy5pby9zzxj2awnlywnjb3vudc9zzxj2awnllwfjy291bnqubmftzsi6imnsdxn0zxitywrtaw4ilcjrdwjlcm5ldgvzlmlvl3nlcnzpy2vhy2nvdw50l3nlcnzpy2utywnjb3vudc51awqioijmoda5zta3os05m2zjltexztgtogi4ns01mju0mda1ndziy2qilcjzdwiioijzexn0zw06c2vydmljzwfjy291bnq6a3vizs1zexn0zw06y2x1c3rlci1hzg1pbij9.rc1x9or8garkhc3p0s-l_pc0e6teuwfbjtxan2w-coarucnco6r4wxxku32ngog86txqcho2wbopxtbj2cparib7fwdxzri6o6lpfzhwnzzo3b-ton2yxhmwecgjpbbqjdgkpkdeldkdxjehdbjm_gfaaudnyypffsp1_t3vvisf2dpcjemlobspryrcekmdie6ehf4rsn1jqb7tvpvtz_wal4crzotjtzdvof75atkiadtvxtxvv_ewzndckuwdupg5jk44qsmj0yig30qyym699l5iflirzd5pj0eepaomeoqsjdp7kvdzim2tbiu8yyl6fj7pg_53wjzrvlsk5pgpls-jpkokixfm9ffb2eeup0ewwlo5wvu5s--a2ekkehaqhtxgigeedudda_5jvijts0m6v9gcbe4_kyrpu7_qd_0tr68c5yxul83kfozj6a_s6idoz-p7ni6ffe_klgqqcguur2mtakjgimjn0gyhnaiqmhiu4yhrt-jffp0-5zclbi5srj-ab4yqgtch9w5_kbyd4s2y6rjv4ko00nzyvi0jahlz6el63tqpwykjypl2mof_p8xcpeodrf6o8bxdzfqlxlqda2nqyo8lmhlxjpe_wfeguwziuxwwth1rur6bisruf86041aa2pejmqjtfau0u_svo-yhmgxzt3o


then amended ~/.kube/config:

$ cat ~/.kube/config
apiversion: v1
kind: config
current-context: dev
preferences:
  colors: true

clusters:
- cluster:
    certificate-authority: cluster-ca.pem
    server: https://k8s.loc:4430
  name: dev

contexts:
- context:
    cluster: dev
    namespace: kube-system
    user: cluster-admin-2
  name: dev

users:
- name: cluster-admin
  user:
    client-certificate: cluster.pem
    client-key: cluster-key.pem
- name: cluster-admin-2
  user:
    token: eyjhbgcioijsuzi1niisinr5cci6ikpxvcj9.eyjpc3mioijrdwjlcm5ldgvzl3nlcnzpy2vhy2nvdw50iiwia3vizxjuzxrlcy5pby9zzxj2awnlywnjb3vudc9uyw1lc3bhy2uioijrdwjllxn5c3rlbsisimt1ymvybmv0zxmuaw8vc2vydmljzwfjy291bnqvc2vjcmv0lm5hbwuioijjbhvzdgvylwfkbwlulxrva2vulxq3czrjiiwia3vizxjuzxrlcy5pby9zzxj2awnlywnjb3vudc9zzxj2awnllwfjy291bnqubmftzsi6imnsdxn0zxitywrtaw4ilcjrdwjlcm5ldgvzlmlvl3nlcnzpy2vhy2nvdw50l3nlcnzpy2utywnjb3vudc51awqioijmoda5zta3os05m2zjltexztgtogi4ns01mju0mda1ndziy2qilcjzdwiioijzexn0zw06c2vydmljzwfjy291bnq6a3vizs1zexn0zw06y2x1c3rlci1hzg1pbij9.rc1x9or8garkhc3p0s-l_pc0e6teuwfbjtxan2w-coarucnco6r4wxxku32ngog86txqcho2wbopxtbj2cparib7fwdxzri6o6lpfzhwnzzo3b-ton2yxhmwecgjpbbqjdgkpkdeldkdxjehdbjm_gfaaudnyypffsp1_t3vvisf2dpcjemlobspryrcekmdie6ehf4rsn1jqb7tvpvtz_wal4crzotjtzdvof75atkiadtvxtxvv_ewzndckuwdupg5jk44qsmj0yig30qyym699l5iflirzd5pj0eepaomeoqsjdp7kvdzim2tbiu8yyl6fj7pg_53wjzrvlsk5pgpls-jpkokixfm9ffb2eeup0ewwlo5wvu5s--a2ekkehaqhtxgigeedudda_5jvijts0m6v9gcbe4_kyrpu7_qd_0tr68c5yxul83kfozj6a_s6idoz-p7ni6ffe_klgqqcguur2mtakjgimjn0gyhnaiqmhiu4yhrt-jffp0-5zclbi5srj-ab4yqgtch9w5_kbyd4s2y6rjv4ko00nzyvi0jahlz6el63tqpwykjypl2mof_p8xcpeodrf6o8bxdzfqlxlqda2nqyo8lmhlxjpe_wfeguwziuxwwth1rur6bisruf86041aa2pejmqjtfau0u_svo-yhmgxzt3o


and then tried to apply the same clusterrole, which rendered the same error:

$ kubectl apply -f clusterrole.yml 
error from server (forbidden): error when creating ""clusterrole.yml"": clusterroles.rbac.authorization.k8s.io ""system:coredns"" is forbidden: attempt to grant extra privileges: [policyrule{resources:[""endpoints""], apigroups:[""""], verbs:[""list""]} policyrule{resources:[""endpoints""], apigroups:[""""], verbs:[""watch""]} policyrule{resources:[""services""], apigroups:[""""], verbs:[""list""]} policyrule{resources:[""services""], apigroups:[""""], verbs:[""watch""]} policyrule{resources:[""pods""], apigroups:[""""], verbs:[""list""]} policyrule{resources:[""pods""], apigroups:[""""], verbs:[""watch""]} policyrule{resources:[""namespaces""], apigroups:[""""], verbs:[""list""]} policyrule{resources:[""namespaces""], apigroups:[""""], verbs:[""watch""]}] user=&amp;{system:serviceaccount:kube-system:cluster-admin f809e079-93fc-11e8-8b85-525400546bcd [system:serviceaccounts system:serviceaccounts:kube-system system:authenticated] map[]} ownerrules=[policyrule{resources:[""selfsubjectaccessreviews""], apigroups:[""authorization.k8s.io""], verbs:[""create""]} policyrule{nonresourceurls:[""/api"" ""/api/*"" ""/apis"" ""/apis/*"" ""/healthz"" ""/swagger-2.0.0.pb-v1"" ""/swagger.json"" ""/swaggerapi"" ""/swaggerapi/*"" ""/version""], verbs:[""get""]}] ruleresolutionerrors=[]


below are the flags which i use to start apiserver:

  containers:
    - name: kube-apiserver
      image: quay.io/coreos/hyperkube:${k8s_ver}
      command:
        - /hyperkube
        - apiserver
        - --bind-address=0.0.0.0
        - --etcd-servers=${etcd_endpoints}
        - --allow-privileged=true
        - --service-cluster-ip-range=${service_ip_range}
        - --secure-port=443
        - --advertise-address=${advertise_ip}
        - --admission-control=namespacelifecycle,limitranger,serviceaccount,defaultstorageclass,resourcequota
        - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
        - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
        - --client-ca-file=/etc/kubernetes/ssl/ca.pem
        - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
        - --runtime-config=extensions/v1beta1/networkpolicies=true
        - --anonymous-auth=false
        - --authorization-mode=alwaysallow,rbac,node


and here are the scripts, which i use to generate my tls certs:

root ca:

openssl genrsa -out ca-key.pem 4096
openssl req -x509 -new -nodes -key ca-key.pem -days 3650 -out ca.pem -subj ""/cn=kube-ca""


apiserver:

cat &gt; openssl.cnf &lt;&lt;eof
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name

[req_distinguished_name]

[v3_req]
basicconstraints = ca:false
keyusage = nonrepudiation, digitalsignature, keyencipherment
subjectaltname = @alt_names

[alt_names]
dns.1 = kubernetes
dns.2 = kubernetes.default
dns.3 = kubernetes.default.svc
dns.4 = kubernetes.default.svc.cluster.local
dns.5 = ${master_lb_dns}
ip.1 = ${k8s_service_ip}
ip.2 = ${master_host}
eof

openssl genrsa -out apiserver-key.pem 4096
openssl req -new -key apiserver-key.pem -out apiserver.csr -subj ""/cn=kube-apiserver"" -config openssl.cnf
openssl x509 -req -in apiserver.csr -ca ca.pem -cakey ca-key.pem -cacreateserial -out apiserver.pem -days 3650 -extensions v3_req -extfile openssl.cnf


cluster-admin:

openssl genrsa -out cluster-admin-key.pem 4096
openssl req -new -key cluster-admin-key.pem -out cluster-admin.csr -subj ""/cn=cluster-admin""
openssl x509 -req -in cluster-admin.csr -ca ca.pem -cakey ca-key.pem -cacreateserial -out cluster-admin.pem -days 3650


i hope this gives you more insight, what's wrong with my system.

edit-2.

i noted a slight discrepancy between my system configuration and what @marcinromaszewicz suggested, thus the namespace of the cluster-admin serviceaccount, in my case it is in the kube-system as opposed to the default namespace:

$ kubectl delete clusterrolebinding cluster-admin-binding 
clusterrolebinding.rbac.authorization.k8s.io ""cluster-admin-binding"" deleted

$ kubectl create clusterrolebinding cluster-admin-binding \
 --clusterrole=cluster-admin --serviceaccount=kube-system:cluster-admin
clusterrolebinding.rbac.authorization.k8s.io ""cluster-admin-binding"" created

$ kubectl apply -f clusterrole.yml 
clusterrole.rbac.authorization.k8s.io ""system:coredns"" created


however it still doesn't work with my certificates... 

edit-3.

as suggested in the comments, in order for apiserver to recognize the user as the cluster-admin, subject line in the certificate of that user must contain the following items: subject: cn = cluster-admin, o = system:masters. one way to generate such a certificate is as follows:

openssl genrsa -out cluster-admin-key.pem 4096
openssl req -new -key cluster-admin-key.pem -out cluster-admin.csr -subj ""/cn=cluster-admin/o=system:masters""
openssl x509 -req -in cluster-admin.csr -ca ca.pem -cakey ca-key.pem -cacreateserial -out cluster-admin.pem -days 3650

",<kubernetes><kubectl><coreos><rbac>,51599748,7,"there isn't enough information here to answer your question.

it sounds like you are running into privilege escalation prevention:
https://kubernetes.io/docs/reference/access-authn-authz/rbac/#privilege-escalation-prevention-and-bootstrapping

this would mean you aren't actually running as cluster-admin. check your kubectl config. you might be running as ""admin"" constrained to a particular name space, for example.

(edit based on comment below)

your identity to k8s is established by the contents of your cluster.pem certificate, not the user name from kubeconfig, since that user name is only valid inside the kubeconfig file. your actual user is determined by that certificate. 

i see that you have a service account named cluster-admin, but it is not a member of ""system:masters"", since groups are something that are a property of the authentication system which authenticates users - you need to create an explicit cluster role binding to bind your cluster-admin service account to the cluster-admin clusterrole.

kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --serviceaccount=default:cluster-admin


you should see the clusterrole now bound with your service account.

$ kubectl get clusterrolebinding cluster-admin-binding -o yaml
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  creationtimestamp: 2018-07-30t22:02:33z
  name: cluster-admin-binding
  resourceversion: ""71152""
  selflink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/cluster-admin-binding
  uid: 42a2862c-9444-11e8-8b71-080027de17da
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: cluster-admin
subjects:
- kind: serviceaccount
  name: cluster-admin
  namespace: default


note at the bottom, that the binding applies to ""serviceaccount"", not group.

your service account has an access token, use that to authenticate instead of your certificate. i made myself a cluster-admin service account, and this is how i get the token:

$ kubectl describe secret $(kubectl get secret | grep cluster-admin | awk '{print $1}')
name:         cluster-admin-token-96vdz
namespace:    default
labels:       &lt;none&gt;
annotations:  kubernetes.io/service-account.name=cluster-admin
              kubernetes.io/service-account.uid=f872f08b-9442-11e8-8b71-080027de17da

type:  kubernetes.io/service-account-token

data
====
token:      eyjhbgcioijsuzi1niisimtpzci6iij9.eyjpc3mioijrdwjlcm5ldgvzl3nlcnzpy2vhy2nvdw50iiwia3vizxjuzxrlcy5pby9zzxj2awnlywnjb3vudc9uyw1lc3bhy2uioijkzwzhdwx0iiwia3vizxjuzxrlcy5pby9zzxj2awnlywnjb3vudc9zzwnyzxqubmftzsi6imnsdxn0zxitywrtaw4tdg9rzw4totz2zhoilcjrdwjlcm5ldgvzlmlvl3nlcnzpy2vhy2nvdw50l3nlcnzpy2utywnjb3vudc5uyw1lijoiy2x1c3rlci1hzg1pbiisimt1ymvybmv0zxmuaw8vc2vydmljzwfjy291bnqvc2vydmljzs1hy2nvdw50lnvpzci6imy4nzjmmdhiltk0nditmtfloc04yjcxlta4mdayn2rlmtdkysisinn1yii6inn5c3rlbtpzzxj2awnlywnjb3vuddpkzwzhdwx0omnsdxn0zxitywrtaw4ifq.&lt;signature snipped&gt;
ca.crt:     1066 bytes
namespace:  7 bytes


update kubeconfig to authenticate yourself using that token, instead of the certificate you are currently using, and you should be successfully authenticated as that cluster-admin service account.

(edit 2)
it turned out that the certificate being used to authenticate into kubernetes did not have any identity claims about the user. kubernetes relies on authentication modules to authenticate users, in this case, based on certificates. it was expecting the certificate to contain a claim which put the user into the ""system:masters"" group, by setting the organization to ""system:masters"".

there are many moving pieces here. the problem had nothing to do with service accounts or roles, but rather in user authentication, which is very opaque.
"
51308341,"error: the server doesn't have a resource type ""svc""","getting error: the server doesn't have a resource type ""svc"" when testing kubectl configuration whilst following this guide:

https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html

detailed error

$ kubectl get svc -v=8

i0712 15:30:24.902035   93745 loader.go:357] config loaded from file /users/matt.canty/.kube/config-test
i0712 15:30:24.902741   93745 round_trippers.go:383] get https://redacted.yl4.us-east-1.eks.amazonaws.com/api
i0712 15:30:24.902762   93745 round_trippers.go:390] request headers:
i0712 15:30:24.902768   93745 round_trippers.go:393]     user-agent: kubectl/v1.10.3 (darwin/amd64) kubernetes/2bba012
i0712 15:30:24.902773   93745 round_trippers.go:393]     accept: application/json, */*
i0712 15:30:25.425614   93745 round_trippers.go:408] response status: 401 unauthorized in 522 milliseconds
i0712 15:30:25.425651   93745 round_trippers.go:411] response headers:
i0712 15:30:25.425657   93745 round_trippers.go:414]     content-type: application/json
i0712 15:30:25.425662   93745 round_trippers.go:414]     content-length: 129
i0712 15:30:25.425670   93745 round_trippers.go:414]     date: thu, 12 jul 2018 14:30:25 gmt
i0712 15:30:25.426757   93745 request.go:874] response body: {""kind"":""status"",""apiversion"":""v1"",""metadata"":{},""status"":""failure"",""message"":""unauthorized"",""reason"":""unauthorized"",""code"":401}
i0712 15:30:25.428104   93745 cached_discovery.go:124] skipped caching discovery info due to unauthorized
i0712 15:30:25.428239   93745 round_trippers.go:383] get https://redacted.yl4.us-east-1.eks.amazonaws.com/api
i0712 15:30:25.428258   93745 round_trippers.go:390] request headers:
i0712 15:30:25.428268   93745 round_trippers.go:393]     accept: application/json, */*
i0712 15:30:25.428278   93745 round_trippers.go:393]     user-agent: kubectl/v1.10.3 (darwin/amd64) kubernetes/2bba012
i0712 15:30:25.577788   93745 round_trippers.go:408] response status: 401 unauthorized in 149 milliseconds
i0712 15:30:25.577818   93745 round_trippers.go:411] response headers:
i0712 15:30:25.577838   93745 round_trippers.go:414]     content-type: application/json
i0712 15:30:25.577854   93745 round_trippers.go:414]     content-length: 129
i0712 15:30:25.577868   93745 round_trippers.go:414]     date: thu, 12 jul 2018 14:30:25 gmt
i0712 15:30:25.578876   93745 request.go:874] response body: {""kind"":""status"",""apiversion"":""v1"",""metadata"":{},""status"":""failure"",""message"":""unauthorized"",""reason"":""unauthorized"",""code"":401}
i0712 15:30:25.579492   93745 cached_discovery.go:124] skipped caching discovery info due to unauthorized
i0712 15:30:25.579851   93745 round_trippers.go:383] get https://redacted.yl4.us-east-1.eks.amazonaws.com/api
i0712 15:30:25.579864   93745 round_trippers.go:390] request headers:
i0712 15:30:25.579873   93745 round_trippers.go:393]     accept: application/json, */*
i0712 15:30:25.579879   93745 round_trippers.go:393]     user-agent: kubectl/v1.10.3 (darwin/amd64) kubernetes/2bba012
i0712 15:30:25.729513   93745 round_trippers.go:408] response status: 401 unauthorized in 149 milliseconds
i0712 15:30:25.729541   93745 round_trippers.go:411] response headers:
i0712 15:30:25.729547   93745 round_trippers.go:414]     content-type: application/json
i0712 15:30:25.729552   93745 round_trippers.go:414]     content-length: 129
i0712 15:30:25.729557   93745 round_trippers.go:414]     date: thu, 12 jul 2018 14:30:25 gmt
i0712 15:30:25.730606   93745 request.go:874] response body: {""kind"":""status"",""apiversion"":""v1"",""metadata"":{},""status"":""failure"",""message"":""unauthorized"",""reason"":""unauthorized"",""code"":401}
i0712 15:30:25.731228   93745 cached_discovery.go:124] skipped caching discovery info due to unauthorized
i0712 15:30:25.731254   93745 factory_object_mapping.go:93] unable to retrieve api resources, falling back to hardcoded types: unauthorized
f0712 15:30:25.731493   93745 helpers.go:119] error: the server doesn't have a resource type ""svc""


screenshot of eks cluster in aws



version

kubectl version

client version: version.info{major:""1"", minor:""10"", gitversion:""v1.10.3"", gitcommit:""2bba0127d85d5a46ab4b778548be28623b32d0b0"", gittreestate:""clean"", builddate:""2018-05-28t20:03:09z"", goversion:""go1.9.3"", compiler:""gc"", platform:""darwin/amd64""}
error: you must be logged in to the server (the server has asked for the client to provide credentials)


config

kubctl config

$ kubectl config view

apiversion: v1
clusters:
- cluster:
    certificate-authority-data: redacted
    server: https://redacted.yl4.us-east-1.eks.amazonaws.com
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: aws
  name: aws
current-context: aws
kind: config
preferences: {}
users:
- name: aws
  user:
    exec:
      apiversion: client.authentication.k8s.io/v1alpha1
      args:
      - token
      - -i
      - test
      command: heptio-authenticator-aws
      env:
      - name: aws_profile
        value: personal


aws config

cat .aws/config

[profile personal]
source_profile = personal 


aws credentials

$ cat .aws/credentials

[personal]
aws_access_key_id = redacted
aws_secret_access_key = redacted


~/.kube/config-test

apiversion: v1
clusters:
- cluster:
    certificate-authority-data: redaceted
    server: https://redacted.yl4.us-east-1.eks.amazonaws.com
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: aws
  name: aws
current-context: aws
kind: config
preferences: {}
users:
- name: aws
  user:
    exec:
      apiversion: client.authentication.k8s.io/v1alpha1
      args:
      - token
      - -i
      - test
      command: heptio-authenticator-aws
      env:
      - name: aws_profile
        value: personal


similar issues


error-the-server-doesnt-have-resource-type-svc
the-connection-to-the-server-localhost8080-was-refused-did-you-specify-the-ri

",<kubernetes><kubectl><amazon-eks>,53169402,7,"i just had a similar issue which i managed to resolve with aws support. the issue i was having was that the cluster was created with a role that was assumed by the user, but kubectl was not assuming this role with the default kube config created by the aws-cli.

i fixed the issue by providing the role in the users section of the kube config

users:
- name: aws
  user:
    exec:
      apiversion: client.authentication.k8s.io/v1alpha1
      args:
      - token
      - -i
      - test
      - -r
      - &lt;arn::of::your::role&gt;
      command: aws-iam-authenticator
      env:
      - name: aws_profile
        value: personal


i believe the heptio-aws-authenticator has now been changed to the aws-iam-authenticator, but this change was what allowed me to use the cluster.
"
59369174,unable to helm install due to deployment manifest issue,"while trying to perform helm install


  error: unable to build kubernetes objects from release manifest:
  [unable to recognize """": no matches for kind ""service"" in version
  ""extensions/v1beta1"", error validating """": error validating data:
  validationerror(deployment.spec): missing required field ""selector"" in
  io.k8s.api.apps.v1.deploymentspec]


my service.yaml looks like below

apiversion: extensions/v1beta1
kind: service
metadata:
  name: helm-xxx-helper-api
spec:
  type: nodeport
  ports:
    - nodeport: 31235
      port: 80
      targetport: 8080
  selector:
     app: helm-xxx-helper


my deployment.yaml 

---
apiversion: apps/v1
kind: deployment
metadata:
  name: helm-xxx-helper
spec:
  replicas: 2
  selector:
    matchlabels:
    name: helm-xxx-helper
  template:
    metadata:
      labels:
        app: helm-xxx-helper
    spec:
      containers:
      - name: helm-xxx-helper
        image: xxxxxxxxx:5001/devops/xxx-helper:latest
        imagepullpolicy: always
        env:
          - name: xxx_stage
            value: ""dev""
        ports:
        - containerport: 8080


what could be the issue here?
",<kubernetes><kubernetes-helm>,59372425,7,"as you received this error it means that you are using version kubernetes 1.16 or newer.

issue 1 - with service

in this version many apiversion has been changed (deployments, statefulset, service). more details can be found here.

in kubernetes 1.16 you need to use apiversion: v1 for service. otherwise you will receive errors like

error: unable to recognize ""stdin"": no matches for kind ""service"" in version ""extensions/v1beta1""
error: unable to recognize ""stdin"": no matches for kind ""service"" in version ""extensions/v1""
error: unable to recognize ""stdin"": no matches for kind ""service"" in version ""apps/v1""


issue 2 - with deployment.


spec.selector.matchlabels does not contain value like name. you need to use value from labels. so in this case instead of name: helm-xxx-helper you need to use app: helm-xxx-helper otherwise you will receive error like:


the deployment ""helm-xxx-helper"" is invalid: spec.template.metadata.labels: invalid value: map[string]string{""app"":""helm-xxx-helper""}: `selector` does not match template `labels`



wrong yaml format. in your code you have


...
selector:
  matchlabels:
  name: helm-xxx-helper
...


value for matchlabels should be under 3rd letter (t). also as i mentioned in previous point you need to change name to app

proper format with correct valye of matchlables:

...
selector:
  matchlabels:
    app: helm-xxx-helper
...


you can read about labels and selectors here.

as you mentioned it is helm, you will need to change kubernetes version to older than 1.16 or change apiversion in each object yaml in  template directory.
there was already a similar case. please check this thread for more information.

below both yamls which will create service and deployment. tested on kubernetes 1.16.1.

apiversion: v1
kind: service
metadata:
  name: helm-xxx-helper-api
spec:
  type: nodeport
  ports:
    - nodeport: 31235
      port: 80
      targetport: 8080
  selector:
    app: helm-xxx-helper
---
apiversion: apps/v1
kind: deployment
metadata:
  name: helm-xxx-helper
spec:
  replicas: 2
  selector:
    matchlabels:
      app: helm-xxx-helper
  template:
    metadata:
      labels:
        app: helm-xxx-helper
    spec:
      containers:
      - name: helm-xxx-helper
        image: nginx # as i dont have your image ive put nginx
        imagepullpolicy: always
        env:
          - name: xxx_stage
            value: ""dev""
        ports:
        - containerport: 8080

"
59375922,"serviceaccount cannot list resource ""pods"" in namespace though it has role with appropriate resources","i have the following definitions in my custom namespace:

apiversion: v1
kind: serviceaccount
metadata:
  name: test-sa
---
kind: role
apiversion: rbac.authorization.k8s.io/v1
metadata:
  name: test
rules:
  - apigroups: [""""]
    resources: [""pods"", ""pods/exec""]
    verbs: [""get"", ""list"", ""delete"", ""patch"", ""create""]
  - apigroups: [""extensions"", ""apps""]
    resources: [""deployments"", ""deployments/scale""]
    verbs: [""get"", ""list"", ""delete"", ""patch"", ""create""]
---
apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: test
subjects:
  - kind: user
    name: test-sa
    apigroup: rbac.authorization.k8s.io
roleref:
  kind: role
  name: test
  apigroup: rbac.authorization.k8s.io


running describe role test

name:         test
labels:       &lt;none&gt;
annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {""apiversion"":""rbac.authorization.k8s.io/v1"",""kind"":""role"",""metadata"":{""annotations"":{},""name"":""test"",""namespace"":""test-namesapce...
policyrule:
  resources                     non-resource urls  resource names  verbs
  ---------                     -----------------  --------------  -----
  pods/exec                     []                 []              [get list delete patch create]
  pods                          []                 []              [get list delete patch create]
  deployments.apps/scale        []                 []              [get list delete patch create]
  deployments.apps              []                 []              [get list delete patch create]
  deployments.extensions/scale  []                 []              [get list delete patch create]
  deployments.extensions        []                 []              [get list delete patch create]


when i'm trying to run the command kubectl get pods in a pod that is using this service account, i'm getting the following error:


  error from server (forbidden): pods is forbidden: user
  ""system:serviceaccount:test-namespace:test-sa"" cannot list resource
  ""pods"" in api group """" in the namespace ""test-namespace""


where is that misconfigured?
",<kubernetes><kubectl><rbac>,59378979,7,"the problem was with the subjects of rolebinding. the correct definition would be:

apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: test
subjects:
  - kind: serviceaccount
    name: test-sa
roleref:
  kind: role
  name: test
  apigroup: rbac.authorization.k8s.io

"
62721059,hpa cannot read metric value (cpu utilization) on gke,"i'm working on google kubernetes engine on a single cluster.
the cluster automatically scales the number of nodes.
i have three created a deployment and set up the auto-scaling policy using the website (workloads -&gt; deployment -&gt; actions -&gt; auto-scaling), so not manually writing the yaml configuration.
based on an official guide, i did not make any mistake.

if you do not specify requests, you can autoscale based only on the
absolute value of the resource's utilization, such as millicpus for
cpu utilization.

the following is the full deployment yaml:
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: student
  name: student
  namespace: ulibretto
spec:
  replicas: 1
  selector:
    matchlabels:
      app: student
  strategy:
    rollingupdate:
      maxsurge: 25%
      maxunavailable: 25%
    type: rollingupdate
  template:
    metadata:
      labels:
        app: student
    spec:
      containers:
        - env:
            - name: cluster_host
              valuefrom:
                configmapkeyref:
                  key: cluster_host
                  name: shared-env-vars
            - name: bind_host
              valuefrom:
                configmapkeyref:
                  key: bind_host
                  name: shared-env-vars
            - name: token_timeout
              valuefrom:
                configmapkeyref:
                  key: token_timeout
                  name: shared-env-vars
          image: gcr.io/ulibretto/github.com/ulibretto/studentservice
          imagepullpolicy: ifnotpresent
          name: studentservice-1
---
apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  labels:
    app: student
  name: student-hpa-n3bp
  namespace: ulibretto
spec:
  maxreplicas: 100
  metrics:
    - resource:
        name: cpu
        targetaverageutilization: 80
      type: resource
  minreplicas: 1
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: student
---
apiversion: v1
kind: service
metadata:
  annotations:
    cloud.google.com/neg: '{&quot;ingress&quot;:true}'
  labels:
    app: student
  name: student-ingress
  namespace: ulibretto
spec:
  clusterip: 10.44.5.59
  ports:
    - port: 5000
      protocol: tcp
      targetport: 5000
  selector:
    app: student
  sessionaffinity: none
  type: clusterip

the problem is that the hpa does not see the metric (average cpu utilization), which is really strange (see image).
hpa cannot read metric value
what i am missing?
",<kubernetes><yaml><google-kubernetes-engine><gcloud>,62752352,7,"edited
you are right. you don't need to specify namespace: ulibretto in scaletargetref: as i mentioned earlier.
as you provided all yamls i was able to find proper root cause.
if you will check gke docs you will find comment in code
    resources:
      # you must specify requests for cpu to autoscale
      # based on cpu utilization
      requests:
        cpu: &quot;250m&quot;
        

your deployment didn't have specified resource requests. i've tried on this (i've removed some parts as i was not able to deploy your container and changed apiversion in hpa):
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: student
  name: student
  namespace: ulibretto
spec:
  replicas: 3
  selector:
    matchlabels:
      app: student
  strategy:
    rollingupdate:
      maxsurge: 25%
      maxunavailable: 25%
    type: rollingupdate
  template:
    metadata:
      labels:
        app: student
    spec:
      containers:
      - image: nginx
        imagepullpolicy: ifnotpresent
        name: studentservice-1
        resources:
          requests:
            cpu: &quot;250m&quot;
---
apiversion: autoscaling/v1
kind: horizontalpodautoscaler
metadata:
  labels:
    app: student
  name: student-hpa
  namespace: ulibretto
spec:
  maxreplicas: 100
  minreplicas: 1
  targetcpuutilizationpercentage: 80
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: student

$ kubectl get all -n ulibretto
name                           ready   status    restarts   age
pod/student-6f797d5888-84xfq   1/1     running   0          7s
pod/student-6f797d5888-b7ctq   1/1     running   0          7s
pod/student-6f797d5888-fbtmd   1/1     running   0          7s
name                      ready   up-to-date   available   age
deployment.apps/student   3/3     3            3           7s
name                                 desired   current   ready   age
replicaset.apps/student-6f797d5888   3         3         3       7s
name                                              reference            targets         minpods   maxpods   replicas   age
horizontalpodautoscaler.autoscaling/student-hpa   deployment/student   &lt;unknown&gt;/80%   1         100       0          7s

after ~1-5 minutes you will receive some metrics.
$ kubectl get all -n ulibretto
name                           ready   status    restarts   age
pod/student-6f797d5888-84xfq   1/1     running   0          95s
pod/student-6f797d5888-b7ctq   1/1     running   0          95s
pod/student-6f797d5888-fbtmd   1/1     running   0          95s

name                      ready   up-to-date   available   age
deployment.apps/student   3/3     3            3           95s

name                                 desired   current   ready   age
replicaset.apps/student-6f797d5888   3         3         3       95s

name                                              reference            targets   minpods   maxpods   replicas   age
horizontalpodautoscaler.autoscaling/student-hpa   deployment/student   0%/80%    1         100       3          95s

same situation if you would like to create hpa using cli:
$ kubectl autoscale deployment student -n ulibretto --cpu-percent=50 --min=1 --max=100
horizontalpodautoscaler.autoscaling/student autoscaled

$ kubectl get hpa -n ulibretto
name      reference            targets         minpods   maxpods   replicas   age
student   deployment/student   &lt;unknown&gt;/50%   1         100       0          3s

and after a while you will receive 0% instead of &lt;unknown&gt;
$ kubectl get all -n ulibretto
name                           ready   status    restarts   age
pod/student-6f797d5888-84xfq   1/1     running   0          4m4s
pod/student-6f797d5888-b7ctq   1/1     running   0          4m4s
pod/student-6f797d5888-fbtmd   1/1     running   0          4m4s
name                      ready   up-to-date   available   age
deployment.apps/student   3/3     3            3           4m5s
name                                 desired   current   ready   age
replicaset.apps/student-6f797d5888   3         3         3       4m5s
name                                          reference            targets   minpods   maxpods   replicas   age
horizontalpodautoscaler.autoscaling/student   deployment/student   0%/50%    1         100       3          58s

"
64609430,how to create a secret file in kubernetes,"i have yaml which i used to create a secret using below command.
kubectl create secret generic -n &lt;namespace&gt; gitlab-openid-connect --from-file=provider=provider.yaml

below is provider.yaml:
name: 'openid_connect'
label: 'openid sso login'
args:
  name: 'openid_connect'
  scope: ['openid','profile','email']
  response_type: 'code'
  issuer: 'https://keycloak.example.com/auth/realms/myrealm'
  discovery: true
  client_auth_method: 'basic'
  client_options:
    identifier: 'gitlab.example.com-oidc'
    secret: '&lt;keycloak clientid secret&gt;'
    redirect_uri: 'https://gitlab.example.com/users/auth/openid_connect/callback'

i want to convert it into a secret yaml file so that i can run kubectl apply -f provider.yaml
i tried to create below file but it does not work, provider-new.yaml
apiversion: v1
kind: secret
type: opaque
metadata:
  name: 'openid_connect'
  label: 'openid sso login'
data:
  scope: ['openid','profile','email']
  response_type: 'code'
  issuer: 'url'
  discovery: true
  client_auth_method: 'basic'
  client_options:
    identifier: 'identifier'
    secret: 'secret-key'
    redirect_uri: 'url'

",<kubernetes><yaml><kubernetes-secrets>,64610245,7,"to make this work you need to use --from-env-file instead --from-file. and the file containing the variables should be in the plain text.

to create a secret from one or more files, use --from-file or
--from-env-file. the file must be plaintext, but the extension of the file does not matter.


when you create the secret using --from-file, the value of the secret
is the entire contents of the file. if the value of your secret
contains multiple key-value pairs, use --from-env-file instead.

file provider.yaml with variables:
scope= ['openid','profile','email']
response_type= 'code'
issuer= 'url'
discovery= true
client_auth_method= 'basic'
identifier= 'identifier'
secret= 'secret-key'
redirect_uri= 'url'

kubectl create secret generic -n default gitlab-openid-connect --from-env-file=provider.yaml

result:
apiversion: v1
data:
  client_auth_method: icdiyxnpyyc=
  discovery: ihrydwu=
  identifier: icdpzgvudglmawvyjw==
  issuer: icd1cmwn
  redirect_uri: icd1cmwn
  response_type: icdjb2rljw==
  scope: ifsnb3blbmlkjywnchjvzmlszscsj2vtywlsj10=
  secret: icdzzwnyzxqta2v5jw==
kind: secret
metadata:
  creationtimestamp: null
  name: gitlab-openid-connect
  namespace: default

another thing is that isn't possible to establish a hierarchy in the secret data scope, so the following isn't gonna work:
client_options
  identifier= 'identifier'
  secret= 'secret-key'
  redirect_uri= 'url'

source: google cloud
"
54060270,how to use podtemplate,"i saw that there is an object named podtemplate which have little documentation.  

it is mentioned:


  pod templates are pod specifications which are included in other
  objects, such as replication controllers, jobs, and daemonsets.


but i am not sure how to mention it on replication controllers, jobs or daemonsets.  

i created a podtemplate like this:  

kubectl apply -f - &lt;&lt;eof
apiversion: v1
kind: podtemplate
metadata:
  name: pod-test
  namespace: default
template:
  metadata:
    name: pod-template
  spec:
    containers:
    - name: container
      image: alpine
      command: [""/bin/sh""]
      args: [""-c"", ""sleep 100""]
eof


i want to use it in a daemonset, how can i do it ?  

here is an example for daemonset yaml:  

kubectl apply -f - &lt;&lt;eof
apiversion: apps/v1
kind: daemonset
metadata:
  name: pod-by-daemonset
  namespace: default
spec:
  selector:
    matchlabels:
      name: selector
  template:
    metadata:
      labels:
        name: selector
    spec:
      containers: # i don't want to specify it, i want to use the template.
      - name: container
        image: alpine 

eof

",<kubernetes><kubernetes-pod>,54061209,7,"interestingly the page for daemonset gives an example of creating a daemonset and says that 


  the .spec.template is a pod template. it has exactly the same schema
  as a pod, except it is nested and does not have an apiversion or kind.


so the intention is to inline the pod schema in the daemonset.

however, there does seem to have been a plan to be able to reference a podtemplate by the form:

templateref:
name: &lt;templatename&gt;


what seems to have happened from the trail on github is that this way of referencing a predefined podtemplate was added but not finished and then removed. you can give it a try but it looks to me like only the inline spec is supported.
"
64541147,ingress nginx cert-manager certificate invalid on browser,"i am experiencing a rather weird issue and have been stuck on this for 2 days. i have a kubernetes cluster running nginx-ingress and cert-manager. everything seems to be working fine though when visiting my website through https, it gives the following error (in chromium edge):
net::err_cert_authority_invalid
if i continue anyways, it loads the site normally but without the certificate.
the certificate is properly being given, secret created, no errors anywhere.
i have the following annotations in my ingress resource:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    kubernetes.io/tls-acme: &quot;true&quot;
    cert-manager.io/cluster-issuer: &quot;letsencrypt-production&quot;
    ingress.kubernetes.io/ssl-redirect: &quot;true&quot;

my cluster issuer:
apiversion: cert-manager.io/v1
kind: clusterissuer
metadata:
  name: letsencrypt-production
spec:
  acme:
    # the acme production api url
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    # email address used for acme registration
    email: *********
    # name of a secret used to store the acme account private key
    privatekeysecretref:
      name: letsencrypt-production
    # enable the http-01 challenge provider
    solvers:
    - http01:
        ingress:
          class: nginx

the certificate resource returns:
 normal  issuing    108s   cert-manager  the certificate has been successfully issued

i am relatively new to kubernetes so let me know if there are any other debugging steps i can take.
",<kubernetes><kubernetes-ingress><nginx-ingress><cert-manager>,64541505,7,"you are using acme staging server: https://acme-staging-v02.api.letsencrypt.org/directory server which does not provide valid certificate.
to get valid certificate you have to use acme production server  server: https://acme-v02.api.letsencrypt.org/directory.
you can try this.
apiversion: cert-manager.io/v1
kind: clusterissuer
metadata:
  name: letsencrypt-production
spec:
  acme:
    # the acme production api url
    server: https://acme-v02.api.letsencrypt.org/directory
    # email address used for acme registration
    email: *********
    # name of a secret used to store the acme account private key
    privatekeysecretref:
      name: letsencrypt-production
    # enable the http-01 challenge provider
    solvers:
    - http01:
        ingress:
          class: nginx


"
52328483,kubernetes ingress rules: how to use wildcard and specific subdomain together,"i want ingress to redirect a specific subdomain to one backend and all others to other backend. basically, i want to define a rule something like the following:


  if subdomain is foo.bar.com then go to s1, for all other subdomains go to s2


when i define the rules as shown below in the ingress spec, i get this exception at deployment:

error: upgrade failed: cannot re-use a name that is still in use


when i change *.bar.com to demo.bar.com it works, however.

here's my ingress resource spec:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: test
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          servicename: s1
          serviceport: 80
  - host: *.bar.com
    http:
      paths:
      - backend:
          servicename: s2
          serviceport: 80


anyone has an idea if it is possible or not?
",<kubernetes><traefik><kubernetes-ingress><traefik-ingress>,71888803,7,"this is now possible in kubernetes with nginx:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
    kubernetes.io/ingress.class: nginx
    kubernetes.io/ingress.global-static-ip-name: web-static-ip
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/server-alias: www.foo.bar
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
  name: foo-bar-ingress
  namespace: test
spec:
  rules:
  - host: 'foo.bar.com'
    http:
      paths:
      - backend:
          servicename: specific-service
          serviceport: 8080
        path: /(.*)
        pathtype: implementationspecific
  - host: '*.bar.com'
    http:
      paths:
      - backend:
          servicename: general-service
          serviceport: 80
        path: /(.*)
        pathtype: implementationspecific

"
65857310,"kubectl : unable to recognize ""csr.yaml"": no matches for kind ""certificatesigningrequest"" in version ""certificates.k8s.io/v1""","i have this template i try to invoke: looking at the docs example here
--- 
apiversion: certificates.k8s.io/v1
kind: certificatesigningrequest
metadata: 
  name: vault-csr
spec: 
  groups: 
    - system: authenticated
  request: ls0tls1crudjtibdrvjusuzjq0fursbsrvfvrvnuls0tls0ktuljrkleq0nbd2ddqvfbd0lerwvnqndhqtfvruf3d1zkbuyxykhrdwrtrjfisff0y0dwewmyohvjm1pqtuljqwpjakfoqmdrcwhrauc5dzbcqvffrkfbt0nbzzhbtuljq0nns0nbz0vbdfjubkfqr2r4bg1xdjhmow1gc29yoxjuck9jctvgtnjmzmrdelzcvevnuev6tdgzswfst1cya2lrnwfrm282d2nstmx1s3nzeul1c0zustfqr2djwjn0exkksdfqmlrommnhmhp4mgvaytjqk3jmvkkwsmvtdxfhnkdmy01rrzruduhzsgjradzuymgyalc5s0rtutvreknzdwo0rlg4bdzxvevilzdsemgwnct0rkdfamxvvktkakjycnvqmnhbc0nqemj2sy9gaehlrjjwrvpza1psnwtcbc80cm1kl2xhutrutysyvw5cbmsvaljjd3g5a0zgwdhucehgwxxxls0k
  signername: kubernetes.io/kubelet-serving
  usages:
  - digital signature
  - key encipherment
  - server auth

the version of kubectl:
$ kubectl version --short
client version: v1.20.0
server version: v1.18.9-eks-d1db3c

and im working with aws eks
i keep getting :
$ kubectl create -f csr.yaml
error: unable to recognize &quot;csr.yaml&quot;: no matches for kind &quot;certificatesigningrequest&quot; in version &quot;certificates.k8s.io/v1&quot;

update
after changing to apiversion: certificates.k8s.io/v1beta1
apiversion: certificates.k8s.io/v1beta1
kind: certificatesigningrequest
metadata: 
  name: vault-csr
spec: 
  groups: 
    - system: authenticated
  request: ls0tls1crudjtibdrvjusuzjq0fursb.....
  usages:
  - digital signature
  - key encipherment
  - server auth

im getting now this error:
$ kubectl create -f csr.yaml
error: error validating &quot;tmp/csr.yaml&quot;: error validating data: validationerror(certificatesigningrequest.spec.groups[0]): invalid type for io.k8s.api.certificates.v1beta1.certificatesigningrequestspec.groups: got &quot;map&quot;, expected &quot;string&quot;; if you choose to ignore these errors, turn validation off with --validate=false

",<kubernetes><kubectl><certificate-signing-request>,65857418,7,"as per the k8s change doc, the certificatesigningrequest api is promoted to certificates.k8s.io/v1 only as part of the k8s 1.19 release.
it was under certificates.k8s.io/v1beta1 before that.
i suspect that to be a problem as your server version is v1.18.
so, try changing your apiversion as below:
apiversion: certificates.k8s.io/v1beta1
"
65795438,kubernetes ingress - load balancer traffic split,"i have a kubernetes ingress of class nginx and two load balancers. running on gke v1.17.
sample ingress yaml:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: my-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/backend-protocol: &quot;https&quot;
    kubernetes.io/ingress.class: &quot;nginx&quot;
    # enable client certificate authentication
    nginx.ingress.kubernetes.io/auth-tls-verify-client: &quot;on&quot;
    # create the secret containing the trusted ca certificates
    nginx.ingress.kubernetes.io/auth-tls-secret: &quot;production/client-cert-secret&quot;
    # specify the verification depth in the client certificates chain
    nginx.ingress.kubernetes.io/auth-tls-verify-depth: &quot;1&quot;
    # automatically redirect http to https
    nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
    # use regex in paths
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    # allow larger request body
    nginx.ingress.kubernetes.io/proxy-body-size: 30m
    # for notifications we add the proxy headers
    nginx.ingress.kubernetes.io/configuration-snippet: |  
      proxy_set_header upgrade $http_upgrade;
      proxy_set_header connection &quot;upgrade&quot;;
spec:
  tls:
    - hosts:
      - my-domain.com
      secretname: my-tls-certificate
  rules:
  - host: my-domain.com
    http:
      paths:
      - path: /(.*)
        backend:
          servicename: load-balancer-1
          serviceport: 443

i wish to split the traffic reached to the ingress between the two load balancers.
for example:
load-balancer-1 will receive 90% of the traffic
load-balancer-2 will receive 10% of the traffic
how can i do that with kubernetes ingress?
",<nginx><kubernetes><kubernetes-ingress><nginx-ingress>,65798740,7,"the nginx ingress controller supports canary deployments through the canary annotations

in some cases, you may want to &quot;canary&quot; a new set of changes by
sending a small number of requests to a different service than the
production service. the canary annotation enables the ingress spec to
act as an alternative service for requests to route to depending on
the rules applied. the following annotations to configure canary can
be enabled after nginx.ingress.kubernetes.io/canary: &quot;true&quot; is set:

nginx.ingress.kubernetes.io/canary-weight: the integer based (0 - 100)
percent of random requests that should be routed to the service
specified in the canary ingress. a weight of 0 implies that no
requests will be sent to the service in the canary ingress by this
canary rule. a weight of 100 means implies all requests will be sent
to the alternative service specified in the ingress.

note that when you mark an ingress as canary, then all the other
non-canary annotations will be ignored (inherited from the
corresponding main ingress) except
nginx.ingress.kubernetes.io/load-balance and
nginx.ingress.kubernetes.io/upstream-hash-by.
known limitations
currently a maximum of one canary ingress can be applied per ingress
rule.

in other words, you can introduce a new ingress object my-ingress-canary where you set the annotations

nginx.ingress.kubernetes.io/canary: &quot;true&quot; (tells nginx ingress to mark this one as canary and associate this ingress with the main ingress by matching host and path.

nginx.ingress.kubernetes.io/canary-weight: &quot;10&quot; (route ten percent traffic to load-balancer-2)


apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: my-ingress-canary
  annotations:
    nginx.ingress.kubernetes.io/canary: &quot;true&quot;
    nginx.ingress.kubernetes.io/canary-weight: &quot;10&quot;
spec:
  rules:
  - host: my-domain.com
    http:
      paths:
      - path: /(.*)
        backend:
          servicename: load-balancer-2
          serviceport: 443


"
66275458,could not access kubernetes ingress in browser on windows home with minikube?,"i am facing the problem which is that i could not access the kubernetes ingress on the browser using it's ip. i have installed k8s and minikube on windows 10 home.
i am following this official document - https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/

first i created the deployment by running this below command on minikube.
kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0


the deployment get created which can be seen on the below image:


next, i exposed the deployment that i created above. for this i ran the below command.
kubectl expose deployment web --type=nodeport --port=8080


this created a service which can be seen by running the below command:
kubectl get service web

the screenshot of the service is shown below:


i can now able to visit the service on the browser by running the below command:
minikube service web


in the below screenshot you can see i am able to view it on the browser.


next, i created an ingress by running the below command:
kubectl apply -f https://k8s.io/examples/service/networking/example-ingress.yaml


by the way the ingress yaml code is:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    spec:
      rules:
        - host: hello-world.info
          http:
            paths:
              - path: /
                pathtype: prefix
                backend:
                  service:
                    name: web
                    port:
                      number: 8080

the ingress gets created and i can verify it by running the below command:
kubectl get ingress

the screenshot for this is given below:

the ingress ip is listed as 192.168.49.2. so that means if i should open it in the browser then it should open, but unfortunately not. it is showing site can't be reached. see the below screeshot.

what is the problem. please provide me a solution for it?
i also added the mappings on etc\hosts file.
192.168.49.2 hello-world.info

then i also tried opening hello-world.info on the browser but no luck.
in the below picture i have done ping to hello-world.info which is going to ip address 192.168.49.2. this shows etc\hosts mapping is correct:

i also did curl to minikube ip and to hello-world.info and both get timeout. see below image:

the kubectl describe services web provides the following details:
name:                     web
namespace:                default
labels:                   app=web
annotations:              &lt;none&gt;
selector:                 app=web
type:                     nodeport
ip:                       10.100.184.92
port:                     &lt;unset&gt;  8080/tcp
targetport:               8080/tcp
nodeport:                 &lt;unset&gt;  31880/tcp
endpoints:                172.17.0.4:8080
session affinity:         none
external traffic policy:  cluster
events:                   &lt;none&gt;

the kubectl describe ingress example-ingress gives the following output:
name:             example-ingress
namespace:        default
address:          192.168.49.2
default backend:  default-http-backend:80 (&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;)
rules:
  host              path  backends
  ----              ----  --------
  hello-world.info
                    /   web:8080   172.17.0.4:8080)
annotations:        nginx.ingress.kubernetes.io/rewrite-target: /$1
events:             &lt;none&gt;

kindly help. thank you.
",<kubernetes><kubectl><minikube>,66303556,7,"having same issue as op and things only work in minikube ssh, sharing the ingress.yaml below.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: frontend-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  defaultbackend:
    service:
      name: default-http-backend
      port:
        number: 80
  rules:
    - host: myapp-com # domain (i.e. need to change host table)
      http:
        paths: # specified path below, only be working when there is more than 1 path; if only having 1 path, it's always using / as path
          - path: /
            pathtype: prefix
            backend:
              service: 
                name: frontend-service # internal service
                port: 
                  number: 8080 # port number that internal service exposes
          - path: /e($|/)(.*)
            pathtype: prefix
            backend:
              service: 
                name: express-service # internal service
                port: 
                  number: 3000 # port number that internal service exposes


"
66985465,kubernetes how to correctly mount windows path in wsl2 backed environment,"i have a local image that runs fine this way:
docker run -p 8080:8080 -v c:\users\moritz\downloads\1\imageservice\examples1:/images -v c:\users\moritz\entwicklung\projekte\imagecluster\logs:/logs imageservice
now i want this to run as kubernetes (using built in from docker-for-windows v1.19.7) deployment:
apiversion: apps/v1
kind: deployment
metadata:
  name: image-service
spec:
  selector:
    matchlabels:
      app: image-service
  template:
    metadata:
      labels:
        app: image-service
    spec:
      containers:
      - name: image-service
        image: &quot;imageservice&quot;
        resources:
          limits:
            cpu: &quot;0.9&quot;
            memory: &quot;1gi&quot;
        ports:
        - name: http
          containerport: 8080
        volumemounts:
          - mountpath: /images
            name: image-volume
          - mountpath: /logs
            name: log-volume  
      volumes:
        - name: image-volume
          hostpath:
            path: &quot;c:\\users\\moritz\\downloads\\1\\imageservice\\examples1&quot;
            type: directory
        - name: log-volume
          hostpath:
            path: /mnt/c/users/moritz/entwicklung/projekte/imagecluster/logs
            type: directory

as you see i tried different ways to set up my host path on windows machine but i always get:
  warning  failedmount  0s (x4 over 4s)  kubelet            mountvolume.setup failed for volume &quot;log-volume&quot; : hostpath type check failed: /mnt/c/users/moritz/entwicklung/projekte/imagecluster/logs is not a directory
  warning  failedmount  0s (x4 over 4s)  kubelet            mountvolume.setup failed for volume &quot;image-volume&quot; : hostpath type check failed: c:\users\moritz\downloads\1\imageservice\examples1 is not a directory 

i also tried other variants (for both):

c:\users\moritz\entwicklung\projekte\imagecluster\logs
c:/users/moritz/entwicklung/projekte/imagecluster/logs

so how to correctly setup these windows host path. (the next step would be to set them as environment variable.)
little update:
removing type: directory helps to get rid of the error and pod is starting but the mounts are not working. if i &quot;look&quot; into container in /images i don't see the images i have on my host and i don't see any logs in log mount while in container /logs contains the expected files.
in meantime i also tried (no avail)

/host_mnt/c/...
/c/users/...
//c/users/...

",<windows><kubernetes><kubernetes-pod>,67053315,7,"as mentioned here, you can use below hostpath to make it work on wsl2.
// c:\somedir\volumedir
hostpath:
  path: /run/desktop/mnt/host/c/somedir/volumedir
  type: directoryorcreate

there is also an example you can use.
apiversion: v1
kind: pod
metadata:
  name: test-localpc
spec:
  containers:
  - name: test-webserver
    image: ubuntu:latest
    command: [&quot;/bin/sh&quot;]
    args: [&quot;-c&quot;, &quot;apt-get update &amp;&amp; apt-get install curl -y &amp;&amp; sleep 600&quot;]
    volumemounts:
    - mountpath: /run/desktop/mnt/host/c/aaa
      name: mydir
    - mountpath: /run/desktop/mnt/host/c/aaa/1.txt
      name: myfile
  volumes:
  - name: mydir
    hostpath:
      # ensure the file directory is created.
      path: /run/desktop/mnt/host/c/aaa
      type: directoryorcreate
  - name: myfile
    hostpath:
      path: /run/desktop/mnt/host/c/aaa/1.txt
      type: fileorcreate

"
67210000,rbac (role based access control) on k3s,"after watching a view videos on rbac (role based access control) on kubernetes (of which this one was the most transparent for me), i've followed the steps, however on k3s, not k8s as all the sources imply. from what i could gather (not working), the problem isn't with the actual role binding process, but rather the x509 user cert which isn't acknowledged from the api service

$ kubectl get pods --kubeconfig userkubeconfig
error: you must be logged in to the server (unauthorized)

also not documented on rancher's wiki on security for k3s (while documented for their k8s implementation)?, while described for rancher 2.x itself, not sure if it's a problem with my implementation, or a k3s &lt;-&gt; k8s thing.
$ kubectl version --short
client version: v1.20.5+k3s1
server version: v1.20.5+k3s1



with duplication of the process, my steps are as follows:

get k3s ca certs

this was described to be under /etc/kubernetes/pki (k8s), however based on this seems to be at /var/lib/rancher/k3s/server/tls/ (server-ca.crt &amp; server-ca.key).

gen user certs from ca certs

#generate user key
$ openssl genrsa -out user.key 2048

#generate signing request from ca
openssl req -new -key user.key -out user.csr -subj &quot;/cn=user/o=rbac&quot;

# generate user.crt from this
openssl x509 -req -in user.csr -ca server-ca.crt -cakey server-ca.key -cacreateserial -out user.crt -days 365

... all good:


creating kubeconfig file for user, based on the certs:

# take user.crt and base64 encode to get encoded crt
cat user.crt | base64 -w0

# take user.key and base64 encode to get encoded key
cat user.key | base64 -w0


created config file:

apiversion: v1
clusters:
- cluster:
    certificate-authority-data: &lt;server-ca.crt base64-encoded&gt;
    server: https://&lt;k3s masterip&gt;:6443
  name: home-pi4
contexts:
- context:
    cluster: home-pi4
    user: user
    namespace: rbac
  name: user-homepi4
current-context: user-homepi4
kind: config
preferences: {}
users:
- name: user
  user:
    client-certificate-data: &lt;user.crt base64-encoded&gt;
    client-key-data: &lt;user.key base64-encoded&gt;


setup role &amp; rolebinding (within specified namespace 'rbac')


role

apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  name: user-rbac
  namespace: rbac
rules:
- apigroups:
  - &quot;*&quot;
  resources:
  - pods
  verbs:
  - get
  - list


rolebinding

apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: user-rb
  namespace: rbac
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: user-rbac
subjects:
  apigroup: rbac.authorization.k8s.io
  kind: user
  name: user 


after all of this, i get fun times of...
$ kubectl get pods --kubeconfig userkubeconfig
error: you must be logged in to the server (unauthorized)

any suggestions please?

apparently this stackoverflow question presented a solution to the problem, but following the github feed, it came more-or-less down to the same approach followed here (unless i'm missing something)?

",<kubernetes><rancher><k3s><kubernetes-rbac>,67336849,7,"as we can find in the kubernetes certificate signing requests documentation:

a few steps are required in order to get a normal user to be able to authenticate and invoke an api.

i will create an example to illustrate how you can get a normal user who is able to authenticate and invoke an api (i will use the user john as an example).

first, create pki private key and csr:
# openssl genrsa -out john.key 2048

note: cn is the name of the user and o is the group that this user will belong to
# openssl req -new -key john.key -out john.csr -subj &quot;/cn=john/o=group1&quot;

# ls
john.csr  john.key

then create a certificatesigningrequest and submit it to a kubernetes cluster via kubectl.
# cat &lt;&lt;eof | kubectl apply -f -
&gt; apiversion: certificates.k8s.io/v1
&gt; kind: certificatesigningrequest
&gt; metadata:
&gt;   name: john
&gt; spec:
&gt;   groups:
&gt;   - system:authenticated
&gt;   request: $(cat john.csr | base64 | tr -d '\n')
&gt;   signername: kubernetes.io/kube-apiserver-client
&gt;   usages:
&gt;   - client auth
&gt; eof
certificatesigningrequest.certificates.k8s.io/john created


# kubectl get csr
name   age   signername                            requestor      condition
john   39s   kubernetes.io/kube-apiserver-client   system:admin   pending

# kubectl certificate approve john
certificatesigningrequest.certificates.k8s.io/john approved

# kubectl get csr
name   age   signername                            requestor      condition
john   52s   kubernetes.io/kube-apiserver-client   system:admin   approved,issued

export the issued certificate from the certificatesigningrequest:
# kubectl get csr john -o jsonpath='{.status.certificate}'  | base64 -d &gt; john.crt

# ls
john.crt  john.csr  john.key

with the certificate created, we can define the role and rolebinding for this user to access kubernetes cluster resources. i will use the role and rolebinding similar to yours.
# cat role.yml 
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  name: john-role
rules:
- apigroups:
  - &quot;&quot;
  resources:
  - pods
  verbs:
  - get
  - list
  
# kubectl apply -f role.yml 
role.rbac.authorization.k8s.io/john-role created

# cat rolebinding.yml 
apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: john-binding
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: john-role
subjects:
- apigroup: rbac.authorization.k8s.io
  kind: user
  name: john
  
# kubectl apply -f rolebinding.yml 
rolebinding.rbac.authorization.k8s.io/john-binding created

the last step is to add this user into the kubeconfig file (see: add to kubeconfig)
# kubectl config set-credentials john --client-key=john.key --client-certificate=john.crt --embed-certs=true
user &quot;john&quot; set.

# kubectl config set-context john --cluster=default --user=john
context &quot;john&quot; created.

finally, we can change the context to john and check if it works as expected.
# kubectl config use-context john
switched to context &quot;john&quot;.

# kubectl config current-context
john

# kubectl get pods
name   ready   status    restarts   age
web    1/1     running   0          30m

# kubectl run web-2 --image=nginx
error from server (forbidden): pods is forbidden: user &quot;john&quot; cannot create resource &quot;pods&quot; in api group &quot;&quot; in the namespace &quot;default&quot;

as you can see, it works as expected (user john only has get and list permissions).
"
40266518,deployment not detecting change of container image tag in init-container,"i have been using init-containers since they became available and find them super useful. my core image (below as web-dev) does not change much, but my init-container image (below as web-data-dev) does change often.

the init-container uses a container image with a version number. i change this version number to the latest value, and then do kubectl apply -f deployment.yaml

for instance, i change eu.gcr.io/project/web-data-dev:187 to eu.gcr.io/project/web-data-dev:188 before running kubectl apply.

when i do this however, no deployment happens, if i make any changes to the image the init-container uses, the deployment will still not happen. i assume this is because the init-container changes are not being detected.

i then tried to just put some garbage in the image field, like this: ""image"": ""thisisnotanimage"" and run kubectl apply -f again, but the update is still not applied.

my question is - how do i make kubectl apply -f detect an image tag change in an init-container? am i doing something wrong, is this a bug, or is this simply not implemented yet because init-containers are alpha?

the full deployment yaml is below.

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: web-deployment
spec:
  replicas: 1
  strategy:
    rollingupdate:
      maxunavailable: 0
  template:
    metadata:
      labels:
        app: web
        tier: frontend
      annotations:
        pod.alpha.kubernetes.io/init-containers: '[
            {
                ""name"": ""initialiser1"",
                ""image"": ""eu.gcr.io/project/web-data-dev:187"",
                ""command"": [""cp"", ""-r"", ""/data-in/"", ""/opt/""],
                ""volumemounts"": [
                    {
                        ""name"": ""file-share"",
                        ""mountpath"": ""/opt/""
                    }
                ]
            }
        ]'
    spec:
      containers:

        - image: eu.gcr.io/project/web-dev:20
          name: web
          resources:
            requests:
              cpu: 10m
              memory: 40mi
          ports:
            - containerport: 80
              name: http
            - containerport: 443
              name: https
          volumemounts:
            - name: file-share
              mountpath: /opt/

      volumes:
        - name: file-share
          emptydir: {}

",<kubernetes><google-cloud-platform><google-kubernetes-engine>,40281436,6,"if you are using kubernetes 1.4, try to change pod.alpha.kubernetes.io/init-containers to pod.beta.kubernetes.io/init-containers.

i can't find a proper issue on github, but behaviour of these two annotations is different. i can do kubectl apply -f with the second one and the deployment will be updated.

you can test it using the example below:

kind: deployment
apiversion: extensions/v1beta1
metadata:
  name: nginx
spec:
  template:
    metadata:
      labels:
        app: nginx
      annotations:
        pod.beta.kubernetes.io/init-containers: '[
            {
                ""name"": ""install"",
                ""image"": ""busybox"",
                ""command"": [""/bin/sh"", ""-c"", ""echo foo &gt; /work-dir/index.html""],
                ""volumemounts"": [
                  {
                    ""name"": ""workdir"",
                    ""mountpath"": ""/work-dir""
                    }
                ]
            }
        ]'
    spec:
      volumes:
        - name: workdir
          emptydir: {}
      containers:
        - name: nginx
          image: nginx
          ports:
            - containerport: 80
          volumemounts:
            - name: workdir
              mountpath: /usr/share/nginx/html


try to change foo to bar and see the result:

$ cat nginx.yaml | kubectl apply -f -
deployment ""nginx"" created
$ curl $(minikube service nginx --url)
waiting, endpoint for service is not ready yet...
foo
$ cat nginx.yaml | sed -e 's/foo/bar/g' | kubectl apply -f -
deployment ""nginx"" configured
$ curl $(minikube service nginx --url)
waiting, endpoint for service is not ready yet...
bar


the same thing using pod.alpha.kubernetes.io/init-containers:

$ curl $(minikube service nginx --url)
waiting, endpoint for service is not ready yet...
foo
$ cat nginx.yaml | sed -e 's/foo/bar/g' | kubectl apply -f -
deployment ""nginx"" configured
$ curl $(minikube service nginx --url)
foo

"
58604021,expose a kubernetes deployment (frontend) using ingress controller,"i am deploying a number of docker containers of micro-services and angular frontend on kubernetes. i have exposed the services using an ingress controller specifying each service using this, and specifying paths in backend.

apiversion: extensions/v1beta1
kind: ingress


for my frontend, i have created a service with type loadbalancer.

apiversion: v1
kind: service
metadata:
  annotations:
    service.beta.kubernetes.io/azure-load-balancer-resource-group: my-resource-group
  name: myapp-svc
  namespace: ui
spec:
  loadbalancerip: some_ip_address
  type: loadbalancer
  ports:
  - port: 80
 selector:
   app: myapp


this works fine but now i have two ip addresses, one for the ui loadbalancer, and other of the ingress controller (for apis). 

can i do this with just one ip address?

how can i expose the ui using ingress controller itself without creating external loadbalancer?
",<kubernetes><load-balancing><kubernetes-ingress><azure-aks>,58604282,6,"try this way -

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/service-upstream: ""true""
  name: rule-name
  namespace: default
spec:
  rules:
  - host: hostname
    http:
      paths:
      - backend:
          servicename: frontend-service
          serviceport: port-number
        path: /(.*)
      - backend:
          servicename: backend-service
          serviceport: port-number
        path: /api/(.*)




you can use the above defined strategy where you can directly map front end at / and use rewrite-target to map anything like hostname/api to backend service.

you can keep frontend and backend services at clusterip level only
"
72020957,how can i provide multipe secrets in one yaml file?,"how can i define multiple secrets in one file?
seems that providing multiple secrets doesn't work.
apiversion: v1
kind: secret
metadata:
  name: ca-secret
  labels:
    app.kubernetes.io/managed-by: helm
type: kubernetes.io/tls
data:
  tls.crt: ls0tldr
  tls.key: ls0tldr
apiversion: v1
kind: secret
metadata:
  name: envoy-secret
  labels:
    app.kubernetes.io/managed-by: helm
type: kubernetes.io/tls
data:
  tls.crt: ls0tls1
  tls.key: ls0tls1

i am not able to use multiple files because i need to generate a single template using helm.
",<kubernetes><kubernetes-helm>,72021094,6,"you can have separate manifests in one yaml file by separating them with ---. this will work:
apiversion: v1
kind: secret
metadata:
  name: ca-secret
  labels:
    app.kubernetes.io/managed-by: helm
type: kubernetes.io/tls
data:
  tls.crt: ls0tldr
  tls.key: ls0tldr
---
apiversion: v1
kind: secret
metadata:
  name: envoy-secret
  labels:
    app.kubernetes.io/managed-by: helm
type: kubernetes.io/tls
data:
  tls.crt: ls0tls1
  tls.key: ls0tls1

"
58426493,"deployment in version ""v1"" cannot be handled as a deployment:","helm install failing with the below error 

command

helm install --name helloworld helm


below is the error once i ran above command 

error: release usagemetrics failed: deployment in version ""v1"" cannot be handled as a deployment: v1.deployment.spec: v1.deploymentspec.template: v1.podtemplatespec.spec: v1.podspec.containers: []v1.container: v1.container.livenessprobe: readobjectstart: expect { or n, but found 9, error found in #10 byte of ...|ssprobe"":9001,""name""|..., bigger context ...|""imagepullpolicy"":""ifnotpresent"",""livenessprobe"":9001,""name"":""usagemetrics-helm"",""ports"":[{""containe|...


below is the deployment.yaml file i feel the issue in liveness and probeness configuration . 

apiversion: apps/v1
kind: deployment
metadata:
  name: release-name-helm
spec:
  replicas: 1
  selector:
    matchlabels:
      app: release-name-helm
      release: release-name
  template:
    metadata:
      labels:
        app: release-name-helm
        release: release-name
    spec:
      containers:
        - name: release-name-helm
          imagepullpolicy: ifnotpresent
          image: hellworld
          ports:
            - name: ""http""
              containerport: 9001
          envfrom:
            - configmapref:
                name: release-name-helm
            - secretref:
                name: release-name-helm
          livenessprobe:
            9001
          readinessprobe:
            9001

",<kubernetes><kubernetes-helm><kubernetes-deployment>,58427227,6,"the problem seems to be related to the livenessprobe and readynessprobe that are both wrong.
an example of livenessprobe of http from the documentation here is:
livenessprobe

  httpget:
    path: /healthz
    port: 8080
    httpheaders:
      - name: custom-header
        value: awesome
    initialdelayseconds: 3
    periodseconds: 3

your yamls if you only want to have a check of the port should be like:
apiversion: apps/v1
kind: deployment
metadata:
  name: release-name-helm
spec:
  replicas: 1
  selector:
    matchlabels:
      app: release-name-helm
      release: release-name
  template:
    metadata:
      labels:
        app: release-name-helm
        release: release-name
    spec:
      containers:
        - name: release-name-helm
          imagepullpolicy: ifnotpresent
          image: hellworld
          ports:
            - name: &quot;http&quot;
              containerport: 9001
          envfrom:
            - configmapref:
                name: release-name-helm
            - secretref:
                name: release-name-helm
          livenessprobe:
            tcpsocket:
              port: 9001
            initialdelayseconds: 5
            periodseconds: 10
          readinessprobe:
            tcpsocket:
              port: 9001
            initialdelayseconds: 5
            periodseconds: 10

"
69277420,"eks ingress with single alb, multiple namespaces, and external dns","i'm trying to configure a single alb across multiple namespaces in aws eks, each namespace has its own  ingress resource.
i'm trying to configure the ingress controller aws-loadbalancer-controller on a k8s v1.20.
the problem i'm facing is that each time i try to deploy a new service it always spin-up a new classic loadbalancer in addition to the shared alb specified in the ingress config.
https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/

# service-realm1-dev.yaml:
apiversion: v1
kind: service
metadata:
  name: sentinel
  annotations:
    external-dns.alpha.kubernetes.io/hostname: realm1.dev.sentinel.mysite.io
  namespace: realm1-dev
  labels:
    run: sentinel
spec:
  ports:
    - port: 5001
      name: ps1
      protocol: tcp
  selector:
    app: sentinel
  type: loadbalancer

# ingress realm1-app
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/group.name: sentinel-ingress
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/healthcheck-protocol: http
    alb.ingress.kubernetes.io/healthcheck-port: traffic-port
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: &quot;15&quot;
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: &quot;5&quot;
    alb.ingress.kubernetes.io/success-codes: 200-300
    alb.ingress.kubernetes.io/healthy-threshold-count: &quot;2&quot;
    alb.ingress.kubernetes.io/unhealthy-threshold-count: &quot;2&quot;
    alb.ingress.kubernetes.io/listen-ports: '[{&quot;http&quot;:80}]'
  name: sentinel-ingress-controller
  namespace: realm1-dev
spec:
  rules:
    - host: realm1.dev.sentinel.mysite.io
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              serviceport: use-annotation
              servicename: sentinel


also i'm using external dns to create a route53 reecodset, and then i use the same configured dns to route requests to the specific eks service, is there any issue with this approach ?
",<kubernetes><kubernetes-ingress><amazon-eks>,69296982,6,"i was able to make it work using only one alb,
@yyashwanth, using nginx was my fallback plan, i'm trying to make the configuration as simple as possible, maybe in the future when we will try to deploy our solution in other cloud providers we will use nginx ingress controller.
1- to start the service type should be node port, use loadbalancer will create a classic lb.
apiversion: v1
kind: service
metadata:
  name: sentinel-srv
  annotations:
    external-dns.alpha.kubernetes.io/hostname: operatorv2.dev.sentinel.mysite.io
  namespace: operatorv2-dev
  labels:
    run: jsflow-sentinel
spec:
  ports:
    - port: 80
      targetport: 80
      name: ps1
      protocol: tcp
  selector:
    app: sentinel-app
  type: nodeport

2- second we need to configure group.name, for the ingress controller to merge all ingress configurations using the annotation alb.ingress.kubernetes.io/group.name
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  annotations:
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: &quot;15&quot;
    alb.ingress.kubernetes.io/healthcheck-path: /
    alb.ingress.kubernetes.io/healthcheck-port: traffic-port
    alb.ingress.kubernetes.io/healthcheck-protocol: http
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: &quot;5&quot;
    alb.ingress.kubernetes.io/healthy-threshold-count: &quot;2&quot;
    alb.ingress.kubernetes.io/listen-ports: '[{&quot;http&quot;: 80} ]'
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/success-codes: &quot;200&quot;
    alb.ingress.kubernetes.io/tags: createdby=aws-controller
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/unhealthy-threshold-count: &quot;2&quot;
    external-dns.alpha.kubernetes.io/hostname: operatorv2.sentinel.mysite.io
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/group.name: sentinel-group
  name: dev-operatorv2-sentinel-ingress-controller
  namespace: operatorv2-dev
spec:
  rules:
    - host: operatorv2.dev.sentinel.mysite.io
      http:
        paths:
          - path: /*
            backend:
              serviceport: 80
              servicename: sentinel-srv


"
55830850,how to import custom dashboards to grafana using helm,"i'm trying to understand helm and i wonder if someone could eli5 to me something or help me with something.

so i did run below:

helm repo add coreos https://s3-eu-west-1.amazonaws.com/coreos-charts/stable/


then i installed kube-prometheus by using below: 

helm install coreos/kube-prometheus --name kube-prometheus -f values.yaml --namespace monitoringtest


everything works fine but i'm trying to add some custom dashboards from json files and i'm struggling to understand how to do it.

i was following this: https://blogcodevalue.wordpress.com/2018/09/16/automate-grafana-dashboard-import-process/

in my values.yaml i added below

serverdashboardconfigmaps:
  - example-dashboards


i understand that if i do:

helm upgrade --install kube-prometheus -f values.yaml --namespace monitoringtest coreos/kube-prometheus


that should cause grafana to pickup a below configmap called example-dashboards and load *.json files from custom-dashboards folder. 

apiversion: v1
kind: configmap
metadata:
  name: example-dashboards
data:
{{ (.files.glob ""custom-dashboards/*.json"").asconfig | indent 2 }}

# or
# 
# data:
#   custom-dashboard.json: |-
# {{ (.files.get ""custom.json"") | indent 4 }}
#
# the filename (and consequently the key under data) must be in the format `xxx-dashboard.json` or `xxx-datasource.json`
# for them to be picked up.


now two questions:

how do i add above configmap to this helm release?

where is this custom-dashboards folder located? is it on my laptop and then is send to grafana?

do i need to copy all of https://s3-eu-west-1.amazonaws.com/coreos-charts/stable/ onto my laptop?

sorry for explaining everything but i'm just trying to understand this.
",<kubernetes><grafana><kubernetes-helm><configmap>,70095865,6,"in the latest version of kube-prometheus-stack chart in 2021, according to this answer on github, you should just create a configmap with dashboard data and right labels and it will be checked by sidecar in grafana pod.
example:
apiversion: v1
kind: configmap
metadata:
  name: grafana-dashboards-custom-1
  namespace: monitoring
  labels:
     grafana_dashboard: &quot;1&quot;
     prometheus: my-value
     release: prometheus

data:
  app-status.json: |-
    {
    &quot;annotations&quot;: {
        &quot;list&quot;: [
        {

prometheus: my-value comes from this helm chart value:
prometheus:
  prometheusspec:
    servicemonitorselector:
      matchlabels:
        prometheus: my-value

"
68090788,how do you send fargate eks fluent bit logs from different services to separate cloudwatch groups?,"i have followed this guide to configure fluent bit and cloudwatch on my eks cluster, but currently all of the logs go to one log group. i tried to follow a separate tutorial that used a kubernetes plugin for fluent bit to tag the services before the reached the [output] configuration. this caused issues because fargate eks currently does not handle fluent bit [input] configurations as per the bottom of this doc.
has anyone encountered this before? i'd like to split the logs up into separate services.
here is my current yaml file .. i added the parser and filter to see if i could gain any additional information to work with over on cloudwatch.
kind: namespace
apiversion: v1
metadata:
  name: aws-observability
  labels:
    aws-observability: enabled
---
kind: configmap
apiversion: v1
metadata:
  name: aws-logging
  namespace: aws-observability
data:
  parsers.conf: |
    [parser]
        name docker
        format json
        time_key time
        time_format %y-%m-%dt%h:%m:%s.%l
        time_keep on
        
  filters.conf: |
    [filter]
        name kubernetes
        match kube.*
        kube_ca_file /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        kube_token_file /var/run/secrets/kubernetes.io/serviceaccount/token
        # kube_tag_prefix kube.var.log.containers.
        kube_url https://kubernetes.default.svc:443
        merge_log on
        merge_log_key log_processed
        use_kubelet true
        buffer_size 0
        dummy_meta true
  
  output.conf: |
    [output]
        name cloudwatch_logs
        match   *
        region us-east-1
        log_group_name fluent-bit-cloudwatch2
        log_stream_prefix from-fluent-bit-
        auto_create_group on

",<kubernetes><amazon-cloudwatch><amazon-eks><aws-fargate><fluent-bit>,68117699,6,"so i found out that it is actually simple to do this.
the default tag of input on fluent bit contains the name of the service you are logging from, so you can actually stack multiple [output] blocks each using the wildcard operator around the name of your service . that was all i had to do to get the streams to get sent to different log groups. here is my yaml for reference.
kind: namespace
apiversion: v1
metadata:
  name: aws-observability
  labels:
    aws-observability: enabled
---
kind: configmap
apiversion: v1
metadata:
  name: aws-logging
  namespace: aws-observability
data:
  output.conf: |
    [output]
        name cloudwatch_logs
        match   *logger*
        region us-east-1
        log_group_name logger-fluent-bit-cloudwatch
        log_stream_prefix from-fluent-bit-
        auto_create_group on
        
    [output]
        name cloudwatch_logs
        match   *alb*
        region us-east-1
        log_group_name alb-fluent-bit-cloudwatch
        log_stream_prefix from-fluent-bit-
        auto_create_group on

"
67982867,"helm: ""template"" keyword","can someone explain to me what the role of the keyword &quot;template&quot; is in this code :
apiversion: v1
kind: secret
metadata:
  name: {{ template &quot;identity-openidconnect&quot; . }}
  namespace: {{ .release.namespace }}
  labels:
    app: {{ template &quot;microservice.name&quot; . }}
    release: &quot;{{ .release.name }}&quot;
    xxxx
xxxxxxxxxxxx

",<kubernetes><kubernetes-helm>,67984458,6,"the keyword &quot;template&quot; means, that helm will find the previously created template and complete the yaml file according to the template in the template. it has to be created in advance. this type of construction allows you to refer to the same scheme many times.
for example, we can define a template to encapsulate a kubernetes block of labels:
{{- define &quot;mychart.labels&quot; }}
  labels:
    generator: helm
    date: {{ now | htmldate }}
{{- end }}

now we can embed this template inside of our existing configmap, and then include it with the  template  action:
{{- define &quot;mychart.labels&quot; }}
  labels:
    generator: helm
    date: {{ now | htmldate }}
{{- end }}
apiversion: v1
kind: configmap
metadata:
  name: {{ .release.name }}-configmap
  {{- template &quot;mychart.labels&quot; }}
data:
  myvalue: &quot;hello world&quot;
  {{- range $key, $val := .values.favorite }}
  {{ $key }}: {{ $val | quote }}
  {{- end }}

when the template engine reads this file, it will store away the reference to  mychart.labels  until  template &quot;mychart.labels&quot;  is called. then it will render that template inline. so the result will look like this:
# source: mychart/templates/configmap.yaml
apiversion: v1
kind: configmap
metadata:
  name: running-panda-configmap
  labels:
    generator: helm
    date: 2016-11-02
data:
  myvalue: &quot;hello world&quot;
  drink: &quot;coffee&quot;
  food: &quot;pizza&quot;

note: a  define  does not produce output unless it is called with a template, as in this example.
for more info about templates you can read this page.
"
76107126,ingress endpoint on k8s v1.26.3,"getting below error while trying to create an ingress endpoint on k8s v1.26.3. i have created a k8s cluster using kops.
error from server

(badrequest): error when creating &quot;ingress.yaml&quot;: ingress in version
&quot;v1&quot; cannot be handled as a ingress: strict decoding error: unknown
field &quot;spec.rules[0].http.paths[0].backend.servicename&quot;, unknown field
&quot;spec.rules[0].http.paths[0].backend.serviceport&quot;

apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  labels:
    env: prod
    app: demo-ms
    layer: frontend
  name: prod-demo-ingress
  namespace: prod
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: demo.abc.in
      http:
        paths:
          - backend:
              servicename: prod-demo-service
              serviceport: 8080
            path: /
~~~

",<kubernetes><kubernetes-ingress>,76107276,6,"apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: prod-demo-ingress
  namespace: prod
  labels:
    env: prod
    app: demo-ms
    layer: frontend
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: demo.abc.in
      http:
        paths:
          - pathtype: prefix
            path: &quot;/&quot;
            backend:
              service:
                name: prod-demo-service
                port:
                  number: 8080

"
59700134,serving a static react application behind a kubernetes ingress,"i'm currently trying to setup a react spa as a deployment/service on kubernetes. like the backend services i have running currently, i want to be able to hit it behind an ingress. 

because the spa is just a bunch of static files, i'm exposing the files through nginx. the container that runs in the deployment has nginx installed to serve the static assets (nginx -g  daemon off in the docker file). this works completely fine if i expose the deployment with a loadbalancer, but if i use an ingress, i get no response. are there any special things to consider when serving static assets behind an ingress? or any known references/resources for doing this?

here's my ingress.yml:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: web-ingress
  annotations:
    kubernetes.io/ingress.global-static-ip-name: web-static-ip
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
  - host: api.my-domain.com
    http:
      paths:
        - backend:
            servicename: web-backend-service
            serviceport: 80
  - host: app.my-domain.com
    http:
      paths:
        - backend:
            servicename: web-frontend-service
            serviceport: 80

",<reactjs><nginx><kubernetes><create-react-app><kubernetes-ingress>,59700672,6,"you need to have an ingress controller deployed on your cluster for an ingress resource to actually take effect. here is the installation guide of nginx ingress controller.an example ingress to serve static content would look like below.

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: web-ingress
  annotations:
    kubernetes.io/ingress.class: ""nginx""
    nginx.ingress.kubernetes.io/add-base-url: ""true""
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: api.my-domain.com
    http:
      paths:
        - backend:
            servicename: web-backend-service
            serviceport: 80
  - host: app.my-domain.com
    http:
      paths:
        - backend:
            servicename: web-frontend-service
            serviceport: 80


here is a guide on how to serve angular 8 application on minikube using nginx ingress. 
"
48627768,redirect in traefik from one domain to another,"according to the traefik 1.7 documentation you should be able to have traefik perform a 302 redirect using:

traefik.ingress.kubernetes.io/redirect-regex
traefik.ingress.kubernetes.io/redirect-replacement

my goal is to simply remove the www. from the address.
this is what i've tried, but i get a 404 service not found.
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: www-redirect
  namespace: public
  annotations:
    kubernetes.io/ingress.class: traefik
    traefik.ingress.kubernetes.io/redirect-regex: ^https?://www.example.com/(.*)
    traefik.ingress.kubernetes.io/redirect-replacement: https://example.com/$1
spec:
  rules:
  - host: www.example.com

unfortunately the documentation isn't explicit on how to use them. at the time of writing the only google hit on this is the documentation (above).
my current work around (assuming it'll help explain the question) is to route www. traffic to nginx which returns a 302.
server {
    listen       80;
    server_name  www.example.com;
    return 302 https://example.com$request_uri;
}

this seems like overkill.

",<kubernetes><traefik><kubernetes-ingress>,52539891,6,"i was having the same issue and ended up making it work with:

---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: www-redirect
  namespace: public
  annotations:
    kubernetes.io/ingress.class: traefik
    traefik.ingress.kubernetes.io/preserve-host: ""true""
    traefik.ingress.kubernetes.io/redirect-permanent: ""true""
    traefik.ingress.kubernetes.io/redirect-regex: ""^https://www.(.*)""
    traefik.ingress.kubernetes.io/redirect-replacement: ""https://$1""
spec:
  tls:
    - hosts:
        - ""example.com""
        - ""www.example.com""
      secretname: example-tls
  rules:
  - host: example.com
  - host: www.example.com


basically i needed both rules.

as a side note, i also start the trafik pod with the following flags:

args:
   - --api
   - --kubernetes
   - --loglevel=info
   - --entrypoints=name:https address::443 tls
   - --entrypoints=name:http address::80 redirect.entrypoint:https
   - --defaultentrypoints=https,http

"
63410690,kubernetes make changes to annotation to force update deployment,"hey i have a wider problem as when i update secrets in kubernetes they are not implemented in pods unless they are ugprades/reschedules or just re-deployed; i saw the other stackoverflow post about it but noone of the solutions fit me update kubernetes secrets doesn&#39;t update running container env vars
also so the in-app solution of python script on pod to update its secret automatically https://medium.com/analytics-vidhya/updating-secrets-from-a-kubernetes-pod-f3c7df51770d but it seems like a long shot and i came up with solution to adding annotation to deployment manifest - and hoping it would re-schedule pods everytime a helm chart would put a new timestamp in it - it does put it but it doesn't reschedule - any thought how to force that behaviour ?
apiversion: apps/v1
kind: deployment
metadata:
  name: xxx
  namespace: xxx
  labels: xxx
  annotations:
    lastupdate: {{ now }}

also i dont feel like adding this patch command to ci/cd deployment, as its arbitraty and - well doesnt feel like right solution
kubectl patch deployment mydeployment -p '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;containers&quot;:[{&quot;name&quot;:&quot;mycontainer&quot;,&quot;env&quot;:[{&quot;name&quot;:&quot;restart_&quot;,&quot;value&quot;:&quot;'$(date +%s)'&quot;}]}]}}}}'

didn't anyone else find better solution to re-deploy pods on changed secrets ?
",<jenkins><kubernetes><kubernetes-helm>,63410927,6,"kubernetes by itself does not do rolling update of a deployment automatically when a secret is changed. so there needs to a controller which will do that for you automatically. take a look at reloader which is a controller that watches if some change happens in configmap and/or secret; then perform a rolling upgrade on relevant deploymentconfig, deployment, daemonset and statefulset.
add reloader.stakater.com/auto annotation to the deployment with name xxx and have a configmap called xxx-configmap or secret called xxx-secret.
this will discover deployments/daemonsets/statefulset automatically where xxx-configmap or xxx-secret is being used either via environment variable or from volume mount. and it will perform rolling upgrade on related pods when xxx-configmap or xxx-secret are updated
apiversion: apps/v1
kind: deployment
metadata:
  name: xxx
  namespace: xxx
  labels: xxx
  annotations:
    reloader.stakater.com/auto: &quot;true&quot;

"
48808887,"error from server (badrequest): error when creating ""pod.yaml"":","i am getting the following error when i run
kubectl create -f pod.yaml

error
error from server (badrequest): error when creating &quot;pod.yaml&quot;: pod in 
version &quot;applicant:v1&quot; cannot be handled as a pod: no kind &quot;pod&quot; is 
registered for version &quot;applicant:v1&quot;

minikube is up and running and i even tried to change it to kind: deployment but i got another error saying:
error: unable to recognize &quot;pod.yaml&quot;: no matches for /, kind=deployment

yaml:
apiversion: apps/v1
kind: deployment
metadata:
  name: customer-applicant
  labels:
    app: applicant-vue
spec:
  replicas: 1
  selector:
    matchlabels:
      app: applicant-vue
  template:
    metadata:
      labels:
        app: applicant-vue 
    spec:
      containers: 
      - name: api-applicant
        image: api-applicant
        ports:
          - containerport: 8080
          - containerport: 8000
        resources: {}
        volumemounts:
          - mountpath: /usr/local/tomcat/logs
            name: api-applicant-claim 

# import       
      - name: applicant-import
        image: applicant-import
        resources: {}

# cache
      - name: cache
        image: cache
        resources:
          limits:
            memory: &quot;536870912&quot;

# storage
      - name: storage
        image: storage
        ports:
         - containerport: 7000
         - containerport: 7001
         - containerport: 7199
         - containerport: 9042
         - containerport: 9160
        resources: {}
# view
      - name: view
        image: view
        ports:
         - containerport: 3000
        resources: {}

      volumes:
        - name: applicant-claim
          persistentvolumeclaim:
            claimname: applicant-claim
 # tomcat
      - name: tomcat
        image: tomcat
# node
      - name: node
        image: node
        resources: {}
# openjdk
      - name: node
      - image: node
        resources: {}

",<kubernetes><kubectl>,48809309,6,"you have a lot of issues here. i've described some of them:

1. 
pod.yaml file's structure is a structure of a deployment object.

2.
apiversion for deployment depends of kubernetes version: 


apps/v1beta1 for versions before 1.8.0
apps/v1beta2 for versions starting from 1.8.0 before 1.9.0
apps/v1 for versions starting from 1.9.0


so, if you deploy your pod.yaml on the latest kubernetes cluster it should be started from:

apiversion: apps/v1
kind: deployment


3.
the part:

spec:
  replicas: 1
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 100mi
  template:
    metadata:
      labels: 
        app: product-ratings-vue


should be changed to:

spec:
  replicas: 1
  template:
    metadata:
      labels: 
        app: product-ratings-vue


4.
second spec block should be moved on the same level as spec.template.metadata:

    spec:
      replicas: 1
      template:
        metadata:
          labels: 
            app: product-ratings-vue
        spec:
          containers: 


the final deployment.yaml is:

apiversion: apps/v1
kind: deployment
metadata:
  name: customer-ratings
  labels:
    app: product-ratings-vue
spec:
  replicas: 1
  selector:
    matchlabels:
      app: product-ratings-vue
  template:
    metadata:
      labels:
        app: product-ratings-vue 
    spec:
      containers: 
      - name: api-service
        image: api-service
        ports:
          - containerport: 8080
          - containerport: 8000
        resources: {}
        volumemounts:
          - mountpath: /usr/local/tomcat/logs
            name: api-service-claim 

# ekomi-import       
      - name: ekomi-import
        image: ekomi-import
        resources: {}

# cache
      - name: cache
        image: cache
        resources:
          limits:
            memory: ""536870912""

# storage
      - name: storage
        image: storage
        ports:
         - containerport: 7000
         - containerport: 7001
         - containerport: 7199
         - containerport: 9042
         - containerport: 9160
        resources: {}
# view
      - name: view
        image: view
        ports:
         - containerport: 3000
        resources: {}

 # tomcat
      - name: tomcat
        image: tomcat
# node
      - name: node
        image: node
        resources: {}
# openjdk
      - name: node
        image: node
        resources: {}

      volumes:
        - name: api-service-claim
          persistentvolumeclaim:
            claimname: api-service-claim

"
62162209,"ingress-nginx connects from outside minikube, but connection is refused from inside minikube","i am trying to access my ingress-nginx service from a service but it gives connection refused. here is my ingress

apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: ingress-service
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/use-regex: ""true""
spec:
  rules:
    - host: ticketing.dev
      http:
        paths:
          - path: /api/users/?(.*)
            backend:
              servicename: auth-srv
              serviceport: 3000
          - path: /api/tickets/?(.*)
            backend:
              servicename: tickets-srv
              serviceport: 3000
          - path: /?(.*)
            backend:
              servicename: client-srv
              serviceport: 3000


apiversion: v1
kind: namespace
metadata:
  name: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
---
kind: service
apiversion: v1
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  externaltrafficpolicy: local
  type: loadbalancer
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
  ports:
    - name: http
      port: 80
      protocol: tcp
      targetport: http
    - name: http
      port: 443
      protocol: tcp
      targetport: https


 kubectl get services -n ingress-nginx
name            type           cluster-ip       external-ip      port(s)                      age
ingress-nginx   loadbalancer   10.101.124.218   10.101.124.218   80:30634/tcp,443:30179/tcp   15m


the ingress-nginx is running on namespace ingress-nginx.
so it should be accessible by http://ingress-nginx.ingress-nginx.svc.cluster.local. but when i access it, it says connection refused 10.101.124.218:80. i am able to access the ingress from outside, i.e. from the ingress ip.

i am using minikube and used ingress by running minikube addons enable ingress. yes and im running the tunnel by minikube tunnel
",<nginx><kubernetes><kubernetes-ingress><minikube>,62180444,6,"i tested your environment and found the same behavior, external access but internally getting connection refused, this is how i solved:


the minikube ingress addon deploys the controller in kube-system namespace. if you try to deploy the service in a newly created namespace, it will not reach the deployment in kube-system namespace. 
it's easy to mix those concepts because the default nginx-ingress deployment uses the namespace ingress-nginx as you were trying.
another issue i found, is that your service does not have all selectors assigned to the controller deployment.
the easiest way to make your deployment work, is to run kubectl expose on the nginx controller:


kubectl expose deployment ingress-nginx-controller --target-port=80 --type=nodeport -n kube-system



using this command to create the nginx-ingress-controller service, all communications were working, both external and internal.




reproduction:


for this example i'm using only two ingress backends to avoid being much repetitive in my explanation.
using minikube 1.11.0
enabled ingress and metallb addons.
deployed two hello apps: v1 and v2, both pods listens on port 8080 and are exposed as node port as follows:


$ kubectl get services
name         type        cluster-ip       external-ip   port(s)          age
hello1-svc   nodeport    10.110.211.119   &lt;none&gt;        8080:31243/tcp   95m
hello2-svc   nodeport    10.96.9.66       &lt;none&gt;        8080:31316/tcp   93m



here is the ingress file, just like yours, just changed the backend services names and ports to match my deployed ones:


apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: ingress-service
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/use-regex: ""true""
spec:
  rules:
    - host: ticketing.dev
      http:
        paths:
          - path: /api/users/?(.*)
            backend:
              servicename: hello1-svc
              serviceport: 8080
          - path: /?(.*)
            backend:
              servicename: hello2-svc
              serviceport: 8080



now i'll create the nginx-ingress service exposing the controller deployment, this way all tags and settings will be inherited:


$ kubectl expose deployment ingress-nginx-controller --target-port=80 --type=nodep
ort -n kube-system
service/ingress-nginx-controller exposed



now we deploy the ingress object:


$ kubectl apply -f ingress.yaml 
ingress.networking.k8s.io/ingress-service created

$ kubectl get ingress
name              class    hosts           address      ports   age
ingress-service   &lt;none&gt;   ticketing.dev   172.17.0.4   80      56s

$ minikube ip
172.17.0.4



testing the ingress from the outside:


$ tail -n 1 /etc/hosts
172.17.0.4 ticketing.dev

$ curl http://ticketing.dev/?foo
hello, world!
version: 2.0.0
hostname: hello2-67bbbf98bb-s78c4

$ curl http://ticketing.dev/api/users/?foo
hello, world!
version: 1.0.0
hostname: hello-576585fb5f-67ph5



then i deployed a alpine pod to test the access from inside the cluster:


$ kubectl run --generator=run-pod/v1 -it alpine --image=alpine -- /bin/sh
/ # nslookup ingress-nginx-controller.kube-system.svc.cluster.local
server:         10.96.0.10
address:        10.96.0.10:53

name:   ingress-nginx-controller.kube-system.svc.cluster.local
address: 10.98.167.112

/ # apk update
/ # apk add curl

/ # curl -h ""host: ticketing.dev"" ingress-nginx-controller.kube-system.svc.cluster.local/?foo
hello, world!
version: 2.0.0
hostname: hello2-67bbbf98bb-s78c4

/ # curl -h ""host: ticketing.dev"" ingress-nginx-controller.kube-system.svc.cluster.local/api/users/?foo
hello, world!
version: 1.0.0
hostname: hello-576585fb5f-67ph5


as you can see, all requests were fulfilled.



note:


as pointed by @suren, when curling ingress, i had to specify the host with -h
the service name needs to be fully fqdn because we are dealing with a service hosted in another namespace, using the format &lt;svc_name&gt;.&lt;namespace&gt;.svc.cluster.local.
in your js app, you will have to pass the host argument in order to reach the ingress.


if you have any question let me know in the comments.
"
52081373,rabbit-mq deployment with kubernetes,"i'm in a progress to migrate to kuberenetes from docker-compose.
one of the services we're using is rabbit-mq.
when i try to deploy rabbit-mq 3.6.16-management i receive the error:

/usr/local/bin/docker-entrypoint.sh: line 382: /etc/rabbitmq/rabbitmq.config: permission denied.

while it works in docker-compose deployment.

kuberentes:

apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: rabbit-mq
  name: rabbit-mq
spec:
  replicas: 1
  selector:
    matchlabels:
      app: rabbit-mq
  strategy:
    type: recreate
  template:
    metadata:
      labels:
         app: rabbit-mq
    spec:
      containers:
      - image: rabbitmq:3.6.16-management
        name: rabbit-mq
        ports:
        - containerport: 15671
        - containerport: 5671
        volumemounts:
        - mountpath: /etc/rabbitmq
          name: rabbit-mq-data
      restartpolicy: always
      hostname: rabbit-mq
      volumes:
      - name: rabbit-mq-data
        persistentvolumeclaim:
          claimname: rabbit-mq-data


pvc:

apiversion: v1
kind: persistentvolumeclaim
metadata:
  labels:
    app: rabbit-mq-data
  name: rabbit-mq-data
spec:
  accessmodes:
  - readwriteonce
  resources:
    requests:
      storage: 16gi


pv:

apiversion: v1
kind: persistentvolume
metadata:
  name: rabbit-mq-data
  labels:
    type: local
spec:
  accessmodes:
  - readwriteonce
  capacity:
    storage: 16gi
  hostpath:
    path: ""/etc/rabbitmq""


docker-compose:

  rabbit-mq:
      image: rabbitmq:3.6.16-management
      ports:
        - ""15671:15671""
        - ""5671:5671""
      container_name: rabbit-mq
      volumes:
        - rabbit-mq-data:/etc/rabbitmq
      restart: on-failure:5

",<docker><kubernetes><rabbitmq><kubectl>,52371754,6,"eventually i've used configmap and secrets to mount files instead of pv and works as expected.

apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: rabbit-mq
  name: rabbit-mq
spec:
  replicas: 1
  selector:
    matchlabels:
      app: rabbit-mq
  template:
    metadata:
      labels:
         app: rabbit-mq
    spec:
      containers:
      - image: rabbitmq:3.6.16-management
        name: rabbit-mq
        ports:
        - containerport: 15671
        - containerport: 5671
        volumemounts:
        - name: rabbit-mq-data
          mountpath: /etc/rabbitmq
          readonly: false
        - name: mq-secret
          mountpath: /etc/rabbitmq/certfiles
          #readonly: true
      volumes:
        - name: mq-secret
          secret:
            defaultmode: 420
            secretname: rabbit-mq-secrets
        - configmap:
            defaultmode: 420
            items:
            - key: rabbitmq.config
              path: rabbitmq.config
            name: mq-config
          name: rabbit-mq-data

"
62651374,how do i access my cassandra/kubernetes cluster from outside the cluster?,"i have started using cass-operator and the setup worked like a charm! https://github.com/datastax/cass-operator.
i have an issue though. my cluster is up and running on gcp. but how do i access it from my laptop (basically from outside)? sorry, i'm new to kubernetes so i do not know how to access the cluster from outside?
i can see the nodes are up on the gcp dashboard. i can ping the external ip of the nodes from my laptop but when i run cqlsh external_ip 9042 then the connection fails.
how do i go about connecting the k8s/cassandra cluster to outside work so that my web application can access it?
i would like to:

have a url so that my web application uses that url to connect to the cassandra/k8s cluster instead of ip address. thus, i need a dns. does it come by default in k8s? would would be the url? would k8s managing the dns mapping for me in some nodes get restarted?
my web application should be able to reach cassandra on 9042. it seems load balancing is done for http/https. the cassandra application is not a http/https request. so i don't need port 80 or 443

i have read few tutorials which talk about service, loadbalancer and ingress. but i am unable to make a start.
i created a service like this
kind: service
apiversion: v1
metadata:
  name: cass-operator-service
spec:
  type: loadbalancer
  ports:
    - port: 9042
  selector:
    name: cass-operator

then created the service - kubectl apply -f ./cass-operator-service.yaml
i checked if the service was created using kubectl get svc and got output
name                    type           cluster-ip      external-ip     port(s)          age 
cass-operator-service   loadbalancer   10.51.249.224   34.91.214.233   9042:30136/tcp   4m17s 
kubernetes              clusterip      10.51.240.1     &lt;none&gt;          443/tcp          10h. 

but when i run cqlsh 34.91.214.233 9042 then the connection fails
it seems that the requests to port 9042 would be forwarded to 30136. but they should be forwarded to 9042 as that is where the cassandra image in the pods is listening for incoming requests
update
tried targetport but still no luck
manuchadha25@cloudshell:~ (copper-frame-262317)$ cat cass-operator-service.yaml
kind: service
apiversion: v1
metadata:
  name: cass-operator-service
spec:
  type: loadbalancer
  ports:
    - port: 9042
      targetport: 9042
  selector:
    name: cass-operator
manuchadha25@cloudshell:~ (copper-frame-262317)$ kubectl get service
name         type        cluster-ip    external-ip   port(s)   age
kubernetes   clusterip   10.51.240.1   &lt;none&gt;        443/tcp   11h
manuchadha25@cloudshell:~ (copper-frame-262317)$ kubectl apply -f ./cass-operator-service.yaml
service/cass-operator-service created
manuchadha25@cloudshell:~ (copper-frame-262317)$ kubectl get service
name                    type           cluster-ip      external-ip   port(s)          age
cass-operator-service   loadbalancer   10.51.255.184   &lt;pending&gt;     9042:30024/tcp   12s
kubernetes              clusterip      10.51.240.1     &lt;none&gt;        443/tcp          11h
manuchadha25@cloudshell:~ (copper-frame-262317)$ kubectl get service
name                    type           cluster-ip      external-ip   port(s)          age
cass-operator-service   loadbalancer   10.51.255.184   &lt;pending&gt;     9042:30024/tcp   37s
kubernetes              clusterip      10.51.240.1     &lt;none&gt;        443/tcp          11h
manuchadha25@cloudshell:~ (copper-frame-262317)$ kubectl get service
name                    type           cluster-ip      external-ip     port(s)          age
cass-operator-service   loadbalancer   10.51.255.184   34.91.214.233   9042:30024/tcp   67s
kubernetes              clusterip      10.51.240.1     &lt;none&gt;          443/tcp          11h
manuchadha25@cloudshell:~ (copper-frame-262317)$ ping 34.91.214.233
ping 34.91.214.233 (34.91.214.233) 56(84) bytes of data.
64 bytes from 34.91.214.233: icmp_seq=1 ttl=109 time=7.89 ms


querying all names spaces reveal the following

but querying pods with namespace cass-operator returns empty result
manuchadha25@cloudshell:~ (copper-frame-262317)$ kubectl get pods -l name=cass-operator
no resources found in default namespace.

",<kubernetes><google-kubernetes-engine>,62659265,6,"
since you are new to kubernetes, you probably are not familiar with statefulsets:


statefulset is the workload api object used to manage stateful applications.
manages the deployment and scaling of a set of  pods,  and provides guarantees about the ordering and uniqueness  of these pods.
like a deployment, a statefulset manages pods that are based on an identical container spec. unlike a deployment, a statefulset maintains a sticky identity for each of their pods. these pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.


i recommend you to read these articles to learn more about it's mechanisms:

kubernetes.io - statefulsets
megalix - statefulsets 101
itnext - exposing statefulsets in kubernetes





how do i go about connecting the k8s/cassandra cluster to outside work so that my web application can access it?


i found out that datastax/cass-operator is still developing their documentation, i found this document that is not merged to master yet, but it explains very well about how to connect to cassandra, i strongly recommend reading.
there are several open issues for documenting methods for connection from outside the cluster.

i followed the guide in https://github.com/datastax/cass-operator to deploy the cass-operator + cassandra datacenter example as from your images i believe you followed as well:
$ kubectl create -f https://raw.githubusercontent.com/datastax/cass-operator/v1.2.0/docs/user/cass-operator-manifests-v1.15.yaml
namespace/cass-operator created
serviceaccount/cass-operator created
secret/cass-operator-webhook-config created
customresourcedefinition.apiextensions.k8s.io/cassandradatacenters.cassandra.datastax.com created
clusterrole.rbac.authorization.k8s.io/cass-operator-cluster-role created
clusterrolebinding.rbac.authorization.k8s.io/cass-operator created
role.rbac.authorization.k8s.io/cass-operator created
rolebinding.rbac.authorization.k8s.io/cass-operator created
service/cassandradatacenter-webhook-service created
deployment.apps/cass-operator created
validatingwebhookconfiguration.admissionregistration.k8s.io/cassandradatacenter-webhook-registration created

$ kubectl create -f https://raw.githubusercontent.com/datastax/cass-operator/v1.2.0/operator/k8s-flavors/gke/storage.yaml
storageclass.storage.k8s.io/server-storage created

$ kubectl -n cass-operator create -f https://raw.githubusercontent.com/datastax/cass-operator/v1.2.0/operator/example-cassdc-yaml/cassandra-3.11.6/example-cassdc-minimal.yaml
cassandradatacenter.cassandra.datastax.com/dc1 created

$ kubectl get all -n cass-operator
name                                ready   status    restarts   age
pod/cass-operator-78c6469c6-6qhsb   1/1     running   0          139m
pod/cluster1-dc1-default-sts-0      2/2     running   0          138m
pod/cluster1-dc1-default-sts-1      2/2     running   0          138m
pod/cluster1-dc1-default-sts-2      2/2     running   0          138m

name                                          type           cluster-ip    external-ip    port(s)             age
service/cass-operator-metrics                 clusterip      10.21.5.65    &lt;none&gt;         8383/tcp,8686/tcp   138m
service/cassandradatacenter-webhook-service   clusterip      10.21.0.89    &lt;none&gt;         443/tcp             139m
service/cluster1-dc1-all-pods-service         clusterip      none          &lt;none&gt;         &lt;none&gt;              138m
service/cluster1-dc1-service                  clusterip      none          &lt;none&gt;         9042/tcp,8080/tcp   138m
service/cluster1-seed-service                 clusterip      none          &lt;none&gt;         &lt;none&gt;              138m

name                            ready   up-to-date   available   age
deployment.apps/cass-operator   1/1     1            1           139m

name                                      desired   current   ready   age
replicaset.apps/cass-operator-78c6469c6   1         1         1       139m

name                                        ready   age
statefulset.apps/cluster1-dc1-default-sts   3/3     138m

$ cass_user=$(kubectl -n cass-operator get secret cluster1-superuser -o json | jq -r '.data.username' | base64 --decode)
$ cass_pass=$(kubectl -n cass-operator get secret cluster1-superuser -o json | jq -r '.data.password' | base64 --decode)

$ echo $cass_user
cluster1-superuser

$ echo $cass_pass
_5rowp851l0e_2cgun_n753e-zvemo5oy31i6c0dbcyiwh5vfjb8_g


from the kubectl get all command above we can see there is an statefulset called statefulset.apps/cluster1-dc1-default-sts which controls the cassandra pods.
in order to create a loadbalancer service that makes available all the pods managed by this statefulset we need to use the same labels assigned to them:

$ kubectl describe statefulset cluster1-dc1-default-sts -n cass-operator
name:               cluster1-dc1-default-sts
namespace:          cass-operator
creationtimestamp:  tue, 30 jun 2020 12:24:34 +0200
selector:           cassandra.datastax.com/cluster=cluster1,cassandra.datastax.com/datacenter=dc1,cassandra.datastax.com/rack=default
labels:             app.kubernetes.io/managed-by=cass-operator
                    cassandra.datastax.com/cluster=cluster1
                    cassandra.datastax.com/datacenter=dc1
                    cassandra.datastax.com/rack=default


now let's create the loadbalancer service yaml and use the labels as selectors for the service:

apiversion: v1
kind: service
metadata:
  name: cassandra-loadbalancer
  namespace: cass-operator
  labels:
    cassandra.datastax.com/cluster: cluster1
    cassandra.datastax.com/datacenter: dc1
    cassandra.datastax.com/rack: default
spec:
  type: loadbalancer
  ports:
  - port: 9042
    protocol: tcp
  selector:
    cassandra.datastax.com/cluster: cluster1
    cassandra.datastax.com/datacenter: dc1
    cassandra.datastax.com/rack: default


&quot;my web application should be able to reach cassandra on 9042. it seems load balancing is done for http/https. the cassandra application is not a http/https request. so i don't need port 80 or 443.&quot;


when you create a service of type  loadbalancer, a google cloud controller wakes up and configures a  network load balancer  in your project. the load balancer has a stable ip address that is accessible from outside of your project.

the network load balancer supports any and all ports. you can use network load balancing to load balance tcp and udp traffic. because the load balancer is a pass-through load balancer, your backends terminate the load-balanced tcp connection or udp packets themselves.

now let's apply the yaml and note the endpoint ips of the pods being listed:


$ kubectl apply -f cassandra-loadbalancer.yaml 
service/cassandra-loadbalancer created

$ kubectl get service cassandra-loadbalancer -n cass-operator 
name                     type           cluster-ip    external-ip    port(s)          age
cassandra-loadbalancer   loadbalancer   10.21.4.253   146.148.89.7   9042:30786/tcp   5m13s

$ kubectl describe svc cassandra-loadbalancer -n cass-operator
name:                     cassandra-loadbalancer
namespace:                cass-operator
labels:                   cassandra.datastax.com/cluster=cluster1
                          cassandra.datastax.com/datacenter=dc1
                          cassandra.datastax.com/rack=default
annotations:              selector:  cassandra.datastax.com/cluster=cluster1,cassandra.datastax.com/datacenter=dc1,cassandra.datastax.com/rack=default
type:                     loadbalancer
ip:                       10.21.4.253
loadbalancer ingress:     146.148.89.7
port:                     &lt;unset&gt;  9042/tcp
targetport:               9042/tcp
nodeport:                 &lt;unset&gt;  30786/tcp
endpoints:                10.24.0.7:9042,10.24.2.7:9042,10.24.3.9:9042
session affinity:         none
external traffic policy:  cluster
events:                   &lt;none&gt;


to test it, i'll use my cloud shell with a cassandra container to emulate your notebook using the loadbalancer ip provided above:

$ docker run -it cassandra /bin/sh

# cqlsh -u cluster1-superuser -p _5rowp851l0e_2cgun_n753e-zvemo5oy31i6c0dbcyiwh5vfjb8_g 146.148.89.7 9042                

connected to cluster1 at 146.148.89.7:9042.
[cqlsh 5.0.1 | cassandra 3.11.6 | cql spec 3.4.4 | native protocol v4]
use help for help.
cluster1-superuser@cqlsh&gt; select * from system.peers;

 peer      | data_center | host_id                              | preferred_ip | rack    | release_version | rpc_address | schema_version                       | tokens
-----------+-------------+--------------------------------------+--------------+---------+-----------------+-------------+--------------------------------------+--------------------------
 10.24.3.9 |         dc1 | bcec6c12-49a1-41d5-be58-5150e99f5dfb |         null | default |          3.11.6 |   10.24.3.9 | e84b6a60-24cf-30ca-9b58-452d92911703 |  {'2248175870989649036'}
 10.24.0.7 |         dc1 | 68409f08-9d6e-4e40-91ff-f43581c8b6f3 |         null | default |          3.11.6 |   10.24.0.7 | e84b6a60-24cf-30ca-9b58-452d92911703 | {'-1105923522927946373'}

(2 rows)



&quot;have a url so that my web application uses that url to connect to the cassandra/k8s cluster instead of ip address. so i need a dns. does it come by default in k8s? would would be the url? would k8s managing the dns mapping for me in some nodes get restarted?&quot;


that documentation on cassandra-operator also has a section about ingress, i recommend reading as well.
kubernetes does not come with a default dns name.
you will have to register a domain, point the dns to the ip of the load balancer this way it will resolve the ip of the network loadbalancer.
the network loadbalancer is bound to a static public ip,  any changes in kubernetes nodes will not cause service unavailability.

if you have any question, let me know in the comments.
"
51057682,persistentvolumeclaim unknown in kubernetes,"i try to deploy a container but unfortunately i have an error when i try to execute kubectl apply -f *.yaml

the error is : 


  error validating data: validationerror(pod.spec.containers[1]):
  unknown field ""persistentvolumeclaim"" in io.k8s.api.core.v1.container;


i dont understand why i get the error because i wrote  claimname: under persistentvolumeclaim: in my pd.yaml config :(

pod.yaml

apiversion: v1
kind: pod
metadata:
  name: karafpod
spec:
  containers:
  - name: karaf
    image: xxx/karaf:ids-1.1.0
    volumemounts:
    - name: karaf-conf-storage
      mountpath: /apps/karaf/etc
  - name: karaf-conf-storage
    persistentvolumeclaim:
      claimname: karaf-conf-claim


persistentvolumeclaimkaraf.yml 

kind: persistentvolumeclaim
apiversion: v1
metadata:
  name: karaf-conf-claim
spec:
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 10mi


deployment.yaml

 apiversion: extensions/v1beta1
 kind: deployment
 metadata:
   name: karaf
   namespace: poc 
 spec:
   replicas: 1
   template:
     metadata:
       labels:
         app: karaf
     spec:
       containers:
       - name: karaf
         image: ""xxx/karaf:ids-1.1.0""
         imagepullpolicy: always
         ports:
         - containerport: 6443
         - containerport: 6100
         - containerport: 6101
         resources: 
         volumemounts:
         - mountpath: /apps/karaf/etc
           name: karaf-conf
       volumes:
       - name: karaf-conf
         persistentvolumeclaim:
           claimname: karaf-conf

",<kubernetes><google-kubernetes-engine>,51057927,6,"the reason you're seeing that error is due to you specifying a persistentvolumeclaim under your pod spec's container specifications. as you can see from the auto generated docs here: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core

persistentvolumeclaims aren't supported at this level/api object, which is what's giving the error you're seeing.

you should modify the pod.yml to specify this as a volume instead.

e.g.:

apiversion: v1
kind: pod
metadata:
  name: karafpod
spec:
  containers:
    - name: karaf
      image: xxx/karaf:ids-1.1.0
      volumemounts:
      - name: karaf-conf-storage
        mountpath: /apps/karaf/etc
  volumes:
    - name: karaf-conf-storage
      persistentvolumeclaim:
        claimname: karaf-conf-claim

"
65919773,ingress nginx proxy to outside website (webflow hosted),"i have an eks cluster, and a separate website built on (and hosted by) webflow.
the cluster is behind cluster.com and the website website.webflow.io
what i would like to achieve is to proxy requests coming to cluster.com/website to website.webflow.io
based on my research, this problem could/might be solved with the externalname service. unfortunately, it doesn't solve it for me, and it's trying to do a dns lookup within the cluster. i tried various other configurations with endpoints as well. the externalname seems the most promising of everything i tried that's why i'm attaching the configuration below.
here is what my configuration looks like:
---
kind: service
apiversion: v1
metadata:
  namespace: development
  name: external-service
spec:
  type: externalname
  externalname: website.webflow.io
  ports:
    - port: 443
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  namespace: development
  name: external-ingress
  annotations:
    ingress.kubernetes.io/preserve-host: &quot;false&quot;
    ingress.kubernetes.io/secure-backends: &quot;true&quot;
    ingress.kubernetes.io/upstream-vhost: &quot;website.webflow.io&quot;
    nginx.ingress.kubernetes.io/backend-protocol: &quot;https&quot;
    nginx.ingress.kubernetes.io/server-snippet: |
      proxy_ssl_name website.webflow.io;
      proxy_ssl_server_name on;
spec:
  rules:
  - host: cluster.com
    http:
      paths:
      - path: /website
        backend:
          servicename: external-service
          serviceport: 443


is there a straight-forward way to achieve this? what stands out as wrong in the configuration?
",<nginx><kubernetes><nginx-ingress><amazon-eks>,65937039,6,"here is what i did.
i applied your config but changed the following annotation name:
ingress.kubernetes.io/upstream-vhost: &quot;website.webflow.io&quot;

to the one i have found in the nginx ingress docs:
nginx.ingress.kubernetes.io/upstream-vhost: &quot;website.webflow.io&quot;
^^^^^^

try it and let me know if it solves it.
edit:
here is a complete yaml i used:
---
kind: service
apiversion: v1
metadata:
  name: external-service
spec:
  type: externalname
  externalname: website.webflow.io
  ports:
    - port: 443

---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: external-ingress
  annotations:
    ingress.kubernetes.io/preserve-host: &quot;false&quot;
    ingress.kubernetes.io/secure-backends: &quot;true&quot;
    nginx.ingress.kubernetes.io/upstream-vhost: &quot;website.webflow.io&quot;
    nginx.ingress.kubernetes.io/backend-protocol: &quot;https&quot;
    nginx.ingress.kubernetes.io/server-snippet: |
      proxy_ssl_name website.webflow.io;
      proxy_ssl_server_name on;
spec:
  rules:
  - host: cluster.com
    http:
      paths:
      - path: /website
        backend:
          servicename: external-service
          serviceport: 443

"
65506388,kubernetes helm pvc,"i have a yml file where i create a pvc on my provider (digital ocean)
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: test-pvc
  namespace: test
spec:
  accessmodes:
  - readwriteonce
  resources:
    requests:
      storage: 10gi
  storageclassname: do-block-storage

i am also using a helm chart and want to be able to point to the already created pvc
how can i do this? this helm chart is deployed after the pvc is already created/deployed
  master:
    persistence:
      enabled: true
      ## mariadb data persistent volume storage class
      ## if defined, storageclassname: &lt;storageclass&gt;
      ## if set to &quot;-&quot;, storageclassname: &quot;&quot;, which disables dynamic provisioning
      ## if undefined (the default) or set to null, no storageclassname spec is
      ##   set, choosing the default provisioner.  (gp2 on aws, standard on
      ##   gke, aws &amp; openstack)
      ##
      # storageclass: &quot;-&quot;
      accessmode: readwriteonce
      size: 10gi

",<kubernetes><kubernetes-helm><kubernetes-pvc>,65506522,6,"to mount your pvc in your helm chart, do this:

disable persistence volume creation in helm chart

master:
    persistence:
      enabled: false #setting it to false


modify your deployment yaml, its located in templates folder of your helm chart. point your pod to mount your own pvc.

something like this:
apiversion: v1
kind: pod
metadata:
  name: task-pv-pod
spec:
  volumes:
    - name: task-pv-storage
      persistentvolumeclaim:
        claimname: test-pvc  # pointing to existing test-pvc
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerport: 80
          name: &quot;http-server&quot;
      volumemounts:
        - mountpath: &quot;/usr/share/nginx/html&quot;
          name: task-pv-storage

update:
specifically in your case, you can specify your pvc name against existingclaim parameter in values.yaml, it will be picked up by your deployment.
"
66778344,networkpolicy in kubernetes doesn't match on podselector,"i have a simple working networkpolicy looking like this
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: monitoring-network-policy-prometheus-jbn
  namespace: monitoring
spec:
  podselector:
    matchlabels:
      app: prometheus
  policytypes:
    - egress
  egress:
    - to: 
      ports:
        - port: 61678

but now i want to restrict this a bit more. instead of allowing egress to all destinations on port 61678 from all pods with label app: prometheus i want to allow only traffic to pods with label k8s-app: aws-node
so i change the policy to:
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: monitoring-network-policy-prometheus-jbn
  namespace: monitoring
spec:
  podselector:
    matchlabels:
      app: prometheus
  policytypes:
    - egress
  egress:
    - to:
      - podselector:
          matchlabels:
            k8s-app: aws-node

according to https://kubernetes.io/docs/concepts/services-networking/network-policies/ a policy that looks like this
  ...
  ingress:
  - from:
    - namespaceselector:
        matchlabels:
          user: alice
    - podselector:
        matchlabels:
          role: client
  ...

is described as  allows connections from pods in the local namespace with the label role=client, or from any pod in any namespace with the label user=alice.
so i would think that this would match a pod with label k8s-app: aws node which is located in the kube-system namespace on any port. but when i try to connect to a pod with that label i get a timeout.
here is the pod i am connecting to
 kubectl get pods -n kube-system -l k8s-app=aws-node
name             ready   status    restarts   age
aws-node-ngmnd   1/1     running   0          46h

i am using aws eks with calio network plugin.
what am i missing here?
",<kubernetes><amazon-eks><kubernetes-networkpolicy>,66782737,6,"this is happening because you omit placing the namespaceselector in your manifest and by default when namespaceselector is not preset the system will select the pods matching podselector in the policy's own namespace.
see here:

podselector
this is a label selector which selects pods. this field follows standard label selector semantics; if present but empty, it selects
all pods. if namespaceselector is also set, then the networkpolicypeer
as a whole selects the pods matching podselector in the namespaces
selected by namespaceselector. otherwise it selects the pods matching
podselector in the policy's own namespace.

what can you do solve it? you could set empty namespace selector as per documents:

namespaceselector
selects namespaces using cluster-scoped labels. this field follows standard label selector semantics; if present but empty, it selects
all namespaces. if podselector is also set, then the
networkpolicypeer as a whole selects the pods matching podselector in
the namespaces selected by namespaceselector. otherwise it selects all
pods in the namespaces selected by namespaceselector.
reference networkpolicypeer

i reproduce this issue and the documentation is correct but a bit misleading about place which should be in fact empty. so the parenthesis should be placed after the matchlabels:
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: monitoring-network-policy-prometheus-jbn
  namespace: monitoring
spec:
  podselector:
    matchlabels:
      app: prometheus
  policytypes:
    - egress
  egress:
    - to:
      - podselector:
          matchlabels:
            k8s-app: aws-node
        namespaceselector:
          matchlabels: {}

to answer your concerns about whether calico might be causing some issues. well in fact it is, but it is suppose to. for network policies to take effect you need to run network plugin that will enforce them.
"
53089294,nginx ingress sub path redirection,"i have an ingress controller and ingress resource running with all /devops mapped to devopsservice in the backend. when i try to hit ""http://hostname/devops"" things work and i get a page (although without css and styles) with a set of hyperlinks for e.g one of them is ""logs"".

when i click on the ""logs"" hyperlink, it is redirecting me to http://hostname/logs whereas i need it to be http://hostname/devops/logs.

any idea what i can do?

apiversion: extensions/v1beta1
kind: ingress
metadata:
 name: my-ingress
 namespace: ingress-nginx
 annotations:
   kubernetes.io/ingress.class: nginx
   nginx.ingress.kubernetes.io/rewrite-target: /
   nginx.ingress.kubernetes.io/add-base-url : ""true""
spec:
 rules:
 - host: master1.dev.local
   http:
     paths:
     - backend:
         servicename: devops1
         serviceport: 10311
       path: /devops

",<nginx><kubernetes><kubernetes-ingress><nginx-ingress>,53095088,6,"if you access  http://hostname/devops/logs directly from your browser, certainly you will get what you want. but since you click the hyperlink in the homepage, then you can only get http://hostname/logs, which will be certainly failed.

so, you need /logs backend configured in your ingress yaml to get it processed, and configure nginx.ingress.kubernetes.io/configuration-snippet to ensure /logs not get rewrote, like this:

apiversion: extensions/v1beta1
kind: ingress
metadata:
 name: my-ingress
 namespace: ingress-nginx
 annotations:
   kubernetes.io/ingress.class: nginx
   nginx.ingress.kubernetes.io/rewrite-target: /
   nginx.ingress.kubernetes.io/add-base-url : ""true""
   nginx.ingress.kubernetes.io/configuration-snippet: |
     rewrite ^/logs /logs break;
spec:
 rules:
 - host: master1.dev.local
   http:
     paths:
     - backend:
         servicename: devops1
         serviceport: 10311
       path: /logs
     - backend:
         servicename: devops1
         serviceport: 10311
       path: /devops

"
52538187,k8s: service unchanged but not listed,"i am trying to create a service on kubernetes but although no error is printed, i am unable to list the service;

working locally with minikube

*$ kubectl get services
name         type        cluster-ip   external-ip   port(s)   age
kubernetes   clusterip   10.96.0.1    &lt;none&gt;        443/tcp   4h
/home/pkara/workspace/gitlab/my-minikube
*$ kubectl apply -f mydb.yaml 
service/mydatabase unchanged
/home/pkara/workspace/gitlab/my-minikube
*$ kubectl get services
name         type        cluster-ip   external-ip   port(s)   age
kubernetes   clusterip   10.96.0.1    &lt;none&gt;        443/tcp   4h


here is the manifest used:

---

apiversion: v1
kind: service
metadata:
  name: mydatabase
  namespace: esa-local-dev
  labels:
    app: mydatabase
spec:
  ports:
  - name: mydatabase-port
    port: 3306
    targetport: 3306
  selector:
    app: mydatabase
  clusterip: none

---
apiversion: apps/v1
kind: deployment
metadata:
  name: mydatabase-deployment
  labels:
    app: mydatabase
spec:
  selector:
    matchlabels:
      app: mydatabase
  strategy:
    type: recreate
  template:
    metadata:
      labels:
        app: mydatabase
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        env:
        - name: mysql_root_password
          value: ""root""
        - name: mysql_database
          value: ""mydatabase""
        - name: mysql_user
          value: ""mydatabase""
        - name: mysql_password
          value: ""mydatabase""
        ports:
        - containerport: 3306
          name: mysql
      restartpolicy: always

",<kubernetes><kubectl><minikube>,52539446,6,"the service is created on esa-local-dev namespace. in your yaml, you have:

apiversion: v1
kind: service
metadata:
  name: mydatabase
  namespace: esa-local-dev
...


you can see the service by running,

$ kubectl get services -n esa-local-dev

"
52718805,one istio-ingressgateway and multiple tls gateways,"brief of the problem:


if i try to attach multiple tls gateways (using the same certificate)
to one ingressgateway, only one tls will work. (the last applied)
attaching multiple non-tls gateways to the same ingressgateway works ok.


error messages:

domain 1 (ok):

 curl -i https://integration.domain.com
http/2 200 
server: envoy
[...]


domain 2 (bad):

 curl -vi https://staging.domain.com    
* rebuilt url to: https://staging.domain.com/
*   trying 35.205.120.133...
* tcp_nodelay set
* connected to staging.domain.com (35.x.x.x) port 443 (#0)
* alpn, offering h2
* alpn, offering http/1.1
* cipher selection: all:!export:!export40:!export56:!anull:!low:!rc4:@strength
* successfully set certificate verify locations:
*   cafile: /etc/ssl/certs/ca-certificates.crt
  capath: /etc/ssl/certs
* tlsv1.2 (out), tls header, certificate status (22):
* tlsv1.2 (out), tls handshake, client hello (1):
* unknown ssl protocol error in connection to staging.domain.com:443 
* curl_http_done: called premature == 1
* stopped the pause stream!
* closing connection 0
curl: (35) unknown ssl protocol error in connection to staging.domain.com:443 


facts:

i have a wildcard tls cert (lets say '*.domain.com') i've put in a secret with:

kubectl create -n istio-system secret tls istio-ingressgateway-certs --key tls.key --cert tls.crt


i have the default istio-ingressgateway attached to a static ip: 

apiversion: v1
kind: service
metadata:
  name: istio-ingressgateway
  namespace: istio-system
  annotations:
  labels:
    chart: gateways-1.0.0
    release: istio
    heritage: tiller
    app: istio-ingressgateway
    istio: ingressgateway
spec:
  loadbalancerip: ""35.x.x.x""
  type: loadbalancer
  selector:
    app: istio-ingressgateway
    istio: ingressgateway
[...]


then i have two gateways in different namespaces, for two domains included on the tls wildcard (staging.domain.com, integration.domain.com):

staging:

apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: domain-web-gateway
  namespace: staging
spec:
  selector:
    istio: ingressgateway # use istio default gateway implementation
  servers:
  - port:
      number: 443
      name: https
      protocol: https
    tls:
      mode: simple
      servercertificate: /etc/istio/ingressgateway-certs/tls.crt
      privatekey: /etc/istio/ingressgateway-certs/tls.key
    hosts:
    - ""staging.domain.com""
  - port:
      number: 80
      name: http
      protocol: http
    hosts:
    - ""staging.domain.com""


integration:

apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: domain-web-gateway
  namespace: integration
spec:
  selector:
    istio: ingressgateway # use istio default gateway implementation
  servers:
  - port:
      number: 443
      name: https
      protocol: https
    tls:
      mode: simple
      servercertificate: /etc/istio/ingressgateway-certs/tls.crt
      privatekey: /etc/istio/ingressgateway-certs/tls.key
    hosts:
    - ""integration.domain.com""
  - port:
      number: 80
      name: http
      protocol: http
    hosts:
    - ""integration.domain.com""

",<ssl><kubernetes><google-kubernetes-engine><istio>,52723870,6,"the problem is that you are using the same name (https) for port 443 in two gateways managed by the same workload (selector). they need to have unique names. this restriction is documented here.

you can fix it by just changing the name of your second gateway, for example:

apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: domain-web-gateway
  namespace: integration
spec:
  selector:
    istio: ingressgateway # use istio default gateway implementation
  servers:
  - port:
      number: 443
      name: https-integration
      protocol: https
    tls:
      mode: simple
      servercertificate: /etc/istio/ingressgateway-certs/tls.crt
      privatekey: /etc/istio/ingressgateway-certs/tls.key
    hosts:
    - ""integration.domain.com""
  - port:
      number: 80
      name: http
      protocol: http
    hosts:
    - ""integration.domain.com""

"
68549818,labels in deployment spec & template,"in the below yaml:
apiversion: apps/v1
kind: deployment
metadata:
  name: my-nginx
  labels:
    app: my-nginx # line 6
spec:             # line 7  
  replicas: 2
  selector:
    matchlabels:
      app: my-nginx    # line 11
  template:
    metadata:
      labels:
        app: my-nginx   # line 15
    spec:                # line 16
      containers:
      - name: my-nginx
        image: nginx:alpine
        ports:
        - containerport: 80
        resources:
          limits:
            memory: &quot;128mi&quot; #128 mb
            cpu: &quot;200m&quot; #200 millicpu (.2 cpu or 20% of the cpu)


deployment is given a label(app: nginx) at line 6.
deployment spec at line 7 uses the pod spec mentioned in line 16


what is the purpose of selector field with matchlabels?

what is the purpose of template field with labels?


",<kubernetes><kubernetes-pod>,68550236,6,"tried to add comments to explain the role of labels:
apiversion: apps/v1
kind: deployment
metadata:
  name: my-nginx
  labels:
    app: my-nginx # label-a: &lt;--this label is to manage the deployment itself. this may be used to filter the deployment based on this label. 
spec:              
  replicas: 2
  selector:
    matchlabels:
      app: my-nginx    #label-b:  &lt;--  field defines how the deployment finds which pods to manage.
  template:
    metadata:
      labels:
        app: my-nginx   #label-c: &lt;--this is the label of the pod, this must be same as label-b
    spec:                
      containers:
      - name: my-nginx
        image: nginx:alpine
        ports:
        - containerport: 80
        resources:
          limits:
            memory: &quot;128mi&quot; #128 mb
            cpu: &quot;200m&quot; #200 millicpu (.2 cpu or 20% of the cpu)

label-a: &lt;--this label is to manage the deployment itself. this may be used to filter the deployment based on this label. example usage of label-a is for deployment management, such as filtering.
k get deployments.apps -l app=my-nginx

label-b:  &lt;--  there must be some place where we tell replication controller to manage the pods. this field defines how the deployment finds which pods to manage. based on these labels of the pod, replication controller ensure they are ready.
label-c: &lt;--this is the label of the pod, which label-b use to monitor. this must be same as label-b
"
68766805,how to build helm chart from existing kubernetes manifest,"i'm a newbie to k8s. i have a homework and this is my situation:
there is an app with microservice-oriented, build with amount ten containers. it had a docker-compose file for easily set up. now my mission is deploy it into kubernetes. my idea: convert docker-compose file to k8s manifest with kompose, and create helm chart for each services.
my question is: i have to modify each chart one-by-one, isn't it? is there any way to generate values.yaml base on existing k8s manifest? example, from this: 
# deployment.yaml
apiversion: apps/v1
kind: deployment
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.22.0 (955b78124)
  creationtimestamp: null
  labels:
    io.kompose.service: bookstore-account-service
  name: bookstore-account-service
...

to this, automatically:
# deployment.yaml
apiversion: apps/v1
kind: deployment
metadata:
  annotations:
    kompose.cmd: {{ .values.cmd }}
    kompose.version: {{ .values.ver }}
  creationtimestamp: null
  labels:
    io.kompose.service: {{ .values.name }}
  name: {{ .values.name }}
...

# values.yaml
cmd: kompose convert
ver: 1.22.0 (955b78124)
name: bookstore-account-service

p/s: sorry for my bad english, it's not my first language :d
",<docker><kubernetes><kubernetes-helm>,68771176,6,"the helm values.yaml file is the main point where you can configure the chart at deployment time.  on the one hand, you can't configure anything that's not referenced in .values; on the other hand, you usually don't want every individual line of the yaml file to be configurable.
if i was going to approach this, i'd start by helm create a new chart.  i'd then switch to the templates directory, move aside most of the boilerplate there (but leave the generated _helpers.tpl file), and run kompose convert.  this will generate a set of yaml files, though with no helm templating.
from here i'd edit the files to make them match typical helm usage.  look at the original files from helm create (or in the helm source) for examples.  i would expect the edited deployment.yaml to look like:
apiversion: apps/v1
kind: deployment
metadata:
  {{-/* delete the kompose annotations: and empty creationtimestamp: */}}
  labels:
    {{-/* get a standard set of labels from _helpers.tpl }}
    {{- include &quot;bookstore.labels&quot; . | nindent 4 }}
  {{-/* get a standard name from _helpers.tpl }}
  name: {{ include &quot;bookstore.fullname&quot; . }}

what should go in the values.yaml file, then?  these are things you'd need to configure at deploy time.  if you need to override the container command: or args:, these would usually be fixed, but if you needed to supply some sort of credentials or host name, those could vary per-deployment.  (if you helm installed the chart twice, what would be different between the installs?)  the helm create template makes resource limits configurable, since these can vary heavily based on the actual workload:
# deployment.yaml (from helm/create.go linked above)
resources:
  {{- toyaml .values.resources | nindent 12 }}

# values.yaml (also from helm/create.go)
resources: {}

you could deploy this with a specific set of values here:
# values.dev.yaml
resources:
  requests:
    memory: 256mi
  limits:
    memory: 1gi

# values.prod.yaml
resources:
  requests:
    memory: 2gi
  limits:
    memory: 4gi

helm install bookstore . -f values.dev.yaml

if you had preserved, for example, the &quot;what version of kompose generated this file&quot; annotation, there'd be no reason to change that between environments, and so you could just leave that as a fixed string.
"
74923859,argocd: why helm app not applying values.yml,"i would like to install a helm release using argocd, i defined a helm app declaratively like the following :
apiversion: argoproj.io/v1alpha1
kind: application
metadata:
  name: moon
  namespace: argocd
spec:
  project: aerokube
  source:
    chart: moon2
    repourl: https://charts.aerokube.com/
    targetrevision: 2.4.0
    helm:
      valuefiles:
      - values.yml
  destination:
    server: &quot;https://kubernetes.default.svc&quot;
    namespace: moon1
  syncpolicy:
    syncoptions:
      - createnamespace=true

where my values.yml:
customingress:
  enabled: true
  annotations:
    cert-manager.io/cluster-issuer: &quot;letsencrypt&quot;   
  ingressclassname: nginx
  host: moon3.benighil-mohamed.com
  tls:
  - secretname: moon-tls
    hosts:
    - moon3.benighil-mohamed.com
configs:
  default:
    containers:
      vnc-server:
        repository: quay.io/aerokube/vnc-server
        resources:
          limits:
            cpu: 400m
            memory: 512mi
          requests:
            cpu: 200m
            memory: 512mi

notice, the app does not take values.yml into consideration, and i get the following error:
rpc error: code = unknown desc = manifest generation error (cached): `helm template . --name-template moon --namespace moon1 --kube-version 1.23 --values /tmp/74d737ea-efd0-42a6-abcf-1d4fea4e40ab/moon2/values.yml --api-versions acme.cert-manager.io/v1 --api-versions acme.cert-manager.io/v1/challenge --api-versions acme.cert-manager.io/v1/order --api-versions admissionregistration.k8s.io/v1 --api-versions admissionregistration.k8s.io/v1/mutatingwebhookconfiguration --api-versions admissionregistration.k8s.io/v1/validatingwebhookconfiguration --api-versions apiextensions.k8s.io/v1 --api-versions apiextensions.k8s.io/v1/customresourcedefinition --api-versions apiregistration.k8s.io/v1 --api-versions apiregistration.k8s.io/v1/apiservice --api-versions apps/v1 --api-versions apps/v1/controllerrevision --api-versions apps/v1/daemonset --api-versions apps/v1/deployment --api-versions apps/v1/replicaset --api-versions apps/v1/statefulset --api-versions argoproj.io/v1alpha1 --api-versions argoproj.io/v1alpha1/appproject --api-versions argoproj.io/v1alpha1/application --api-versions argoproj.io/v1alpha1/applicationset --api-versions autoscaling/v1 --api-versions autoscaling/v1/horizontalpodautoscaler --api-versions autoscaling/v2 --api-versions autoscaling/v2/horizontalpodautoscaler --api-versions autoscaling/v2beta1 --api-versions autoscaling/v2beta1/horizontalpodautoscaler --api-versions autoscaling/v2beta2 --api-versions autoscaling/v2beta2/horizontalpodautoscaler --api-versions batch/v1 --api-versions batch/v1/cronjob --api-versions batch/v1/job --api-versions batch/v1beta1 --api-versions batch/v1beta1/cronjob --api-versions ceph.rook.io/v1 --api-versions ceph.rook.io/v1/cephblockpool --api-versions ceph.rook.io/v1/cephblockpoolradosnamespace --api-versions ceph.rook.io/v1/cephbucketnotification --api-versions ceph.rook.io/v1/cephbuckettopic --api-versions ceph.rook.io/v1/cephclient --api-versions ceph.rook.io/v1/cephcluster --api-versions ceph.rook.io/v1/cephfilesystem --api-versions ceph.rook.io/v1/cephfilesystemmirror --api-versions ceph.rook.io/v1/cephfilesystemsubvolumegroup --api-versions ceph.rook.io/v1/cephnfs --api-versions ceph.rook.io/v1/cephobjectrealm --api-versions ceph.rook.io/v1/cephobjectstore --api-versions ceph.rook.io/v1/cephobjectstoreuser --api-versions ceph.rook.io/v1/cephobjectzone --api-versions ceph.rook.io/v1/cephobjectzonegroup --api-versions ceph.rook.io/v1/cephrbdmirror --api-versions cert-manager.io/v1 --api-versions cert-manager.io/v1/certificate --api-versions cert-manager.io/v1/certificaterequest --api-versions cert-manager.io/v1/clusterissuer --api-versions cert-manager.io/v1/issuer --api-versions certificates.k8s.io/v1 --api-versions certificates.k8s.io/v1/certificatesigningrequest --api-versions coordination.k8s.io/v1 --api-versions coordination.k8s.io/v1/lease --api-versions crd.projectcalico.org/v1 --api-versions crd.projectcalico.org/v1/bgpconfiguration --api-versions crd.projectcalico.org/v1/bgppeer --api-versions crd.projectcalico.org/v1/blockaffinity --api-versions crd.projectcalico.org/v1/caliconodestatus --api-versions crd.projectcalico.org/v1/clusterinformation --api-versions crd.projectcalico.org/v1/felixconfiguration --api-versions crd.projectcalico.org/v1/globalnetworkpolicy --api-versions crd.projectcalico.org/v1/globalnetworkset --api-versions crd.projectcalico.org/v1/hostendpoint --api-versions crd.projectcalico.org/v1/ipamblock --api-versions crd.projectcalico.org/v1/ipamconfig --api-versions crd.projectcalico.org/v1/ipamhandle --api-versions crd.projectcalico.org/v1/ippool --api-versions crd.projectcalico.org/v1/ipreservation --api-versions crd.projectcalico.org/v1/kubecontrollersconfiguration --api-versions crd.projectcalico.org/v1/networkpolicy --api-versions crd.projectcalico.org/v1/networkset --api-versions discovery.k8s.io/v1 --api-versions discovery.k8s.io/v1/endpointslice --api-versions discovery.k8s.io/v1beta1 --api-versions discovery.k8s.io/v1beta1/endpointslice --api-versions events.k8s.io/v1 --api-versions events.k8s.io/v1/event --api-versions events.k8s.io/v1beta1 --api-versions events.k8s.io/v1beta1/event --api-versions flowcontrol.apiserver.k8s.io/v1beta1 --api-versions flowcontrol.apiserver.k8s.io/v1beta1/flowschema --api-versions flowcontrol.apiserver.k8s.io/v1beta1/prioritylevelconfiguration --api-versions flowcontrol.apiserver.k8s.io/v1beta2 --api-versions flowcontrol.apiserver.k8s.io/v1beta2/flowschema --api-versions flowcontrol.apiserver.k8s.io/v1beta2/prioritylevelconfiguration --api-versions moon.aerokube.com/v1 --api-versions moon.aerokube.com/v1/browserset --api-versions moon.aerokube.com/v1/config --api-versions moon.aerokube.com/v1/deviceset --api-versions moon.aerokube.com/v1/license --api-versions moon.aerokube.com/v1/quota --api-versions networking.k8s.io/v1 --api-versions networking.k8s.io/v1/ingress --api-versions networking.k8s.io/v1/ingressclass --api-versions networking.k8s.io/v1/networkpolicy --api-versions node.k8s.io/v1 --api-versions node.k8s.io/v1/runtimeclass --api-versions node.k8s.io/v1beta1 --api-versions node.k8s.io/v1beta1/runtimeclass --api-versions objectbucket.io/v1alpha1 --api-versions objectbucket.io/v1alpha1/objectbucket --api-versions objectbucket.io/v1alpha1/objectbucketclaim --api-versions operator.tigera.io/v1 --api-versions operator.tigera.io/v1/apiserver --api-versions operator.tigera.io/v1/imageset --api-versions operator.tigera.io/v1/installation --api-versions operator.tigera.io/v1/tigerastatus --api-versions policy/v1 --api-versions policy/v1/poddisruptionbudget --api-versions policy/v1beta1 --api-versions policy/v1beta1/poddisruptionbudget --api-versions policy/v1beta1/podsecuritypolicy --api-versions rbac.authorization.k8s.io/v1 --api-versions rbac.authorization.k8s.io/v1/clusterrole --api-versions rbac.authorization.k8s.io/v1/clusterrolebinding --api-versions rbac.authorization.k8s.io/v1/role --api-versions rbac.authorization.k8s.io/v1/rolebinding --api-versions scheduling.k8s.io/v1 --api-versions scheduling.k8s.io/v1/priorityclass --api-versions snapshot.storage.k8s.io/v1 --api-versions snapshot.storage.k8s.io/v1/volumesnapshot --api-versions snapshot.storage.k8s.io/v1/volumesnapshotclass --api-versions snapshot.storage.k8s.io/v1/volumesnapshotcontent --api-versions snapshot.storage.k8s.io/v1beta1 --api-versions snapshot.storage.k8s.io/v1beta1/volumesnapshot --api-versions snapshot.storage.k8s.io/v1beta1/volumesnapshotclass --api-versions snapshot.storage.k8s.io/v1beta1/volumesnapshotcontent --api-versions storage.k8s.io/v1 --api-versions storage.k8s.io/v1/csidriver --api-versions storage.k8s.io/v1/csinode --api-versions storage.k8s.io/v1/storageclass --api-versions storage.k8s.io/v1/volumeattachment --api-versions storage.k8s.io/v1beta1 --api-versions storage.k8s.io/v1beta1/csistoragecapacity --api-versions v1 --api-versions v1/configmap --api-versions v1/endpoints --api-versions v1/event --api-versions v1/limitrange --api-versions v1/namespace --api-versions v1/node --api-versions v1/persistentvolume --api-versions v1/persistentvolumeclaim --api-versions v1/pod --api-versions v1/podtemplate --api-versions v1/replicationcontroller --api-versions v1/resourcequota --api-versions v1/secret --api-versions v1/service --api-versions v1/serviceaccount --include-crds` failed exit status 1: error: open /tmp/74d737ea-efd0-42a6-abcf-1d4fea4e40ab/moon2/values.yml: no such file or directory

notice both application.yml and values.yml are located in the same directory on my local machine, ie: the structure of the 2 files in question looks like :
.
 application.yml
 values.yml

any help please ?
",<kubernetes><kubernetes-helm><argocd><gitops><argo>,74984288,5,"cleanest way to achieve what you want is using the remote chart as dependency:
chart.yaml
name: mychartname
version: 1.0.0
apiversion: v2
dependencies:
  - name: moon2
    version: &quot;2.4.0&quot;
    repository: &quot;https://charts.aerokube.com/&quot;

and overriding its values like this:
values.yaml
moon2:
  customingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: &quot;letsencrypt&quot;   
    ingressclassname: nginx
    host: moon3.benighil-mohamed.com
    tls:
    - secretname: moon-tls
      hosts:
      - moon3.benighil-mohamed.com
  configs:
    default:
      containers:
        vnc-server:
          repository: quay.io/aerokube/vnc-server
          resources:
            limits:
              cpu: 400m
              memory: 512mi
            requests:
              cpu: 200m
              memory: 512mi

pay attention to this file. you need to create a key in your values file with the same name as the dependency(moon2 in your case), and indent the values you want to override one level.
you need to upload both of these files to a repository and point your argocd application url to this repository.
this has the advantage that whenever the upstream helm chart gets updated, all you need to do is increase the version in chart.yaml
"
75204589,minikube ingress on macos appears to be working but never connects (times out no error),"macos big sur 11.6.8
minikube version: v1.28.0
following several tutorials on ingress and attempting to get it working locally. everything appears to work: manual minikube service foo works, kubectl get ingress shows an ip, pinging the designated host name resolves the expected ip, etc. i went through a few different tutes with the same results.
i boiled it down to the simplest replication from the tutorial at kubernetes.io :
# kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0
# kubectl expose deployment web --type=nodeport --port=8080
# kubectl get service web (ensure it's a node port)
# minikube service web --url (test url)
# kubectl apply -f ingress_hello_world.yaml
# curl localkube.com
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
    - host: localkube.com
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: web
                port:
                  number: 8080

manual service works:
&gt;minikube service web --url
http://127.0.0.1:50111
  because you are using a docker driver on darwin, the terminal needs to be open to run it.

&gt;curl http://127.0.0.1:50111
hello, world!
version: 1.0.0
hostname: web-84fb9498c7-hnphb

ingress looks good:
&gt;minikube addons list | grep ingress
| ingress                     | minikube | enabled    | kubernetes                     |

&gt;kubectl get ingress
name              class   hosts           address        ports   age
example-ingress   nginx   localkube.com   192.168.49.2   80      15m

ping resolves the address mapped in /etc/hosts:
&gt;ping localkube.com
ping localkube.com (192.168.49.2): 56 data bytes

i have looked through similar questions with no positive results. i have gone from this simple example to apache to mongo deployments via config files. each time i can get to the app through a manual service mapping or by creating an external service (loadbalancer / nodeport), but when i get to the ingress part the config applies with no errors and everything appears to be working except for it actually... working.
",<kubernetes><kubernetes-ingress><kubectl><minikube>,75214415,5,"based on veera's answer, i looked into the ingress issue with macos and minikube tunnel. to save others the hassle, here is how i resolved the issue:

ingress doesn't seem to work on macos (the different pages say &quot;with docker&quot; but i had the same outcome with other drivers like hyperkit.
the issue seems to be ip / networking related. you can not get to the minikube ip from your local workstation. if you first run minikube ssh you can ping and curl the minikube ip and the domain name you mapped to that ip in /etc/hosts. however, this does not help trying to access the service from a browser.
the solution is to map the domain names to 127.0.0.1 in /etc/hosts (instead of the ingress assigned ip) and use ingress components to control the domain-name -&gt; service mappings as before...
then starting a tunnel with sudo minikube tunnel will keep a base tunnel open, and create tunneling for any existing or new ingress components. this combined with the ingress rules will mimic host header style connecting to any domain resolving to the local host.

here is a full example of a working solution on mac. dump this to a file named ingress_hello_world.yaml and follow the commented instructions to achieve a simple ingress solution that routes 2 domains to 2 different services (note this will work with pretty much any internal service, and can be a clusterip instead of nodeport):
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: example-ingress
spec:
  ingressclassname: nginx
  rules:
    - host: test1.com
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: web
                port:
                  number: 8080
    - host: test2.com
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: web2
                port:
                  number: 8080

# instructions:
# start minikube if not already
# &gt;minikube start --vm-driver=docker
#
# enable ingress if not already
# &gt;minikube addons enable ingress
# &gt;minikube addons list | grep &quot;ingress &quot;
# | ingress                     | minikube | enabled    | kubernetes                     |
#
# &gt;kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0
# deployment.apps/web created
#
# &gt;kubectl expose deployment web --type=nodeport --port=8080
# service/web exposed
#
# &gt;kubectl create deployment web2 --image=gcr.io/google-samples/hello-app:2.0
# deployment.apps/web2 created
#
# &gt;kubectl expose deployment web2 --port=8080 --type=nodeport
# service/web2 exposed
#
# &gt;kubectl get service | grep web
# web          nodeport    10.101.19.188   &lt;none&gt;        8080:31631/tcp   21m
# web2         nodeport    10.102.52.139   &lt;none&gt;        8080:30590/tcp   40s
#
# &gt;minikube service web --url
# http://127.0.0.1:51813
#   because you are using a docker driver on darwin, the terminal needs to be open to run it.
#
# ------ in another console ------
# &gt;curl http://127.0.0.1:51813
#                          ^---- this must match the port from the output above
# hello, world!
# version: 1.0.0     &lt;---- will show version 2.0.0 for web2
# hostname: web-84fb9498c7-7bjtg
# --------------------------------
# ctrl+c to kill tunnel in original tab, repeat with web2 if desired
#
# ------ in another console ------
# &gt;sudo minikube tunnel
#   tunnel successfully started
#
# (leave open, will show the following when you start an ingress component)
# starting tunnel for service example-ingress.
# --------------------------------
#
# &gt;kubectl apply -f ingress_hello_world.yaml
# ingress.networking.k8s.io/example-ingress created
#
# &gt;kubectl get ingress example-ingress --watch
# name              class   hosts                 address   ports   age
# example-ingress   nginx   test1.com,test2.com             80      15s
# example-ingress   nginx   test1.com,test2.com   192.168.49.2   80      29s
#                wait for this to be populated ----^
#
# &gt;cat /etc/hosts | grep test
# 127.0.0.1    test1.com
# 127.0.0.1    test2.com
#        ^---- set this to localhost ip
#
# &gt;ping test1.com
# ping test1.com (127.0.0.1): 56 data bytes
#
# &gt;curl test1.com
# hello, world!
# version: 1.0.0
# hostname: web-84fb9498c7-w6bkc
#
# &gt;curl test2.com
# hello, world!
# version: 2.0.0
# hostname: web2-7df4dcf77b-66g5b

# ------- cleanup:
# stop tunnel
#
# &gt;kubectl delete -f ingress_hello_world.yaml
# ingress.networking.k8s.io &quot;example-ingress&quot; deleted
#
# &gt;kubectl delete service web
# service &quot;web&quot; deleted
#
# &gt;kubectl delete service web2
# service &quot;web2&quot; deleted
#
# &gt;kubectl delete deployment web
# deployment.apps &quot;web&quot; deleted
#
# &gt;kubectl delete deployment web2
# deployment.apps &quot;web2&quot; deleted

"
58783945,how to give all kubernetes service accounts access to a specific namespace?,"i have a rbac enabled kubernetes cluster in gcp

there are one namespace for tiller and multiple for services

right now, i can assign reader role for a specific service account given it's full name

apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: ""true""
  name: tiller-reader
  namespace: tiller
rules:
  - apigroups: [""""]
    resources: [""pods""]
    verbs:
    - ""get""
    - ""watch""
    - ""list""
---
apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: tiller-reader-role-binding
  namespace: tiller
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: tiller-reader
subjects:
  - apigroup: rbac.authorization.k8s.io
    kind: user
    name: system:serviceaccount:my-namespace-1:my-namespace-1-service-account


the service namespaces and accounts are created dynamically. how do i automatically give all services accounts access to the tiller namespace, for example: to get pods?
",<kubernetes><kubernetes-helm><rbac>,58789337,5,"to grant a role to all service accounts you must use the group system:serviceaccounts. 

you can try the following config :

apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: ""true""
  name: tiller-reader
  namespace: tiller
rules:
  - apigroups: [""""]
    resources: [""pods""]
    verbs:
    - ""get""
    - ""watch""
    - ""list""
---
apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: tiller-reader-role-binding
  namespace: tiller
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: tiller-reader
subjects:
  - apigroup: rbac.authorization.k8s.io
    kind: group
    name: system:serviceaccounts

"
60741296,kubernetes : deploy only in one node-pool,"i'm currently creating a kubernetes cluster for a production environment.
in my cluster, i have 2 node-pool, let's call them api-pool and web-pool

in my api-pool, i have 2 nodes with 4cpu and 15gb of ram each.

i'm trying to deploy 8 replicas of my api in my api-pool, each replicas should have 1cpu and 3.5gi of ram.

my api.deployment.yaml looks something like this : 

---
apiversion: apps/v1
kind: deployment
metadata:
  name: api-dev
spec:
  replicas: 8
  selector:
    matchlabels:
      app: my-api
  template:
    metadata:
      labels:
        app: my-api
    spec:
      containers:
      - name: api-docker
        image: //my_image
        imagepullpolicy: always
        envfrom:
          - configmapref:
              name: api-dev-env
          - secretref:
              name: api-dev-secret
        ports:
          - containerport: 80
        resources:
          requests:
            cpu: ""1""
            memory: ""3.5gi""


but my problem is that kubernetes is deploying the pods on nodes on my web-pool as well as in my api-pool but i want those pods to be deployed only in my api-pool.

i tried to label my nodes of the api-pool to use a selector that matches labels but it doesn't work and i'm not sure it's supposed to work that way.

how can i precise to k8s to deploy those 8 replicas only in my api-pool ?
",<kubernetes><deployment><google-kubernetes-engine>,60741438,5,"you can use a nodeselector which is the simplest recommended form of node selection constraint.

label the nodes of api-pool with pool=api

kubectl label nodes nodename pool=api


add nodeselector in pod spec.

---
apiversion: apps/v1
kind: deployment
metadata:
  name: api-dev
spec:
  replicas: 8
  selector:
    matchlabels:
      app: my-api
  template:
    metadata:
      labels:
        app: my-api
    spec:
      containers:
      - name: api-docker
        image: //my_image
        imagepullpolicy: always
        envfrom:
          - configmapref:
              name: api-dev-env
          - secretref:
              name: api-dev-secret
        ports:
          - containerport: 80
        resources:
          requests:
            cpu: ""1""
            memory: ""3.5gi""
      nodeselector:
        pool: api


for mode advanced use cases you can use node affinity.
"
44306554,how to deserialize kubernetes yaml file,"how can i deserialize a kubernetes yaml file into an go struct? i took a look into the kubectl code, but somehow i get an error for every yaml file:

no kind ""deployment"" is registered for version ""apps/v1beta1""


this is an mwe:

package main

import (
    ""fmt""

    ""k8s.io/client-go/pkg/api""
)

var service = `
apiversion: apps/v1beta1
kind: deployment
metadata:
  name: my-nginx
spec:
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx
        ports:
        - containerport: 80
`

func main() {
    decode := api.codecs.universaldecoder().decode
    //decode := api.codecs.universaldeserializer().decode

    obj, _, err := decode([]byte(service), nil, nil)
    if err != nil {
        panic(err)
    }

    fmt.printf(""%#v\n"", obj)
}


i am using client version 2.0.0. the glide.yaml looks like this:

package: test/stackoverflow
import:
- package: k8s.io/client-go
  version: ^2.0.0


these are the references to kubectl:


https://github.com/kubernetes/kubernetes/blob/43ac38e29e6ecf83e78bc7c5d9f804310b051c95/pkg/kubectl/cmd/apply.go#l637
https://github.com/kubernetes/kubernetes/blob/43ac38e29e6ecf83e78bc7c5d9f804310b051c95/pkg/kubectl/cmd/util/factory_client_access.go#l205-l213


unfortunately, the docs are very confusing to me, so i have no idea how to tackle this problem.

edit:

this problem also exists with other resource types:


no kind ""service"" is registered for version ""v1""

",<go><kubernetes><kubernetes-go-client>,44310259,5,"you need to import _ ""k8s.io/client-go/pkg/apis/extensions/install"" otherwise the schema is empty, see also docs.

the complete working example is:

$ go get -u github.com/golang/dep/cmd/dep
$ dep init
$ go run main.go


with the following main.go:

package main

import (
    ""fmt""

    ""k8s.io/client-go/pkg/api""
    _ ""k8s.io/client-go/pkg/api/install""
    _ ""k8s.io/client-go/pkg/apis/extensions/install""
)

var deployment = `
apiversion: extensions/v1beta1
kind: deployment
metadata:
name: my-nginx
spec:
replicas: 2
template:
  metadata:
    labels:
      run: my-nginx
  spec:
    containers:
    - name: my-nginx
      image: nginx
      ports:
      - containerport: 80
`

func main() {
    // decode := api.codecs.universaldecoder().decode
    decode := api.codecs.universaldeserializer().decode

    obj, _, err := decode([]byte(deployment), nil, nil)
    if err != nil {
        fmt.printf(""%#v"", err)
    }

    fmt.printf(""%#v\n"", obj)
}


note that i also imported _ ""k8s.io/client-go/pkg/api/install"" for you so that you can use objects in v1 such as pods or services.

edit: kudos to my colleague stefan schimanski who proposed the initial solution.
"
58911806,how to expose kubernetes metric server api to curl from inside the pod?,"i am using metric server to get the usage of my kubernetes cluster. but in order to use it from outside the host, i need to use ""kubectl proxy"". but i don't want to do that as it is not intended to run on background. i want it to be run as a service continuously 

how can i achieve these

expected output
 curl clusterip:8001/apis/metrics.k8s.io/v1beta1/nodes

{
  ""kind"": ""nodemetricslist"",
  ""apiversion"": ""metrics.k8s.io/v1beta1"",
  ""metadata"": {
    ""selflink"": ""/apis/metrics.k8s.io/v1beta1/nodes""
  },
  ""items"": [
    {
      ""metadata"": {
        ""name"": ""manhattan-master"",
        ""selflink"": ""/apis/metrics.k8s.io/v1beta1/nodes/manhattan-master"",
        ""creationtimestamp"": ""2019-11-15t04:26:47z""
      },
      ""timestamp"": ""2019-11-15t04:26:33z"",
      ""window"": ""30s"",
      ""usage"": {
        ""cpu"": ""222998424n"",
        ""memory"": ""3580660ki""
      }
    }
  ]


i tried by using loadbalancig service
metrics-server-service.yaml

apiversion: v1
kind: service
metadata:
  name: metrics-server
  namespace: kube-system
  labels:
    kubernetes.io/name: ""metrics-server""
    kubernetes.io/cluster-service: ""true""
spec:
  selector:
    k8s-app: metrics-server
  ports:
  - port: 443
    protocol: tcp
    targetport: main-port
  externaltrafficpolicy: local
  type: loadbalancer


kubectl describe service metrics-master -n kube-system

[root@manhattan-master 1.8+]# kubectl describe service metrics-server -n kube-system
name:                     metrics-server
namespace:                kube-system
labels:                   kubernetes.io/cluster-service=true
                          kubernetes.io/name=metrics-server
annotations:              kubectl.kubernetes.io/last-applied-configuration:
                            {""apiversion"":""v1"",""kind"":""service"",""metadata"":{""annotations"":{},""labels"":{""kubernetes.io/cluster-service"":""true"",""kubernetes.io/name"":""me...
selector:                 k8s-app=metrics-server
type:                     loadbalancer
ip:                       10.110.223.216
port:                     &lt;unset&gt;  443/tcp
targetport:               main-port/tcp
nodeport:                 &lt;unset&gt;  31043/tcp
endpoints:                10.32.0.7:4443
session affinity:         none
external traffic policy:  local
healthcheck nodeport:     32208
events:                   &lt;none&gt;

",<kubernetes><kubectl><kubernetes-service>,58932268,5,"this is possible by creating a new service to expose the metrics server. your metrics server service should look like this: 

apiversion: v1
kind: service
metadata:
  labels:
    kubernetes.io/name: metrics-server-ext
  name: metrics-server-ext
  namespace: kube-system
  selflink: /api/v1/namespaces/kube-system/services/metrics-server
spec:
  ports:
  - port: 443
    protocol: tcp
    targetport: https
  selector:
    k8s-app: metrics-server
  sessionaffinity: none
  type: loadbalancer


if you try to access this service you will face some problems with authorization and you need to do some things to give all necessary authorizations.

after creating the service you will need to create a cluster role binding so our service can have access to the data:

$ kubectl create clusterrolebinding node-admin-default-svc --clusterrole=cluster-admin --serviceaccount=default:default


before running curl command we need to get the token so we can pass this on our curl command: 

$ token=$(kubectl get secrets -o jsonpath=""{.items[?(@.metadata.annotations['kubernetes\.io/service-account\.name']=='default')].data.token}""|base64 --decode)


get your service external ip:

kubectl get svc/metrics-server-ext -n kube-system -o jsonpath='{..ip}'


your curl command should pass the token key to get authorization: 

curl -k https://34.89.228.98/apis/metrics.k8s.io/v1beta1/nodes --header ""authorization: bearer $token"" --insecure


sample output: 

{
 ""kind"": ""nodemetricslist"",
 ""apiversion"": ""metrics.k8s.io/v1beta1"",
 ""metadata"": {
   ""selflink"": ""/apis/metrics.k8s.io/v1beta1/nodes""
 },
 ""items"": [
   {
     ""metadata"": {
       ""name"": ""gke-lab-default-pool-993de7d7-ntmc"",
       ""selflink"": ""/apis/metrics.k8s.io/v1beta1/nodes/gke-lab-default-pool-993de7d7-ntmc"",
       ""creationtimestamp"": ""2019-11-19t10:26:52z""
     },
     ""timestamp"": ""2019-11-19t10:26:17z"",
     ""window"": ""30s"",
     ""usage"": {
       ""cpu"": ""52046272n"",
       ""memory"": ""686768ki""
     }
   },
   {
     ""metadata"": {
       ""name"": ""gke-lab-default-pool-993de7d7-tkj9"",
       ""selflink"": ""/apis/metrics.k8s.io/v1beta1/nodes/gke-lab-default-pool-993de7d7-tkj9"",
       ""creationtimestamp"": ""2019-11-19t10:26:52z""
     },
     ""timestamp"": ""2019-11-19t10:26:21z"",
     ""window"": ""30s"",
     ""usage"": {
       ""cpu"": ""52320505n"",
       ""memory"": ""687252ki""
     }
   },
   {
     ""metadata"": {
       ""name"": ""gke-lab-default-pool-993de7d7-v7m3"",
       ""selflink"": ""/apis/metrics.k8s.io/v1beta1/nodes/gke-lab-default-pool-993de7d7-v7m3"",
       ""creationtimestamp"": ""2019-11-19t10:26:52z""
     },
     ""timestamp"": ""2019-11-19t10:26:17z"",
     ""window"": ""30s"",
     ""usage"": {
       ""cpu"": ""45602403n"",
       ""memory"": ""609968ki""
     }
   }
 ]
}


edit:

you can also optionally access it from your pods since you created a cluster role binding in your default service account with cluster-admin role. 

as example, create a pod from a image that includes curl command:

$ kubectl run bb-$random --rm -i --image=ellerbrock/alpine-bash-curl-ssl --restart=never --tty -- /bin/bash


than you need to exec into you pod and run:

$ curl -k -x get https://kubernetes.default/apis/metrics.k8s.io/v1beta1/nodes --header ""authorization: bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)"" --insecure


here we are passing the same token mentioned before in a complete different way. 
"
60244923,how to mount a secret to kubernetes statefulset,"so, looking at the kubernetes api documentation: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#statefulsetspec-v1-apps it appears that i can indeed have a volume because it uses a podspec and the podspec does have a volume field, so i could list the secret and then mount it like in a deployment, or any other pod.

the problem is that kubernetes seems to think that volumes are not actually in the podspec for statefulset? is this right? how do i mount in a secret to my statefulset if this is true.

error: error validating ""mysql-stateful-set.yaml"": error validating data: validationerror(statefulset.spec.template.spec.containers[0]): unknown field ""volumes"" in io.k8s.api.core.v1.container; if you choose to ignore these errors, turn validation off with --validate=false

statefulset:

apiversion: v1
kind: service
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  ports:
  - port: 3306
    name: database
  selector:
    app: mysql
---
apiversion: apps/v1
kind: statefulset
metadata:
  name: mysql
spec:
  selector:
    matchlabels:
      app: mysql # has to match .spec.template.metadata.labels
  servicename: ""mysql""
  replicas: 1
  template:
    metadata:
      labels:
        app: mysql
    spec:
      terminationgraceperiodseconds: 60
      containers:
      - name: mysql
        image: mysql
        ports:
        - containerport: 3306
          name: database
        volumemounts:
        - name: data
          mountpath: /var/lib/mysql
        - name: mysql
          mountpath: /run/secrets/mysql
        env:
        - name: mysql_root_password_file
          value: /run/secrets/mysql/root-pass
        volumes:
          - name: mysql
            secret:
              secretname: mysql
              items:
                - key: root-pass
                  path: root-pass
                  mode: 511
  volumeclaimtemplates:
  - metadata:
      name: data
    spec:
      accessmodes: [ ""readwriteonce"" ]
      storageclassname: do-block-storage
      resources:
        requests:
          storage: 10gi```

",<kubernetes><kubernetes-secrets><statefulset>,60245558,5,"the volume field should come inside template spec and not inside container (as done in your template). refer this for the exact structure (https://godoc.org/k8s.io/api/apps/v1#statefulsetspec), go to podtemplatespec and you will find volumes field.

below template should work for you:

apiversion: v1
kind: service
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  ports:
  - port: 3306
    name: database
  selector:
    app: mysql
---
apiversion: apps/v1
kind: statefulset
metadata:
  name: mysql
spec:
  selector:
    matchlabels:
      app: mysql # has to match .spec.template.metadata.labels
  servicename: ""mysql""
  replicas: 1
  template:
    metadata:
      labels:
        app: mysql
    spec:
      terminationgraceperiodseconds: 60
      containers:
      - name: mysql
        image: mysql
        ports:
        - containerport: 3306
          name: database
        volumemounts:
        - name: data
          mountpath: /var/lib/mysql
        - name: mysql
          mountpath: /run/secrets/mysql
        env:
        - name: mysql_root_password_file
          value: /run/secrets/mysql/root-pass
      volumes:
       - name: mysql
         secret:
           secretname: mysql
           items:
            - key: root-pass
              path: root-pass
               mode: 511
  volumeclaimtemplates:
  - metadata:
      name: data
    spec:
      accessmodes: [ ""readwriteonce"" ]
      storageclassname: do-block-storage
      resources:
        requests:
          storage: 10gi

"
70832556,kubernetes nginx ingress configuration for wildcard rule,"i am struggling with the following issues. i have 2 services running. i am using a wildcard for handling subdomains. see the example conf below:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
    kubernetes.io/ingress.class: nginx
    kubernetes.io/ingress.global-static-ip-name: web-static-ip
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/server-alias: www.foo.bar
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
  name: foo-bar-ingress
  namespace: test
spec:
  rules:
  - host: '*.foo.bar'
    http:
      paths:
      - backend:
          servicename: legacy-service
          serviceport: 80
        path: /(.*)
        pathtype: implementationspecific
  - host: foo.bar
    http:
      paths:
      - backend:
          servicename: new-service
          serviceport: 8080
        path: /(.*)
        pathtype: implementationspecific

using the app in the way that abc.foo.bar -&gt; legacy-service and foo.bar -&gt; new-service work perfectly fine. however, when i access the app with www prefix, it gets under the wildcard subdomain path, meaning www.foo.bar goes into legacy-service, which is what i want to avoid. afaiu this &quot;www&quot; is caught by this asterisk regexp and goes in the wrong way. i would like it go to new-service.
is there any way i can achieve this with the nginx ingress configuration?
",<nginx><kubernetes><kubernetes-ingress><nginx-ingress>,70832796,5,"also redirecting requests from www.foo.bar can be achieved by also specifying the hostname. please note that the order of the hosts does matter as they are translated into the envoy filter chain. therefore, the wildcard host should be the last host.
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
    kubernetes.io/ingress.class: nginx
    kubernetes.io/ingress.global-static-ip-name: web-static-ip
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/server-alias: www.foo.bar
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
  name: foo-bar-ingress
  namespace: test
spec:
  rules:
  - host: 'foo.bar'
    http:
      paths:
      - backend:
          servicename: new-service
          serviceport: 8080
        path: /(.*)
        pathtype: implementationspecific
  - host: 'www.foo.bar'
    http:
      paths:
      - backend:
          servicename: new-service
          serviceport: 8080
        path: /(.*)
        pathtype: implementationspecific
  - host: '*.foo.bar'
    http:
      paths:
      - backend:
          servicename: legacy-service
          serviceport: 80
        path: /(.*)
        pathtype: implementationspecific

"
60806249,how to route all traffic from a container through another container in the same kubernetes pod?,"i'm creating a web application that comprises a react frontend and a node.js (express) server. the frontend makes an internal api call to the express server and the express server then makes an external api call to gather some data. the frontend and the server are in different containers within the same kubernetes pod.

the frontend service is an nginx:1.14.0-alpine image. the static files are built (npm build) in a ci pipeline and the build directory is copied to the image during docker build. the package.json contains a proxy key, ""proxy"": ""http://localhost:8080"", that routes traffic from the app to localhost:8080 - which is the port that the express server is listening on for an internal api call. i think the proxy key will have no bearing once the files are packaged into static files and served up onto an nginx image?

when running locally, i.e. running npm start instead of npm build, this all works. the express server picks up the api requests sent out by the frontend on port 8080.

the express server is a simple service that adds authentication to the api call that the frontend makes, that is all. but the authentication relies on secrets as environment variables, making them incompatible with react. the server is started by running node server.js; locally the server service successfully listens (app.listen(8080))to the api calls from the react frontend, adds some authentication to the request, then makes the the request to the external api and passes the response back to the frontend once it is received.

in production, in a kubernetes pod, things aren't so simple. the traffic from the react frontend proxying through the node server needs to be handled by kubernetes now, and i haven't been able to figure it out.

it may be important to note that there are no circumstances in which the frontend will make any external api calls directly, they will all go through the server.

react frontend dockerfile

from nginx:1.14.0-alpine

# copy static files
copy client/build/ /usr/share/nginx/html/

# the rest has been redacted for brevity but is just copying of favicons etc.


express node server

from node:10.16.2-alpine

# create app directory
workdir /app

# install app dependencies
copy server/package*.json .

run npm install

expose 8080

cmd [ ""node"", ""server.js"" ]


kubernetes manifest - redacted for brevity

apiversion: apps/v1beta1
kind: deployment

containers:
      - name: frontend
        image: frontend-image:1.0.0
        imagepullpolicy: ifnotpresent
        ports:
        - name: http
          containerport: 80
        volumemounts:
        - mountpath: /etc/nginx/conf.d/default.conf
          name: config-dir
          subpath: my-config.conf

      - name: server
              image: server-image:1.0.0
              imagepullpolicy: ifnotpresent
              volumes:
              - name: config-tmpl
                configmap:
                  name: app-config
                  defaultmode: 0744
              - name: my-config-directory
                emptydir: {}
---
apiversion: v1
kind: configmap
metadata:
  name: app-config
  namespace: my-namespace
data:
  my-conf.conf: |-
    server {

        listen 80;

        server_name _;

        location api/ {
          proxy_pass  http://127.0.0.1:8080/;
        }

.....


",<node.js><docker><nginx><kubernetes><amazon-eks>,60806628,5,"in kubernetes, thes pod share the same network interface with all container inside it, so for the frontend container localhost:8080 is the backend, and for the backend container localhost:80 is the frontend.
as for any container application, you should ensure that they are listening in other interfaces than 127.0.0.1 if you want traffic from outside.

migrating an aplication from one server - where every application talks from 127.0.0.1 - to a pod was intended to be simple as in a dedicated machine.

your nginx.conf looks a little bit strange, should be location /api/ {.

here is functional example:

nginx.conf

server {
    server_name   localhost;
    listen        0.0.0.0:80;

    error_page    500 502 503 504  /50x.html;

    location      / {
        root      html;
    }
    location /api/ {
      proxy_pass  http://127.0.0.1:8080/;
    }

}


create this configmap:

kubectl create -f nginx.conf


app.js

const express = require('express')
const app = express()
const port = 8080

app.get('/', (req, res) =&gt; res.send('hello from express!'))

app.listen(port, () =&gt; console.log(`example app listening on port ${port}!`))


dockerfile

from alpine

run apk add nodejs npm &amp;&amp; mkdir -p /app

copy . /app

workdir /app

run npm install express --save

expose 8080

cmd node app.js


you can build this image or use the one i've made hectorvido/express.

then, create the pod yaml definition:

pod.yml

apiversion: v1
kind: pod
metadata:
  name: front-back
  labels:
    app: front-back
spec:
  containers:
  - name: front
    image: nginx:alpine
    volumemounts:
    - name: nginx-conf
      mountpath: /etc/nginx/conf.d/
    ports:
    - containerport: 80
  - name: back
    image: hectorvido/express
    ports:
    - containerport: 8080      
  volumes:
  - name: nginx-conf
    configmap:
      name: nginx


put on the cluster:

kubectl create -f pod.yml


get the ip:

kubectl get pods -o wide


i tested with minikube, so if the pod ip was 172.17.0.7 i have to do:

minikube ssh
curl -l 172.17.0.7/api


if you had an ingress in the front, it should still working. i enabled an nginx ingress controller on minikube, so we need to create a service and a ingress:

service

kubectl expose pod front-back --port 80


ingress.yml

apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: front-back
spec:
  rules:
  - host: fb.192-168-39-163.nip.io # minikube ip
    http:
      paths:
      - path: /
        backend:
          servicename: front-back
          serviceport: 80


the test still works:

curl -vl http://fb.192-168-39-163.nip.io/api/

"
64712410,kubenetes mongo deployment loses data after pod restart,"recently, the managed pod in my mongo deployment onto gke was automatically deleted and a new one was created in its place. as a result, all my db data was lost.
i specified a pv for the deployment and the pvc was bound too, and i used the standard storage class (google persistent disk). the persistent volume claim had not been deleted either.
here's an image of the result from kubectl get pv:
pvc
my mongo deployment along with the persistent volume claim and service deployment were all created by using kubernets' kompose tool from a docker-compose.yml for a prisma 1 + mongodb deployment.
here are my yamls:
mongo-deployment.yaml
apiversion: apps/v1
kind: deployment
metadata:
  annotations:
    kompose.cmd: kompose -f docker-compose.yml convert
    kompose.version: 1.21.0 (992df58d8)
  creationtimestamp: null
  labels:
    io.kompose.service: mongo
  name: mongo
  namespace: dbmode
spec:
  replicas: 1
  selector:
    matchlabels:
      io.kompose.service: mongo
  strategy:
    type: recreate
  template:
    metadata:
      annotations:
        kompose.cmd: kompose -f docker-compose.yml convert
        kompose.version: 1.21.0 (992df58d8)
      creationtimestamp: null
      labels:
        io.kompose.service: mongo
    spec:
      containers:
      - env:
        - name: mongo_initdb_root_password
          value: prisma
        - name: mongo_initdb_root_username
          value: prisma
        image: mongo:3.6
        imagepullpolicy: &quot;&quot;
        name: mongo
        ports:
        - containerport: 27017
        resources: {}
        volumemounts:
        - mountpath: /var/lib/mongo
          name: mongo
      restartpolicy: always
      serviceaccountname: &quot;&quot;
      volumes:
      - name: mongo
        persistentvolumeclaim:
          claimname: mongo
status: {}

mongo-persistentvolumeclaim.yaml
apiversion: v1
kind: persistentvolumeclaim
metadata:
  creationtimestamp: null
  labels:
    io.kompose.service: mongo
  name: mongo
  namespace: dbmode
spec:
  accessmodes:
  - readwriteonce
  resources:
    requests:
      storage: 100mi
status: {}


mongo-service.yaml
apiversion: v1
kind: service
metadata:
  annotations:
    kompose.cmd: kompose -f docker-compose.yml convert
    kompose.version: 1.21.0 (992df58d8)
  creationtimestamp: null
  labels:
    io.kompose.service: mongo
  name: mongo
  namespace: dbmode
spec:
  ports:
  - name: &quot;27017&quot;
    port: 27017
    targetport: 27017
  selector:
    io.kompose.service: mongo
status:
  loadbalancer: {}

i've tried checking the contents mounted in /var/lib/mongo and all i got was an empty lost+found/ folder, and i've tried to search the google persistent disks but there was nothing in the root directory and i didn't know where else to look.
i guess that for some reason the mongo deployment is not pulling from the persistent volume for the old data when it starts a new pod, which is extremely perplexing.
i also have another kubernetes project where the same thing happened, except that the old pod still showed but had an evicted status.
",<mongodb><kubernetes><google-kubernetes-engine><kubernetes-pod><persistent-volumes>,64724162,5,"
i've tried checking the contents mounted in /var/lib/mongo and all i
got was an empty lost+found/ folder,

ok, but have you checked it was actually saving data there, before the pod restart and data loss ? i guess it was never saving any data in that directory.
i checked the image you used by running a simple pod:
apiversion: v1
kind: pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-pod
    image: mongo:3.6

when you connect to it by running:
kubectl exec -ti my-pod -- /bin/bash

and check the default mongo configuration file:
root@my-pod:/var/lib# cat /etc/mongod.conf.orig
# mongod.conf

# for documentation of all options, see:
#   http://docs.mongodb.org/manual/reference/configuration-options/

# where and how to store data.
storage:
  dbpath: /var/lib/mongodb # 
  journal:
    enabled: true
#  engine:
#  mmapv1:
#  wiredtiger:

you can see among other things that dbpath is actually set to /var/lib/mongodb and not to /var/lib/mongo.
so chances are that your mongo wasn't actually saving any data to your pv i.e. to /var/lib/mongo directory, where it was mounted, but to /var/lib/mongodb as stated in its configuration file.
you should be able to check it easily by kubectl exec to your running mongo pod:
kubectl exec -ti &lt;mongo-pod-name&gt; -- /bin/bash

and verify where the data is saved.
if you didn't overwrite in any way the original config file (e.g. by providing a configmap), mongo should save its data to /var/lib/mongodb and this directory, not being a mount point for your volume, is part of a pod filesystem and its ephemeral.
update:
the above mentioned /etc/mongod.conf.orig is only a template so it doesn't reflect the actual configuration that has been applied.
if you run:
kubectl logs your-mongo-pod

it will show where the data directory is located:
$ kubectl logs my-pod 
2020-12-16t22:20:47.472+0000 i control  [initandlisten] mongodb starting : pid=1 port=27017 dbpath=/data/db 64-bit host=my-pod
2020-12-16t22:20:47.473+0000 i control  [initandlisten] db version v3.6.21
...

as we can see, data is saved in /data/db:
dbpath=/data/db

"
65007977,helm charts create secrets in different namespace,"i have the following secrets.yaml in templetes in helm charts:
apiversion: v1
kind: secret
metadata:
  name: mysecret
type: opaque
data:
  user_name: ywrtaw4=
  password: mwyyzdflmmu2n2rm 

i need to create the same secret in different namespace, for example, namespace test1, test2, test3, test4, how to specify the different namespace with the same secrets so the same secret can be created in different namespace?
",<kubernetes><kubernetes-helm>,65014492,5,"you can set the namespace name in the metadata section like
apiversion: v1
kind: secret
metadata:
  name: mysecret
  namespace: test1
type: opaque
data:
  user_name: ywrtaw4=
  password: mwyyzdflmmu2n2rm

you can set a for loop with helm to create one secret definition in each namespace.
update.
# values.yaml
namespaces:
  - test1
  - test2

# templates.secrets.tpl
{{- range .values.namespaces }}
---
apiversion: v1
kind: secret
metadata:
  name: mysecret
  namespace: {{ . | quote }}
type: opaque
data:
  user_name: ywrtaw4=
  password: mwyyzdflmmu2n2rm

{{- end }}


### output ###
---
# source: base/templates/secrets.tpl

---
apiversion: v1
kind: secret
metadata:
  name: mysecret
  namespace: &quot;test1&quot;
type: opaque
data:
  user_name: ywrtaw4=
  password: mwyyzdflmmu2n2rm
---
apiversion: v1
kind: secret
metadata:
  name: mysecret
  namespace: &quot;test2&quot;
type: opaque
data:
  user_name: ywrtaw4=
  password: mwyyzdflmmu2n2rm

"
72489600,specify service account name with namespace for kubernetes pod,"i am trying to use workflow identity for my kubernetes cluster. i have created the service account on a new namespace. my issue is that i am not able to specify the name space when i am trying to add the service account name on the pod deployment yml.
following is my pod spect file:
apiversion: apps/v1
kind: deployment
metadata:
  name: test-scheduler
spec:
  replicas: 1
  selector:
    matchlabels:
      app: test-scheduler
  template:
    metadata:
      labels:
        app: test-scheduler
    spec:
      serviceaccountname: test-na/test-k8-sa
      nodeselector:
        iam.gke.io/gke-metadata-server-enabled: &quot;true&quot;
      containers:
        - name: test-scheduler
          image: gcr.io/project_id/image:tag
          ports:
            - name: scheduler-port
              containerport: 8002
              protocol: tcp
          env:
            - name: namespace
              value: test-scheduler
            - name: container_name
              value: test-scheduler
---
apiversion: v1
kind: service
metadata:
  name: test-scheduler
spec:
  selector:
    app: test-scheduler
  ports:
    - port: 8002
      protocol: tcp
      targetport: scheduler-port 


when i deploy this code using github actions i get this error:
the deployment &quot;test-scheduler&quot; is invalid: spec.template.spec.serviceaccountname: invalid value: &quot;test-na/test-k8-sa&quot;: a lowercase rfc 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.',

when i remove the namespace in a file like this:
serviceaccountname: test-k8-sa

it searches for the service account on default name space and fails.
my question here is what is the right way to specify the custom namespace with the service account in kubernetes?
i can start using the default but i am inclined to keep the namespace. i saw some reference to service account file but i don't really understand how to use them.
by the way, i am using this guide https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#gcloud_3
",<kubernetes><google-cloud-platform><google-kubernetes-engine><kubernetes-pod>,72491396,5,"...i have created the service account on a new namespace. my issue is that i am not able to specify the name space when i am trying to add the service account name on the pod deployment yml.
to assign the created service account to your deployment, you can create the deployment in the same namespace as the service account:
apiversion: apps/v1
kind: deployment
metadata:
  name: test-scheduler
  namespace: test-na  # &lt;-- add this line with the namespace where the service account resides
spec:
  ...
  template:
    ...
    spec:
      serviceaccountname: test-k8-sa
      ...

"
57879351,how can i generate a configmap from a directory of files that need to be templated?,"i can generate the configmap from the directory but they are not translating the template directives or values. below is an example of the release.namespace template directive not being output in the configmap.

.
|____chart.yaml
|____charts
|____.helmignore
|____templates
| |____my-scripts.yaml
|____values.yaml
|____test-files
  |____test1.txt


---
# templates/myscripts.yaml
apiversion: v1
kind: configmap
metadata:
  name: my-scripts
  namespace: {{ .release.namespace }}
data:
  test.txt: |-
{{ .files.get ""test-files/test1.txt"" | indent 4}}


# test-files/test1.txt
test file
{{ .release.namespace }}


when i run helm install . --dry-run --debug --namespace this-should-print here's what i'm getting vs what i'm expecting:

actual:

---
# source: test/templates/my-scripts.yaml
# templates/myscripts.yaml
apiversion: v1
kind: configmap
metadata:
  name: my-scripts
  namespace: test
data:
  test.txt: |-
    # test-files/test1.txt
    test file
    {{ .release.namespace }}


expected:

---
# source: test/templates/my-scripts.yaml
# templates/myscripts.yaml
apiversion: v1
kind: configmap
metadata:
  name: my-scripts
  namespace: test
data:
  test.txt: |-
    # test-files/test1.txt
    test file
    this-should-print


alternatively, i would be interested in every file in a specified directory being output in the format like:

&lt;filename&gt;: |-
  &lt;content&gt;

",<kubernetes><kubernetes-helm>,57886569,5,"i've found a way of doing it using the tpl function:

---
# templates/myscripts.yaml
apiversion: v1
kind: configmap
metadata:
  name: my-scripts
  namespace: {{ .release.namespace }}
data:
  test.txt: |-
{{ tpl ( .files.get ""test-files/test1.txt"" ) . | indent 4 }}


the new output is exactly as expected:

# templates/myscripts.yaml
apiversion: v1
kind: configmap
metadata:
  name: my-scripts
  namespace: this-should-print
data:
  test.txt: |-
    # test-files/test1.txt
    test file
    this-should-print


and for bonus points, getting all files from a directory without having to update this list within the config map:

---
# templates/myscripts.yaml
apiversion: v1
kind: configmap
metadata:
  name: my-scripts
  namespace: {{ .release.namespace }}
data:
{{ tpl (.files.glob ""groovy-scripts/*"").asconfig . | indent 4 }}

"
57869937,horizontal pod autoscaler scales custom metric too aggressively on gke,"i have the below horizontal pod autoscaller configuration on google kubernetes engine to scale a deployment by a custom metric - rabbitmq messages ready count for a specific queue: foo-queue.
it picks up the metric value correctly.
when inserting 2 messages it scales the deployment to the maximum 10 replicas.
i expect it to scale to 2 replicas since the targetvalue is 1 and there are 2 messages ready.
why does it scale so aggressively?
hpa configuration:
apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  name: foo-hpa
  namespace: development
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: foo
  minreplicas: 1
  maxreplicas: 10
  metrics:
  - type: external
    external:
      metricname: &quot;custom.googleapis.com|rabbitmq_queue_messages_ready&quot;
      metricselector:
        matchlabels:
          metric.labels.queue: foo-queue
      targetvalue: 1

",<kubernetes><rabbitmq><google-kubernetes-engine><kubernetes-hpa>,57947108,5,"i think you did a great job explaining how targetvalue works with horizontalpodautoscalers. however, based on your question, i think you're looking for targetaveragevalue instead of targetvalue. 

in the kubernetes docs on hpas, it mentions that using targetaveragevalue instructs kubernetes to scale pods based on the average metric exposed by all pods under the autoscaler. while the docs aren't explicit about it, an external metric (like the number of jobs waiting in a message queue) counts as a single data point. by scaling on an external metric with targetaveragevalue, you can create an autoscaler that scales the number of pods to match a ratio of pods to jobs.

back to your example:

apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  name: foo-hpa
  namespace: development
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: foo
  minreplicas: 1
  maxreplicas: 10
  metrics:
  - type: external
    external:
      metricname: ""custom.googleapis.com|rabbitmq_queue_messages_ready""
      metricselector:
        matchlabels:
          metric.labels.queue: foo-queue
      # aim for one pod per message in the queue
      targetaveragevalue: 1


will cause the hpa to try keeping one pod around for every message in your queue (with a max of 10 pods).

as an aside, targeting one pod per message is probably going to cause you to start and stop pods constantly. if you end up starting a ton of pods and process all of the messages in the queue, kubernetes will scale your pods down to 1. depending on how long it takes to start your pods and how long it takes to process your messages, you may have lower average message latency by specifying a higher targetaveragevalue. ideally, given a constant amount of traffic, you should aim to have a constant number of pods processing messages (which requires you to process messages at about the same rate that they are enqueued).
"
67985163,kubernetes dashboard not accessible when providing path in ingress,"i have deployed minikube on windows vm and the minikube vm is created on virtualbox with the host-only ip.
i have deployed the kubernetes dashboard with nodeport ip so i can access it from outside the cluster. the svc is as follows:
ps c:\users\xxx\desktop\ingress&gt; kubectl get svc -n kubernetes-dashboard
name                        type        cluster-ip      external-ip   port(s)         age
dashboard-metrics-scraper   clusterip   10.111.167.61   &lt;none&gt;        8000/tcp        5d20h
kubernetes-dashboard        nodeport    10.111.220.57   &lt;none&gt;        443:30613/tcp   5d20h

with the help of the minikube ingress addon, i installed the ingress controller which is of nginx. its svc details are as follows:
ps c:\users\xxx\desktop\ingress&gt; kubectl get svc -n ingress-nginx
name                                 type        cluster-ip    external-ip   port(s)                      age
ingress-nginx-controller             nodeport    10.98.29.41   &lt;none&gt;        80:32628/tcp,443:31194/tcp   5d20h
ingress-nginx-controller-admission   clusterip   10.96.35.36   &lt;none&gt;        443/tcp                      5d20h

then i have created an ingress rule for my dashboard application as follows:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: dashboard-ingress
  namespace: kubernetes-dashboard
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/add-base-url: &quot;true&quot;
    nginx.ingress.kubernetes.io/backend-protocol: &quot;https&quot;
    nginx.ingress.kubernetes.io/secure-backends: &quot;true&quot;
    nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
    ingress.kubernetes.io/configuration-snippet: |
      rewrite ^(/dashboard)$ $1/ permanent;
spec:
  rules:
  - host: k8s.dashboard.com
    http:
      paths:
      - path: /dashboard
        pathtype: prefix
        backend:
          service:
            name: kubernetes-dashboard
            port:
              number: 443

but now when i am trying to access the dashboard with the following url https://k8s.dashboard.com/dashboard then i am facing the error of 404 not found. i also tried multiple url to access the dashboard such as :
https://k8s.dashboard.com:30613/dashboard
http://k8s.dashboard.com:30613/dashboard
https://k8s.dashboard.com/dashboard

but this url is working for me: https://k8s.dashboard.com:30613
i have added the minikube ip to hosts files in the windows machine.
ingress rule describe the output is as follows:
ps c:\users\xxx\desktop\ingress&gt; kubectl describe ingress -n kubernetes-dashboard
name:             dashboard-ingress
namespace:        kubernetes-dashboard
address:          192.168.56.100
default backend:  default-http-backend:80 (&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;)
rules:
  host               path  backends
  ----               ----  --------
  k8s.dashboard.com
                     /dashboard   kubernetes-dashboard:443 (172.17.0.4:8443)
annotations:         ingress.kubernetes.io/configuration-snippet: rewrite ^(/dashboard)$ $1/ permanent;
                     kubernetes.io/ingress.class: nginx
                     nginx.ingress.kubernetes.io/add-base-url: true
                     nginx.ingress.kubernetes.io/backend-protocol: https
                     nginx.ingress.kubernetes.io/force-ssl-redirect: false
                     nginx.ingress.kubernetes.io/rewrite-target: /
                     nginx.ingress.kubernetes.io/secure-backends: true
events:
  type    reason  age                   from                      message
  ----    ------  ----                  ----                      -------
  normal  sync    26m (x16 over 5d20h)  nginx-ingress-controller  scheduled for sync

any help regarding this really helps. thanks
edited
my ingress controller logs are as follows:
192.168.56.1 - - [16/jun/2021:06:57:00 +0000] &quot;get /dashboard http/2.0&quot; 200 746 &quot;-&quot; &quot;mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/91.0.4472.77 safari/537.36&quot; 418 0.019 [kubernetes-dashboard-kubernetes-dashboard-443] [] 172.17.0.4:8443 746 0.018 200 1a2793052f70031c6c9fa59b0d4374d1
192.168.56.1 - - [16/jun/2021:06:57:00 +0000] &quot;get /styles.aa1f928b22a88c391404.css http/2.0&quot; 404 548 &quot;https://k8s.dashboard.com/dashboard&quot; &quot;mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/91.0.4472.77 safari/537.36&quot; 101 0.002 [upstream-default-backend] [] 127.0.0.1:8181 548 0.002 404 1974258442f8b4c46d8badd1dda3e3f5
192.168.56.1 - - [16/jun/2021:06:57:00 +0000] &quot;get /runtime.2a456dd93bf6c4890676.js http/2.0&quot; 404 548 &quot;https://k8s.dashboard.com/dashboard&quot; &quot;mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/91.0.4472.77 safari/537.36&quot; 49 0.008 [upstream-default-backend] [] 127.0.0.1:8181 548 0.007 404 96c17c52e6337f29dd8b2b2b68b088ac
192.168.56.1 - - [16/jun/2021:06:57:00 +0000] &quot;get /polyfills.f4f05ad675be9638106e.js http/2.0&quot; 404 548 &quot;https://k8s.dashboard.com/dashboard&quot; &quot;mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/91.0.4472.77 safari/537.36&quot; 40 0.008 [upstream-default-backend] [] 127.0.0.1:8181 548 0.007 404 096ae29cb168523aa9191f27a967e47a
192.168.56.1 - - [16/jun/2021:06:57:00 +0000] &quot;get /scripts.128068f897fc721c4673.js http/2.0&quot; 404 548 &quot;https://k8s.dashboard.com/dashboard&quot; &quot;mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/91.0.4472.77 safari/537.36&quot; 38 0.008 [upstream-default-backend] [] 127.0.0.1:8181 548 0.007 404 728f73f75276167b387dc87a69b65a72
192.168.56.1 - - [16/jun/2021:06:57:00 +0000] &quot;get /en.main.09bf52db2dbc808e7279.js http/2.0&quot; 404 548 &quot;https://k8s.dashboard.com/dashboard&quot; &quot;mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/91.0.4472.77 safari/537.36&quot; 38 0.014 [upstream-default-backend] [] 127.0.0.1:8181 548 0.014 404 b11e5ae324a828508d488816306399c2

and this is dashboard logs
172.17.0.1 - - [16/jun/2021:06:59:46 +0000] &quot;get / http/1.1&quot; 200 6 &quot;&quot; &quot;kube-probe/1.20&quot;
172.17.0.1 - - [16/jun/2021:06:59:56 +0000] &quot;get / http/1.1&quot; 200 6 &quot;&quot; &quot;kube-probe/1.20&quot;
172.17.0.1 - - [16/jun/2021:07:00:00 +0000] &quot;get /healthz http/1.1&quot; 200 13 &quot;&quot; &quot;dashboard/v2.2.0&quot;
172.17.0.1 - - [16/jun/2021:07:00:06 +0000] &quot;get / http/1.1&quot; 200 6 &quot;&quot; &quot;kube-probe/1.20&quot;
172.17.0.1 - - [16/jun/2021:07:00:16 +0000] &quot;get / http/1.1&quot; 200 6 &quot;&quot; &quot;kube-probe/1.20&quot;
172.17.0.1 - - [16/jun/2021:07:00:26 +0000] &quot;get / http/1.1&quot; 200 6 &quot;&quot; &quot;kube-probe/1.20&quot;
172.17.0.1 - - [16/jun/2021:07:00:30 +0000] &quot;get /healthz http/1.1&quot; 200 13 &quot;&quot; &quot;dashboard/v2.2.0&quot;
172.17.0.1 - - [16/jun/2021:07:00:36 +0000] &quot;get / http/1.1&quot; 200 6 &quot;&quot; &quot;kube-probe/1.20&quot;
{&quot;level&quot;:&quot;error&quot;,&quot;msg&quot;:&quot;error scraping node metrics: the server could not find the requested resource (get nodes.metrics.k8s.io)&quot;,&quot;time&quot;:&quot;2021-06-16t07:00:41z&quot;}

",<kubernetes><kubernetes-ingress><minikube>,67999266,5,"according to this issue this is a limitation/bug of the kubernetes dashboard.
they suggest using this config as a workaround:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: kubernetes-dashboard
  labels:
    app.kubernetes.io/name: kubernetes-dashboard  
  annotations:
    kubernetes.io/ingress.class: nginx
    # add https backend protocol support for ingress-nginx
    nginx.ingress.kubernetes.io/backend-protocol: &quot;https&quot;
    nginx.ingress.kubernetes.io/configuration-snippet: |
      proxy_set_header accept-encoding &quot;&quot;;
      sub_filter '&lt;base href=&quot;/&quot;&gt;' '&lt;base href=&quot;/dashboard/&quot;&gt;';
      sub_filter_once on;
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  rules:
    - host: my.example.com
      http:
        paths:
          - path: /dashboard(/|$)(.*)
            backend:
              servicename: kubernetes-dashboard
              serviceport: 443

"
68150171,istio - redirect request to external url,"i'm trying to set up a proxy service in the kubernetes cluster using istio. i have created two different domains. if the domain is foo.com it should be redirected to an external url else it should be routed to an app server. i have configured this using virtual service and service entry. but when i hit foo.com it is skipping the authorization header. i need an authorization header to process the request. is there any way to fix this issue? thanks in advance.
virtualservice.yaml
apiversion: networking.istio.io/v1beta1
kind: serviceentry
metadata:
  name: external-svc-https
spec:
  hosts:
  - foo.com
  location: mesh_external
  ports:
  - number: 443
    name: https
    protocol: tls
  resolution: dns
---
kind: virtualservice
apiversion: networking.istio.io/v1alpha3
metadata:
  name: redirect
  namespace: default
  labels:
    app: foo
    env: staging
spec:
  hosts:
    - foo.com
  gateways:
    - istio-system/gateway
  http:
    - match:
        - uri:
            prefix: /
      redirect:
        authority: bar.com

",<kubernetes><google-kubernetes-engine><istio><istio-gateway>,68152336,5,"if to redirect when foo.com domain get hit
apiversion: networking.istio.io/v1alpha3
kind: serviceentry
metadata:
  name: github
spec:
  hosts:
  - &quot;raw.githubusercontent.com&quot;
  location: mesh_external
  ports:
  - number: 443
    name: https
    protocol: tls
  resolution: dns

and
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: webserver
spec:
  hosts:
  - foo.com
  http:
  - match:
    - uri:
        regex: &quot;.*&quot;
    rewrite:
      uri: &quot;/mcasperson/nodejsproxy/master/externalservice1.txt&quot;
      authority: raw.githubusercontent.com
    route:
    - destination:
        host: raw.githubusercontent.com
        port:
          number: 443

rule
apiversion: networking.istio.io/v1alpha3
kind: destinationrule
metadata:
  name: github
spec:
  host: &quot;raw.githubusercontent.com&quot;
  trafficpolicy:
    tls:
      mode: simple

read more at : https://octopus.com/blog/istio/istio-serviceentry
"
69586297,issue with self-signed certificate with cert-manager in kubernetes,"i'm trying to add a self-signed certificate in my aks cluster using cert-manager.
i created a clusterissuer for the ca certificate (to sign the certificate) and a second clusterissuer for the certificate (self-signed) i want to use.
i am not sure if the certificate2 is being used correctly by ingress as it looks like it is waiting for some event.
am i following the correct way to do this?
this is the first clusterissuer &quot;clusterissuer.yml&quot;:
apiversion: cert-manager.io/v1alpha2
kind: clusterissuer
metadata:
  name: selfsigned
spec:
  selfsigned: {} 

this is the ca certificate &quot;certificate.yml&quot;:
apiversion: cert-manager.io/v1alpha2
kind: certificate
metadata:
  name: selfsigned-certificate
spec:
  secretname: hello-deployment-tls-ca-key-pair
  dnsnames:
  - &quot;*.default.svc.cluster.local&quot;
  - &quot;*.default.com&quot;
  isca: true
  issuerref:
    name: selfsigned
    kind: clusterissuer

this is the second clusterissuer &quot;clusterissuer2.yml&quot; for the certificate i want to use:
apiversion: cert-manager.io/v1alpha2
kind: clusterissuer
metadata:
 name: hello-deployment-tls
spec:
 ca:
   secretname: hello-deployment-tls-ca-key-pair

and finally this is the self-signed certificate &quot;certificate2.yml&quot;:
apiversion: cert-manager.io/v1alpha2
kind: certificate
metadata:
  name: selfsigned-certificate2
spec:
  secretname: hello-deployment-tls-ca-key-pair2
  dnsnames:
  - &quot;*.default.svc.cluster.local&quot;
  - &quot;*.default.com&quot;
  isca: false
  issuerref:
    name: hello-deployment-tls
    kind: clusterissuer

i am using this certificate in an ingress:
--- 
apiversion: extensions/v1beta1
kind: ingress
metadata: 
  annotations: 
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: &quot;hello-deployment-tls&quot;
    nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
  name: sonar-ingress
spec: 
  tls: 
  - secretname: &quot;hello-deployment-tls-ca-key-pair2&quot;
  rules: 
  - http: 
      paths: 
      - pathtype: prefix
        path: &quot;/&quot;
        backend: 
          servicename: sonarqube
          serviceport: 80


as i do not have any registered domain name i just want to use the public ip to access the service over https://&lt;public_ip&gt;.
when i access to the service https://&lt;public_ip&gt; i can see that &quot;kubernetes ingress controller fake certificate&quot; so i guess this is because the certificate is not globally recognize by the browser.
the strange thing is here. theoretically the ingress deployment is using the selfsigned-certificate2 but looks like it is not ready:
kubectl get certificate
name                      ready   secret                              age
selfsigned-certificate    true    hello-deployment-tls-ca-key-pair    4h29m
selfsigned-certificate2   false   hello-deployment-tls-ca-key-pair2   3h3m
selfsigned-secret         true    selfsigned-secret                   5h25m

kubectl describe certificate selfsigned-certificate2
. 
.
.
spec:
  dns names:
    *.default.svc.cluster.local
    *.default.com
  issuer ref:
    kind:       clusterissuer
    name:       hello-deployment-tls
  secret name:  hello-deployment-tls-ca-key-pair2
status:
  conditions:
    last transition time:  2021-10-15t11:16:15z
    message:               waiting for certificaterequest &quot;selfsigned-certificate2-3983093525&quot; to complete
    reason:                inprogress
    status:                false
    type:                  ready
events:                    &lt;none&gt;

any idea?
thank you in advance.
",<kubernetes><ssl-certificate><kubernetes-ingress><azure-aks><cert-manager>,69617656,5,"apiversions
first i noticed you're using v1alpha2 apiversion which is depricated and will be removed in 1.6 cert-manager:
$ kubectl apply -f cluster-alpha.yaml
warning: cert-manager.io/v1alpha2 clusterissuer is deprecated in v1.4+, unavailable in v1.6+; use cert-manager.io/v1 clusterissuer

i used apiversion: cert-manager.io/v1 in reproduction.
same for v1beta1 ingress, consider updating it to networking.k8s.io/v1.
what happens
i started reproducing your setup step by step.
i applied clusterissuer.yaml:
$ kubectl apply -f clusterissuer.yaml
clusterissuer.cert-manager.io/selfsigned created

$ kubectl get clusterissuer
name         ready   age
selfsigned   true    11s

pay attention that ready is set to true.
next i applied certificate.yaml:
$ kubectl apply -f cert.yaml
certificate.cert-manager.io/selfsigned-certificate created

$ kubectl get cert
name                     ready   secret                             age
selfsigned-certificate   true    hello-deployment-tls-ca-key-pair   7s

next step is to add the second clusterissuer which is referenced to hello-deployment-tls-ca-key-pair secret:
$ kubectl apply -f clusterissuer2.yaml
clusterissuer.cert-manager.io/hello-deployment-tls created

$ kubectl get clusterissuer
name                   ready   age
hello-deployment-tls   false   6s
selfsigned             true    3m50

clusterissuer hello-deployment-tls is not ready. here's why:
$ kubectl describe clusterissuer hello-deployment-tls
...
events:
  type     reason         age                from          message
  ----     ------         ----               ----          -------
  warning  errgetkeypair  10s (x5 over 75s)  cert-manager  error getting keypair for ca issuer: secret &quot;hello-deployment-tls-ca-key-pair&quot; not found
  warning  errinitissuer  10s (x5 over 75s)  cert-manager  error initializing issuer: secret &quot;hello-deployment-tls-ca-key-pair&quot; not found

this is expected behaviour since:

when referencing a secret resource in clusterissuer resources (eg
apikeysecretref) the secret needs to be in the same namespace as the
cert-manager controller pod. you can optionally override this by using
the --cluster-resource-namespace argument to the controller.

reference
answer - how to move forward
i edited the cert-manager deployment so it will look for secrets in default namespace (this is not ideal, i'd use issuer instead in default namespace):
$ kubectl edit deploy cert-manager -n cert-manager

spec:
  containers:
  - args:
    - --v=2
    - --cluster-resource-namespace=default

it takes about a minute for cert-manager to start. redeployed clusterissuer2.yaml:
$ kubectl delete -f clusterissuer2.yaml
clusterissuer.cert-manager.io &quot;hello-deployment-tls&quot; deleted

$ kubectl apply -f clusterissuer2.yaml
clusterissuer.cert-manager.io/hello-deployment-tls created

$ kubectl get clusterissuer
name                   ready   age
hello-deployment-tls   true    3s
selfsigned             true    5m42s

both are ready. moving forward with certificate2.yaml:
$ kubectl apply -f cert2.yaml
certificate.cert-manager.io/selfsigned-certificate2 created

$ kubectl get cert
name                      ready   secret                              age
selfsigned-certificate    true    hello-deployment-tls-ca-key-pair    33s
selfsigned-certificate2   true    hello-deployment-tls-ca-key-pair2   6s

$ kubectl get certificaterequest
name                            approved   denied   ready   issuer                 requestor                                         age
selfsigned-certificate-jj98f    true                true    selfsigned             system:serviceaccount:cert-manager:cert-manager   52s
selfsigned-certificate2-jwq5c   true                true    hello-deployment-tls   system:serviceaccount:cert-manager:cert-manager   25s

ingress
when host is not added to ingress, it doesn't create any certificates and seems to used some fake one from ingress which is issued by cn = kubernetes ingress controller fake certificate.
events from ingress:
events:
  type     reason     age   from                      message
  ----     ------     ----  ----                      -------
  warning  badconfig  5s    cert-manager              tls entry 0 is invalid: secret &quot;example-cert&quot; for ingress tls has no hosts specified

when i added dns to ingress:
events:
  type     reason             age                from                      message
  ----     ------             ----               ----                      -------
normal   createcertificate  4s                 cert-manager              successfully created certificate &quot;example-cert&quot;

answer, part 2 (about ingress, certificates and issuer)
you don't need to create a certificate if you're referencing to issuer in ingress rule. ingress will issue certificate for you when all details are presented, such as:

annotation cert-manager.io/cluster-issuer: &quot;hello-deployment-tls&quot;
spec.tls part with host within
spec.rules.host

or
if you want to create certificate manually and ask ingress to use it, then:

remove annotation cert-manager.io/cluster-issuer: &quot;hello-deployment-tls&quot;
create certificate manually
refer to it in ingress rule.

you can check certificate details in browser and find that it no longer has issuer as cn = kubernetes ingress controller fake certificate, in my case it's empty.
note - cert-manager v1.4
initially i used a bit outdated cert-manager v1.4 and got this issue which has gone after updating to 1.4.1.
it looks like:
$ kubectl describe certificaterequest selfsigned-certificate2-45k2c

events:
  type     reason           age   from          message
  ----     ------           ----  ----          -------
  normal   cert-manager.io  41s   cert-manager  certificate request has been approved by cert-manager.io
  warning  decodeerror      41s   cert-manager  failed to decode returned certificate: error decoding certificate pem block

useful links:

setting up self-singed issuer
setting up ca issuers
cluster issuers

"
68060456,how do i create a kubernetes custom resource using javascript client,"my custom definition
apiversion: something.com/v1alpha1
 kind: mykind
 metadata:
   name: test
 spec:
   size: 1
   image: myimage
   

here is an answer that shows how to create a deployment using a javascript client. however, i need to create a custom resource using a javascript client
",<javascript><node.js><kubernetes><kubernetes-apiserver><kubernetes-custom-resources>,68077770,5,"const k8s = require('@kubernetes/client-node')
const kc = new k8s.kubeconfig();
kc.loadfromdefault();
const k8sclient = kc.makeapiclient(k8s.customobjectsapi);

var body = {
    &quot;apiversion&quot;: &quot;something.com/v1alpha1&quot;,
    &quot;kind&quot;: &quot;mykind&quot;,
    &quot;metadata&quot;: {
        &quot;name&quot;: &quot;mycustomobject&quot;,
    },
    &quot;spec&quot;: {
        &quot;size&quot;: &quot;1&quot;,
        &quot;image&quot;: &quot;myimage&quot;
    }
}

k8sclient.createnamespacedcustomobject('something.com','v1alpha1','default','mykinds', body)
    .then((res)=&gt;{
        console.log(res)
    })
    .catch((err)=&gt;{
        console.log(err)
    })

"
68274671,pod affinity settings is not working properly in my deployment,"i try to deploy 2 replicas on k3s - each one to a different node. according to documentation, it should be pretty easy. however, i must be doing some silly mistake i am not able to find. when i apply the deploy file, both of my pods are running the same node (node1). in case i switch that node off, these pods start on another 2 nodes (node2, node3). when i start the node1 back and redeploy the app, it runs again on the same node1.
if somebody can advise please, what i have wrong in my configuration, i would be really grateful.
(i run fresh new k3s on 3 servers with the same hw configuration)
apiversion: apps/v1
kind: deployment
metadata:
  name: tbd-node-js-runner
  namespace: default
spec:
  replicas: 2
  selector:
    matchlabels:
      app: tbd-node-js-runner
  template:
    metadata:
      labels:
        app: tbd-node-js-runner
    spec:
      affinity:
        podantiaffinity:
          requiredduringschedulingignoredduringexecution:
          - labelselector:
              matchexpressions:
              - key: app
                operator: in
                values:
                - tbd-node-js-runner
            topologykey: topology.kubernetes.io/hostname
      containers:
        - name: tbd-node-js-runner
          image: tabidoo/nodejs-runner:latest
          ports:
          - containerport: 
          env:
          - name: connection_string
            value: &quot;...&quot;
            ...
          imagepullpolicy: always
      imagepullsecrets:
        - name: regcred

",<kubernetes><kubernetes-pod><affinity>,68276317,5,"
it is due to incorrect topologykey , it should be 'kubernetes.io/hostname' not 'topology.kubernetes.io/hostname' .

so it would be as following :


apiversion: apps/v1
kind: deployment
metadata:
  name: tbd-node-js-runner
  namespace: default
spec:
  replicas: 2
  selector:
    matchlabels:
      app: tbd-node-js-runner
  template:
    metadata:
      labels:
        app: tbd-node-js-runner
    spec:
      affinity:
        podantiaffinity:
          requiredduringschedulingignoredduringexecution:
          - labelselector:
              matchexpressions:
              - key: app
                operator: in
                values:
                - tbd-node-js-runner
            topologykey: &quot;kubernetes.io/hostname&quot;
      containers:
        - name: tbd-node-js-runner
          image: tabidoo/nodejs-runner:latest

"
55347770,nginx.ingress.kubernetes.io/proxy-body-size not working,"i want to increase size of post body of each request in ingress. so i add the

nginx.ingress.kubernetes.io/proxy-body-size: 8m


in yaml file ingress(in view/edit yaml file of rancher) but it doesnt work. when i get the describe of ingress with kubectl i dont see the added annotation but i see the new added mapping.
hereis the configs:

yaml file:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    ingress.kubernetes.io/configuration-snippet: |-
      set $test_host ""testdms.test.com""
      if ($host == $test_host) {
        return 301 $scheme://$test_host/webui/;
      }
    nginx.ingress.kubernetes.io/proxy-body-size: 8m
  creationtimestamp: 2018-09-11t12:19:02z
  generation: 116
  name: test-dms
  namespace: test-dms
  resourceversion: ""95490045""
  selflink: /apis/extensions/v1beta1/namespaces/test-dms/ingresses/test-dms
  uid: de7c4c1b-b5bc-11e8-84c0-005056bf6431
spec:
  rules:
  - host: testdms.test.com
    http:
      paths:
      - backend:
          servicename: ingress-e5a45b0dc688c653b79d4b5942ebbe7c
          serviceport: 80
        path: /test
status:
  loadbalancer:
    ingress:
    - {}
    - ip: 198.100.101.171
    - ip: 198.100.101.172
    - ip: 198.100.101.173
    - ip: 198.100.101.61


describe result:

annotations:
  configuration-snippet:  set $test_host ""testdms.test.com""
if ($host == $test_host) {
  return 301 $scheme://$test_host/webui/;
}
events:
  type    reason  age                       from                      message
  ----    ------  ----                      ----                      -------
  normal  update  36s (x38 over 2h)         nginx-ingress-controller  ingress test-dms/test-dms
  normal  update  21s (x47 over 23d)        nginx-ingress-controller  ingress test-dms/test-dms
  normal  update  &lt;invalid&gt; (x47 over 23d)  nginx-ingress-controller  ingress test-dms/test-dms
  normal  update  &lt;invalid&gt; (x84 over 64d)  nginx-ingress-controller  ingress test-dms/test-dms
  normal  update  &lt;invalid&gt; (x39 over 12d)  nginx-ingress-controller  ingress test-dms/test-dms

",<nginx><kubernetes><kubernetes-ingress><rancher>,55868546,5,"amendment of the ingress objects in k8s sometimes misbehave, so it's recommended to re-create rather than edit. 

if it still didn't work, try to set this value globally for all ingress rules using a configmap

    apiversion: v1
    kind: configmap
    metadata:
      name: nginx
      namespace: ingress-nginx
      labels:
        app: ingress-nginx
    data:
      proxy-body-size: ""8m""

"
55630746,kubernetes ingress configuration rewrite problem,"i'm creating a configuration to host some apps in a kubernetes cluster on aws. i have two different apps, with separate service/pod/selector but i want to expose them with a single ingress for the moment.

so i created the following ingress controller

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /foo
        backend:
          servicename: foo
          serviceport: 8080
      - path: /bar
        backend:
          servicename: bar
          serviceport: 8080 


and the ingress obtain the elb from aws without any problem, but when i try to browse the app (java application using tomcat appserver) i always receive the following page



it's the classic old tomcat welcome page but every request always returns the index.html (no css/img loaded) and also if i try to use the correct context path for the application i receive this page.

if i expose the apps using a service (loadbalancer) i can use it without these problems, so i think there is something wrong with ingress configuration.

any ideas?



update

if i use an ingress with a single path like this

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          servicename: foo
          serviceport: 8080


using ingresshost url i can see the tomcat home with img/css and if i browse to ingresshost/appcontext i can use the app without problem
",<tomcat><kubernetes><kubernetes-ingress><nginx-ingress><amazon-eks>,55638499,5,"if you have recently changed the version of your nginx-ingress controller then maybe the cause can be a recent change done to it. now it uses regex rewrite rules and maybe your rewrite target is just always being rewritten to ""/"". i think the changes were introduced in version 0.22 in january.

the new correct syntax for your ingress would be:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
  - http:
      paths:
      - path: /foo(.*)
        backend:
          servicename: foo
          serviceport: 8080
      - path: /bar(.*)
        backend:
          servicename: bar
          serviceport: 8080 

"
61097712,passing values from initcontainers to container spec,"i have a kubernetes deployment with the below spec that gets installed via helm 3.

apiversion: apps/v1
kind: deployment
metadata:
  name: gatekeeper
spec:
  replicas: 1
  template:
    spec:
      containers:
        - name: gatekeeper
          image: my-gatekeeper-image:some-sha
          args:
            - --listen=0.0.0.0:80
            - --client-id=gk-client
            - --discovery-url={{ .values.discoveryurl }}


i need to pass the discoveryurl value as a helm value, which is the public ip address of the nginx-ingress pod that i deploy via a different helm chart. i install the above deployment like below:

helm3 install my-nginx-ingress-chart
ingress_ip=$(kubectl get svc -lapp=nginx-ingress -o=jsonpath='{.items[].status.loadbalancer.ingress[].ip}')
helm3 install my-gatekeeper-chart --set discovery_url=${ingress_ip}


this works fine, however, now instead of these two helm3 install, i want to have a single helm3 install, where both the nginx-ingress and the gatekeeper deployment should be created.

i understand that in the initcontainer of my-gatekeeper-image we can get the nginx-ingress ip address, but i am not able to understand how to set that as an environment variable or pass to the container spec.

there are some stackoverflow questions that mention that we can create a persistent volume or secret to achieve this, but i am not sure, how that would work if we have to delete them. i do not want to create any extra objects and maintain the lifecycle of them.
",<kubernetes><kubernetes-helm>,61264376,5,"it is not possible to do this without mounting a persistent volume. but the creation of persistent volume can be backed by just an in-memory store, instead of a block storage device. that way, we do not have to do any extra lifecycle management. the way to achieve that is:

apiversion: v1
kind: configmap
metadata:
  name: gatekeeper
data:
  gatekeeper.sh: |-
    #!/usr/bin/env bash
    set -e

    ingress_ip=$(kubectl get svc -lapp=nginx-ingress -o=jsonpath='{.items[].status.loadbalancer.ingress[].name}')
    # do other validations/cleanup
    echo $ingress_ip &gt; /opt/gkconf/discovery_url;
    exit 0

---
apiversion: apps/v1
kind: deployment
metadata:
  name: gatekeeper
  labels:
    app: gatekeeper
spec:
  replicas: 1
  selector:
    matchlabels:
      app: gatekeeper
  template:
    metadata:
      name: gatekeeper
      labels:
        app: gatekeeper
    spec:
      initcontainers:
        - name: gkinit
          command: [ ""/opt/gk-init.sh"" ]
          image: 'bitnami/kubectl:1.12'
          volumemounts:
            - mountpath: /opt/gkconf
              name: gkconf
            - mountpath: /opt/gk-init.sh
              name: gatekeeper
              subpath: gatekeeper.sh
              readonly: false
      containers:
        - name: gatekeeper
          image: my-gatekeeper-image:some-sha
          # entrypoint of above image should read the
          # file /opt/gkconf/discovery_url and then launch
          # the actual gatekeeper binary
          imagepullpolicy: always
          ports:
            - containerport: 80
              protocol: tcp
          volumemounts:
            - mountpath: /opt/gkconf
              name: gkconf
      volumes:
        - name: gkconf
          emptydir:
            medium: memory
        - name: gatekeeper
          configmap:
            name: gatekeeper
            defaultmode: 0555

"
61542804,kubernetes health checks fail with custom nginx webserver configuration,"my health checks fail with the following setup.

nginx.conf

user                            root;
worker_processes                auto;

error_log                       /var/log/nginx/error.log warn;

events {
    worker_connections          1024;
}

http {
    server {
        listen                  80;
        server_name             subdomain.domain.com
        auth_basic              ""restricted"";
        auth_basic_user_file    /etc/nginx/.htpasswd;
    }
    server {
        listen                  80;
        auth_basic              off;
    }
    server {
        listen                  2222;
        auth_basic              off;
        location /healthz {
            return 200;
        }
    }
}


dockerfile

from nginx:alpine
copy index.html /usr/share/nginx/html/index.html
volume /usr/share/nginx/html
copy /server/nginx.conf /etc/nginx/
copy /server/htpasswd /etc/nginx/.htpasswd
cmd [""nginx"", ""-g"", ""daemon off;""]
expose 80
expose 2222


deployment.yaml

apiversion: apps/v1
kind: deployment
metadata:
  name: my-app
  namespace: my-namespace
  labels:
    app: my-app
spec:
  replicas: 1
  selector:
    matchlabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-app
          image: gcr.io/google_cloud_project/my-app
          ports:
            - containerport: 80
            - containerport: 2222
          livenessprobe:
            httpget:
              path: /healthz
              port: 2222
          readinessprobe:
            httpget:
              path: /healthz
              port: 2222


it definitely works when i delete the ""server_name"" row in nginx.conf and delete the second server block.
could this be an issue with ingress/load balancer, since i do not know how long it takes to update (i experienced a healthy pod go unhealthy after a few minutes yesterday). running it on google kubernetes engine (gke) with google's own ingress controller (not nginx ingress!)

what am i doing wrong?
",<docker><nginx><kubernetes><google-kubernetes-engine><basic-authentication>,61595472,5,"the issue was that gke's load balancer does its own health checks. these look at / by default and expect a 200 in return. only when health checks in the deployment/pod have another path declared, the load balancer health check will pick up those paths.

the load balancer is provisioned after ingress yaml is applied. any changes in the deployment or ingress that affect the load balancer will not be accepted as long as the load balancer runs. this means i had to delete the load balancer first and then apply the deployment, service and ingress yamls (ingress automatically sets up the load balancer then). instead of deleting the load balancer one can enter the correct path manually (and wait a few minutes).

since it seems the load balancer does health checks on each open port, i deleted my 2222 port and added location /healthz to each server block with port 80 in nginx with auth_basic off.

see: https://cloud.google.com/load-balancing/docs/health-check-concepts and https://stackoverflow.com/a/61222826/2534357 and https://stackoverflow.com/a/38511357/2534357

new nginx.conf

user                            root;
worker_processes                auto;

error_log                       /var/log/nginx/error.log warn;

events {
    worker_connections          1024;
}

http {
    server {
        listen                  80;
        server_name             subdomain1.domain.com;
        root                    /usr/share/nginx/html;
        index                   index.html;
        auth_basic              ""restricted"";
        auth_basic_user_file    /etc/nginx/.htpasswd_subdomain1;
        location /healthz {
            auth_basic          off;
            allow               all;
            return              200;
        }
    }
    server {
        listen                  80;
        server_name             subdomain2.domain.com;
        root                    /usr/share/nginx/html;
        index                   index.html;
        auth_basic              ""restricted"";
        auth_basic_user_file    /etc/nginx/.htpasswd_subdomain2;
        location /healthz {
            auth_basic          off;
            allow               all;
            return              200;
        }
    }
    server {
        listen                  80;
        server_name             domain.com www.domain.com;
        root                    /usr/share/nginx/html;
        index                   index.html;
        auth_basic              ""restricted"";
        auth_basic_user_file    /etc/nginx/.htpasswd_domain;
        location /healthz {
            auth_basic          off;
            allow               all;
            return              200;
        }
    }
    ## next block probably not necessary
    server {
        listen                  80;
        auth_basic              off;
        location /healthz {
            return              200;
        }
    }
}


my new deployment.yaml

apiversion: apps/v1
kind: deployment
metadata:
  name: my-app
  namespace: my-namespace
  labels:
    app: my-app
spec:
  replicas: 1
  selector:
    matchlabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-app
          image: gcr.io/google_cloud_project/my-app
          ports:
            - containerport: 80
          livenessprobe:
            httpget:
              path: /healthz
              port: 80
          readinessprobe:
            httpget:
              path: /healthz
              port: 80

"
57494369,kubectl apply deployment to specified node group - aws eks,"i have created multiple stacks (node groups) within my eks cluster, and each group runs on a different instance type (for example, one group runs on gpu instances). i have added an entry in maproles of aws-auth-cm.yaml file for each of the node groups. now i would like to deploy some deployments on another. the deployment files look something like this:

apiversion: apps/v1
kind: deployment
metadata:
  name: deployment-1
spec:
  replicas: 1
  selector:
    matchlabels:
      component: component-1
  template:
    metadata:
      labels:
        component: component-1
    spec:
      containers:
        - name: d1
          image: docker-container
          ports:
            - containerport: 83


the documentation shows that i can run the standard command kubectl apply. is there any way to specify the group? maybe something like


  kubectl apply -f server-deployment.yaml -group node-group-1

",<amazon-web-services><kubernetes><amazon-eks>,57494603,5,"sadly something that you mentioned doesn't exist, but you can read about affinity and it should solve your problem.
tl;dr you have to add labels or use existing labels on nodes and use these labels to assign pods to correct nodes.
assuming you have label beta.kubernetes.io/instance-type=highmem
apiversion: apps/v1
kind: deployment
metadata:
  name: deployment-1
spec:
  replicas: 1
  selector:
    matchlabels:
      component: component-1
  template:
    metadata:
      labels:
        component: component-1
    spec:
      affinity:
        nodeaffinity:
          requiredduringschedulingignoredduringexecution:
            nodeselectorterms:
            - matchexpressions:
              - key: beta.kubernetes.io/instance-typ
                operator: in
                values:
                - highmem
      containers:
        - name: d1
          image: docker-container
          ports:
            - containerport: 83

"
59448042,kubernetes hpa deployment cannot find target resource,"i'm trying to deploy my own hpa without a success.
although while trying to deploy the official php of kubernetes, it worked as planned.
when i tryied to deploy my own test deployment with the hpa, it didn't work.
compare 2 hpa deployments processes - kubernetes official deployment vs my test deployment:
deploy the official kubernetes template image:
$ kubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80
service &quot;php-apache&quot; created
deployment &quot;php-apache&quot; created

my own test deployment result
{
   &quot;apiversion&quot;: &quot;autoscaling/v1&quot;,
   &quot;kind&quot;: &quot;horizontalpodautoscaler&quot;,
   &quot;metadata&quot;: {
       &quot;annotations&quot;: {
           &quot;autoscaling.alpha.kubernetes.io/conditions&quot;: &quot;[{\&quot;type\&quot;:\&quot;abletoscale\&quot;,\&quot;status\&quot;:\&quot;false\&quot;,\&quot;lasttransitiontime\&quot;:\&quot;2019-12-22t20:39:59z\&quot;,\&quot;reason\&quot;:\&quot;failedgetscale\&quot;,\&quot;message\&quot;:\&quot;the hpa controller was unable to get the target's current scale: deployments/scale.apps \\\&quot;gw-autoscale-t6\\\&quot; not found\&quot;}]&quot;
       },
       &quot;creationtimestamp&quot;: &quot;2019-12-22t20:39:44z&quot;,
       &quot;labels&quot;: {
           &quot;app&quot;: &quot;gw-autoscale-t6&quot;
       },
       &quot;name&quot;: &quot;gw-autoscale-t6&quot;,
       &quot;namespace&quot;: &quot;dev&quot;,
       &quot;resourceversion&quot;: &quot;17299134&quot;,
       &quot;selflink&quot;: &quot;/apis/autoscaling/v1/namespaces/dev/horizontalpodautoscalers/gw-autoscale-t6&quot;,
       &quot;uid&quot;: &quot;2f7e014c-24fb-11ea-a4d8-a28620329da6&quot;
   },
   &quot;spec&quot;: {
       &quot;maxreplicas&quot;: 3,
       &quot;minreplicas&quot;: 1,
       &quot;scaletargetref&quot;: {
           &quot;apiversion&quot;: &quot;apps/v1&quot;,
           &quot;kind&quot;: &quot;deployment&quot;,
           &quot;name&quot;: &quot;gw-autoscale-t6&quot;
       },
       &quot;targetcpuutilizationpercentage&quot;: 80
   },
   &quot;status&quot;: {
       &quot;currentreplicas&quot;: 0,
       &quot;desiredreplicas&quot;: 0
   }
}

the hpa deployment yaml i used for both of the deployment above
(official deployment and my test deployment):
apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  name: {{ .values.name }}
  labels:
    app: {{ .values.name }}
  namespace: {{ .values.namespace }}
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: {{ .values.name }}
  minreplicas: {{ .values.spec.replicacountmin }}
  maxreplicas: {{ .values.spec.replicacountmax }}
  targetcpuutilizationpercentage: 50
  selector:
    matchlabels:
      app: {{ .values.name }}
  template:
    metadata:
      labels:
        app: {{ .values.name }}
        release: {{ .release.name }}
        heritage: {{ .release.service }}
    spec:
      containers:
      - name: {{ .chart.name }}
        image: &quot;{{ .values.image.repository }}:{{ .values.image.tag }}&quot;

the results:
php hpa is working as planned
name        reference              targets  minpods   maxpods   replicas   age
php-apache  deployment/php-apache  0%/50%    1        3         1          3d7h

test deployment hpa is not working
name              reference                    targets         minpods  maxpods   replicas   age
gw-autoscale-t6   deployment/gw-autoscale-t6   &lt;unknown&gt;/80%   1        3         0          11m

the test deployment hpa error
name:                                                  gw-autoscale-t6
namespace:                                             dev
labels:                                                app=gw-autoscale-t6
annotations:                                           &lt;none&gt;
creationtimestamp:                                     sun, 22 dec 2019 22:39:44 +0200
reference:                                             deployment/gw-autoscale-t6
metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  &lt;unknown&gt; / 80%
min replicas:                                          1
max replicas:                                          3
deployment pods:                                       0 current / 0 desired
conditions:
  type         status  reason          message
  ----         ------  ------          -------
  abletoscale  false   failedgetscale  the hpa controller was unable to get the target's current scale: deployments/scale.apps &quot;gw-autoscale-t6&quot; not found
events:
  type     reason          age                 from                       message
  ----     ------          ----                ----                       -------
  warning  failedgetscale  27s (x81 over 20m)  horizontal-pod-autoscaler  deployments/scale.apps &quot;gw-autoscale-t6&quot; not found


i have also tried a lot of other types of deployment files.

my metrics-service is installed.

i'm installing my deployment with helm.


what can i do to solve this one?

the solution i found was to add the 'resources' property to the test deployment yaml file:
for instance: in case of scale by cpu usage, use it as followed in the deployment file.
resources:
    requests:
       cpu: {{ .values.request.cpu }}
    limits:
       cpu: {{ .values.limits.cpu }}

",<node.js><kubernetes><kubernetes-helm><kubectl>,60250204,5,"the way i solved it is by adding the cpu resources to the deployment's file and keeping only the necessary hpa deployment yaml fields.
hpa deployment file
apiversion: autoscaling/v1
kind: horizontalpodautoscaler
metadata:
  name: {{ .values.name }}
  labels:
    app: {{ .values.name }}
  namespace: {{ .values.namespace }}
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: {{ .values.name }}
  minreplicas: {{ .values.spec.replicacountmin }} 
  maxreplicas: {{ .values.spec.replicacountmax }} 
  metrics:
    - type: resource
      resource:
        name: cpu
        target:
          type: utilization
          averageutilization: {{ .values.spec.cpuutil }} 


deployment file - resource addition
apiversion: apps/v1beta2
kind: deployment
metadata:
  name: {{ .values.name }}
  labels:
    app: {{ .values.name }}
  namespace: {{ .values.namespace }}
spec:
  replicas: {{ .values.replicacount }}
  template:
    metadata:
      labels:
        app: {{ .values.name }}
        release: {{ .release.name }}
        heritage: {{ .release.service }}
    spec:
      containers:
      - name: {{ .chart.name }}
        image: &quot;{{ .values.image.repository }}:{{ .values.image.tag }}&quot;
        resources:
          requests:
            cpu: {{ .values.request.cpu }}
          limits:
            cpu: {{ .values.limits.cpu }} 

you must add this code:
resources:
  requests:
     cpu: {{ .values.request.cpu }}
  limits:
     cpu: {{ .values.limits.cpu }}

hope it helps :)
"
72751763,why does k8s statefulset change from 1/1 to 0/0,"i deployed a helm chart and used it for more than ten days, but when i used it today, i found that the pod was missing, and the statefulset instance became 0/0. i checked the history through kubectl rollout history, and the result was 1, which has not been modified. , what is the reason for this problem?
ps: some records are as follows
kubectl get sts -n proxy
name            ready   age
sts-test   0/0     16d

$ kubectl rollout history sts/sts-test
statefulset.apps/sts-test
revision
1

kubectl get sts sts-test -o yaml
apiversion: apps/v1
kind: statefulset
metadata:
  creationtimestamp: &quot;2022-06-09t03:22:23z&quot;
  generation: 2
  labels:
    app.kubernetes.io/instance: sts-test-integration
    app.kubernetes.io/managed-by: tiller
    app.kubernetes.io/name: sts-test
    app.kubernetes.io/version: &quot;1.0&quot;
    helm.sh/chart: sts-test-0.1.0
  managedfields:
  - apiversion: apps/v1
    fieldstype: fieldsv1
    fieldsv1:
      f:metadata:
        f:labels:
          .: {}
          f:app.kubernetes.io/instance: {}
          f:app.kubernetes.io/managed-by: {}
          f:app.kubernetes.io/name: {}
          f:app.kubernetes.io/version: {}
          f:helm.sh/chart: {}
      f:spec:
        f:podmanagementpolicy: {}
        f:revisionhistorylimit: {}
        f:selector:
          f:matchlabels:
            .: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/name: {}
        f:servicename: {}
        f:template:
          f:metadata:
            f:labels:
              .: {}
              f:app.kubernetes.io/instance: {}
              f:app.kubernetes.io/name: {}
          f:spec:
            f:containers:
              k:{&quot;name&quot;:&quot;sts-test&quot;}:
                .: {}
                f:image: {}
                f:imagepullpolicy: {}
                f:name: {}
                f:ports:
                  .: {}
                  k:{&quot;containerport&quot;:3306,&quot;protocol&quot;:&quot;tcp&quot;}:
                    .: {}
                    f:containerport: {}
                    f:name: {}
                    f:protocol: {}
                f:readinessprobe:
                  .: {}
                  f:failurethreshold: {}
                  f:initialdelayseconds: {}
                  f:periodseconds: {}
                  f:successthreshold: {}
                  f:tcpsocket:
                    .: {}
                    f:port: {}
                  f:timeoutseconds: {}
                f:resources: {}
                f:securitycontext: {}
                f:terminationmessagepath: {}
                f:terminationmessagepolicy: {}
                f:volumemounts:
                  .: {}
                  k:{&quot;mountpath&quot;:&quot;/data/mysql&quot;}:
                    .: {}
                    f:mountpath: {}
                    f:name: {}
                    f:subpath: {}
                  k:{&quot;mountpath&quot;:&quot;/var/log/mysql&quot;}:
                    .: {}
                    f:mountpath: {}
                    f:name: {}
                    f:subpath: {}
            f:dnspolicy: {}
            f:restartpolicy: {}
            f:schedulername: {}
            f:securitycontext: {}
            f:serviceaccount: {}
            f:serviceaccountname: {}
            f:terminationgraceperiodseconds: {}
        f:updatestrategy:
          f:type: {}
        f:volumeclaimtemplates: {}
    manager: go-http-client
    operation: update
    time: &quot;2022-06-09t03:22:23z&quot;
  - apiversion: apps/v1
    fieldstype: fieldsv1
    fieldsv1:
      f:spec:
        f:replicas: {}
    manager: kubectl
    operation: update
    time: &quot;2022-06-24t11:37:22z&quot;
  - apiversion: apps/v1
    fieldstype: fieldsv1
    fieldsv1:
      f:status:
        f:collisioncount: {}
        f:currentrevision: {}
        f:observedgeneration: {}
        f:replicas: {}
        f:updaterevision: {}
    manager: kube-controller-manager
    operation: update
    time: &quot;2022-06-24t11:37:24z&quot;
  name: sts-test
  namespace: proxy
  resourceversion: &quot;5821333&quot;
  selflink: /apis/apps/v1/namespaces/proxy/statefulsets/sts-test
  uid: 8bb73b11-8ee9-44e1-8ead-f5b7c07c5f2e
spec:
  podmanagementpolicy: parallel
  replicas: 0
  revisionhistorylimit: 10
  selector:
    matchlabels:
      app.kubernetes.io/instance: sts-test-integration
      app.kubernetes.io/name: sts-test
  servicename: sts-test-svc
  template:
    metadata:
      creationtimestamp: null
      labels:
        app.kubernetes.io/instance: sts-test-integration
        app.kubernetes.io/name: sts-test
    spec:
      containers:
      - image: sts-test-integration:v0.1.0
        imagepullpolicy: ifnotpresent
        name: sts-test
        ports:
        - containerport: 3306
          name: mysql
          protocol: tcp
        resources: {}
        securitycontext: {}
        terminationmessagepath: /dev/termination-log
        terminationmessagepolicy: file
        volumemounts:
        - mountpath: /data/mysql
          name: sts-test-data
          subpath: mysqldata-pvc
        - mountpath: /var/log/mysql
          name: sts-test-data
          subpath: mysqllog-pvc
      dnspolicy: clusterfirst
      restartpolicy: always
      schedulername: default-scheduler
      securitycontext: {}
      serviceaccount: sts-test
      serviceaccountname: sts-test
      terminationgraceperiodseconds: 30
  updatestrategy:
    type: rollingupdate
  volumeclaimtemplates:
  - apiversion: v1
    kind: persistentvolumeclaim
    metadata:
      creationtimestamp: null
      name: sts-test-data
    spec:
      accessmodes:
      - readwriteonce
      resources:
        requests:
          storage: 50g
      storageclassname: local-path
      volumemode: filesystem
    status:
      phase: pending
status:
  collisioncount: 0
  currentrevision: sts-test-6f95c95b57
  observedgeneration: 2
  replicas: 0
  updaterevision: sts-test-6f95c95b57

",<kubernetes><kubernetes-statefulset>,72752031,5,"...
- apiversion: apps/v1
    fieldstype: fieldsv1
    fieldsv1:
      f:spec:
        f:replicas: {}  # &lt;-- replicas count has changed
    manager: kubectl
    operation: update
    time: &quot;2022-06-24t11:37:22z&quot;  # &lt;-- at this time
...

kubectl scale statefulset sts-test --replicas=0 will not create a new history record by default.
"
49858027,cannot query kubernetes (unauthorized): endpoints is forbidden: user cannot list endpoints in the namespace,"i am running kubernetes 1.9.4 on my gke cluster

i have two pods , gate which is trying to connect to coolapp, both written in elixir

i am using libcluster to connect my nodes
i get the following error:

[libcluster:app_name] cannot query kubernetes (unauthorized): endpoints is forbidden: user ""system:serviceaccount:staging:default"" cannot list endpoints in the namespace ""staging"": unknown user ""system:serviceaccount:staging:default""

here is my config in gate under config/prod:

 config :libcluster,
 topologies: [
   app_name: [
     strategy: cluster.strategy.kubernetes,
     config: [
       kubernetes_selector: ""tier=backend"",
       kubernetes_node_basename: system.get_env(""my_pod_namespace"") || ""${my_pod_namespace}""]]]


here is my configuration:

vm-args

## name of the node
-name ${my_pod_namespace}@${my_pod_ip}
## cookie for distributed erlang
-setcookie ${erlang_cookie}
# enable smp automatically based on availability
-smp auto


creating the secrets:

kubectl create secret generic erlang-config --namespace staging --from-literal=erlang-cookie=xxxxxx
kubectl create configmap vm-config --namespace staging --from-file=vm.args


gate/deployment.yaml

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: gate
  namespace: staging
spec:
  replicas: 1
  revisionhistorylimit: 1
  strategy:
      type: rollingupdate
  template:
    metadata:
      labels:
        app: gate
        tier: backend
    spec:
      securitycontext:
        runasuser: 0
        runasnonroot: false
      containers:
      - name: gate
        image: gcr.io/development/gate:0.1.7
        args:
          - foreground
        ports:
        - containerport: 80
        volumemounts:
        - name: config-volume
          mountpath: /beamconfig
        env:
        - name: my_pod_namespace
          value: staging
        - name: my_pod_ip
          valuefrom:
            fieldref:
              fieldpath: status.podip
        - name: my_pod_name
          valuefrom:
            fieldref:
              fieldpath: metadata.name
        - name: release_config_dir
          value: /beamconfig
        - name: erlang_cookie
          valuefrom:
            secretkeyref:
              name: erlang-config
              key: erlang-cookie
      volumes:
      - name: config-volume
        configmap:
          name: vm-config


coolapp/deployment.yaml:

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: coolapp
  namespace: staging
spec:
  replicas: 1
  revisionhistorylimit: 1
  strategy:
      type: rollingupdate
  template:
    metadata:
      labels:
        app: coolapp
        tier: backend
    spec:
      securitycontext:
        runasuser: 0
        runasnonroot: false
     # volumes
      volumes:
      - name: config-volume
        configmap:
          name: vm-config
      containers:
      - name: coolapp
        image: gcr.io/development/coolapp:1.0.3
        volumemounts:
        - name: secrets-volume
          mountpath: /secrets
          readonly: true
        - name: config-volume
          mountpath: /beamconfig
        ports:
        - containerport: 80
        args:
          - ""foreground""
        env:
        - name: my_pod_namespace
          value: staging
        - name: my_pod_ip
          valuefrom:
            fieldref:
              fieldpath: status.podip
        - name: my_pod_name
          valuefrom:
            fieldref:
              fieldpath: metadata.name
        - name: replace_os_vars
          value: ""true""
        - name: release_config_dir
          value: /beamconfig
        - name: erlang_cookie
          valuefrom:
            secretkeyref:
              name: erlang-config
              key: erlang-cookie
        # proxy_container
      - name: cloudsql-proxy
        image: gcr.io/cloudsql-docker/gce-proxy:1.11
        command: [""/cloud_sql_proxy"", ""--dir=/cloudsql"",
            ""-instances=staging:us-central1:com-staging=tcp:5432"",
            ""-credential_file=/secrets/cloudsql/credentials.json""]
        volumemounts:
          - name: cloudsql-instance-credentials
            mountpath: /secrets/cloudsql
            readonly: true
          - name: cloudsql
            mountpath: /cloudsql

",<erlang><kubernetes><elixir><google-kubernetes-engine>,49859497,5,"the default service account for the staging namespace (in which apparently your pods using libcluster are running) lacks rbac permissions to get endpoints in that namespace.

likely your application requires a number of other permissions (that are not mentioned in the above error message) to work correctly; identifying all such permissions is out of scope for so.

a way to resolve this issue is to grant superuser permissions that service account. this is not a secure solution but a stop gap fix.

$ kubectl create clusterrolebinding make-staging-sa-cluster-admin \
    --serviceaccount=staging:default \
    --clusterrole=cluster-admin

clusterrolebinding ""make-staging-sa-cluster-admin"" created


to grant the specific permission only (get endpoints in the staging namespace) you would need to create a role first:

apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  name: some-permissions
  namespace: staging
rules:
- apigroups: [""""]
  resources: [""endpoints""]
  verbs: [""get"", ""list"", ""watch""]


and create a rolebinding for the default service account in the staging namespace:

apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: give-default-sa-some-permissions
  namespace: staging
subjects:
- kind: serviceaccount
  name: default
  namespace: staging
roleref:
  kind: role
  name: some-permissions
  apigroup: rbac.authorization.k8s.io

"
73862532,only allow requests to certain paths using k8s ingress,"i've set up an ingress to route traffic to my http server, however i would like to leave some routes inaccessible from outside of the cluster.
example routes:
/status -&gt; end point to determine service status

/users/names -&gt; returns users

/users/ages -&gt; returns ages

current ingress:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  namespace: my-namespace
  name: my-app-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: localhost
    http:
      paths:
      - pathtype: prefix
        path: /
        backend:
          service:
            name: my-service
            port:
              number: 8080

this works currently but leaves all routes accessible. what i want to do is only have routes that fall under the /users path open, so that would be both /users/names and /users/ages. that would leave /status inaccessible from outside of the cluster. is this achievable from changing the ingress configuration? any help would be appreciated.
",<kubernetes><kubernetes-ingress>,73862757,5,"just specify the path that you want to expose via the ingress like this:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  namespace: my-namespace
  name: my-app-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: localhost
    http:
      paths:
      - pathtype: prefix
        path: /users # &lt;- add the path here
        backend:
          service:
            name: my-service
            port:
              number: 8080

"
48857092,how to expose nginx on public ip using nodeport service in kubernetes?,"i'm executing kubectl create -f nginx.yaml which creates the pods successfully. but the pods aren't exposed on public ip of my instance. following is the yaml used be me with service type as nodeport:

 apiversion: apps/v1
kind: deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerport: 80

---
apiversion: v1
kind: service
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  type: nodeport
  ports:
    - port: 80
      nodeport: 30080
      name: http
    - port: 443
      nodeport: 30443
      name: https
  selector:
    name: nginx


what could be in-correct in my approach or above yaml file to expose the pod on deployment to the public ip?

ps: firewall and acls are open to internet on all tcp
",<kubernetes><containers><kubectl>,48862498,5,"jeel is right. your service selector is mismatch with pod labels.

if you fix that like what jeel already added in this answer

apiversion: v1
kind: service
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  type: nodeport
  ports:
    - port: 80
      nodeport: 30080
      name: http
  selector:
    name: nginx


your service will be exposed in node ip address. because your service type is nodeport.

if your node ip is, lets say, 35.226.16.207, you can connect to your pod using this ip and nodeport

$ curl 35.226.16.207:30080


in this case, your node must have a public ip. otherwise, you can't access

second option, you can create loadbalancer type service

apiversion: v1
kind: service
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  type: loadbalancer
  ports:
    - port: 80
      name: http
  selector:
    name: nginx


this will provide you a public ip.

for more details, check this
"
51702872,default path on multiple nginx ingress rewrite,"here is my situation, i'm on kubernetes (ingress), with two docker images: one dedicated to the web and the second one to the api.

under the next configuration (at the end of the message): /web will show the front-end that will make some calls to /api, all good there.

but / is a 404 since nothing is defined, i couldn't find a way to tell in the ingress config that / should redirect to /web

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: dev-ingress
  annotations:
    kubernetes.io/tls-acme: ""true""
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
  - hosts:
    - demo.com
    secretname: tls-secret
  rules:
  - host: demo.com
    http: 
      paths:
      - path: /api
        backend:
          servicename: api-app
          serviceport: 8080
      - path: /web
        backend:
          servicename: web-app
          serviceport: 80

",<kubernetes><kubernetes-ingress>,53260993,5,"this depends on what your frontend and backend apps expect in terms of paths. normally the frontend will need to be able to find the backend on a certain external path and in your case it sounds like your backend needs to be made available on a different path externally (/api) from what it works on within the cluster (/). you can rewrite the target for requests to the api so that /api will go to / when the request is routed to the backend:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: dev-ingress-backend
  annotations:
    kubernetes.io/tls-acme: ""true""
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
  - hosts:
    - demo.com
    secretname: tls-secret
  rules:
  - host: demo.com
    http: 
      paths:
      - path: /api
        backend:
          servicename: api-app
          serviceport: 8080


and you can also define a separate ingress (with a different name) for the frontend that does not rewrite the target, so that a request to /web will go to /web for it:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: dev-ingress-frontend
  annotations:
    kubernetes.io/tls-acme: ""true""
spec:
  tls:
  - hosts:
    - demo.com
    secretname: tls-secret
  rules:
  - host: demo.com
    http: 
      paths:
      - path: /web
        backend:
          servicename: web-app
          serviceport: 80

"
63342051,"kubernetes ingress is not working , default backend 404","i'm new to kubernetes. i'm using gke managed service for k8s. there are 2 deployments nginx, httpd, and created nodeport services for those 2 deploys.
i'm trying to create ingress rule for the services. the nginx ingress controller is installed through helm. i have a domain from freenom and set the google cloud dns to use the static public ip. when i try to hit the ingress url (domain/nginx), it's giving:

&quot;default backend - 404&quot;


deployment:
apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx
  replicas: 1
---
apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: httpd
  labels:
    name: httpd
spec:
  selector:
    matchlabels:
      app: httpd
  template:
    metadata:
      labels:
        app: httpd
    spec:
      containers:
        - name: httpd
          image: httpd
  replicas: 1

services:
apiversion: v1
kind: service
metadata:
  labels:
    name: nginx
  name: nginx-service
spec:
  ports:
  - port: 80
    protocol: tcp
    targetport: 80
  selector:
    app: nginx
  type: nodeport

same like for httpd service
ingress:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: test
  annotations:
    kubernetes.io/ingress.global-static-ip-name: testingk8s
spec:
  rules:
    - host: xyz.tk
      http: 
        paths:
          - path: /nginx
            backend:
              servicename: nginx-service
              serviceport: 80
          - path: /httpd
            backend:
              servicename: httpd-service
              serviceport: 80

ingress describe:
default backend:  default-http-backend:80 (10.48.0.7:8080)
rules:
  host           path  backends
  ----           ----  --------
  xyz.tk
                 /nginx   nginx-service:80 (10.48.0.25:80)
                 /httpd   httpd-service:80 (10.48.0.26:80)
annotations:     ingress.kubernetes.io/backends:
                   {&quot;k8s-be-30916--413d33a91e61ca5d&quot;:&quot;healthy&quot;,&quot;k8s-be-31376--413d33a91e61ca5d&quot;:&quot;healthy&quot;,&quot;k8s-be-32702--413d33a91e61ca5d&quot;:&quot;healthy&quot;}


ingress controller pod logs:
i0812 09:38:34.405188       6 event.go:278] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;nginx&quot;, name:&quot;test&quot;, uid:&quot;61991dbd-a361-47d2-88cc-548a7c43e743&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;316030&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'create' ingress nginx/test
i0812 09:38:34.405815       6 controller.go:139] configuration changes detected, backend reload required.
i0812 09:38:34.532163       6 controller.go:155] backend successfully reloaded.
i0812 09:38:41.369315       6 status.go:275] updating ingress nginx/test status from [] to [{35.192.136.218 }]
i0812 09:38:41.374080       6 event.go:278] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;nginx&quot;, name:&quot;test&quot;, uid:&quot;61991dbd-a361-47d2-88cc-548a7c43e743&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;316057&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'update' ingress nginx/test


",<kubernetes><google-kubernetes-engine><nginx-ingress>,63343670,5,"add annotations kubernetes.io/ingress.class: nginx and nginx.ingress.kubernetes.io/rewrite-target: /. so the ingress looks like below
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: test
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: nginx
spec:
  rules:
    - host: xyz.tk
      http: 
        paths:
          - path: /nginx
            backend:
              servicename: nginx-service
              serviceport: 80
          - path: /httpd
            backend:
              servicename: httpd-service
              serviceport: 80

"
75984109,how do i import dashboards by id from grafana.com? without json files and configmap,"i have a helm chart that deploys a kube-prometheus stack (prometheus, grafana, node-exporter), there are some json files (dashboards) in the grafana configuration, they are transferred to the grafana pod via configmap (common practice). i have a task to optimize this configuration to add grafana dashboards via their id from grafana.com and not to use json files (as they are very big). i know how to create a folder in grafana and specify the prometheus resource, but i don't understand how to export dashboards by id.
to create a folder i have a file (yaml) which is added via configmap to the directory /etc/grafana/provisioning/dashboards
- name: 'default'
  org_id: 1
  folder: 'my-dashboards'
  type: 'file'
  options:
    folder: '/var/lib/grafana/dashboards'

how to do it and in which file i need to insert this configuration to make it work. i will be grateful for help.
i tried to create configurations i found on github (dashboardproviders), but it only creates a folder (the code i specified), without dashboards
upd: what i have now
apiversion: 1
providers:
  # &lt;string&gt; an unique provider name. required
  - name: 'prometheus'
    # &lt;int&gt; org id. default to 1
    orgid: 1
    # &lt;string&gt; name of the dashboard folder.
    folder: 'my-dashboards'
    # &lt;string&gt; folder uid. will be automatically generated if not specified
    folderuid: ''
    # &lt;string&gt; provider type. default to 'file'
    type: file
    # &lt;bool&gt; disable dashboard deletion
    disabledeletion: false
    # &lt;int&gt; how often grafana will scan for changed dashboards
    updateintervalseconds: 10
    # &lt;bool&gt; allow updating provisioned dashboards from the ui
    allowuiupdates: false
    options:
      # &lt;string, required&gt; path to dashboard files on disk. required when using the 'file' type
      path: /var/lib/grafana/dashboards
      # &lt;bool&gt; use folder names from filesystem to create folders in grafana
      foldersfromfilesstructure: true

dashboards:
  default:
    minio:
      gnetid: 13502
      revision: 2
      datasource: prometheus


but it still doesn't work...why?
",<kubernetes><kubernetes-helm><grafana><dashboard>,75985106,5,"it is (or can be) a part of the grafana helm chart, from what i remember when used it in the past. that configuration needs to go into the values.yaml when using that helm chart.
specifically here to enable/configure dashboardproviders and here to provision the dashboard using dashboard id from the grafana website.
can also refer to some documentation here.
hope it helps.
update:
using the below config i was able to import the minio dashboard (the one op tried to import):
dashboardproviders:
  dashboardproviders.yaml:
   apiversion: 1
   providers:
   - name: 'default'
     orgid: 1
     folder: 'default'
     type: file
     disabledeletion: true
     editable: true
     options:
       path: /var/lib/grafana/dashboards/standard

dashboards:
  default:
    minio:
      gnetid: 13502
      revision: 2
      datasource: prometheus


ofcourse i don't have the prometheus data source, hence the warning sign(s).
"
51253016,k8s dashboard not logging in (k8s version 1.11),"i did k8s(1.11) cluster using kubeadm tool. it 1 master and one node in the cluster. 


i applied dashboard ui there. 
kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
created service account (followed this link: https://github.com/kubernetes/dashboard/wiki/creating-sample-user)



apiversion: v1
    kind: serviceaccount
    metadata:
      name: admin-user
      namespace: kube-system



and 

apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrolebinding
metadata:
  name: admin-user
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: cluster-admin
subjects:
- kind: serviceaccount
  name: admin-user
  namespace: kube-system


start kube proxy: kubectl proxy --address 0.0.0.0 --accept-hosts '.*' 

and access dashboard from remote host using this url: http://&lt;k8s master node ip&gt;:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login

its asking for token for login: got token using this command: kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}') 

after copy and apply the token in browser.. its not logging in. its not showing authentication error too not sure wht is wrong with this? is my token wrong or my kube proxy command wrong?  
",<kubernetes><kubectl><kubeadm>,51270399,5,"i recreated all the steps in accordance to what you've posted.

turns out the issue is in the &lt;k8s master node ip&gt;, you should use localhost in this case. so to access the proper dashboard, you have to use:

http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login

when you start kubectl proxy - you create a tunnel to your apiserver on the master node. by default, dashboard is starting with servicetype: clusterip. the port on the master node in this mode is not open, and that is the reason you can't reach it on the 'master node ip'. if you would like to use master node ip, you have to change the servicetype to nodeport.

you have to delete the old service and update the config by changing service type to nodeport as in the example below (note that clusterip is not there because it is assumed by default).

create a new yaml file name newservice.yaml

---
# ------------------- dashboard service ------------------- #

kind: service
apiversion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  type: nodeport
  ports:
    - port: 443
      targetport: 8443
  selector:
    k8s-app: kubernetes-dashboard


delete the old service

 kubectl delete service kubernetes-dashboard -n kube-system


apply the new service

kubectl apply -f newservice.yaml


run describe service

kubectl describe svc kubernetes-dashboard -n kube-system | grep ""nodeport""


and you can use that port with the ip address of the master node

type:                   nodeport
nodeport:           &lt;unset&gt; 30518/tcp

http://&lt;k8s master node ip&gt;:30518/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login


note that the port number is generated randomly and yours will be probably different. 
"
54125470,deny access to route outside of kubernetes cluster,"i'm working with a large monolithic application with some private routes. these private routes are currently managed by an plain classic nginx  server.

i need to migrate this to kubernetes, and i must deny all external access to these routes. i'm using gke, and afaik, privatize routes can be done inside the nginx-ingress controller.

i'm trying with server-snippet, but it doesn't seems to work.
here's the current code:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: ""true""
    nginx.org/websocket-services: service-ws
    nginx.org/server-snippet: |
      location /private {
        allow 10.100.0.0/16; #pods ips
        allow 10.200.0.0/16; #pods ips
        deny all; 
      }
  generation: 3


the result is that /private routes always return 200 instead of 401/403.
i've also tried to create a redirection instead of allow/deny, and also get 200 instead of 301 redirections.

do you have some ideas or tips to make this work?
",<kubernetes><google-kubernetes-engine><nginx-ingress>,54182688,5,"following many links, the trick was the prefix is not up-to-date in most documentation:

here's a working sample:

apiversion: extensions/v1beta1
kind: ingress
metadata:
    annotations:
        kubernetes.io/ingress.class: nginx
        nginx.ingress.kubernetes.io/server-snippet: |-
            location /management_api {
                allow 1.2.3.4/16;  # pod address range
                allow 1.3.4.5/16;   # pod address range
                deny all;

                proxy_http_version 1.1;
                proxy_redirect off;
                proxy_intercept_errors on;
                proxy_set_header connection """";
                proxy_set_header x-cf-visitor $http_cf_visitor;
                proxy_set_header host $host;
                proxy_set_header x-real-ip $remote_addr;
                proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;
                proxy_set_header x-forwarded-host $host;
                proxy_set_header x-forwarded-port $server_port;
                proxy_set_header x-forwarded-proto $scheme;
                proxy_pass http://10.11.12.13;
            }


enjoy!
"
54047248,cannot fetch token error when using cloudsql-proxy with gke,"i am using gke with istio add-on enabled. myapp somehow gives 503 errors using when using websocket. i am starting to think that maybe the websocket is working but the database connection is not and that causes 503's, as the cloudsql-proxy logs give errors:

$ kubectl logs myapp-54d6696fb4-bmp5m cloudsql-proxy
2019/01/04 21:56:47 using credential file for authentication; email=proxy-user@myproject.iam.gserviceaccount.com
2019/01/04 21:56:47 listening on 127.0.0.1:5432 for myproject:europe-west4:mydatabase
2019/01/04 21:56:47 ready for new connections
2019/01/04 21:56:51 new connection for ""myproject:europe-west4:mydatabase""
2019/01/04 21:56:51 couldn't connect to ""myproject:europe-west4:mydatabase"": post https://www.googleapis.com/sql/v1beta4/projects/myproject/instances/mydatabase/createephemeral?alt=json: oauth2: cannot fetch token: post https://oauth2.googleapis.com/token: read tcp 10.44.11.21:60728-&gt;108.177.126.95:443: read: connection reset by peer
2019/01/04 22:14:56 new connection for ""myproject:europe-west4:mydatabase""
2019/01/04 22:14:56 couldn't connect to ""myproject:europe-west4:mydatabase"": post https://www.googleapis.com/sql/v1beta4/projects/myproject/instances/mydatabase/createephemeral?alt=json: oauth2: cannot fetch token: post https://oauth2.googleapis.com/token: read tcp 10.44.11.21:36734-&gt;108.177.127.95:443: read: connection reset by peer


looks like the required authentication details should be in the credentials of the proxy service account i created and thus is provided for:

{
  ""type"": ""service_account"",
  ""project_id"": ""myproject"",
  ""private_key_id"": ""myprivekeyid"",
  ""private_key"": ""-----begin private key-----\myprivatekey-----end private key-----\n"",
  ""client_email"": ""proxy-user@myproject.iam.gserviceaccount.com"",
  ""client_id"": ""myclientid"",
  ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",
  ""token_uri"": ""https://oauth2.googleapis.com/token"",
  ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",
  ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/proxy-user%40myproject.iam.gserviceaccount.com""
}


my question:
how do i get rid of the errors/ get a proper google sql config from gke?

at cluster creation i selected the mtls 'permissive' option.

my config:
myapp_and_router.yaml:

apiversion: v1
kind: service
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  ports:
  - port: 8089
    # 'name: http' apparently does not work
    name: db
  selector:
    app: myapp    
---
apiversion: apps/v1
kind: deployment
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  selector:
    matchlabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp
          image: gcr.io/myproject/firstapp:v1
          imagepullpolicy: always
          ports:
            - containerport: 8089
          env:
            - name: postgres_db_host
              value: 127.0.0.1:5432
            - name: postgres_db_user
              valuefrom:
                secretkeyref:
                  name: mysecret
                  key: username
            - name: postgres_db_password
              valuefrom:
                secretkeyref:
                  name: mysecret
                  key: password
          ## custom healthcheck for ingress
          readinessprobe:
            httpget:
              path: /healthz
              scheme: http
              port: 8089
            initialdelayseconds: 5
            timeoutseconds: 5
          livenessprobe:
            httpget:
              path: /healthz
              scheme: http
              port: 8089
            initialdelayseconds: 5
            timeoutseconds: 20             
        - name: cloudsql-proxy
          image: gcr.io/cloudsql-docker/gce-proxy:1.11
          command: [""/cloud_sql_proxy"",
                    ""-instances=myproject:europe-west4:mydatabase=tcp:5432"",
                    ""-credential_file=/secrets/cloudsql/credentials.json""]
          securitycontext:
            runasuser: 2
            allowprivilegeescalation: false
          volumemounts:
            - name: cloudsql-instance-credentials
              mountpath: /secrets/cloudsql
              readonly: true
      volumes:
        - name: cloudsql-instance-credentials
          secret:
            secretname: cloudsql-instance-credentials
---
###########################################################################
# ingress resource (gateway)
##########################################################################
apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: myapp-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      # 'name: http' apparently does not work
      name: db 
      protocol: http
    hosts:
    - ""*""
---
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: myapp
spec:
  hosts:
  - ""*""
  gateways:
  - myapp-gateway
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        host: myapp
      weight: 100
    websocketupgrade: true
---


edit 1: i had not enabled permissions (scopes) for the various google services when creating the cluster, see here. after creating a new cluster with the permissions i now get a new errormessage:

kubectl logs mypod cloudsql-proxy
2019/01/11 20:39:58 using credential file for authentication; email=proxy-user@myproject.iam.gserviceaccount.com
2019/01/11 20:39:58 listening on 127.0.0.1:5432 for myproject:europe-west4:mydatabase
2019/01/11 20:39:58 ready for new connections
2019/01/11 20:40:12 new connection for ""myproject:europe-west4:mydatabase""
2019/01/11 20:40:12 couldn't connect to ""myproject:europe-west4:mydatabase"": post https://www.googleapis.com/sql/v1beta4/projects/myproject/instances/mydatabase/createephemeral?alt=json: oauth2: cannot fetch token: 400 bad request
response: {
  ""error"": ""invalid_grant"",
  ""error_description"": ""invalid jwt signature."" 
}


edit 2: looks like new error was caused by the service accounts keys no longer being valid. after making new ones i can connect to the database!
",<kubernetes><google-cloud-platform><google-cloud-sql><google-kubernetes-engine><cloud-sql-proxy>,54115186,5,"i saw similar errors but was able to get cloudsql-proxy working in my istio cluster on gke by creating the following service entries (with some help from https://github.com/istio/istio/issues/6593#issuecomment-420591213):

apiversion: networking.istio.io/v1alpha3
kind: serviceentry
metadata:
  name: google-apis
spec:
  hosts:
  - ""*.googleapis.com""
  ports:
  - name: https
    number: 443
    protocol: https
---
apiversion: networking.istio.io/v1alpha3
kind: serviceentry
metadata:
  name: cloudsql-instances
spec:
  hosts:
  # use `gcloud sql instances list` to get the addresses of instances
  - 35.226.125.82
  ports:
  - name: tcp
    number: 3307
    protocol: tcp


also, i still saw those connection errors during initialization until i added a delay in my app startup (sleep 10 before running server) to give the istio-proxy and cloudsql-proxy containers time to get set up first. 

edit 1: here are logs with the errors, then the successful ""new connection/client closed"" lines once things are working:

2019/01/10 21:54:38 new connection for ""my-project:us-central1:my-db""
2019/01/10 21:54:38 throttling refreshcfg(my-project:us-central1:my-db): it was only called 44.445553175s ago
2019/01/10 21:54:38 couldn't connect to ""my-project:us-central1:my-db"": post https://www.googleapis.com/sql/v1beta4/projects/my-project/instances/my-db/createephemeral?alt=json: oauth2: cannot fetch token: post https://accounts.google.com/o/oauth2/token: dial tcp 108.177.112.84:443: getsockopt: connection refused
2019/01/10 21:54:38 new connection for ""my-project:us-central1:my-db""
2019/01/10 21:54:38 throttling refreshcfg(my-project:us-central1:my-db): it was only called 44.574562959s ago
2019/01/10 21:54:38 couldn't connect to ""my-project:us-central1:my-db"": post https://www.googleapis.com/sql/v1beta4/projects/my-project/instances/my-db/createephemeral?alt=json: oauth2: cannot fetch token: post https://accounts.google.com/o/oauth2/token: dial tcp 108.177.112.84:443: getsockopt: connection refused
2019/01/10 21:55:15 new connection for ""my-project:us-central1:my-db""
2019/01/10 21:55:16 client closed local connection on 127.0.0.1:5432
2019/01/10 21:55:17 new connection for ""my-project:us-central1:my-db""
2019/01/10 21:55:17 new connection for ""my-project:us-central1:my-db""
2019/01/10 21:55:27 client closed local connection on 127.0.0.1:5432
2019/01/10 21:55:28 new connection for ""my-project:us-central1:my-db""
2019/01/10 21:55:30 client closed local connection on 127.0.0.1:5432
2019/01/10 21:55:37 client closed local connection on 127.0.0.1:5432
2019/01/10 21:55:38 new connection for ""my-project:us-central1:my-db""
2019/01/10 21:55:40 client closed local connection on 127.0.0.1:5432


edit 2: ensure that cloud sql api is within scope of your cluster.
"
63104035,basic helm dependency clarification,"to ease local development/testing, i have an umbrella chart that deploys all my sub-charts. those applications make use of resources (e.g. mongodb, kafka, etc) and i want to make sure that if you are installing the umbrella chart to a cluster, it will also install those resources.
to do this, i have the following:
apiversion: v2
name: my-cool-project
type: application
version: 0.1.0
appversion: 0.1.0
dependencies:
  - name: my-cool-app-1
    repository: &quot;file://my-cool-app-1&quot;
  - name: my-cool-app-2
    repository: &quot;file://my-cool-app-2&quot;
  - name: bitnami/kafka
    version: 2.5.0
    repository: &quot;https://charts.bitnami.com/bitnami&quot;

unfortunately, installing this chart throws the following error:
error: found in chart.yaml, but missing in charts/ directory: bitnami/kafka

this seems so fundamental to the concept of helm that the fact it's not working means i'm clearly missing something basic. even the official docs are pretty clear this is the right approach.
most documentation/guides instruct you to simply helm install it straight to the cluster. while this might solve my immediate problem of needing kafka or mongodb on the cluster, my desire is to code-ify the need for that resource so that i can achieve &quot;single chart installs everything to an empty cluster and it just works&quot; status.
what am i missing?
",<kubernetes><kubernetes-helm>,63108028,5,"this worked for me :
apiversion: v2
name: my-cool-project
type: application
version: 0.1.0
appversion: 0.1.0
dependencies:
  - name: my-cool-app-1
    repository: &quot;file://my-cool-app-1&quot;
  - name: my-cool-app-2
    repository: &quot;file://my-cool-app-2&quot;
  - name: kafka 
    version: 11.6.0 
    repository: &quot;https://charts.bitnami.com/bitnami&quot;

then update on the dependencies on your local helm chart:
  helm dependency update
hang tight while we grab the latest from your chart repositories...
...successfully got an update from the &quot;bitnami&quot; chart repository
update complete. happy helming!
saving 1 charts
downloading kafka from repo https://charts.bitnami.com/bitnami
deleting outdated charts


"
53285851,how can you perform variable substitution using kubectl?,"i am trying to create a role and rolebinding so i can use helm. i want to use variable substitution somehow to replace {{namespace}} with something when i run an apply command.

kind: role
apiversion: rbac.authorization.k8s.io/v1
metadata:
  name: tiller-manager-{{namespace}}
  namespace: {{namespace}}
rules:
- apigroups: ["""", ""batch"", ""extensions"", ""apps""]
  resources: [""*""]
  verbs: [""*""]


i want to pass the namespace something like this:


  kubectl apply --file role.yaml --namespace foo


i have seen that kubectl apply has a --template parameter but i can't see much information about how it might be used.
",<kubernetes><kubectl>,53286058,5,"you can do it in following way.


write role file like this:

kind: role
apiversion: rbac.authorization.k8s.io/v1
metadata:
  name: tiller-manager-${namespace}
  namespace: ${namespace}
rules:
- apigroups: ["""", ""batch"", ""extensions"", ""apps""]
  resources: [""*""]
  verbs: [""*""]

set namespace environment variable to your desired value.
then create role using following command

envsubst &lt; role.yaml | kubectl apply -f -


"
64541808,kubernetes - multiple configuration in one ingress,"i have different applications running in the same kubernetes cluster.
i would like multiple domains to access my kubernetes cluster and be redirected depending the domain.
for each domain i would like different annotations/configuration.
without the annotations i have an ingress deployment such as:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: frontdoor
  namespace: myapps
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
spec:
  type: loadbalancer
  tls:
    - hosts:
        - foo.bar.dev
        - bar.foo.dev
      secretname: tls-secret
  rules:
    - host: foo.bar.dev
      http:
        paths:
          - backend:
              servicename: foobar
              serviceport: 9000
            path: /(.*)
    - host: bar.foo.dev
      http:
        paths:
          - backend:
              servicename: varfoo
              serviceport: 80
            path: /(.*)

but they need to have multiple configuration, for example, one need to have the following annotation
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    nginx.ingress.kubernetes.io/affinity: &quot;cookie&quot;
    nginx.ingress.kubernetes.io/session-cookie-name: &quot;phpsessid&quot;
    nginx.ingress.kubernetes.io/session-cookie-expires: &quot;172800&quot;
    nginx.ingress.kubernetes.io/session-cookie-max-age: &quot;172800&quot;

and another would have this one
    nginx.ingress.kubernetes.io/backend-protocol: &quot;fcgi&quot;
    nginx.ingress.kubernetes.io/fastcgi-index: &quot;index.php&quot;
    nginx.ingress.kubernetes.io/fastcgi-params-configmap: &quot;example-cm&quot;

those configurations are not compatible, and i can't find a way to specify a configuration by host.
i also understand that it's impossible to have 2 ingress serving external http request.
so what am i not understanding / doing wrong ?
",<nginx><kubernetes><kubernetes-ingress>,64575791,5,"
i also understand that it's impossible to have 2 ingress serving external http request

i am not sure where you've found this but you totally can do this.
you should be able to create two separate ingress objects like following:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: frontdoor-bar
  namespace: myapps
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod

    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    nginx.ingress.kubernetes.io/affinity: &quot;cookie&quot;
    nginx.ingress.kubernetes.io/session-cookie-name: &quot;phpsessid&quot;
    nginx.ingress.kubernetes.io/session-cookie-expires: &quot;172800&quot;
    nginx.ingress.kubernetes.io/session-cookie-max-age: &quot;172800&quot;

spec:
  type: loadbalancer
  tls:
    - hosts:
        - bar.foo.dev
      secretname: tls-secret-bar
  rules:
    - host: bar.foo.dev
      http:
        paths:
          - backend:
              servicename: barfoo
              serviceport: 80
            path: /(.*)

---
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: frontdoor-foo
  namespace: myapps
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod

    nginx.ingress.kubernetes.io/backend-protocol: &quot;fcgi&quot;
    nginx.ingress.kubernetes.io/fastcgi-index: &quot;index.php&quot;
    nginx.ingress.kubernetes.io/fastcgi-params-configmap: &quot;example-cm&quot;

spec:
  type: loadbalancer
  tls:
    - hosts:
        - foo.bar.dev
      secretname: tls-secret-foo
  rules:
    - host: foo.bar.dev
      http:
        paths:
          - backend:
              servicename: foobar
              serviceport: 9000
            path: /(.*)

this is a completely valid ingress configuration, and most probably the only valid one that will solve your problem.
each ingress object configures one domain.
"
65454891,put yaml (as data) into a config map,"is there a way to store yaml data in a config map?
in my values.yaml i have something like the below
config:
  filters:
    - kind: pod
      apiversion: v1
...

in my config map, i'm currently doing
...
data:
  config.yaml: |-
    {{ .values.config }}

but in the resulting configmap the data is &quot;inlined&quot; and formatted as this
...
data:
  config.yaml: &gt;-
    map[filters:[map[apiversion:v1...

which isn't yaml and therefore can't be parsed by the app reading it.
",<kubernetes><kubernetes-helm><configmap>,65455430,5,"let's say  you have a demo-chart:
.
 demo
     charts
     chart.yaml
     templates
      configmap.yaml
      _helpers.tpl
      notes.txt
     values.yaml


values.yaml:
config:
  filters:
    - kind: pod
      apiversion: v1

configmap.yaml:
{{- if .values.config }}
apiversion: v1
kind: configmap
metadata:
  name: demo-name
data:
  config.yml: |- {{- toyaml .values.config | nindent 4 }}
{{- end }}

explanation: toyaml parse the data loaded from the values.yaml into yaml and nindent put 4 spaces in front of every line.
check:
$ helm template demo
---
# source: demo/templates/configmap.yaml
apiversion: v1
kind: configmap
metadata:
  name: demo-name
data:
  config.yml: |-
    filters:
    - apiversion: v1
      kind: pod

"
67073909,error scaling up in hpa in gke: apiserver was unable to write a json response: http2: stream closed,"following the guide that google made for deploying an hpa in google kubernetes engine: https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics
and adding the right permissions because i am using workload identity with this guide: https://github.com/googlecloudplatform/k8s-stackdriver/tree/master/custom-metrics-stackdriver-adapter
and also adding the firewall-rule commented here: https://github.com/kubernetes-sigs/prometheus-adapter/issues/134
i am stuck in a point where the hpa returns me this error:
kubectl describe hpa -n test-namespace
name:                  my-hpa
namespace:             test-namespace
labels:                &lt;none&gt;
annotations:           &lt;none&gt;
creationtimestamp:     tue, 13 apr 2021 12:47:56 +0200
reference:             statefulset/my-set
metrics:               ( current / target )
  &quot;my-metric&quot; on pods:  &lt;unknown&gt; / 1
min replicas:          1
max replicas:          60
statefulset pods:      1 current / 0 desired
conditions:
  type           status  reason               message
  ----           ------  ------               -------
  abletoscale    true    succeededgetscale    the hpa controller was able to get the target's current scale
  scalingactive  false   failedgetpodsmetric  the hpa was unable to compute the replica count: unable to get metric my-metric: no metrics returned from custom metrics api
events:
  type     reason                        age                   from                       message
  ----     ------                        ----                  ----                       -------
  warning  failedgetpodsmetric           8m26s (x40 over 18m)  horizontal-pod-autoscaler  unable to get metric my-metric: no metrics returned from custom metrics api
  warning  failedcomputemetricsreplicas  3m26s (x53 over 18m)  horizontal-pod-autoscaler  failed to compute desired number of replicas based on listed metrics for statefulset/test-namespace/my-set: invalid metrics (1 invalid out of 1), first error is: failed to get pods metric value: unable to get metric my-metric: no metrics returned from custom metrics api

but the apiservices are in true,
kubectl get apiservices
name                                     service                                             available   age
...
v1beta1.custom.metrics.k8s.io            custom-metrics/custom-metrics-stackdriver-adapter   true        24h
v1beta1.external.metrics.k8s.io          custom-metrics/custom-metrics-stackdriver-adapter   true        24h
v1beta2.custom.metrics.k8s.io            custom-metrics/custom-metrics-stackdriver-adapter   true        24h
...

and when i try to retrieve the metric data it returns ok,
kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta2/namespaces/test-namespace/pods/*/my-metric&quot; | jq .
{
  &quot;kind&quot;: &quot;metricvaluelist&quot;,
  &quot;apiversion&quot;: &quot;custom.metrics.k8s.io/v1beta2&quot;,
  &quot;metadata&quot;: {
    &quot;selflink&quot;: &quot;/apis/custom.metrics.k8s.io/v1beta2/namespaces/test-namespace/pods/%2a/my-metric&quot;
  },
  &quot;items&quot;: [
    {
      &quot;describedobject&quot;: {
        &quot;kind&quot;: &quot;pod&quot;,
        &quot;namespace&quot;: &quot;test-namespace&quot;,
        &quot;name&quot;: &quot;my-metrics-api-xxxx&quot;,
        &quot;apiversion&quot;: &quot;/__internal&quot;
      },
      &quot;metric&quot;: {
        &quot;name&quot;: &quot;my-metric&quot;,
        &quot;selector&quot;: null
      },
      &quot;timestamp&quot;: &quot;2021-04-13t11:15:30z&quot;,
      &quot;value&quot;: &quot;5&quot;
    }
  ]
}

but the stackdriver gives me this error:
2021-04-13t11:01:30.432634z apiserver was unable to write a json response: http2: stream closed
2021-04-13t11:01:30.432679z apiserver received an error that is not an metav1.status: &amp;errors.errorstring{s:&quot;http2: stream closed&quot;}

i had to configure the adapter that google provides like this:
apiversion: v1
kind: namespace
metadata:
  name: custom-metrics
---
apiversion: v1
kind: serviceaccount
metadata:
  name: custom-metrics-stackdriver-adapter
  namespace: custom-metrics
---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: custom-metrics:system:auth-delegator
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: system:auth-delegator
subjects:
- kind: serviceaccount
  name: custom-metrics-stackdriver-adapter
  namespace: custom-metrics
---
apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: custom-metrics-auth-reader
  namespace: kube-system
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: extension-apiserver-authentication-reader
subjects:
- kind: serviceaccount
  name: custom-metrics-stackdriver-adapter
  namespace: custom-metrics
---
apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrolebinding
metadata:
  name: custom-metrics-resource-reader
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: view
subjects:
- kind: serviceaccount
  name: custom-metrics-stackdriver-adapter
  namespace: custom-metrics
---
apiversion: apps/v1
kind: deployment
metadata:
  name: custom-metrics-stackdriver-adapter
  namespace: custom-metrics
  labels:
    run: custom-metrics-stackdriver-adapter
    k8s-app: custom-metrics-stackdriver-adapter
spec:
  replicas: 1
  selector:
    matchlabels:
      run: custom-metrics-stackdriver-adapter
      k8s-app: custom-metrics-stackdriver-adapter
  template:
    metadata:
      labels:
        run: custom-metrics-stackdriver-adapter
        k8s-app: custom-metrics-stackdriver-adapter
        kubernetes.io/cluster-service: &quot;true&quot;
    spec:
      serviceaccountname: custom-metrics-stackdriver-adapter
      containers:
      - image: gcr.io/gke-release/custom-metrics-stackdriver-adapter:v0.12.0-gke.0
        imagepullpolicy: always
        name: pod-custom-metrics-stackdriver-adapter
        command:
        - /adapter
        - --use-new-resource-model=true
        - --cert-dir=/tmp
        - --secure-port=4443
        resources:
          limits:
            cpu: 250m
            memory: 200mi
          requests:
            cpu: 250m
            memory: 200mi
        securitycontext:
          runasnonroot: true
          runasuser: 1000
---
apiversion: v1
kind: service
metadata:
  labels:
    run: custom-metrics-stackdriver-adapter
    k8s-app: custom-metrics-stackdriver-adapter
    kubernetes.io/cluster-service: 'true'
    kubernetes.io/name: adapter
  name: custom-metrics-stackdriver-adapter
  namespace: custom-metrics
spec:
  ports:
  - port: 443
    protocol: tcp
    targetport: 4443
  selector:
    run: custom-metrics-stackdriver-adapter
    k8s-app: custom-metrics-stackdriver-adapter
  type: clusterip
---
apiversion: apiregistration.k8s.io/v1
kind: apiservice
metadata:
  name: v1beta1.custom.metrics.k8s.io
spec:
  insecureskiptlsverify: true
  group: custom.metrics.k8s.io
  grouppriorityminimum: 100
  versionpriority: 100
  service:
    name: custom-metrics-stackdriver-adapter
    namespace: custom-metrics
  version: v1beta1
---
apiversion: apiregistration.k8s.io/v1
kind: apiservice
metadata:
  name: v1beta2.custom.metrics.k8s.io
spec:
  insecureskiptlsverify: true
  group: custom.metrics.k8s.io
  grouppriorityminimum: 100
  versionpriority: 200
  service:
    name: custom-metrics-stackdriver-adapter
    namespace: custom-metrics
  version: v1beta2
---
apiversion: apiregistration.k8s.io/v1
kind: apiservice
metadata:
  name: v1beta1.external.metrics.k8s.io
spec:
  insecureskiptlsverify: true
  group: external.metrics.k8s.io
  grouppriorityminimum: 100
  versionpriority: 100
  service:
    name: custom-metrics-stackdriver-adapter
    namespace: custom-metrics
  version: v1beta1
---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  name: external-metrics-reader
rules:
- apigroups:
  - &quot;external.metrics.k8s.io&quot;
  resources:
  - &quot;*&quot;
  verbs:
  - list
  - get
  - watch
---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: external-metrics-reader
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: external-metrics-reader
subjects:
- kind: serviceaccount
  name: horizontal-pod-autoscaler
  namespace: kube-system

because it was disabled the port 443 and i had to change to 4443 and put also the --cert-dir=/tmp option because without that option, stackdriver returns me the error:
&quot;unable to run custom metrics adapter: error creating self-signed certificates: mkdir apiserver.local.config: permission denied&quot;

i think that i explained all the steps that i did to configure it, without success. any ideas?
",<kubernetes><google-cloud-platform><google-kubernetes-engine><stackdriver><hpa>,67083280,5,"resolved for me!
after a several of test, changing in the hpa yaml,
the metric from pod to external, and the metric name with custom.google.apis/my-metric, it works!
apiversion: autoscaling/v2beta2
kind: horizontalpodautoscaler
metadata:
  name: my-hpa
  namespace: test-namespace
spec:
  maxreplicas: 60
  minreplicas: 1
  scaletargetref:
    apiversion: apps/v1
    kind: statefulset
    name: my-set
  metrics:
  - type: external
    external:
      metric: 
        name: custom.googleapis.com/my-metric #custom.googleapis.com/my-metric
      target:
        averagevalue: 1
        type: averagevalue

"
67469775,eks cron job fails to deploy,"i am trying to deploy my first cron job.
starting with a very simple one, as described in the k8s tutorial:
apiversion: batch/v1
kind: cronjob
metadata:
  name: hello
spec:
  schedule: &quot;*/1 * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            imagepullpolicy: ifnotpresent
            command:
            - /bin/sh
            - -c
            - date; echo hello from the kubernetes cluster
          restartpolicy: onfailure

i am connected to my eks cluster. this is the command and output:
  dev kubectl apply -f cronjob.yaml                                         
error: unable to recognize &quot;cronjob.yaml&quot;: no matches for kind &quot;cronjob&quot; in version &quot;batch/v1&quot; 

and batch/v1 does exists in my apiversion list.
can't understand what is wrong..
",<amazon-web-services><kubernetes><yaml><amazon-eks>,67470815,5,"the cronjob apiversion in the kubernetes 1.18 is batch/v1beta1
apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: hello
spec:
  schedule: &quot;*/1 * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo hello from the kubernetes cluster

source
"
67351729,ingress - simple fanout configuration not working,"i'm using ubuntu 20.04.2 lts. i installed microk8s 1.20.6 rev 2143 and experimenting with ingress. i must be missing something - but it doesn't work as i expect it to. i tracked the strange behavior down to the following configuration:
ingress.yaml:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: my-ubuntu
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: my-ubuntu
    http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: nginx
            port:
              number: 80
      - path: /nginx
        pathtype: prefix
        backend:
          service:
            name: nginx
            port:
              number: 80

nginx-service.yaml:
apiversion: v1
kind: service
metadata:
  name: nginx
spec:
  ports:
    - port: 80
      name: http
    - port: 443
      name: https
  type: clusterip
  selector:
    app: nginx

now,
curl my-ubuntu/                     # this returns welcome page, as expected
curl my-ubuntu/nginx                # this returns welcome page, as expected
curl my-ubuntu/bad-page.html        # this returns 404 not found, as expected
curl my-ubuntu/nginx/bad-page.html  # this returns welcome page. why?

any request under my-ubuntu/nginx/* returns welcome page, even when the url is correct and should have returned different content. did i configure something wrong?
i was able to reproduce the same strange behavior using docker for windows + wsl2 + ubuntu + ingress installed using:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.41.2/deploy/static/provider/cloud/deploy.yaml

edit
nginx-deployment.yaml i used:
apiversion: apps/v1
kind: deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 1
  revisionhistorylimit: 0
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        imagepullpolicy: always
        name: nginx

when i try /nginx/ instead of /nginx like @harshmanvar suggested, i get this behavior:
curl my-ubuntu/                     # this returns welcome page, as expected
curl my-ubuntu/bad-page.html        # this returns 404 not found, as expected
curl my-ubuntu/nginx                # this returns 404 not found
curl my-ubuntu/nginx/               # this returns welcome page
curl my-ubuntu/nginx/bad-page.html  # this returns welcome page

kubernetes ingress documentation about simple fanout also does use /nginx pattern but not working as described above.
",<kubernetes><kubernetes-ingress><nginx-ingress><docker-for-windows><microk8s>,67379339,5,"https://kubernetes.github.io/ingress-nginx/examples/rewrite/ explains how to use rewrite-target annotation. i was able to make it work with the following ingress.yaml:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: my-ubuntu
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
  - host: localhost
    http:
      paths:
      - path: /(.*)
        pathtype: prefix
        backend:
          service:
            name: nginx
            port:
              number: 80
      - path: /nginx($|/.*)
        pathtype: prefix
        backend:
          service:
            name: nginx
            port:
              number: 80

each path defines regular expression with ( ), which yields $1, $2, etc., aka. regex capture group variables. now you put rewrite-target using those variables, and that will be the actual url that is passed to the service's container that handles the request.
maybe there is another way, but this is the only way i was able to make it work.
"
39942571,"mysql lower_case_table_names = 1 with kubernetes yml file, mysql server start up error","i am using yo jhipster:kubernetes to generate kubernetes file, i want to set lower_case_table_names=1 for mysql to make mysql case insensitive. below file is generated by command

apiversion: extensions/v1beta1
kind: deployment
metadata
name: app-mysql
spec:
replicas: 1
template:
metadata:
labels:
app: app-mysql
spec:
volumes:
  - name: data
    emptydir: {}
  containers:
  - name: mysql
    image: mysql:5.6.22
    env:
    - name: mysql_user
      value: root
    - name: mysql_allow_empty_password
      value: 'yes'
    - name: mysql_database
      value: app
    command:
    - mysqld
    - --lower_case_table_names=1
    - --skip-ssl
    ports:
    - containerport: 3306
    volumemounts:
    - name: data
      mountpath: /var/lib/mysql/
apiversion: v1
kind: service
metadata:
name: app-mysql
spec:
selector:
app: app-mysql
ports:
port: 3306


mysql is not starting i am getting below error on mysql startup on linux machine due to command node in file:-

       2016-10-09 10:08:35 1 [note] plugin 'federated' is disabled. mysqld: 
    table 'mysql.plugin' doesn't exist 2016-10-09 10:08:35 1 
    [error] can't open the mysql.plugin table. please run mysql_upgrade to create it. 2016-10-09 10:08:35 1 
    [note] innodb: using atomics to ref count buffer pool pages 2016-10-09 10:08:35 1 [note] innodb: the innodb memory heap is disabled 2016-10-09 10:08:35 1 [note] innodb: mutexes and rw_locks use gcc atomic builtins 2016-10-09 10:08:35 1 [note] 
    innodb: memory barrier is not used 2016-10-09 10:08:35 1 [note] innodb: compressed tables use zlib 1.2.7 2016-10-09 10:08:35 1 [note] innodb: using linux native aio 2016-10-09 10:08:35 1 
[note] innodb: using cpu crc32 instructions 2016-10-09 10:08:35 1 
[note] innodb: initializing buffer pool, size = 128.0m 2016-10-09 10:08:35 1 
[note] innodb: completed initialization of buffer pool 2016-10-09 10:08:35 1 
[note] innodb: highest supported file format is barracuda. 2016-10-09 10:08:35 1 
[note] innodb: log scan progressed past the checkpoint lsn 49463 2016-10-09 10:08:35 1 
[note] innodb: database was not shutdown normally! 2016-10-09 10:08:35 1 
[note] innodb: starting crash recovery. 2016-10-09 10:08:35 1 [note] innodb: reading tablespace information from the .ibd files... 2016-10-09 10:08:35 1 
[note] innodb: restoring possible half-written data pages 2016-10-09 10:08:35 1 
[note] innodb: from the doublewrite buffer... innodb: doing recovery: scanned up to log sequence number 1600607 2016-10-09 10:08:35 1 
[note] innodb: starting an apply batch of log records to the database... innodb: progress in percent: 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 innodb: apply batch completed 2016-10-09 

10:08:35 1 [note] innodb: 128 rollback segment(s) are active. 2016-10-09 10:08:35 1 [note] innodb: waiting for purge to start 2016-10-09 10:08:35 1 

[note] innodb: 5.6.22 started; log sequence number 1600607 2016-10-09 10:08:35 1 [note] server hostname (bind-address): '*'; port: 3306 2016-10-09 10:08:35 1 [note] ipv6 is available. 2016-10-09 10:08:35 1 
[note] - '::' resolves to '::'; 2016-10-09 10:08:35 1 [note] server socket created on ip: '::'. 2016-10-09 10:08:35 1 [error] fatal error: can't open and lock privilege tables: table 'mysql.user' doesn't exist


any idea how to set lower_case_table_names=1 in kubernetes yml file?
",<mysql><kubernetes><google-cloud-platform><jhipster><google-kubernetes-engine>,39968323,4,"could you try using args instead of command?
that is to say,

args:
- --lower_case_table_names=1
- --skip-ssl


if it still doesn't work, how about creating a config volume?
in your yaml file on mysql pod, you can define like

spec:
      containers:
      - name: mysql
        image:  mysql:5.6
        imagepullpolicy: ifnotpresent
        volumemounts:
        - mountpath: /var/lib/mysql
          name: data
        - mountpath: /etc/mysql/conf.d/
          name: config
          readonly: true
        ports:
        - containerport: 3306
        env:
        - name: mysql_allow_empty_password
          value: ""yes""
      volumes:
      - name: data
        hostpath:
          path: /var/lib/data
      - name: config
        configmap:
          name: mysql-config


and then you can pass additional config parameters by loading mysql-config written as 

apiversion: v1
kind: configmap
metadata:
  name: mysql-config
data:
  my.conf: |
    [mysqld]
    lower_case_table_names=1
    skip_ssl


then no modification of command or args values on kuberenetes yaml required. at least on our local development environment, we can change as  innodb_file_format=barracuda in the latter way.
"
39667667,how to (re) use an existent static ip address when creating a tls ingress resource?,"i'm creating an (tls enabled) ingress resource using following configurations:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-app-apis
spec:
  tls:
  - secretname: tls-secret
  backend:
    servicename: my-web-service
    serviceport: 80


a new static ip address is provisioned everytime.
is it possible to reuse an existent one ?

(i'm using kubernetes running on gke)
",<google-compute-engine><kubernetes><google-kubernetes-engine>,39922899,4,"you can specify the ip address in an annotation on the ingress (it looks like you specify it by name rather than ip address). this is only picked up by the gce controller so don't expect it to work anywhere other than gce/gke.

https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/controller/utils.go#l48

something like this should work:

apiversion: extensions/v1beta1
kind: ingress
metadata:
 name: myingress
 annotations:
   ""kubernetes.io/ingress.global-static-ip-name"": my-ip-name
spec:
  ...

"
40763718,kubernetes https ingress in google container engine,"i want to expose a http service running in google container engine over https only load balancer.

how to define in ingress object that i want https only load balancer instead of default http?

or is there a way to permanently drop http protocol from created load balancer? when i add https protocol and then drop http protocol, http is recreated after few minutes by the platform.

ingress:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: myapp-ingress
spec:
  backend:
    servicename: myapp-service
    serviceport: 8080

",<kubernetes><google-cloud-platform><google-kubernetes-engine>,40897263,4,"in order to have https service exposed only, you can block traffic on port 80 as mentioned on this link:


  you can block traffic on :80 through an annotation. you might want to do this if all your clients are only going to hit the loadbalancer through https and you don't want to waste the extra gce forwarding rule, eg:


apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: test
  annotations:
    kubernetes.io/ingress.allow-http: ""false""
spec:
  tls:
  # this assumes tls-secret exists.
  # to generate it run the make in this directory.
  - secretname: tls-secret
  backend:
    servicename: echoheaders-https
    serviceport: 80

"
60316148,ingress controller nodeport not reachable from outside,"i have a cluster of 3 vm on which i install kubernetes and deployed some pods and services that i would like to be accessible from outside (my local pc for exemple)

i followed this tutorial https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal to install my ingress controller.
i have created a service of type nodeport.
i have created a ingress that looks like this:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: test-ingress
  annotations:
      kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: my.service.com
    http:
      paths:
      - path: /
        backend:
          servicename: myservice
          serviceport: 9090


kubectl get svc myservice gives me:

name        type       cluster-ip        external-ip   port(s)          age
myservice   nodeport   xxx.xxx.xxx.xxx   &lt;none&gt;        9090:31220/tcp   47m


kubectl get ingress test-ingress gives me

name            hosts            address          ports   age
test-ingress    my.service.com   xx.xxx.xxx.xxx   80      49m


on my local computer i have added to /etc/hosts i have mapped the ip address of the ingress to the name my.service.com
when i try to ping my.service.com or directly the ip i go request timeout 100% packet loss.
i tried to reach my service visual interface with the web browser and it does not work either.

how can i investigate further why i can't access to my service from outside the cluster ?
",<kubernetes><kubernetes-ingress>,60317191,4,"there is probably an issue with your firewall. check the firewall rules if they are blocking access from outside to that port.

also you should access the service from the nodeport of the service you created while deploying the ingress controller.

also edit your ingress and add the tls and hosts property with a self signed certificate for my.service.com

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: test-ingress
  annotations:
      ingress.kubernetes.io/rewrite-target: /
      kubernetes.io/ingress.class: nginx
spec:
  tls:
  - hosts:
    - my.service.com
    secretname: tls-secret-for my service
  rules:
  - host: my.service.com
    http:
      paths:
      - path: /
        backend:
          servicename: myservice
          serviceport: 9090

"
45165855,kubernetes: configure deployment to mount directory from local kubernetes host?,"i need to provide access to the file /var/docker.sock on the kubernetes host (actually, a gke instance) to a container running on that host. 

to do this i'd like to mount the directory into the container, by configuring the mount in the deployment.yaml for the container deployment.

how would i specify this in the deployment configuration?

here is the current configuration, i have for the deployment:

apiversion: apps/v1beta1
 kind: deployment
metadata:
   name: appd-sa-agent
 spec:
  replicas: 1
   template: 
     metadata:
      labels:
        run: appd-sa-agent
     spec:
      containers:
      - name: appd-sa-agent
        image: docker.io/archbungle/appd-sa-agent:latest
        ports:
        - containerport: 443
        env:
        - name: appd_host  
          value: ""https://graffiti201707132327203.saas.appdynamics.com""


how would i specify mounting the localhost file path to a directory mountpoint on the container?

thanks!
t.
",<kubernetes><google-kubernetes-engine>,45166832,4,"you need to define a hostpath volume.

apiversion: apps/v1beta1
kind: deployment
metadata:
   name: appd-sa-agent
 spec:
  replicas: 1
   template: 
     metadata:
      labels:
        run: appd-sa-agent
     spec:
      volumes:
      - name: docker-socket
        hostpath:
          path: /var/run/docker.sock
      containers:
      - name: appd-sa-agent
        image: docker.io/archbungle/appd-sa-agent:latest
        volumemounts:
        - name: docker-socket
          mountpath: /var/run/docker.sock
        ports:
        - containerport: 443
        env:
        - name: appd_host  
          value: ""https://graffiti201707132327203.saas.appdynamics.com""

"
59065279,"error: failed to prepare subpath for volumemount ""postgres-storage"" of container ""postgres""","i am trying to use persistent volume claims and facing this issue
this is my postgres-deployment.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: postgres-deployment
spec:
  replicas: 1
  selector:
    matchlabels:
      component: postgres
  template:
    metadata:
      labels:
        component: postgres
    spec:
      volumes:
        - name: postgres-storage
          persistentvolumeclaim:
            claimname: database-persistent-volume-claim
      containers:
      - name: postgres
        image: postgres
        ports:
        - containerport: 5432
        volumemounts:
          - mountpath: /var/lib/postgresql/data
            name: postgres-storage
            subpath: postgres

when i debug pod using describe
kubectl describe pod postgres-deployment-8576df7bfc-8mp5t

events:
  type     reason     age                  from                     message
  ----     ------     ----                 ----                     -------
  normal   scheduled  3m4s                 default-scheduler        successfully assigned default/postgres-deployment-8576df7bfc-8mp5t to docker-desktop
  normal   pulled     67s (x8 over 2m58s)  kubelet, docker-desktop  successfully pulled image &quot;postgres&quot;
  warning  failed     67s (x8 over 2m58s)  kubelet, docker-desktop  error: failed to prepare subpath for volumemount &quot;postgres-storage&quot; of container &quot;postgres&quot;
  normal   pulling    53s (x9 over 3m3s)   kubelet, docker-desktop  pulling image &quot;postgres&quot;

my pod is showing me this error
$ kubectl get pods
name                                   ready   status                       restarts   age
postgres-deployment-8576df7bfc-8mp5t   0/1     createcontainerconfigerror   0          5m5

i am not sure where is the problem in the config. but i am sure this is related to volumes because after adding volumes this problem appears
",<postgresql><kubernetes><kubernetes-pvc>,59065369,4,"remove subpath. can you try below yaml

apiversion: apps/v1
kind: deployment
metadata:
  name: postgres-deployment
spec:
  replicas: 1
  selector:
    matchlabels:
      component: postgres
  template:
    metadata:
      labels:
        component: postgres
    spec:
      volumes:
        - name: postgres-storage
          persistentvolumeclaim:
            claimname: database-persistent-volume-claim
      containers:
      - name: postgres
        image: postgres
        ports:
        - containerport: 5432
        volumemounts:
          - mountpath: /var/lib/postgresql/data
            name: postgres-storage


i just deployed and it works

master $ kubectl get deploy
name                  ready   up-to-date   available   age
postgres-deployment   1/1     1            1           4m13s
master $ kubectl get po
name                                   ready   status    restarts   age
postgres-deployment-6b66bdd748-5q76h   1/1     running   0          4m13s

"
60754598,"can't upgrade deployment from apiversion extensions/v1beta1 to apps/v1, it uses extensions/v1beta1 automatically","i currently have a gke kubernetes 1.15 cluster and i'm planning to upgrade to 1.16. since 1.16 doesn't support certain apis i have to change my deployments from extensions/v1beta1 to apps/v1.

using this simple deployment.yml:

apiversion: apps/v1
kind: deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerport: 80


when i apply it into my 1.15 cluster: kubectl -n mynamespace deployment.yml, what is actually see is the following (kubectl -n mynamespace get deployments nginx-deployment):

apiversion: extensions/v1beta1
kind: deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: ""1""
    kubectl.kubernetes.io/last-applied-configuration: |
...


as you can see the actual apiversion is extensions/v1beta1 instead of apps/v1. why isn't it applying the version i specified?

update:

this is my kubectl version:

client version: version.info{major:""1"", minor:""17"", gitversion:""v1.17.4"", gitcommit:""8d8aa39598534325ad77120c120a22b3a990b5ea"", gittreestate:""clean"", builddate:""2020-03-12t23:41:24z"", goversion:""go1.14"", compiler:""gc"", platform:""darwin/amd64""}
server version: version.info{major:""1"", minor:""15+"", gitversion:""v1.15.9-gke.24"", gitcommit:""39e41a8d6b7221b901a95d3af358dea6994b4a40"", gittreestate:""clean"", builddate:""2020-02-29t01:24:35z"", goversion:""go1.12.12b4"", compiler:""gc"", platform:""linux/amd64""}

",<kubernetes><google-kubernetes-engine><kubectl><kubernetes-deployment>,61034945,4,"the apiversion returned from kubectl get won't necessarily match up with the actual apiversion of your current configuration.

see here: https://github.com/kubernetes/kubernetes/issues/62283#issuecomment-380968868

quote:


  kubectl get uses server-preferred order, which will prefer the extensions api group for backward compatibility, until extensions is removed. that is to say, kubectl get deployment uses extenions/v1beta1 endpoint by default.
  
  to get deployments under apps api group, you can use kubectl get deployment.apps, which returns you apps/v1 deployments.

"
55191936,unable to access kubernetes clusterip services via nginx-ingress-controller,"i'm a kubernetes amateur trying to use nginx ingress controller on gke. i'm following this google cloud documentation to setup nginx ingress for my services, but, i'm facing issues in accessing the nginx locations.

what's working?


ingress-controller deployment using helm (rbac enabled)
clusterip service deployments


what's not working?


ingress resource to expose multiple clusterip services using unique paths (fanout routing)


k8s services

[msekar@ebs kube-base]$ kubectl get services -n payment-gateway-7682352
name                            type           cluster-ip      external-ip      port(s)                      age
nginx-ingress-controller        loadbalancer   10.35.241.255   35.188.161.171   80:31918/tcp,443:31360/tcp   6h
nginx-ingress-default-backend   clusterip      10.35.251.5     &lt;none&gt;           80/tcp                       6h
payment-gateway-dev             clusterip      10.35.254.167   &lt;none&gt;           5000/tcp                     6h
payment-gateway-qa              clusterip      10.35.253.94    &lt;none&gt;           5000/tcp                     6h


k8s ingress

[msekar@ebs kube-base]$ kubectl get ing -n payment-gateway-7682352
name                hosts     address          ports     age
pgw-nginx-ingress   *         104.198.78.169   80        6h

[msekar@ebs kube-base]$ kubectl describe ing pgw-nginx-ingress -n payment-gateway-7682352
name:             pgw-nginx-ingress
namespace:        payment-gateway-7682352
address:          104.198.78.169
default backend:  default-http-backend:80 (10.32.1.4:8080)
rules:
  host  path  backends
  ----  ----  --------
  *     
        /dev/   payment-gateway-dev:5000 (&lt;none&gt;)
        /qa/    payment-gateway-qa:5000 (&lt;none&gt;)
annotations:
  kubectl.kubernetes.io/last-applied-configuration:  {""apiversion"":""extensions/v1beta1"",""kind"":""ingress"",""metadata"":{""annotations"":{""kubernetes.io/ingress.class"":""nginx"",""nginx.ingress.kubernetes.io/ssl-redirect"":""false""},""name"":""pgw-nginx-ingress"",""namespace"":""payment-gateway-7682352""},""spec"":{""rules"":[{""http"":{""paths"":[{""backend"":{""servicename"":""payment-gateway-dev"",""serviceport"":5000},""path"":""/dev/""},{""backend"":{""servicename"":""payment-gateway-qa"",""serviceport"":5000},""path"":""/qa/""}]}}]}}

  kubernetes.io/ingress.class:               nginx
  nginx.ingress.kubernetes.io/ssl-redirect:  false
events:                                      &lt;none&gt;


last applied configuration in the annotations (ingress description output) shows the ingress resource manifest. but, i'm pasting it below for reference

---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: pgw-nginx-ingress
  namespace: payment-gateway-7682352
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
spec:
  rules:
  - http:
      paths:
      - backend:
          servicename: payment-gateway-dev
          serviceport: 5000
        path: /dev/
      - backend:
          servicename: payment-gateway-qa
          serviceport: 5000
        path: /qa/


additional info

the services i'm trying to access are springboot services that use contexts, so, the root location isn't a valid end-point.

the container's readiness and liveliness probes are defined accordingly.

for example, ""payment-gateway-dev"" service is using a context /pgw/v1 context, so, the deployment can only be accessed through the context. to access application's swagger spec you would use the url

http://&lt;>/pgw/v1/swagger-ui.html

behaviour of my deployment


  ingress-controller-lb-ip = 35.188.161.171



accessing ingress controller load balancer ""http://35.188.161.171"" takes me to default 404 backend
accessing ingress controller load balancer health ""http://35.188.161.171/healthz"" returns 200 http response as expected
trying to access the services using the urls below returns ""404: page not found"" error


http://35.188.161.171/dev/pgw/v1/swagger-ui.html
http://35.188.161.171/qa/pgw/v1/swagger-ui.html



any suggestions about or insights into what i might be doing wrong will be much appreciated.
",<nginx><kubernetes><kubernetes-ingress>,55193468,4,"+1 for this well asked question.

your setup seemed right to me. in you explanation, i could find that your services would require http://&lt;&gt;/pgw/v1/swagger-ui.html as context. however, in your setup the path submitted to the service will be http://&lt;&gt;/qa/pgw/v1/swagger-ui.html if your route is /qa/.

to remove the prefix, what you would need to do is to add a rewrite rule to your ingress:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: pgw-nginx-ingress
  namespace: payment-gateway-7682352
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
  - http:
      paths:
      - backend:
          servicename: payment-gateway-dev
          serviceport: 5000
        path: /dev/(.+)
      - backend:
          servicename: payment-gateway-qa
          serviceport: 5000
        path: /qa/(.+)


after this, you service should receive the correct contexts.

ref:


rewrite: https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/rewrite/readme.md
ingress route matching: https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/ingress-path-matching.md

"
55167725,sticky session for asp.net core on kubernetes deployment,"i try to port an asp.net core 1 application with identity to kubernetes. the login doesn't work and i got different errors like the anti-forgery token could not be decrypted. the problem is that i'm using a deployment with three replica sets so that further request were served by different pods that don't know about the anti-forgery token. using replicas: 3 it works.

in the same question i found a sticky session documentation which seems a solution for my problem. the cookie name .aspnetcore.identity.application is from my browser tools. 

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: myapp-k8s-test
  annotations:
    nginx.ingress.kubernetes.io/affinity: ""cookie""
    nginx.ingress.kubernetes.io/session-cookie-name: "".aspnetcore.identity.application""
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: myapp-k8s
    spec:
      containers:
      - name: myapp-app
        image: myreg/myapp:0.1
        ports:
        - containerport: 80
        env:
        - name: ""aspnetcore_environment""
          value: ""production""
      imagepullsecrets:
      - name: registrypullsecret


this doesn't work, either with or without leading dot at the cookie name. i also tried adding the following annotations

kubernetes.io/ingress.class: nginx
nginx.ingress.kubernetes.io/session-cookie-hash: sha1


what is required to allow sticky sessions on kubernetes with asp.net core?
",<nginx><asp.net-core><kubernetes><kubernetes-ingress><kubernetes-deployment>,55167864,4,"found out that i made two logical mistakes: 


sticky sessions doesn't work this way


i assumed that kubernetes will look into the cookie and create some mapping of cookie hashes to pods. but instead, another session is generated and append to our http header. nginx.ingress.kubernetes.io/session-cookie-name is only the name of those generated cookie. so per default, it's not required to change them. 


scope to the right object


the annotations must be present on the ingress, not the deployment (stupid c&amp;p mistake)

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: myapp-k8s-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/affinity: ""cookie""
    nginx.ingress.kubernetes.io/session-cookie-hash: sha1

spec:
  tls:
  - hosts:
    - myapp-k8s.local
  rules:
  - host: myapp-k8s.local
    http:
      paths:
      - path: /
        backend:
          servicename: myapp-svc
          serviceport: 80


this works as expected. 
"
54474960,dynamic configmap helm template,"i would like to create serveral confimaps with one helm telmplate. therefore i have created a folder for the configs/values and one configfile per configmap. i have read the helm template guide and found nothing helpful for my problem. maybe i missunderstood the possibilities of helm. 

afterwards there is a possibility to create one configmap from serveral files:

apiversion: v1
kind: configmap
metadata:
  name: {{ .release.name }}-configmap
data:
  {{- $files := .files }}
  {{- range tuple ""file1.yaml"" ""file2.yaml"" }}
  {{ $files.get . }}
  {{- end }}


any recommendations would be helpful,
thanks,

best wishes
",<templates><kubernetes><kubernetes-helm>,54492773,4,"thank you for the response. i have something different in mind. my new code makes it a little bit more clearly. 

{{ range $k, $v :=  .values.configs }}
apiversion: v1
kind: configmap
metadata:
  name: configmap
  namespace: {{ $.values.namespace }}
  labels:
    app: ""{{base $v}}""
data:
  key: {{$k}}
  value: {{$v}}
{{ $.files.get  $v }}
{{ end }}


i have a loop over the configmap. my values.yaml looks like

configs
   name: configs/file1
   name: configs/file2


the values are in a separate folder configs, one file per configmap. 

the current problem is, that the result is one configmap with the values of file2. i would expect two configmaps. what is wrong here in my template. 

thank you very much.
"
54942723,vernemq on kubernetes cluster,"i'm trying to install vernemq on a kubernetes cluster over oracle oci usign helm chart.

the kubernetes infrastructure seems to be up and running, i can deploy my custom microservices without a problem.

i'm following the instructions from https://github.com/vernemq/docker-vernemq

here the steps:


helm install --name=""broker"" ./ from helm/vernemq directory


the output is:

name:   broker
last deployed: fri mar  1 11:07:37 2019
namespace: default
status: deployed

resources:
==&gt; v1/rolebinding
name            age
broker-vernemq  1s

==&gt; v1/service
name                     type       cluster-ip    external-ip  port(s)   age
broker-vernemq-headless  clusterip  none          &lt;none&gt;       4369/tcp  1s
broker-vernemq           clusterip  10.96.120.32  &lt;none&gt;       1883/tcp  1s

==&gt; v1/statefulset
name            desired  current  age
broker-vernemq  3        1        1s

==&gt; v1/pod(related)
name              ready  status             restarts  age
broker-vernemq-0  0/1    containercreating  0         1s

==&gt; v1/serviceaccount
name            secrets  age
broker-vernemq  1        1s

==&gt; v1/role
name            age
broker-vernemq  1s


notes:
1. check your vernemq cluster status:
  kubectl exec --namespace default broker-vernemq-0 /usr/sbin/vmq-admin cluster show

2. get vernemq mqtt port
  echo ""subscribe/publish mqtt messages there: 127.0.0.1:1883""
  kubectl port-forward svc/broker-vernemq 1883:1883


but when i do this check

kubectl exec --namespace default broker-vernemq-0 vmq-admin cluster show

i got

node 'vernemq@broker-vernemq-0..default.svc.cluster.local' not responding to pings.
command terminated with exit code 1


i think there is something wrong with subdomain (the double dots without nothing between them)

whit this command

kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name | head -1) -c kubedns


the last log line is

i0301 10:07:38.366826       1 dns.go:552] could not find endpoints for service ""broker-vernemq-headless"" in namespace ""default"". dns records will be created once endpoints show up.


i've also tried with this  custom yaml:

apiversion: apps/v1
kind: statefulset
metadata:
  namespace: default
  name: vernemq
  labels:
    app: vernemq
spec:
  servicename: vernemq
  replicas: 3
  selector:
    matchlabels:
      app: vernemq
  template:
    metadata:
      labels:
        app: vernemq
    spec:
      containers:
      - name: vernemq
        image: erlio/docker-vernemq:latest
        imagepullpolicy: always
        ports:
          - containerport: 1883
            name: mqtt
          - containerport: 8883
            name: mqtts
          - containerport: 4369
            name: epmd
        env:
        - name: docker_vernemq_kubernetes_namespace
          valuefrom:
            fieldref:
              fieldpath: metadata.namespace
        - name: my_pod_name
          valuefrom:
            fieldref:
              fieldpath: metadata.name
        - name: docker_vernemq_allow_anonymous
          value: ""off""
        - name: docker_vernemq_discovery_kubernetes
          value: ""1""
        - name: docker_vernemq_kubernetes_app_label
          value: ""vernemq""
        - name: docker_vernemq_vmq_passwd__password_file
          value: ""/etc/vernemq-passwd/vmq.passwd""
        volumemounts:
          - name: vernemq-passwd
            mountpath: /etc/vernemq-passwd
            readonly: true

      volumes:
      - name: vernemq-passwd
        secret:
          secretname: vernemq-passwd
---
apiversion: v1
kind: service
metadata:
  name: vernemq
  labels:
    app: vernemq
spec:
  clusterip: none
  selector:
    app: vernemq
  ports:
  - port: 4369
    name: epmd
---
apiversion: v1
kind: service
metadata:
  name: mqtt
  labels:
    app: mqtt
spec:
  type: clusterip
  selector:
    app: vernemq
  ports:
  - port: 1883
    name: mqtt
---
apiversion: v1
kind: service
metadata:
  name: mqtts
  labels:
    app: mqtts
spec:
  type: loadbalancer
  selector:
    app: vernemq
  ports:
  - port: 8883
    name: mqtts


any suggestion?

many thanks
jack
",<kubernetes><oracle-cloud-infrastructure><mqtt-vernemq><kubernetes-helm>,55026577,4,"it seems to be a bug in the docker image. the suggestion on github is to built your own image or use the later vernemq image (after 1.6.x) where it has been fixed.

suggestion mentioned here: https://github.com/vernemq/docker-vernemq/pull/92

pull-request for a possible fix: https://github.com/vernemq/docker-vernemq/pull/97

edit: 

i only got it to work without helm. using kubectl create -f ./cluster.yaml, with the following cluster.yaml:

---
apiversion: apps/v1
kind: statefulset
metadata:
  name: vernemq
  namespace: default
spec:
  servicename: vernemq
  replicas: 3
  selector:
    matchlabels:
      app: vernemq
  template:
    metadata:
      labels:
        app: vernemq
    spec:
      serviceaccountname: vernemq
      containers:
      - name: vernemq
        image: erlio/docker-vernemq:latest
        ports:
        - containerport: 1883
          name: mqttlb
        - containerport: 1883
          name: mqtt
        - containerport: 4369
          name: epmd
        - containerport: 44053
          name: vmq
        - containerport: 9100
        - containerport: 9101
        - containerport: 9102
        - containerport: 9103
        - containerport: 9104
        - containerport: 9105
        - containerport: 9106
        - containerport: 9107
        - containerport: 9108
        - containerport: 9109
        env:
        - name: docker_vernemq_discovery_kubernetes
          value: ""1""
        - name: docker_vernemq_kubernetes_app_label
          value: ""vernemq""
        - name: docker_vernemq_kubernetes_namespace
          valuefrom:
            fieldref:
              fieldpath: metadata.namespace
        - name: my_pod_name
          valuefrom:
           fieldref:
             fieldpath: metadata.name
        - name: docker_vernemq_erlang__distribution__port_range__minimum
          value: ""9100""
        - name: docker_vernemq_erlang__distribution__port_range__maximum
          value: ""9109""
        - name: docker_vernemq_kubernetes_insecure
          value: ""1""
        # only allow anonymous access for development / testing purposes!
        # - name: docker_vernemq_allow_anonymous
        #   value: ""on""
---
apiversion: v1
kind: service
metadata:
  name: vernemq
  labels:
    app: vernemq
spec:
  clusterip: none
  selector:
    app: vernemq
  ports:
  - port: 4369
    name: empd
  - port: 44053
    name: vmq
---
apiversion: v1
kind: service
metadata:
  name: mqttlb
  labels:
    app: mqttlb
spec:
  type: loadbalancer
  selector:
    app: vernemq
  ports:
  - port: 1883
    name: mqttlb
---
apiversion: v1
kind: service
metadata:
  name: mqtt
  labels:
    app: mqtt
spec:
  type: nodeport
  selector:
    app: vernemq
  ports:
  - port: 1883
    name: mqtt
---
apiversion: v1
kind: serviceaccount
metadata:
  name: vernemq
---
kind: role
apiversion: rbac.authorization.k8s.io/v1
metadata:
  name: endpoint-reader
rules:
- apigroups: ["""", ""extensions"", ""apps""]
  resources: [""endpoints"", ""deployments"", ""replicasets"", ""pods""]
  verbs: [""get"", ""list""]
---
kind: rolebinding
apiversion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: endpoint-reader
subjects:
- kind: serviceaccount
  name: vernemq
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: endpoint-reader


needs a few seconds to get the pods ready.
"
54762993,no load balancer created and static ip assigned to traefik ingress on gke,"when i set up an ingress controller to point to the traefik service, i expect load balancers to be created for that ingress controller on gke in the same way a loadbalancer service would. i could then point to the static ip created.

however, when i get my ingresses, there is no static ip assigned.

$ kubectl get ingresses -n kube-system
name              hosts                 address   ports     age
traefik-ingress   traefik-ui.minikube             80        4m


traefik-ingress.yml

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: traefik-ingress
  namespace: kube-system
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
  rules:
    - host: traefik-ui.minikube
      http:
        paths:
          - path: ""/""
            backend:
              servicename: traefik-ingress-service
              serviceport: 80


traefik-deployment.yml

---
apiversion: v1
kind: serviceaccount
metadata:
  name: traefik-ingress-controller
  namespace: kube-system
---
kind: deployment
apiversion: apps/v1
metadata:
  name: traefik-ingress-controller
  namespace: kube-system
  labels:
    k8s-app: traefik-ingress-lb
spec:
  replicas: 1
  selector:
    matchlabels:
      k8s-app: traefik-ingress-lb
  template:
    metadata:
      labels:
        k8s-app: traefik-ingress-lb
        name: traefik-ingress-lb
    spec:
      serviceaccountname: traefik-ingress-controller
      terminationgraceperiodseconds: 60
      containers:
        - image: traefik
          name: traefik-ingress-lb
          ports:
            - name: http
              containerport: 80
            - name: admin
              containerport: 8080
          args:
            - --api
            - --kubernetes
            - --loglevel=info
---
kind: service
apiversion: v1
metadata:
  name: traefik-ingress-service
  namespace: kube-system
spec:
  selector:
    k8s-app: traefik-ingress-lb
  ports:
    - protocol: tcp
      port: 80
      name: web
    - protocol: tcp
      port: 8080
      name: admin
  type: nodeport

",<kubernetes><google-cloud-platform><traefik><google-kubernetes-engine>,54764942,4,"you are creating a service object for the traefik deployment, but you have used the nodeport type, which is only accesible from inside the cluster. if you want kubernetes to create a loadbalancer for a service, you need to specify the type loadbalancer in your service, so your traefik service would look like

kind: service
apiversion: v1
metadata:
  name: traefik-ingress-service
  namespace: kube-system
spec:
  selector:
    k8s-app: traefik-ingress-lb
  ports:
    - protocol: tcp
      port: 80
      name: web
    - protocol: tcp
      port: 8080
      name: admin
  type: loadbalancer


this will talk to the gke api and create a loadbalancer with an ip for you.
"
72257944,kubernetes deployment with storageclass,"i have a storageclass with provisioner: kubernetes.io/aws-ebs
and i have a deployment where i need to mount some volumes. so, i need to use this storageclass
here's my sc.yaml
apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  annotations:
  name: gp2
parameters:
  fstype: ext4
  type: gp2
provisioner: kubernetes.io/aws-ebs
reclaimpolicy: delete
volumebindingmode: waitforfirstconsumer

here's my deployment.yaml
apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: postgres
  namespace: var.namespace
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - name: postgres
          image: postgres:10.4
          imagepullpolicy: &quot;ifnotpresent&quot;
          ports:
            - containerport: 5432
          envfrom:
            - configmapref:
                name: postgres-config
          volumemounts:
            - mountpath: /var/lib/postgresql/data
              name: postgredb
      # how can i specify my storage class here?
      volumes:
        - name: postgredb
          persistentvolumeclaim:
            claimname: postgres-pv-claim

i need to specify the storage class in my deployment. can someone help me?
",<kubernetes><kubernetes-deployment>,72260374,4,"
i need to specify the storage class in my deployment. can someone help me?

before we answer, first let's explain how stroageclass, persistentvolume, persistent volume claim.
terminology
stroageclass

we can look at storageclass as a driver (software).
a driver which responsible for the &quot;communication&quot; with the storage hardware.
usually but not a must, the storageclass is supplied by the storage provider (hardware or virtual)

persistentvolume

a persistentvolume (pv) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using storage classes.

persistentvolumeclaim

a persistentvolumeclaim (pvc) is a request for storage by a user (usually pod)


general diagram (describing k8 storage objects)

tl;dr; explanation

you have physical storage (disk, ssd, virtual, etc)
someone (usually the storage or the cloud provider) supplied you with the storageclass object. by the way, you don't need to define/declare it most of the time and k8s will supply default storage for you (emptydir).
then you define persistentvolume (pv) which will &quot;create&quot; storage based upon the type (storageclass) you require.
next step is to define persistentvolumeclaim (pvc). the pvc is the allocation of the &quot;physical&quot; storage mounted from the (pv) which you defined in the previous step.
the last step is to &quot;assign&quot; volume to your execution (pod, deployment, statefulset, etc) which is done using volumes.

** notes

as mentioned above most of the time you can simply use volumes without the need to define storageclass or pv/pvc. simply use a volume in the required  resources and k8s will take care of that for you.
there are some exceptions (without getting into too many details here like staefulset).
if no storageclass is specified, then the default storageclass will be used


now let's answer your question

you have defined a storageclass

apiversion: storage.k8s.io/v1
kind: storageclass
...
provisioner: kubernetes.io/aws-ebs


in your deployment, you specified volumes (identation left as as)

apiversion: extensions/v1beta1
kind: deployment
...
        
        # --&gt; here you define the actual mount path you need for your pods
        #     the name (pvc) is corresponding to the one you 
        #     defined below under volumes
        volumemounts:
            - mountpath: /var/lib/postgresql/data
              name: postgredb
      # -&gt;&gt;&gt;&gt; how can i specify my storage class here?
      # you don't need to specify storage class, you need to define pvc,
      # this is the missing piece in your code.
      volumes:
        - name: postgredb
          persistentvolumeclaim:
            claimname: postgres-pv-claim

the missing piece ...
kind: persistentvolumeclaim
apiversion: v1
metadata:
  name: postgres-pv-claim ### &lt;&lt;&lt; the name as mentioned above in your deploymnet
  labels:
    app: postgres
spec:
  # the name of the storage class u defined earlier
  storageclassname: gp2

  # the access modes are:
  #   readwriteonce - the volume can be mounted as read-write by a single node
  #   readwritemany - the volume can be mounted as read-write by a many nodes
  #   readonlymany  - the volume can be mounted as read-only  by many nodes
  accessmodes:
    - readwritemany
  resources:
    requests:
      storage: 1gi


hope it helped you out.
"
64682659,creating a docker container that runs forever using bash,"i'm trying to create a pod with a container in it for testing purposes that runs forever using the k8s api. i have the following yaml spec for the pod which would run a container and exit straight away:
apiversion: v1
kind: pod
metadata:
  name: pod-example
spec:
  containers:
  - name: ubuntu
    image: ubuntu:trusty
    command: [&quot;echo&quot;]
    args: [&quot;hello world&quot;]

i can't find any documentation around the command: tag but ideally i'd like to put a while loop in there somewhere printing out numbers forever.
",<kubernetes><kubernetes-pod>,64695049,4,"if you want to keep printing hello every few seconds you can use:
apiversion: v1
kind: pod
metadata:
  name: busybox2
  labels:
    app: busybox
spec:
  containers:
  - name: busybox
    image: busybox
    ports:
    - containerport: 80
    command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;while :; do echo 'hello'; sleep 5 ; done&quot;]

you can see the output using kubectl logs &lt;pod-name&gt;
other option to keep a container running without printing anything is to use sleep command on its own, for example:
command: [&quot;/bin/sh&quot;, &quot;-ec&quot;, &quot;sleep 10000&quot;]

"
58489453,k8s ingress redirect to endpoint outside the cluster,"i am using google clould, gke.

i have this example ingress.yaml:

  1 apiversion: extensions/v1beta1
  2 kind: ingress
  3 metadata:
  4   name: kuard
  5   namespace: sample
  6   annotations:
  7     kubernetes.io/ingress.class: ""nginx""
  8     cert-manager.io/issuer: ""letsencrypt-prod""
  9     nginx.ingress.kubernetes.io/permanent-redirect: https://www.google.com
 10 spec:
 11   tls:
 12   - hosts:
 13     - example.gordion.io
 14     secretname: quickstart-example-tls
 15   rules:
 16   - host: example.gordion.io
 17     http:
 18       paths:
 19       - path: /
 20         backend:
 21           servicename: kuard
 22           serviceport: 80


i need that when user request specific host, like: example-2.gordion.io, to be redirected to other site, outside the cluster, (on other google cluster actually), using nginx.

currently i am aware only to the specific annonation nginx.ingress.kubernetes.io/permanent-redirect which seems to be global. how is it possible to redirct based on specific requested host in this ingress file?
",<nginx><kubernetes><google-cloud-platform><kubernetes-ingress>,58493229,4,"you combine an externalname service with another ingress file:
in the following yaml file we define an externalname service with the name example-2.gordion.io-service, which will lead to the real site service in the other cluster:

kind: service
apiversion: v1
metadata:
  name: example-2.gordion.io-service
spec:
  type: externalname
  externalname: internal-name-of-example-2.gordion.io


and an ingress file to direct example-2.gordion.io to example-2.gordion.io-service:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: example-ingress
  annotations:
    kubernetes.io/ingress.class: ""nginx""
spec:
  rules:
  - host: example-2.gordion.io
    http:
      paths:
      - path: /
        backend:
          servicename: example-2.gordion.io-service
          serviceport: 80


"
72223045,how to configure nginx ingress to use single ingress load balancer for multiple hosts and multiple namespaces,"i'm planning to deploy more than 30 apps in 5 namespaces. i will be using existing aws eks 1.21 cluster. all the apps will be requiring external access because it's being used by clients. i don't want to use multiple load balancers to ease the management and also avoiding extra cost on aws side (because elb is charged based on hourly usage too.)
what i'm trying to do it basically ;
 apiversion: networking.k8s.io/v1
 kind: ingress
 metadata:
   name: random-ingress
 spec:
   rules:
   - host: randomhost-in-namespace1.com
     http:
       paths:
       - path: /
         backend:
           servicename: randomhost-in-namespace1 (in first namespace)
           serviceport: 80
   - host: randomhost-in-namespace2.com
     http:
       paths:
       - path: /
         backend:
           servicename: randomhost-in-namespace2 (in second namespace)
           serviceport: 80
   - host: randomhost-in-namespace3.com
     http:
       paths:
       - path: /
         backend:
           servicename: randomhost-in-namespace3 (in third namespace)
           serviceport: 80

something like this.
is it possible to cover all these apps in all these namespaces with a single ingress load balancer? i didn't find any clear information about this.
any help will be highly appreciated.
thank you.
",<kubernetes><amazon-eks><ingress-controller>,72416735,4,"using the aws lb controller and not nginx lb, you can have 1x alb, re-used by each namespace.
define ingress.yaml file per namespace and annotate them with the same alb-group-name.

if group.name specified, all ingresses with this ingressclass will
belong to the same ingressgroup specified and result in a single alb.

the aws lb controller will then create 1x alb, it desired rules, listeners to tg's and register the right ec2 nodes etc.
this can be something like this:
ingress-namespace1.yaml
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: random-ingress
  namespace: namespace1
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/group.name: my-group
    alb.ingress.kubernetes.io/scheme: internet-facing
spec:
  rules:
    - host: randomhost-in-namespace1.com
      http:
        paths:
          - path: /
            backend:
              servicename: randomhost-in-namespace1 (in first namespace)
              serviceport: 80

ingress-namespace2.yaml
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: random-ingress
  namespace: namespace2
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/group.name: my-group
    alb.ingress.kubernetes.io/scheme: internet-facing
spec:
  rules:
    - host: randomhost-in-namespace2.com
      http:
        paths:
          - path: /
            backend:
              servicename: randomhost-in-namespace2 (in second namespace)
              serviceport: 80

where both files contain same group.name and differ by their namespace and host rule.
you can also follow aws lbc logs to see if everything has been created successfully (should contain no errors on logs):
kubectl logs deploy/aws-load-balancer-controller -n kube-system --follow

"
57852419,can't use persistent host key on sftp kubernetes deployment,"i'm running atmoz/sftp deployment for sftp on gke. i succeeded mounting a persistent volume and using a configmap to mount public keys for users, but i can't mount a host key so every time my container is restarting i'm getting a warning that my host key has changed.. 

i tried to mount it to /etc/ssh and changing sshd_config and nothing worked - it says file already exists, overwrite? (y/n) and i can't manipulate it because it's inside the container.

and even if i try to run a command, any command like echo, the container is turning into crashloopbackhoff

my configmap:

apiversion: v1
data:
  ssh_host_rsa_key: |
    &lt;my key&gt;
kind: configmap
metadata:
  name: ssh-host-rsa
  namespace: default


my deployment yaml:

apiversion: apps/v1
kind: deployment
metadata:
  annotations:
  name: sftp
  namespace: default
spec:
  progressdeadlineseconds: 600
  replicas: 1
  revisionhistorylimit: 10
  selector:
    matchlabels:
      app: sftp
  strategy:
    rollingupdate:
      maxsurge: 25%
      maxunavailable: 25%
    type: rollingupdate
  template:
    metadata:
      creationtimestamp: null
      labels:
        app: sftp
    spec:
      containers:
      - args:
        - client::::sftp
        env:
        - name: sftp
          value: ""1""
        image: atmoz/sftp
        imagepullpolicy: ifnotpresent
        name: sftp
        ports:
        - containerport: 22
          name: sftp
          protocol: tcp
        resources: {}
        securitycontext:
          capabilities:
            add:
            - sys_admin
          procmount: default
        terminationmessagepath: /dev/termination-log
        terminationmessagepolicy: file
        volumemounts:
        - mountpath: /home/client/sftp
          name: sftp
        - mountpath: /home/client/.ssh/keys
          name: sftp-public-keys
        - mountpath: /etc/ssh
          name: ssh-host-ed25519
      dnspolicy: clusterfirst
      restartpolicy: always
      schedulername: default-scheduler
      securitycontext:
        fsgroup: 100
      terminationgraceperiodseconds: 30
      volumes:
      - name: sftp
        persistentvolumeclaim:
          claimname: sftp-uat
      - configmap:
          defaultmode: 420
          name: sftp-public-keys
        name: sftp-public-keys
      - configmap:
          defaultmode: 420
          name: ssh-host-ed25519
        name: ssh-host-ed25519


the echo test: 

containers:
          - args:
            - client::::sftp
            env:
            - name: sftp
              value: ""1""
            image: atmoz/sftp
            command:
            - ""echo hi""
            imagepullpolicy: ifnotpresent
            name: sftp
            ports:
            - containerport: 22
              name: sftp
              protocol: tcp
            resources: {}
            securitycontext:
              capabilities:
                add:
                - sys_admin
              procmount: default
            terminationmessagepath: /dev/termination-log
            terminationmessagepolicy: file
            volumemounts:
            - mountpath: /home/client/sftp
              name: sftp
            - mountpath: /home/client/.ssh/keys
              name: sftp-public-keys
            - mountpath: /etc/ssh
              name: ssh-host-ed25519


any ideas?
",<ssh><kubernetes><containers><sftp><google-kubernetes-engine>,57885755,4,"""not sure if you're still looking for a way to get host keys to persist, but mounting host key secrets into their relevant /etc/ssh/ files seems to work for me, eg.""

kind: deployment
...
spec:
  template:
    spec:
      #secrets and config
      volumes:
      ...
      - name: sftp-host-keys
        secret:
          secretname: sftp-host-keys
          defaultmode: 0600
      ...
      containers:
        #the sftp server itself
        - name: sftp
          image: atmoz/sftp:latest
          ...
          volumemounts:
          - mountpath: /etc/ssh/ssh_host_ed25519_key
            name: sftp-host-keys
            subpath: ssh_host_ed25519_key
            readonly: true
          - mountpath: /etc/ssh/ssh_host_ed25519_key.pub
            name: sftp-host-keys
            subpath: ssh_host_ed25519_key.pub
            readonly: true
          - mountpath: /etc/ssh/ssh_host_rsa_key
            name: sftp-host-keys
            subpath: ssh_host_rsa_key
            readonly: true
          - mountpath: /etc/ssh/ssh_host_rsa_key.pub
            name: sftp-host-keys
            subpath: ssh_host_rsa_key.pub
            readonly: true
            ...
---
apiversion: v1
kind: secret
metadata:
  name: sftp-host-keys
  namespace: sftp
stringdata:
  ssh_host_ed25519_key: |
    -----begin openssh private key-----
    ...
    -----end openssh private key-----
  ssh_host_ed25519_key.pub: |
    ssh-ed25519 aaaa...
  ssh_host_rsa_key: |
    -----begin rsa private key-----
    ...
    -----end rsa private key-----
  ssh_host_rsa_key.pub: |
    ssh-rsa aaaa...
type: opaque

"
67662353,four different errors when using kubectl apply command,"my docker-compose file is as below:
version: '3'

services:
    nginx:
        build: .
        container_name: nginx
        ports:
            - '80:80'

and my dockerfile:
from nginx:alpine

i used kompose konvert and it created two files called nginx-deployment.yml and nginx-service.yml with the below contents.
nginx-deployment.yml:
apiversion: apps/v1
kind: deployment
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.22.0 (955b78124)
  creationtimestamp: null
  labels:
    io.kompose.service: nginx
  name: nginx
spec:
  replicas: 1
  selector:
    matchlabels:
      io.kompose.service: nginx
  strategy: {}
  template:
    metadata:
      annotations:
        kompose.cmd: kompose convert
        kompose.version: 1.22.0 (955b78124)
      creationtimestamp: null
      labels:
        io.kompose.service: nginx
    spec:
      containers:
        - image: nginx:alpine
          name: nginx
          ports:
            - containerport: 80
          resources: {}
      restartpolicy: always
status: {}

and nginx-service.yml:
apiversion: v1
kind: service
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.22.0 (955b78124)
  creationtimestamp: null
  labels:
    io.kompose.service: nginx
  name: nginx
spec:
  ports:
    - name: &quot;80&quot;
      port: 80
      targetport: 80
  selector:
    io.kompose.service: nginx
status:
  loadbalancer: {}

my kustomization.yml:
resources:
        - nginx-deployment.yml
        - nginx-service.yml

all files are in the same path /home.
i run these three commands but for each of them i got a different error:

kubectl apply -f kustomization.yml:

error: error validating &quot;kustomization.yml&quot;: error validating data: [apiversion not set, kind not set]; if you choose to ignore these errors, turn validation off with --validate=false


kubectl apply -k .:

error: accumulating resources: accumulation err='accumulating resources from 'nginx-deployment.yml': evalsymlink failure on '/home/nginx-deployment.yml' : lstat /home/nginx-deployment.yml: no such file or directory': evalsymlink failure on '/home/nginx-deployment.yml' : lstat /home/nginx-deployment.yml: no such file or directory


kubectl apply -f kustomization.yml --validate=false:

error: unable to decode &quot;kustomization.yml&quot;: object 'kind' is missing in '{&quot;resources&quot;:[&quot;nginx-deployment.yml&quot;,&quot;nginx-service.yml&quot;]}'


kubectl apply -k . --validate=false:

error: accumulating resources: accumulation err='accumulating resources from 'nginx-deployment.yml': evalsymlink failure on '/home/nginx-deployment.yml' : lstat /home/nginx-deployment.yml: no such file or directory': evalsymlink failure on '/home/nginx-deployment.yml' : lstat /home/nginx-deployment.yml: no such file or directory

the kubernetes is a single node.
why i'm getting these errors and how may i run my container in this environment?
",<kubernetes><kubectl><kustomize>,67662695,4,"your kustomization.yml file has two errors. the files generated by kompose have .yaml extensions but you are referring to .yml and you are missing the kind and apiversion lines.
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization

resources:
  - nginx-deployment.yaml
  - nginx-service.yaml

kubectl apply -k .

"
68086502,why does eks say my fluent-bit.conf is not valid,"i am trying to setup fluent bit for kuberentes on eks + fargate. i was able to get logs all going to one general log group on cloudwatch but now when i add fluent-bit.conf: | to the data: field  and try to apply the update to my cluster, i get this error:

for: &quot;fluentbit-config.yaml&quot;: admission webhook &quot;0500-amazon-eks-fargate-configmaps-admission.amazonaws.com&quot; denied the request: fluent-bit.conf is not valid. please only provide output.conf, filters.conf or parsers.conf in the logging configmap

what sticks out the most to me is that the error message is asking me to only provide output, filter or parser configurations.
it matches up with other examples i found online, but it seems like i do not have the fluent-bit.conf file on the cluster that i am updating or something. the tutorials i have followed do not mention installing a file so i am lost as to why i am getting this error.
the
my fluentbit-config.yaml file looks like this
kind: namespace
apiversion: v1
metadata:
  name: aws-observability
  labels:
    aws-observability: enabled
---
kind: configmap
apiversion: v1
metadata:
  name: aws-logging
  namespace: aws-observability
  labels:
    k8s-app: fluent-bit
data:
  fluent-bit.conf: |
    @include input-kubernetes.conf
    
  input-kubernetes.conf: |
    [input]
        name tail
        parser docker
        tag logger
        path /var/log/containers/*logger-server*.log
        
  output.conf: |
    [output]
        name cloudwatch_logs
        match logger
        region us-east-1
        log_group_name fluent-bit-cloudwatch
        log_stream_prefix from-fluent-bit-
        auto_create_group on

",<amazon-web-services><kubernetes><logging><amazon-eks><fluent-bit>,68086878,4,"as per docs (at the very bottom of that page and yeah, we're in the process of improving them, not happy with the current state) you have a couple of sections in there that are not allowed in the context of eks on fargate logging, more specifically what can go into the configmap. what you want is something along the lines of the following (note: this is from an actual deployment i'm using, slightly adapted):
kind: configmap
apiversion: v1
metadata:
  name: aws-logging
  namespace: aws-observability
data:
  output.conf: |
     [output]
        name cloudwatch_logs
        match *
        region eu-west-1
        log_group_name something-fluentbit
        log_stream_prefix fargate-
        auto_create_group on
     [output]
        name  es
        match *
        host blahblahblah.eu-west-1.es.amazonaws.com
        port 443
        index something
        type  something_type
        aws_auth on
        aws_region eu-west-1
        tls   on


with this config, you're streaming logs to both cw and aes, so feel free to drop the second output section if not needed. however, you notice that there can not be the other sections that you had there such as input-kubernetes.conf for example.
"
69423932,container initialization order in pod on k8s,"i want to run two containers on a single pod.
container1 is a test that tries to connect to a sql server database that runs on container2.
how can i make sure that the sql container (container2) will run and be ready before container1 starts?
initcontainer won't work here, as it will run before both containers.
this is my compose.yaml:
apiversion: v1
kind: pod
metadata:
  name: sql-test-pod
  labels:
    name: sql-test
spec:
  restartpolicy: never
  containers:
    - name: my-sqldb
      image: docker-registry.com/database
      imagepullpolicy: always
      resources:
        limits:
          memory: &quot;4096mi&quot;
          cpu: &quot;750m&quot;
        requests:
          memory: &quot;4096mi&quot;
          cpu: &quot;750m&quot;
    - name: tests
      tty: true
      stdin: true
      image: docker-registry.com/test
      imagepullpolicy: always
      resources:
        limits:
          memory: &quot;4096mi&quot;
          cpu: &quot;750m&quot;
        requests:
          memory: &quot;4096mi&quot;
          cpu: &quot;750m&quot;
      env:
      - name: sqlhostname
        value: &quot;sqlhostnameplaceholder&quot;
  nodeselector:
    kubernetes.io/os: windows
  tolerations:
  - key: &quot;windows&quot;
    operator: &quot;equal&quot;
    value: &quot;2019&quot;
    effect: &quot;noschedule&quot;

",<kubernetes><kubernetes-pod>,69484044,4,"in order to make sure that container1 will start only after container2's sql server is up the only way i found is to use poststart container's lifecycle event.
poststart triggered after after the container is create, it is true that  there is no guarantee, that the poststart handler is called before the container's entrypoint is called, but it turns out that the kubelet code that starts the container blocks the start of the next container until the post-start handler terminates.
and this is how my new compose file will look like:
apiversion: v1
kind: pod
metadata:
  name: sql-test-pod
  labels:
    name: sql-test
spec:
  restartpolicy: never
  containers:
    - name: my-sqldb
      image: docker-registry.com/database
      imagepullpolicy: always
      lifecycle:
        poststart:
          exec:
            command: ['powershell.exe', '-command', &quot;$connectionstring = 'server=sql-test-pod;user id=user;password=password'; $sqlconnection = new-object system.data.sqlclient.sqlconnection $connectionstring; $i=0; while($i -lt 6) {try { $i++;$sqlconnection.open();$sqlconnection.close(); return}catch {write-error $_; start-sleep 30}}&quot;]
      resources:
        limits:
          memory: &quot;4096mi&quot;
          cpu: &quot;750m&quot;
        requests:
          memory: &quot;4096mi&quot;
          cpu: &quot;750m&quot;
    - name: tests
      tty: true
      stdin: true
      image: docker-registry.com/test
      imagepullpolicy: always
      resources:
        limits:
          memory: &quot;4096mi&quot;
          cpu: &quot;750m&quot;
        requests:
          memory: &quot;4096mi&quot;
          cpu: &quot;750m&quot;
      env:
      - name: sqlhostname
        value: &quot;sql-test-pod&quot;
  nodeselector:
    kubernetes.io/os: windows
  tolerations:
  - key: &quot;windows&quot;
    operator: &quot;equal&quot;
    value: &quot;2019&quot;
    effect: &quot;noschedule&quot;

similar case you can find here
"
55935077,single service with multiple exposed ports on a pod with multiple containers,"i have gotten multiple containers to work in the same pod.

 kubectl apply  -f myymlpod.yml

kubectl expose pod mypod --name=myname-pod --port 8855 --type=nodeport


then i was able to test the ""expose""

minikube service list


..

|-------------|-------------------------|-----------------------------|
|  namespace  |          name           |             url             |
|-------------|-------------------------|-----------------------------|
| default     | kubernetes              | no node port                |
| default     | myname-pod              | http://192.168.99.100:30036 |
| kube-system | kube-dns                | no node port                |
| kube-system | kubernetes-dashboard    | no node port                |
|-------------|-------------------------|-----------------------------|


now, my myymlpod.yml has multiple containers in it.
one container has a service running on 8855, and one on 8877.

the below article ~hints~ at what i need to do .

https://www.mirantis.com/blog/multi-container-pods-and-container-communication-in-kubernetes/


  exposing multiple containers in a pod
  
  while this example shows how to
  use a single container to access other containers in the pod, its
  quite common for several containers in a pod to listen on different
  ports  all of which need to be exposed. to make this happen, you can
  either create a single service with multiple exposed ports, or you can
  create a single service for every poirt youre trying to expose.


""create a single service with multiple exposed ports""

i cannot find anything on how to actually do this, expose multiple ports.

how does one expose multiple ports on a single service?

thank you.

append:

k8containers.yml (below)

apiversion: v1
kind: pod
metadata:
  name: mypodkindmetadataname
  labels:
    example: mylabelname

spec:

  containers:

  - name: containername-springbootfrontend
    image: mydocker.com/webfrontendspringboot:latest 
    resources:
      limits:
        memory: ""800mi""
        cpu: ""800m"" 
      requests:
        memory: ""612mi""
        cpu: ""400m""
    ports:
      - containerport: 8877


  - name: containername-businessservicesspringboot
    image: mydocker.com/businessservicesspringboot:latest
    resources:
      limits:
        memory: ""800mi""
        cpu: ""800m"" 
      requests:
        memory: ""613mi""
        cpu: ""400m"" 
    ports:
      - containerport: 8855




kubectl apply  -f k8containers.yml
pod ""mypodkindmetadataname"" created

kubectl get pods
name                    ready     status    restarts   age
mypodkindmetadataname   2/2       running   0          11s




k8services.yml (below)

apiversion: v1
kind: service
metadata: 
  name: myymlservice
  labels: 
    name: myservicemetadatalabel
spec: 
  type: nodeport
  ports:
  - name: myrestservice-servicekind-port-name
    port: 8857
    targetport: 8855
  - name: myfrontend-servicekind-port-name
    port: 8879
    targetport: 8877
  selector: 
    name: mypodkindmetadataname


........

kubectl apply  -f k8services.yml
service ""myymlservice"" created


........

 minikube service myymlservice --url
http://192.168.99.100:30784
http://192.168.99.100:31751


........

 kubectl describe service myymlservice


name:                     myymlservice
namespace:                default
labels:                   name=myservicemetadatalabel
annotations:              kubectl.kubernetes.io/last-applied-configuration={""apiversion"":""v1"",""kind"":""service"",""metadata"":{""annotations"":{},""labels"":{""name"":""myservicemetadatalabel""},""name"":""myymlservice"",""namespace"":""default""...
selector:                 name=mypodkindmetadataname
type:                     nodeport
ip:                       10.107.75.205
port:                     myrestservice-servicekind-port-name  8857/tcp
targetport:               8855/tcp
nodeport:                 myrestservice-servicekind-port-name  30784/tcp
endpoints:                &lt;none&gt;
port:                     myfrontend-servicekind-port-name  8879/tcp
targetport:               8877/tcp
nodeport:                 myfrontend-servicekind-port-name  31751/tcp
endpoints:                &lt;none&gt;
session affinity:         none
external traffic policy:  cluster
events:                   &lt;none&gt;


....

unfortunately, it is still not working when i try to invoke the ""exposed"" items.

calling

http://192.168.99.100:30784/myrestmethod

does not work

and calling

http://192.168.99.100:31751

or

http://192.168.99.100:31751/index.html

does not work

anyone see what i'm missing.

append (working now)

the selector does not match on ""name"", it matches on label(s).

k8containers.yml (partial at the top)

apiversion: v1
kind: pod
metadata:
  name: mypodkindmetadataname
  labels:
    myexamplelabelone: mylabelonevalue
    myexamplelabeltwo: mylabeltwovalue

spec:

  containers:

  # main application container
  - name: containername-springbootfrontend
    image: mydocker.com/webfrontendspringboot:latest 
    resources:
      limits:
        memory: ""800mi""
        cpu: ""800m"" 
      requests:
        memory: ""612mi""
        cpu: ""400m""
    ports:
      - containerport: 8877


  - name: containername-businessservicesspringboot
    image: mydocker.com/businessservicesspringboot:latest
    resources:
      limits:
        memory: ""800mi""
        cpu: ""800m"" 
      requests:
        memory: ""613mi""
        cpu: ""400m"" 
    ports:
      - containerport: 8855


k8services.yml

apiversion: v1
kind: service
metadata: 
  name: myymlservice
  labels: 
    name: myservicemetadatalabel
spec: 
  type: nodeport
  ports:
  - name: myrestservice-servicekind-port-name
    port: 8857
    targetport: 8855
  - name: myfrontend-servicekind-port-name
    port: 8879
    targetport: 8877
  selector: 
    myexamplelabelone: mylabelonevalue
    myexamplelabeltwo: mylabeltwovalue

",<docker><kubernetes><kubernetes-pod>,55935176,4,"yes you can create one single service with multiple ports open or service port connect pointing to container ports. 

kind: service
apiversion: v1
metadata:
  name: mymlservice
spec:
  selector:
    app: mymlapp
  ports:
  - name: servicename-1
    port: 4444
    targetport: 8855
  - name: servicename-2
    port: 80
    targetport: 8877


where target ports are poting out to your container ports.
"
71558824,enable cors kubernetes ingress issues,"i have a laravel backend api and an angular frontend. i deploy them with kubernetes  on minikube.
name                                      ready   status    restarts      age
pod/backend-deployment-bd4f98697-c2scp    1/1     running   1 (22m ago)   23m
pod/frontend-deployment-8bc989f89-cxj67   1/1     running   0             23m
pod/mysql-0                               1/1     running   0             23m

name                       type        cluster-ip      external-ip   port(s)          age
service/backend-service    nodeport    10.108.40.53    &lt;none&gt;        8000:30670/tcp   23m
service/frontend-service   nodeport    10.105.57.226   &lt;none&gt;        4200:32353/tcp   23m
service/kubernetes         clusterip   10.96.0.1       &lt;none&gt;        443/tcp          25m
service/mysql              clusterip   none            &lt;none&gt;        3306/tcp         23m

name                                  ready   up-to-date   available   age
deployment.apps/backend-deployment    1/1     1            1           23m
deployment.apps/frontend-deployment   1/1     1            1           23m

name                                            desired   current   ready   age
replicaset.apps/backend-deployment-bd4f98697    1         1         1       23m
replicaset.apps/frontend-deployment-8bc989f89   1         1         1       23m

name                     ready   age
statefulset.apps/mysql   1/1     23m

i can access both the front service and the back service with minikube service service-name.

this work perfectly. 


i have also an ingress for the frontend.
name               class    hosts                          address     ports   age
frontend-ingress   &lt;none&gt;   kubiapp-frontend-group35.com   localhost   80      27m

i can access the ingress with a curl. curl http://kubiapp-frontend-group35.com. 
but, when i check the url on a browser, i get some cors errors.

the ingress works but i have cors error. 


frontend-ingress:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/enable-cors: &quot;true&quot;
    nginx.ingress.kubernetes.io/cors-allow-origin: &quot;*&quot;
    nginx.ingress.kubernetes.io/cors-allow-methods: &quot;put, get, post, options, delete&quot;
    nginx.ingress.kubernetes.io/cors-allow-headers: &quot;dnt,x-customheader,x-lang,keep-alive,user-agent,x-requested-with,if-modified-since,cache-control,content-type,x-api-key,x-device-id,access-control-allow-origin&quot;
  name: frontend-ingress
  labels:
    name: frontend-ingress
spec:
  rules:
    - host: kubiapp-frontend-group35.com
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: frontend-service
                port:
                  number: 4200

i don't understand the ingress annotation well, does it enable cors policy for the frontend? shouldn't i make it for the backend? do i have to create a backend ingress?
edit:
the error message was:

cross-origin request blocked: the same origin policy disallows reading the remote resource at http://backend-service:8000/api/health. (reason: cors request did not succeed). status code: (null).


tl;dr :
i can access the ingress url, but i have cors errors. how to solve it with kubernetes
",<kubernetes><cors><google-kubernetes-engine><nginx-ingress>,71572449,4,"the mistake i made was using the kubernetes annotations for the frontend ingress.
what i had to do was create a backend ingress as well, that used the annotations:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/enable-cors: &quot;true&quot;
    nginx.ingress.kubernetes.io/cors-allow-origin: &quot;*&quot;
    nginx.ingress.kubernetes.io/cors-allow-methods: &quot;put, get, post, options, delete&quot;
    nginx.ingress.kubernetes.io/cors-allow-headers: &quot;dnt,x-customheader,x-lang,keep-alive,user-agent,x-requested-with,if-modified-since,cache-control,content-type,x-api-key,x-device-id,access-control-allow-origin&quot;
  name: backend-ingress
  labels:
    name: backend-ingress
spec:
  rules:
    - host: kubiapp-backend-group35.com
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: backend-service
                port:
                  number: 8000

the frontend app, will now communicate correctly with http://kubiapp-backend-group35.com.
"
71496560,kustomize using strategic merge patch on helmreleases,"we keep in our flux repo our helmreleases. we use kustomize to edit some of the keys in the helmreleases. i tried using strategic merge patch in order to append a value into a list but instead the list was overwritten (which is the default it seems..)
is there a way to use strategic merge patch on helmreleases in a way that will allow me to append values to a list (patch - merge) ?
my base.yaml is :
apiversion: helm.toolkit.fluxcd.io/v2beta1
kind: helmrelease
metadata:
  name: myreleasex
spec:
  releasename: servicexrelease
  chart:
    spec:
      chart: servicexchart
      sourceref:
        kind: helmrepository
        name: my-repo
      valuesfiles:
        - values.yaml
  values:
     env:
        - name: account
          value: &quot;x5&quot; 

my kustomization file under overlay dir:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
resources:
  - ../base/servicex
patchesstrategicmerge:
  - servicex.yaml

i want to add in my overlay another env variable (i don't want to overwrite it the existing env).
when i tried the following in my overlay/servicex.yaml the list was overwritten and i had only one value:
apiversion: helm.toolkit.fluxcd.io/v2beta1
kind: helmrelease
metadata:
  name: servicexrelease
spec:

  values:
     env:
       - name: new_env
         value: &quot;val&quot;

is my only option using json patches instead of strategic merge patch like suggested here (just use merge instead of replace) ?
",<kubernetes><kubernetes-helm><kustomize>,71498143,4,"personally, i prefer the solution @the fool suggested. however, in my case that solution didn't work, might be related to kustomize's version or the apiversion i used (v4.4.1).
the following is the solution i used (json patches) :
my base/servicex.yaml is kept the same as i posted.
the kustomization file
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
resources:
  - ../base/servicex # 

patches:
  - path: patch-env.yaml
    target:
      group: helm.toolkit.fluxcd.io
      version: v2beta1
      kind: helmrelease
      name: myreleasex

the patch file :
- op: add
  path: &quot;/spec/values/env/0&quot;
  value:
    name: new_env
    value: val

"
57638780,kubernetes istio ingress gateway responds with 503 always,"i'm configuring istio using helm. here you can find my istio-config.yaml:

global:
  proxy:
    accesslogfile: ""/dev/stdout""
    resources:
      requests:
        cpu: 10m
        memory: 40mi

  disablepolicychecks: false

sidecarinjectorwebhook:
  enabled: true
  rewriteapphttpprobe: false

pilot:
  autoscaleenabled: false
  tracesampling: 100.0
  resources:
    requests:
      cpu: 10m
      memory: 100mi

mixer:
  policy:
    enabled: true
    autoscaleenabled: false
    resources:
      requests:
        cpu: 10m
        memory: 100mi

  telemetry:
    enabled: true
    autoscaleenabled: false
    resources:
      requests:
        cpu: 50m
        memory: 100mi

  adapters:
    stdio:
      enabled: true

grafana:
  enabled: true

tracing:
  enabled: true

kiali:
  enabled: true
  createdemosecret: true

gateways:
  istio-ingressgateway:
    autoscaleenabled: false
    resources:
      requests:
        cpu: 10m
        memory: 40mi

  istio-egressgateway:
    enabled: true
    autoscaleenabled: false
    resources:
      requests:
        cpu: 10m
        memory: 40mi

global:
  controlplanesecurityenabled: false

  mtls:
    enabled: false



then i deployed a bunch of microservices using istioctl, all of them are simple rest call using http. they can communicate with each other without any issue. if i exposed them with nodeports i can reach and communicate with them correctly.

here are my services:

$ kubectl get svc --all-namespaces
namespace              name                             type           cluster-ip       external-ip   port(s)                                                                                                                                      age
default                activemq                         clusterip      none             &lt;none&gt;        61616/tcp                                                                                                                                    3h17m
default                activemq-np                      nodeport       10.110.76.147    &lt;none&gt;        8161:30061/tcp                                                                                                                               3h17m
default                api-exchange                     clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h16m
default                api-response                     clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h16m
default                authorization-server             clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h17m
default                de-communication                 clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h16m
default                gateway                          clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h17m
default                gateway-np                       nodeport       10.96.123.57     &lt;none&gt;        8080:30080/tcp                                                                                                                               3h17m
default                identity                         clusterip      none             &lt;none&gt;        88/tcp,8080/tcp                                                                                                                              3h18m
default                kubernetes                       clusterip      10.96.0.1        &lt;none&gt;        443/tcp                                                                                                                                      3h19m
default                matchengine                      clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h16m
default                monitor-redis                    clusterip      none             &lt;none&gt;        8081/tcp                                                                                                                                     3h17m
default                monitor-redis-np                 nodeport       10.106.178.13    &lt;none&gt;        8081:30082/tcp                                                                                                                               3h17m
default                postgres                         clusterip      none             &lt;none&gt;        5432/tcp                                                                                                                                     3h18m
default                postgres-np                      nodeport       10.106.223.216   &lt;none&gt;        5432:30032/tcp                                                                                                                               3h18m
default                redis                            clusterip      none             &lt;none&gt;        6379/tcp                                                                                                                                     3h18m
default                redis-np                         nodeport       10.101.167.194   &lt;none&gt;        6379:30079/tcp                                                                                                                               3h18m
default                synchronization                  clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h15m
default                tx-flow                          clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h15m
default                tx-manager                       clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h15m
default                tx-scheduler                     clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h15m
default                ubc-config                       clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h16m
default                ubc-services-config              clusterip      none             &lt;none&gt;        8888/tcp                                                                                                                                     3h18m
default                ubc-services-config-np           nodeport       10.110.11.213    &lt;none&gt;        8888:30088/tcp                                                                                                                               3h18m
default                user-admin                       clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h17m
default                web-exchange-np                  nodeport       10.105.244.194   &lt;none&gt;        80:30081/tcp                                                                                                                                 3h15m
istio-system           grafana                          clusterip      10.97.134.230    &lt;none&gt;        3000/tcp                                                                                                                                     3h22m
istio-system           istio-citadel                    clusterip      10.99.159.56     &lt;none&gt;        8060/tcp,15014/tcp                                                                                                                           3h22m
istio-system           istio-egressgateway              clusterip      10.97.71.204     &lt;none&gt;        80/tcp,443/tcp,15443/tcp                                                                                                                     3h22m
istio-system           istio-galley                     clusterip      10.98.111.27     &lt;none&gt;        443/tcp,15014/tcp,9901/tcp                                                                                                                   3h22m
istio-system           istio-ingressgateway             loadbalancer   10.96.182.202    &lt;pending&gt;     15020:30936/tcp,80:31380/tcp,443:31390/tcp,31400:31400/tcp,15029:31913/tcp,15030:30606/tcp,15031:32127/tcp,15032:30362/tcp,15443:31416/tcp   3h22m
istio-system           istio-pilot                      clusterip      10.101.117.169   &lt;none&gt;        15010/tcp,15011/tcp,8080/tcp,15014/tcp                                                                                                       3h22m
istio-system           istio-policy                     clusterip      10.97.247.54     &lt;none&gt;        9091/tcp,15004/tcp,15014/tcp                                                                                                                 3h22m
istio-system           istio-sidecar-injector           clusterip      10.101.219.141   &lt;none&gt;        443/tcp                                                                                                                                      3h22m
istio-system           istio-telemetry                  clusterip      10.109.108.78    &lt;none&gt;        9091/tcp,15004/tcp,15014/tcp,42422/tcp                                                                                                       3h22m
istio-system           jaeger-agent                     clusterip      none             &lt;none&gt;        5775/udp,6831/udp,6832/udp                                                                                                                   3h22m
istio-system           jaeger-collector                 clusterip      10.97.255.231    &lt;none&gt;        14267/tcp,14268/tcp                                                                                                                          3h22m
istio-system           jaeger-query                     clusterip      10.104.80.162    &lt;none&gt;        16686/tcp                                                                                                                                    3h22m
istio-system           kiali                            clusterip      10.104.41.71     &lt;none&gt;        20001/tcp                                                                                                                                    3h22m
istio-system           kiali-np                         nodeport       10.100.99.141    &lt;none&gt;        20001:30085/tcp                                                                                                                              29h
istio-system           prometheus                       clusterip      10.110.46.60     &lt;none&gt;        9090/tcp                                                                                                                                     3h22m
istio-system           tracing                          clusterip      10.111.173.205   &lt;none&gt;        80/tcp                                                                                                                                       3h22m
istio-system           zipkin                           clusterip      10.101.144.199   &lt;none&gt;        9411/tcp                                                                                                                                     3h22m
kube-system            kube-dns                         clusterip      10.96.0.10       &lt;none&gt;        53/udp,53/tcp,9153/tcp                                                                                                                       54d
kube-system            tiller-deploy                    clusterip      10.105.162.195   &lt;none&gt;        44134/tcp                                                                                                                                    24d


i created an ingress gateway and one virtualservice to route calls from outside the cluster. here are my gateway and virtual services configurations:

gateway:

$ kubectl describe gateway iris-gateway
name:         iris-gateway
namespace:    default
labels:       &lt;none&gt;
annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {""apiversion"":""networking.istio.io/v1alpha3"",""kind"":""gateway"",""metadata"":{""annotations"":{},""name"":""iris-gateway"",""namespace"":""default""},""s...
api version:  networking.istio.io/v1alpha3
kind:         gateway
metadata:
  creation timestamp:  2019-08-23t17:25:20z
  generation:          1
  resource version:    7093263
  self link:           /apis/networking.istio.io/v1alpha3/namespaces/default/gateways/iris-gateway
  uid:                 4c4fac7d-a698-4c9c-97e6-ebc7416c96a8
spec:
  selector:
    istio:  ingressgateway
  servers:
    hosts:
      *
    port:
      name:      http
      number:    80
      protocol:  http
events:          &lt;none&gt;


virtual services:

$ kubectl describe virtualservice apiexg
name:         apiexg
namespace:    default
labels:       &lt;none&gt;
annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {""apiversion"":""networking.istio.io/v1alpha3"",""kind"":""virtualservice"",""metadata"":{""annotations"":{},""name"":""apiexg"",""namespace"":""default""},""...
api version:  networking.istio.io/v1alpha3
kind:         virtualservice
metadata:
  creation timestamp:  2019-08-23t19:26:16z
  generation:          1
  resource version:    7107510
  self link:           /apis/networking.istio.io/v1alpha3/namespaces/default/virtualservices/apiexg
  uid:                 861bca0d-be98-4bfb-bf92-b2bd2f1b703f
spec:
  gateways:
    iris-gateway
  hosts:
    *
  http:
    match:
      uri:
        prefix:  /api-exchange
    route:
      destination:
        host:  api-exchange.default.svc.cluster.local
        port:
          number:  8080
events:            &lt;none&gt;


when i make a call to the service i always got a 503 service unavailable:

curl -x post http://172.30.7.129:31380/api-exchange/ -vvv
* about to connect() to 172.30.7.129 port 31380 (#0)
*   trying 172.30.7.129...
* connected to 172.30.7.129 (172.30.7.129) port 31380 (#0)
&gt; post /api-exchange/ http/1.1
&gt; user-agent: curl/7.29.0
&gt; host: 172.30.7.129:31380
&gt; accept: */*
&gt; 
&lt; http/1.1 503 service unavailable
&lt; content-length: 19
&lt; content-type: text/plain
&lt; date: fri, 23 aug 2019 21:49:33 gmt
&lt; server: istio-envoy
&lt; 
* connection #0 to host 172.30.7.129 left intact
no healthy upstream


here is log output for istio-ingressgateway pod:

[2019-08-23 21:49:34.185][38][warning][upstream] [external/envoy/source/common/upstream/original_dst_cluster.cc:110] original_dst_load_balancer: no downstream connection or no original_dst.


versions:

$ istioctl version --remote
client version: 1.2.4
citadel version: 1.2.4
egressgateway version: 94746ccd404a8e056483dd02e4e478097b950da6-dirty
galley version: 1.2.4
ingressgateway version: 94746ccd404a8e056483dd02e4e478097b950da6-dirty
pilot version: 1.2.4
policy version: 1.2.4
sidecar-injector version: 1.2.4
telemetry version: 1.2.4

$ kubectl version
client version: version.info{major:""1"", minor:""15"", gitversion:""v1.15.1"", gitcommit:""4485c6f18cee9a5d3c3b4e523bd27972b1b53892"", gittreestate:""clean"", builddate:""2019-07-18t09:18:22z"", goversion:""go1.12.5"", compiler:""gc"", platform:""linux/amd64""}
server version: version.info{major:""1"", minor:""15"", gitversion:""v1.15.0"", gitcommit:""e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529"", gittreestate:""clean"", builddate:""2019-06-19t16:32:14z"", goversion:""go1.12.5"", compiler:""gc"", platform:""linux/amd64""}


istio installation:

$ helm install /opt/istio-1.2.4/install/kubernetes/helm/istio-init --name istio-init --namespace istio-system
$ helm install /opt/istio-1.2.4/install/kubernetes/helm/istio --name istio --namespace istio-system --values istio-config/istio-config.yaml


environment:

i did the same configuration over a oracle virtual appliance virtual server with rhel 7 and over a cluster of 3 physical servers with rhel 7.
",<kubernetes><kubernetes-helm><kubernetes-ingress><istio>,57912587,4,"i solve this problem. istio-gateway was not capable to do redirect due to one of my services have a clusterip assigned:

$ kubectl get svc --all-namespaces
namespace              name                             type           cluster-ip       external-ip   port(s)                                                                                                                                      age
default                activemq                         clusterip      none             &lt;none&gt;        61616/tcp                                                                                                                                    3h17m
default                api-exchange                     clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h16m
default                api-response                     clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h16m
default                authorization-server             clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h17m
default                de-communication                 clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h16m
default                gateway                          clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h17m
default                identity                         clusterip      none             &lt;none&gt;        88/tcp,8080/tcp                                                                                                                              3h18m
default                kubernetes                       clusterip      10.96.0.1        &lt;none&gt;        443/tcp                                                                                                                                      3h19m
default                matchengine                      clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h16m
default                monitor-redis                    clusterip      none             &lt;none&gt;        8081/tcp                                                                                                                                     3h17m
default                postgres                         clusterip      none             &lt;none&gt;        5432/tcp                                                                                                                                     3h18m
default                redis                            clusterip      none             &lt;none&gt;        6379/tcp                                                                                                                                     3h18m
default                synchronization                  clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h15m
default                tx-flow                          clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h15m
default                tx-manager                       clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h15m
default                tx-scheduler                     clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h15m
default                ubc-config                       clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h16m
default                ubc-services-config              clusterip      none             &lt;none&gt;        8888/tcp                                                                                                                                     3h18m
default                user-admin                       clusterip      none             &lt;none&gt;        8080/tcp                                                                                                                                     3h17m


here one of my yaml with clusterip: none:

apiversion: v1
kind: service
metadata:
  name: ubc-config
  labels:
    app: ubc-config
spec:
  clusterip: none
  ports:
  - port: 8080
    name: ubc-config
  selector:
    app: ubc-config
---
apiversion: apps/v1
kind: deployment
metadata:
  name: ubc-config
spec:
  selector:
    matchlabels:
      app: ubc-config
  replicas: 1
  template:
    metadata:
      labels:
        app: ubc-config
    spec:
      containers:
      - name: ubc-config
        image: ubc-config
        ports:
        - containerport: 8080


as you can see, service.spec.clusterip is set to none. to solve the problem i only change my yaml configuration to:

apiversion: v1
kind: service
metadata:
  name: ubc-config
  labels:
    app: ubc-config
spec:
  ports:
  - port: 8080
    name: http-ubcconfig
  selector:
    app: ubc-config
---
apiversion: apps/v1
kind: deployment
metadata:
  name: ubc-config
spec:
  selector:
    matchlabels:
      app: ubc-config
  replicas: 1
  template:
    metadata:
      labels:
        app: ubc-config
    spec:
      containers:
      - name: ubc-config
        image: ubc-config
        ports:
        - containerport: 8080
          name: http-ubcconfig


i hope this helps someone.
"
71137291,kubernetes annotations are not applied,"in my kubernetes deployment file i have a annotation as below
spec:
  template:
    metadata:
      annotations:
        prometheus.io/port: &quot;24231&quot;
        prometheus.io/scrape: &quot;true&quot;

but when i apply the deployment file it will be replaced with
spec:
  template:
    metadata:
      creationtimestamp: null
      labels:
        app: my-app
        version: 4.0.5-164

not sure why my annotations are not coming. it is getting replaced with data from a metadata section as shown below
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: my-app
    appid: xxxxx-xxxx-xxxx
    groupid: default
    version: 4.0.5-164

k8s version 1.18
",<kubernetes><kubernetes-pod>,71137578,4,"you are showing us different parts of the deployment manifest here, so i think you are confusing the different metadata sections in the same file.
the first section, .metadata, is applied to the deployment itself.
the .spec.template.metadata section is applied to the pods that are created by the deployment and those annotations will not appear in the top .metadata section of the deployment.
summary:
if you want to specify labels/annotations for the deployment, use the .metadata section.
if you want to specify labels/annotations that are applied to your pod, use the .spec.template.metadata section.
if you want to specify labels/annotations for both, specify them in both places.
example:
---
apiversion: apps/v1
kind: deployment
metadata:
  name:  myapp
  # labels/annotations that are applied to the deployment
  labels:
    app:  myapp
    appid: xxxxx-xxxx-xxxx
    groupid: default
    version: 4.0.5-164
  annotations:
    whatever: isapplied
spec:
  ...
  template:
    metadata:
      # labels/annotations that are applied to the pods
      labels:
        app:  myapp
        appid: xxxxx-xxxx-xxxx
        groupid: default
        version: 4.0.5-164
      annotations:
        prometheus.io/port: &quot;24231&quot;
        prometheus.io/scrape: &quot;true&quot;

"
61107694,changing secrets of kiali in istio is not working,"i have deployed istio in my eks cluster with demo profile. demo has kiali deployment with it. the access secret for kiali dashboard is ( username:admin,password:admin ).i was able to access my dashboard with this credentials. then i created my own secrets. 

$ echo shajaltest | base64
$ c2hhamfsdgvzdao=


deleted the secrets for kiali.

$ kubectl delete secrets kiali -n istio-system


deployed the secrets again with this yaml

apiversion: v1
kind: secret
metadata:
  name: kiali
  namespace: istio-system
  labels:
    app: kiali
type: opaque
data:
  username: c2hhamfsdgvzdao=
  passphrase: c2hhamfsdgvzdao=


after all of that i deleted the pod of kiali.
after that i can not access my dashboard with this username and password. what should i do ? 

i also checked the secrets of kiali. it has updated with recent secret value. 

here is the log of kiali pod.

i0408 18:30:30.194890       1 kiali.go:66] kiali: version: v1.15.1, commit: 
3263b7692bcc06ad40292bedea5a9213e04aa9db
i0408 18:30:30.195179       1 kiali.go:205] using authentication strategy [login]
i0408 18:30:30.195205       1 kiali.go:87] kiali: console version: 1.15.0
i0408 18:30:30.195212       1 kiali.go:286] updating base url in index.html with [/kiali]
i0408 18:30:30.195376       1 kiali.go:267] generating env.js from config
i0408 18:30:30.197274       1 server.go:57] server endpoint will start at [:20001/kiali]
i0408 18:30:30.197285       1 server.go:58] server endpoint will serve static content from [/opt/kiali/console]
i0408 18:30:30.197297       1 metrics_server.go:18] starting metrics server on [:9090]
i0408 18:30:30.197367       1 kiali.go:137] secret is now available.

",<kubernetes><istio><amazon-eks><kiali>,61116599,4,"have you tried to follow the istio documentation about changing the credentials in kiali?



i made a reproduction of your issue with below steps and everything worked just fine.


  enter a kiali username when prompted:


kiali_username=$(read -p 'kiali username: ' uval &amp;&amp; echo -n $uval | base64)



  enter a kiali passphrase when prompted:


kiali_passphrase=$(read -sp 'kiali passphrase: ' pval &amp;&amp; echo -n $pval | base64)



  to create a secret, run the following commands:


namespace=istio-system




cat &lt;&lt;eof | kubectl apply -f -
apiversion: v1
kind: secret
metadata:
  name: kiali
  namespace: $namespace
  labels:
    app: kiali
type: opaque
data:
  username: $kiali_username
  passphrase: $kiali_passphrase
eof


and simply recreate the kiali pod with

kubectl delete pod &lt;name_of_the_kiali_pod&gt; -n istio-system




edit

as @shajal ahamed mentioned in comments the problem was absence of -n, if you want to use just echo, then use.

echo -n username | base64
echo -n passphrase | base64

"
69801758,why these ingress rules don't expose a service to outside? (net::err_cert_authority_invalid/err_too_many_redirects),"i'm trying to setup and expose a service (argocd) to outside a cluster. note: i'm fairly new to kubernetes, so quite probably i have some misconceptions. if you can see one, please help me get rid of it. if more information is needed to diagnose what's happening, please let me know, i'll add it.
i have nginx-ingress ingress controller installed in the cluster in the namespace nginx. i have installed argocd via helm into argocd namespace*. kubectl get service -n argocd shows (omitting age column):
name                                        type        cluster-ip       external-ip   port(s)
projectname-argocd-application-controller   clusterip   10.100.249.133   &lt;none&gt;        8082/tcp
projectname-argocd-dex-server               clusterip   10.100.80.187    &lt;none&gt;        5556/tcp,5557/tcp
projectname-argocd-redis                    clusterip   10.100.230.170   &lt;none&gt;        6379/tcp
projectname-argocd-repo-server              clusterip   10.100.221.87    &lt;none&gt;        8081/tcp
projectname-argocd-server                   clusterip   10.100.22.26     &lt;none&gt;        80/tcp,443/tcp

as far as i understand, service projectname-argocd-server is the one i should expose to get argocd webui. trying to do so, i've created an ingress (based on docs):
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-routing

spec:
  rules:
  - host: test2.projectname.org
    http:
      paths:
      - path: /
        pathtype: prefix # presumably may comment this out
        backend:
          service:
            name: projectname-argocd-server
            port:
              number: 80
  # this was added later while trying to figure the problem out
  defaultbackend:
    service:
      name: projectname-argocd-server
      port:
        number: 80
  ingressclassname: nginx

and applied it via kubectl apply -f routing.yaml -n argocd. now i can see the ingress is created along with the one created by deployment of argocd, and the output of kubectl get ing -a is (omitting age, and ports that are 80; &lt;url&gt; is url of loadbalancer shown in aws console):
namespace   name                        class    hosts                   address
argocd      projectname-argocd-server   nginx    test.projectname.org    &lt;url&gt;
argocd      ingress-routing             nginx    test2.projectname.org   &lt;url&gt;

by the way, kubectl get svc -n nginx shows that nginx-ingress-ingress-nginx-controller is loadbalancer with url &lt;url&gt; (80:30538/tcp).
kubectl describe ingress -n argocd shows that ingress ingress-routing is ok, with correct address, default backend and rules; for ingress projectname-argocd-server it shows ok address and rules (path /), although default backend is shown as default-http-backend:80 (&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;).
now let me also show the dns settings to complete the picture:

i've created a hosted zone for projectname.org (in route 53), put its dns servers to ns-entries of domain register
i've created a cname entry in the hosted zone, pointing test.projectname.org to &lt;url&gt;
i've created an a entry for test2.projectname.org, selected the load balancer from the list and so it points to dualstack.&lt;url&gt;

i expected to see argocd interface at least at one of http://test.projectname.org/ and http://test2.projectname.org/. what actually happens is:

when i open http://test.projectname.org/, it redirects me to https url and shows net::err_cert_authority_invalid. if i insist on visiting, browser shows err_too_many_redirects.

before i added ingress class and moved ingress-routing from nginx namespace to argocd namespace, http://test2.projectname.org/ gave me 404; now it also redirects to https and then gives err_too_many_redirects

i've also checked the /healthz addresses but they give the same result as the / ones. (in contrast, http://&lt;url&gt;/healthz gives an empty page)


my question is: what else am i missing, why i don't get the ui?
is it impossible to expose a service before setting some ssl certificate? can 2 ingresses conflict when trying to expose the same thing on different subdomains (test.projectname.org and test2.projectname.org)? can i see at least one service (argocd) without using projectname.org to check if it is configured and deployed properly? (to separate if it's an ingress/routing/dns issue or a configuration issue)
(*) here's the chart that i used to install argocd:
apiversion: v2
name: argo-cd
appversion: v2.1.5
description: a declarative, gitops continuous delivery tool for kubernetes
version: 3.26.3

dependencies:
  - name: argo-cd
    version: 3.26.3
    repository: https://argoproj.github.io/argo-helm

and values-overwrite.yaml that i've used is just default values wrapped into argo-cd: thing since these should be applied to the dependency. notably, those have enabled: false in ingress:, so the fact that ingress projectname-argocd-server is created is somewhat unexpected.
ps the nginx ingressclass was generated, not created manually, so it may be useful to see it as well (i've substituted ids and timestamps with &quot;...&quot;), as shown by kubectl get ingressclass nginx -o yaml:
apiversion: networking.k8s.io/v1
kind: ingressclass
metadata:
  annotations:
    meta.helm.sh/release-name: nginx-ingress
    meta.helm.sh/release-namespace: nginx
  creationtimestamp: ...
  generation: 1
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: nginx-ingress
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/version: 1.0.3
    helm.sh/chart: ingress-nginx-4.0.5
  name: nginx
  resourceversion: &quot;5750&quot;
  uid: ...
spec:
  controller: k8s.io/ingress-nginx

",<kubernetes><kubernetes-ingress><nginx-ingress>,69979684,4,"right, the issue was somewhat complicated, but i've figured it out. basically, it consists of 2 problems:

https configuration and
ingress configuration

the main problem about https configuration was solved in a separate question and is reduced to switching acme server from staging to production. i've provided more details it in my answer.
now, the ingress configuration is somewhat tricky since argocd has some redirections, ~inner tls requirements~, and also serves more than one protocol at :443. fortunately, i've found this tutorial which shows ssl-passthrough settings, more ingress annotations, including nginx.ingress.kubernetes.io/backend-protocol: &quot;https&quot; which fixes the err_too_many_redirects error. here's my ingress config which works fine with https set up (note also changes in port and tls secret):
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-argocd-routing
  namespace: argocd
  annotations:
    cert-manager.io/cluster-issuer: &lt;cluster issuer name&gt;
    kubernetes.io/tls-acme: &quot;true&quot;
    nginx.ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;
    nginx.ingress.kubernetes.io/backend-protocol: &quot;https&quot;
spec:
  ingressclassname: nginx
  tls:
    - hosts:
      - test2.projectname.org # switched to argocd. later
      secretname: argocd-secret # do not change, this is provided by argo cd
  rules:
    - host: test2.projectname.org # switched to argocd. later
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: projectname-argocd-server
                port:
                  number: 443

"
48452556,setting up a kuberentes cluster with http load balancing ingress for rstudio and shiny results in error pages,"i'm attempting to create a cluster on google kubernetes engine that runs nginx, rstudio server and two shiny apps, following and adapting this guide.

i have 4 workloads that are all green in the ui, deployed via:

kubectl run nginx --image=nginx --port=80
kubectl run rstudio --image gcr.io/gcer-public/persistent-rstudio:latest --port 8787
kubectl run shiny1 --image gcr.io/gcer-public/shiny-googleauthrdemo:latest --port 3838
kubectl run shiny5 --image=flaviobarros/shiny-wordcloud --port=80


they were then all exposed as node ports via:

kubectl expose deployment nginx --target-port=80  --type=nodeport
kubectl expose deployment rstudio --target-port=8787  --type=nodeport
kubectl expose deployment shiny1 --target-port=3838  --type=nodeport
kubectl expose deployment shiny5 --target-port=80  --type=nodeport


..that are all green in the ui.

i then deployed this ingress backend

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: r-ingress
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          servicename: nginx
          serviceport: 80
      - path: /rstudio/
        backend:
          servicename: rstudio
          serviceport: 8787
      - path: /shiny1/
        backend:
          servicename: shiny1
          serviceport: 3838
      - path: /shiny5/
        backend:
          servicename: shiny5
          serviceport: 80


the result is that the nginx routing works great, i can see ""welcome to nginx"" webpage from home, but the three other paths i get:


/rstudio/ - input/output error
/shiny1/ - page not found (the shiny 404 page)
/shiny5/ - page not found (the shiny 404 page)


the rstudio and shiny workloads both work when exposing via the single load balancer, mapped to 8787 and 3838 respectively.

can anyone point to where i'm going wrong?

qs:


do the dockerfiles need to be adapted so they all give a 200 status on port 80 when requesting ""/""?  do i need to change the health checker?  i tried changing the rstudio login page (that 302 to /auth-sign-in if you are not logged in) but no luck
both rstudio and shiny need websockets - does this affect this?
does session affinity need to be on?  i tried adding that with ip but same errors. 

",<kubernetes><rstudio-server><kubernetes-health-check><google-kubernetes-engine>,48954776,4,"as radek suggested, ingress.kubernetes.io/rewrite-target: / is required to re-write your requests. however, this is not currently supported by the gke ingress controller and is the reason that you're receiving 404 responses.

instead, on gke, you must use an nginx ingress controller.

you will then be able to configure ingress for your rstudio and shiny images that obeys the rewrite rule:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: r-ingress
  annotations:
    kubernetes.io/ingress.class: ""nginx""
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - backend:
          servicename: rstudio
          serviceport: 8787
        path: /rstudio/*
      - backend:
          servicename: shiny1
          serviceport: 3838
        path: /shiny1/*
      - backend:
          servicename: shiny5
          serviceport: 80
        path: /shiny5/*

"
59553878,why does a kubernetes ingress create *two* healthchecks by default on its load balancers?,"after i create a very basic ingress (yaml below) with no special definition of health checks (nor any in other kubernetes objects), the ingress creates a  gcp load balancer. 

why does this lb have two health checks (""backend services"") defined against different nodeports, one against root path / and one against /healthz? i would expect to see only one.

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress1
spec:
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          servicename: myservice
          serviceport: 80

",<kubernetes><google-kubernetes-engine><health-monitoring>,59554289,4,"reason for / health check 

one of the limitation of gke ingress controller is below:

for the gke ingress controller to use your readinessprobes as health checks, the pods for an ingress must exist at the time of ingress creation. if your replicas are scaled to 0 or pods don't exist when the ingress is created, the default health check using / applies.

because of the above it created two health checks.

there are lot of caveats for the health-check from readiness probe to work:


the pod's containerport field must be defined
the service's targetport field must point to the pod port's
containerport value or name. note that the targetport defaults to the
port value if not defined
the pods must exist at the time of ingress creation
the readiness probe must be exposed on the port matching the
serviceport specified in the ingress
the readiness probe cannot have special requirements like headers the
probe timeouts are translated to gce health check timeouts


based on above it makes sense to have a default fallback health check using /

reason for /healthz health check

as per this faq all gce url maps require at least one default backend, which handles all requests that don't match a host/path. in ingress, the default backend is optional, since the resource is cross-platform and not all platforms require a default backend. if you don't specify one in your yaml, the gce ingress controller will inject the default-http-backend service that runs in the kube-system namespace as the default backend for the gce http lb allocated for that ingress resource.

some caveats concerning the default backend:


it is the only backend service that doesn't directly map to a user
specified nodeport service
it's created when the first ingress is created, and deleted when the
last ingress is deleted, since we don't want to waste quota if the
user is not going to need l7 loadbalancing through ingress
it has a http health check pointing at /healthz, not the default /,
because / serves a 404 by design


so gke ingress deploys a default backend service using below yaml and setup a /healthz healthcheck for that service.

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: l7-default-backend
  namespace: kube-system
  labels:
    k8s-app: glbc
    kubernetes.io/name: ""glbc""
    kubernetes.io/cluster-service: ""true""
spec:
  replicas: 1
  selector:
    matchlabels:
      k8s-app: glbc
  template:
    metadata:
      labels:
        k8s-app: glbc
        name: glbc
    spec:
      containers:
      - name: default-http-backend
        # any image is permissible as long as:
        # 1. it serves a 404 page at /
        # 2. it serves 200 on a /healthz endpoint
        image: k8s.gcr.io/defaultbackend-amd64:1.5
        livenessprobe:
          httpget:
            path: /healthz
            port: 8080
            scheme: http
          initialdelayseconds: 30
          timeoutseconds: 5
        ports:
        - containerport: 8080
        resources:
          limits:
            cpu: 10m
            memory: 20mi
          requests:
            cpu: 10m
            memory: 20mi
---
apiversion: v1
kind: service
metadata:
  # this must match the --default-backend-service argument of the l7 lb
  # controller and is required because gce mandates a default backend.
  name: default-http-backend
  namespace: kube-system
  labels:
    k8s-app: glbc
    kubernetes.io/cluster-service: ""true""
    kubernetes.io/name: ""glbcdefaultbackend""
spec:
  # the default backend must be of type nodeport.
  type: nodeport
  ports:
  - port: 80
    targetport: 8080
    protocol: tcp
    name: http
  selector:
    k8s-app: glbc

"
73066299,kubernetes networkpolicy: blocks all egress though no blocks are specified,"i have a service that i want to be able to access the internet and noother pods. however, all egress rules seem to block all egress.
# no networkpolicies
kubectl -n mytestnamespace exec service-c-78f784b475-qsdqg -- bin/bash -c 'curl www.google.com'

with no networkpolicy my pod can access the internet. next i make a networkpolicy that allows egress to all ip-addresses.
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: allow-all
  namespace: mytestnamespace
spec:
  podselector: {}
  policytypes:
    - egress
  egress:
    - to:
      - ipblock:
          cidr: 0.0.0.0/0

but now curl can no longer access the internet, but why??????
with networkpolicies allowing egress to all ips!
kubectl -n mytestnamespace exec service-c-78f784b475-qsdqg -- bin/bash -c 'curl www.google.com'

why does this networkpolicy block all egress? makes no sense!
",<kubernetes><kubernetes-networkpolicy><egress>,73077931,4,"it turns out that despite opening up to all ip-addresses, the networkpolicy does not allow egress to the dns pod, which is in another namespace.
# identifying dns pod
kubectl get pods -a | grep dns

# identifying dns pod label
kubectl describe pods -n kube-system coredns-64cfd66f7-rzgwk

next i add the dns label to the egress policy:
# network_policy.yaml
kind: networkpolicy
apiversion: networking.k8s.io/v1
metadata:
  name: allow-all
  namespace: mytestnamespace
spec:
  podselector: {}
  policytypes:
  - egress
  - ingress
  egress:
  - to:
    - ipblock:
        cidr: &quot;0.0.0.0/0&quot;
  - to:
    - namespaceselector:
        matchlabels:
          kubernetes.io/metadata.name: &quot;kube-system&quot;
    - podselector:
        matchlabels:
          k8s-app: &quot;kube-dns&quot;

i apply the network policy and test the curl calls:
# setting up policy
kubectl apply -f network_policy.yaml

# testing curl call
kubectl -n mytestnamespace exec service-c-78f784b475-qsdqg -- bin/bash -c 'curl www.google.com'

success! now i can make egress calls, next i just have to block the appropriate ip-addresses in the private network.
"
59961285,facing some issues when trying to create secrets in kubernetes,"i need to store some passwords and usernames in the secrets.yaml .but after the deployment getting this error .so unable to build the secret and access it in the pods.
attaching the deployment.yaml and secretes .yaml below.

--secrets.yaml

    apiversion: v1
    kind: secret
    metadata:
      name: mysecret
    data:
       cassandrasettings__cassandrapassword: [[ .environment ]]-abcd-cassandra-password


---deployment.yaml

     env:
          - name: password
            valuefrom:
                secretkeyref:
                  name: mysecret
                  key: cassandrasettings__cassandrapassword



after deployment in teamcity getting this error


  error from server (badrequest): error when creating ""stdin"": secret in
  version ""v1"" cannot be handled as a secret: v1.secret.objectmeta:
  v1.objectmeta.typemeta: kind: data: decode base64: illegal base64 data
  at input byte 3, error found in #10 byte of
  ...|-password""},""kind"":""|..., bigger context
  ...|_cassandrapassword"":""dev-bling-cassandra-password""},""kind"":""secret"",""metadata"":{""annotations"":{""kube|...
  error parsing stdin: error converting yaml to json: yaml: line 33: did
  not find expected '-' indicator

",<kubernetes><devops><kubernetes-pod><configmap><kubernetes-secrets>,59961954,4,"looks like type is missing, can you try as below,

---secrets.yaml

apiversion: v1
kind: secret
metadata:
  name: mysecret
type: opaque
data:
  cassandrasettings__cassandrapassword: [[ .environment ]]-abcd-cassandra-password


---deployment.yaml

    env:
      - name: password
        valuefrom:
             secretkeyref:
               name: mysecret
               key: cassandrasettings__cassandrapassword

"
73049535,"kubernetes networkpolicy: allow egress only to internet, and allow ingress only from ingress controller and promtail","by default pods can communicate with each other in kubernetes, which is unwanted should a pod be compromised. we want to use networkpolicies to control inbound (ingress) and outbound (egress) traffic to/from pods.
specifically pods should only be able to:

egress: call services on the internet
ingress: receive requests from the nginx-ingress controller
ingress: send logs via promtail to loki

what i have tried
1. denying all ingress and egress
this is the default policy that we want to gradually open up. it blocks all ingress and egress.
kind: networkpolicy
apiversion: networking.k8s.io/v1
metadata:
  name: default-deny-all
  namespace: mynamespace
spec:
  podselector: {}
  policytypes:
    - ingress
    - egress

2. opening egress to internet only
we allow egress only to ip-adresses that are not reserved for private networks according to wikipedia.
kind: networkpolicy
apiversion: networking.k8s.io/v1
metadata:
  name: egress-allow-internet-only
  namespace: mynamespace
spec:
  podselector: {}
  policytypes:
  - egress
  egress:
  - to:
    - ipblock:
        cidr: 0.0.0.0/0
        except:
        - 10.0.0.0/8
        - 172.16.0.0/12
        - 192.168.0.0/16

3. opening ingress from ingress controller and loki
we have deployed the standard nginx ingress controller in namespace default, and it has the lable app.kubernetes.io/name=ingress-nginx. we have also deployed the standard loki-grafana stack to the default namespace, which uses promtail to transfer logs to loki. here i allow pods to recieve ingress from the promtail and ingress-nginx pods.
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: ingress-allow-ingress-controller-and-promptail
  namespace: mynamespace
spec:
  podselector: {}
  ingress:
  - from:
    - namespaceselector:
        matchlabels:
          kubernetes.io/metadata.name=default
    - podselector:
        matchlabels:
          app.kubernetes.io/name=ingress-nginx
  - from:
    - namespaceselector:
        matchlabels:
          kubernetes.io/metadata.name=default
    - podselector:
        matchlabels:
          app.kubernetes.io/name=promtail

so, does this configuration look right?
i am new to kubernetes, so i hope you guys can help point me in the right direction. does this configuration do what i intent it to do, or have i missed something? e.g. is it enough that i have just blocked egress within the private network to ensure that the pods are isolated from each other, or should i also make the ingress configuration as i have done here?
",<kubernetes><network-programming><nginx-ingress><grafana-loki><kubernetes-networkpolicy>,73077959,4,"i have compared your ingress with k8 doc and egress with this so  and deny both ingress and egress seems to be correct.the only thing we need to do is check whether all the name space is given correct or not. seems to be correct as per your yaml file.
but kubernetes pods use the dns server inside kubernetes; due to this dns server being blocked, we need to define more specific ip ranges to allow dns lookups. follow this so to define dns config at pod levels and to get curl calls with domain names allow egress to core dns from kube-system(by adding a namespace selecter (kube-system) and a pod selector (dns pods)).
how to identify dns pod
# identifying dns pod
kubectl get pods -a | grep dns

# identifying dns pod label
kubectl describe pods -n kube-system coredns-64cfd66f7-rzgwk

adding dns pod to networkpolicy
kind: networkpolicy
apiversion: networking.k8s.io/v1
metadata:
  name: egress-allow-internet-only
  namespace: mynamespace
spec:
  podselector: {}
  policytypes:
  - egress
  egress:
  - to:
    - ipblock:
        cidr: 0.0.0.0/0
        except:
        - 10.0.0.0/8
        - 172.16.0.0/12
        - 192.168.0.0/16
  - to:
    - namespaceselector:
        matchlabels:
          kubernetes.io/metadata.name: &quot;kube-system&quot;
    - podselector:
        matchlabels:
          k8s-app: &quot;kube-dns&quot;

"
51795709,unable to access websocket over kubernetes ingress,"i have deployed two services to a kubernetes cluster on gcp:

one is a spring cloud api gateway implementation:

apiversion: v1
kind: service
metadata:
  name: api-gateway
spec:
  ports:
  - name: main
    port: 80
    targetport: 8080
    protocol: tcp
  selector:
    app: api-gateway
    tier: web
  type: nodeport


the other one is a backend chat service implementation which exposes a websocket at /ws/ path.

apiversion: v1
kind: service
metadata:
 name: chat-api
spec:
  ports:
  - name: main
    port: 80
    targetport: 8080
    protocol: tcp
  selector:
    app: chat
    tier: web
  type: nodeport


the api gateway is exposed to internet through a contour ingress controller:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: api-gateway-ingress
  annotations:
    kubernetes.io/tls-acme: ""true""
    certmanager.k8s.io/cluster-issuer: ""letsencrypt-prod""
    ingress.kubernetes.io/force-ssl-redirect: ""true""
spec:
  tls:
  - secretname: api-gateway-tls
    hosts:
    - api.mydomain.com.br
  rules:
  - host: api.mydomain.com.br
    http:
      paths:
      - backend:
          servicename: api-gateway
          serviceport: 80


the gateway routes incoming calls to /chat/ path to the chat service on /ws/:

@bean
public routelocator routes(routelocatorbuilder builder) {
    return builder.routes()
            .route(r -&gt; r.path(""/chat/**"")
                    .filters(f -&gt; f.rewritepath(""/chat/(?&lt;segment&gt;.*)"", ""/ws/(?&lt;segment&gt;.*)""))
                    .uri(""ws://chat-api""))
            .build();
}


when i try to connect to the websocket through the gateway i get a 403 error:

error: unexpected server response: 403

i even tried to connect using http, https, ws and wss but the error remains.

anyone has a clue?
",<spring><websocket><kubernetes><kubernetes-ingress>,52326431,4,"i had the same issue using ingress resource with contour 0.5.0 but i managed to solve it by
upgrading contour to v0.6.0-beta.3 with ingressroute  (be aware, though, that it's a beta version).

you can add an ingressroute resource (crd) like this (remove your previous ingress resource): 

#ingressroute.yaml
apiversion: contour.heptio.com/v1beta1
kind: ingressroute
metadata:
  name: api-gateway-ingress
  namespace: default
spec:
  virtualhost:
    fqdn: api.mydomain.com.br
    tls:
      secretname: api-gateway-tls
  routes:
    - match: /
      services:
        - name: api-gateway
          port: 80
    - match: /chat
      enablewebsockets: true # setting this to true enables websocket for all paths that match /chat
      services:
        - name: api-gateway
          port: 80


then apply it

websockets  will be authorized only on the /chat path.

see here for more detail about contour ingressroute.
"
51089657,error while creating k8s secret through rest api,"i want to create a secret through kubectl api.
below is the script, i am running but getting error in parsing yaml file.
please help

    curl -vk \
    -x post \
    -d @- \
    -h ""authorization: bearer $(cat token)"" \
    -h 'accept: application/json' \
    -h 'content-type: application/json' \
    https://ip:port/api/v1/namespaces/nginx-ingress/secrets &lt;&lt;'eof'
{
""apiversion"": ""v1"",
""kind"": ""secret"",
""metadata"": {
""namespace"": ""nginx-ingress"",
},
""type"": ""opaque""
""data"": {
""username"": ""ywrtaw4="",
""password"": ""mwyyzdflmmu2n2rm""
}
eof


error:


  message"": ""the object provided is unrecognized (must be of type secret): couldn't get version/kind; json parse error: invalid character '}' looking for beginning of object key string ({\""apiversion\"": \""v1\"",\""kind\"": \""s ...)"",
    ""reason"": ""badrequest"",

",<curl><kubernetes><kubernetes-secrets>,51090773,4,"i changed my json structure in which i added the curly brace which i missed in the end and one comma in the type key.
curl -vk \
    -x post \
    -d @- \
    -h &quot;authorization: bearer $(cat token)&quot; \
    -h 'accept: application/json' \
    -h 'content-type: application/json' \
    https://192.168.2.100:6443/api/v1/namespaces/nginx-ingress/secrets &lt;&lt;'eof'
{
 &quot;apiversion&quot;:&quot;v1&quot;,
 &quot;kind&quot; :&quot;secret&quot;,
 &quot;metadata&quot; :{&quot;namespace&quot; :&quot;nginx-ingress&quot;,&quot;name&quot;:&quot;mysecret1&quot;},
 &quot;type&quot;: &quot;opaque&quot;,
 &quot;data&quot;: {&quot;username&quot;: &quot;ywrtaw4=&quot;,&quot;password&quot;: &quot;mwyyzdflmmu2n2rm&quot;}
}

it worked.
"
74194356,prevent argocd from syncing a single ressource,"i have an argocd app which is generating symbols
apiversion: v1
kind: secret
metadata:
  labels:
    {{- include &quot;myapp.labels&quot; . | nindent 4 }}
  annotations:
    helm.sh/hook: pre-install,post-delete
  name: {{ include &quot;myapp.fullname&quot; . }}
type: opaque
data:
  {{- if .values.password }}
  password: {{ .values.password | b64enc | quote }}
  {{- else }}
  password: {{ randalphanum 10 | b64enc | quote }}
  {{- end }}

a second service is adding additional values to the secrets later. i don't want this secrets in my chart
now when something is changed on the app, the secrets are recreated.
how can i change this behaviour in argocd?
",<kubernetes><kubernetes-helm><argocd>,74194477,4,"add annotation to that particular object and it should work
apiversion: v1
kind: secret
metadata:
  labels:
    {{- include &quot;myapp.labels&quot; . | nindent 4 }}
  annotations:
    helm.sh/hook: pre-install,post-delete
    argocd.argoproj.io/sync-options: prune=false


some sync options can defined as annotations in a specific resource. most of the sync options are configured in the application resource spec.syncpolicy.syncoptions attribute. multiple sync options which are configured with the argocd.argoproj.io/sync-options annotation can be concatenated with a , in the annotation value; white spaces will be trimmed.

no-prune-resources
or if you dont want to apply auto sync on this object then you can try applyoutofsynconly=false
selective-sync
"
74204723,how to set an springboot array property as a kubernetes secret?,"i want to use the direct translation from k8s secret-keys to springboot properties.
therefore i have a helm chart (but similar with plain k8s):
apiversion: v1
data:
  app.entry[0].name: {{.values.firstentry.name | b64enc }}
kind: secret
metadata:
  name: my-secret
type: opaque

with that my intention is that this behaves as if i'd set the spring property file:
app.entry[0].name: &quot;somename&quot;

but when i do this i get an error:
 invalid value: &quot;[app.entry[0].name]&quot;: a valid config key must consist of alphanumeric characters, '-', '_' or '.' (e.g. 'key.name',  or 'key_name',  or 'key-name', regex used for validation is '[-._a-za-z0-9]+'),

so, [0] seems not to be allowed as a key name for the secrets.
any idea how i can inject an array entry into spring directly from a k8s secret name?
shooting around wildly i tried these that all failed:

  app.entry[0].name: ... -- k8s rejects '['
  app.entry__0.name: ... -- k8s ok, but spring does not recognize this as array (i think)
  &quot;app.entry[0].name&quot;: ...  -- k8s rejects '['
  'app.entry[0].name': ... -- k8s rejects '['

",<spring-boot><kubernetes><kubernetes-secrets>,74339014,4,"you should be able to use environnment variables like described in sprint-boot-env.
app.entry[0].name property will be set using app_entry_0_name environment variable. this could be set in your deployment.
using secret like:
apiversion: v1
data:
  value: {{.values.firstentry.name | b64enc }}
kind: secret
metadata:
  name: my-secret
type: opaque

and then use it with
       env:
       - name: app_entry_0_name
         valuefrom:
           secretkeyref:
             name: my-secret
             key: value

"
63548794,createcontainerconfigerror found when i deploy pods in kubernetes with secret,"apiversion: v1
kind: secret
metadata:
  name: mysecret
type: opaque
data:
  db_url: &quot;base64&quot;
---
apiversion: v1
kind: namespace
metadata:
  name: my-ns
---
apiversion: apps/v1
kind: deployment
metadata:
  namespace: my-ns
  name: my-api
spec:
  selector:
    matchlabels:
      app: my-api
  replicas: 1
  revisionhistorylimit: 5
  template:
    metadata:
      labels:
        app: my-api
    spec:
      containers:
        - name: my-api
          image: myimage:latest
          ports:
            - containerport: 5000
          env:
            - name: db_url
              valuefrom:
                secretkeyref:
                  name: mysecret
                  key: db_url

i've encountered createcontainerconfigerror after deploying above yaml file in my kubernetes cluster.
",<kubernetes><amazon-eks>,63548858,4,"this is because the deployment is using a secret mysecret which needs to exist in the namespace where the the deployment is being created. the namespace of the deployment is my-ns but secret is in default namespace because in the secret you have not mentioned any namespace.
changing the secret yaml as below should fix it.
apiversion: v1
kind: secret
metadata:
  name: mysecret
  namespace: my-ns
type: opaque
data:
  db_url: &quot;base64&quot;

"
74054955,powershell pod failing in kubernetes cluster,"i need to run powershell as an container in kubernetes
i am using following deployment file sample.yaml
apiversion: v1
kind: pod
metadata:
  name: powershell
spec:
  containers:
  - name: powershell
    image: mcr.microsoft.com/powershell:latest

when i run kubectl apply -f sample.yaml
i get the following error on kubectl get pods
powershell        0/1     crashloopbackoff   3 (50s ago)   92s

i did check the log kubectl logs powershell
powershell 7.2.6
copyright (c) microsoft corporation.

https://aka.ms/powershell
type 'help' to get help.

ps /&gt; [?1h

but when i run same image as a docker container with following command its working
docker run --rm -it mcr.microsoft.com/powershell:latest

",<powershell><kubernetes><kubernetes-pod>,74056099,4,"if you want to keep a container for running, you should write like this yaml..
apiversion: v1
kind: pod
metadata:
  name: powershell
spec:
  containers:
  - name: powershell
    image: mcr.microsoft.com/powershell:latest
    command: [&quot;pwsh&quot;]
    args: [&quot;-command&quot;, &quot;start-sleep&quot;, &quot;3600&quot;]


[root@master1 ~]# kubectl get pod powershell
name         ready   status    restarts   age
powershell   1/1     running   0          3m32s
[root@master1 ~]# kubectl exec -it powershell -- pwsh
powershell 7.2.6
copyright (c) microsoft corporation.

https://aka.ms/powershell
type 'help' to get help.

ps /&gt; date
thu oct 13 12:50:24 pm utc 2022
ps /&gt;


"
53637647,kubernetes multiple service conflict,"i'm newbie in kubernetes. i created a kubernetes cluster on amazon eks.
i'm trying to setup multiple kubernetes services to run multiple asp.net applications in one cluster. but facing a weird problem. 

everything runs fine when there is only 1 service. but whenever i create 2nd service for 2nd application it creates a conflict. the conflict is sometime service 1 url load service 2 application and sometime it loads service 1 application and same happens with service 2 url on simple page reload.

i've tried both amazon classic elb (with loadbalancer service type) and nginx ingress controller (with clusterip service type). this error is common in both approaches.

both services and deployments are running on port 80, i even tried different ports for both services and deployments to avoid port conflict but same problem.

i've checked the deployment &amp; service status, and pod log everything looks fine. no error or warning at all

please guide how i can fix this error.
here is the yaml file of both services for nginx ingress

# service 1 for deployment 1 (container port: 1120)
apiversion: v1
kind: service
metadata:
  creationtimestamp: 2018-12-05t14:54:21z
  labels:
    run: load-balancer-example
  name: app1-svc
  namespace: default
  resourceversion: ""463919""
  selflink: /api/v1/namespaces/default/services/app1-svc
  uid: a*****-****-****-****-**********c
spec:
  clusterip: 10.100.102.224
  ports:
  - port: 1120
    protocol: tcp
    targetport: 1120
  selector:
    run: load-balancer-example
  sessionaffinity: none
  type: clusterip
status:
  loadbalancer: {}


2nd service

# service 2 for deployment 2 (container port: 80)
apiversion: v1
kind: service
metadata:
  creationtimestamp: 2018-12-05t10:13:33z
  labels:
    run: load-balancer-example
  name: app2-svc
  namespace: default
  resourceversion: ""437188""
  selflink: /api/v1/namespaces/default/services/app2-svc
  uid: 6******-****-****-****-************0
spec:
  clusterip: 10.100.65.46
  ports:
  - port: 80
    protocol: tcp
    targetport: 80
  selector:
    run: load-balancer-example
  sessionaffinity: none
  type: clusterip
status:
  loadbalancer: {}


thanks
",<amazon-web-services><asp.net-core><kubernetes><kubernetes-ingress><amazon-eks>,53638694,4,"the problem is with the selector in the services. they both have the same selector and that's why you are facing that problem. so they both will point to same set of pods.


  the set of pods targeted by a service is (usually) determined by a label selector


since deployemnt 1 and deployment 2 are different(i think), you should use different selector in them. then expose the deployments. for example:

apiversion: apps/v1
kind: deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.15.4
        ports:
        - containerport: 80


--

apiversion: apps/v1
kind: deployment
metadata:
  name: hello-deployment
  labels:
    app: hello
spec:
  replicas: 3
  selector:
    matchlabels:
      app: hello
  template:
    metadata:
      labels:
        app: hello
    spec:
      containers:
      - name: hello
        image: nightfury1204/hello_server
        args:
        - serve
        ports:
        - containerport: 8080


above two deployment nginx-deployment and hello-deployment has different selector. so expose them to service will not colide each other.

when you use kubectl expose deployment app1-deployment --type=clusterip --name=app1-svc to expose deployment, the service will have the same selector as the deployment.
"
53625500,kubernetes ingress path not finding resources,"i'm having some issues when using a path to point to a different 
kubernetes service. 

i'm pointing to a secondary service using the path /secondary-app and i can see through my logs that i am correctly reaching that service. 

my issue is that any included resource on the site, let's say /css/main.css for example, end up not found resulting in a 404.

here's a slimmed down version of my ingress:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: ""true""
    nginx.ingress.kubernetes.io/proxy-body-size: 50m
    nginx.ingress.kubernetes.io/rewrite-target: /
  name: my-app
spec:
  rules:
  - host: my-app.example.com
    http:
      paths:
      - backend:
        path: /
          servicename: my-app
          serviceport: http
      - backend:
        path: /secondary-app        
          servicename: secondary-app
          serviceport: http


i've tried a few things and haven't yet been able to make it work. do i maybe need to do some apache rewrites?

any help would be appreciated.

edit - solution

thanks to some help from @mk_sta i was able to get my secondary service application working by using the nginx.ingress.kubernetes.io/configuration-snippet annotation like so:

  nginx.ingress.kubernetes.io/configuration-snippet: |
      if ($request_uri = '/?%secondary-app') { rewrite /(.*) secondary-app/$1 break; }


it still needs a bit of tweaking for my specific app but that worked exactly how i needed it to.
",<apache><kubernetes><kubernetes-ingress>,53631559,4,"i guess nginx.ingress.kubernetes.io/rewrite-target: / annotation in your ingress configuration doesn't bring any success for multipath rewrite target paths, read more here. however, you can consider to use nginx plus ingress controller, shipped with nginx.org/rewrites: annotation and can be used for pointing uri paths to multiple services as described in this example.

you can also think about using nginx.ingress.kubernetes.io/configuration-snippet annotation for the existing ingress, which can adjust rewrite rules to nginx location, something like:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: ""true""
    nginx.ingress.kubernetes.io/proxy-body-size: 50m
    nginx.ingress.kubernetes.io/configuration-snippet: |
      rewrite /first-app/(.*) $1 break;
      rewrite /secondary-app/(.*) /$1 break;
  name: my-app
spec:
  rules:
  - host: my-app.example.com
    http:
      paths:
      - backend:
        path: /first-app
          servicename: my-app
          serviceport: http
      - backend:
        path: /secondary-app        
          servicename: secondary-app
          serviceport: http

"
53787321,mount two persistent volume claims in one deployment cause error,"i created two persistentvolumeclaims(one for redis, one for persistent logs) and tried to mount both in a single deployment, but after creating the deployment, i get the following error:

nodes are available: 3 node(s) didn't match node selector, 4 node(s) had no available volume zone.

however as soon as i remove one pvc from the deployment yaml file, it works fine. i am running it on google cloud platform using kubernetes engine.

pvc1:

kind: persistentvolumeclaim
apiversion: v1
metadata:
  name: pvc-log
spec:
  accessmodes:
    - readwriteonce
  volumemode: filesystem
  resources:
    requests:
      storage: 20gi
  storageclassname: standard


pvc2:

kind: persistentvolumeclaim
apiversion: v1
metadata:
  name: pvc-redis
spec:
  accessmodes:
    - readwriteonce
volumemode: filesystem
resources:
  requests:
    storage: 20gi
storageclassname: standard


deployment:

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: 'prod-deployment'
spec:
  replicas: 1
template:
  metadata:
    labels:
      app: foo
      release: canary
      environment: production
  spec:
    containers:
      - name: api-server
        image: 'foo:latest'
        volumemounts:
          - mountpath: /logs
            name: log-storage
      - name: redis
        image: 'redis'
        volumemounts:
          - mountpath: /data
            name: redis-data
    volumes:
      - name: redis-data
        persistentvolumeclaim:
          claimname: pvc-redis
      - name: log-storage
        persistentvolumeclaim:
          claimname: pvc-log

",<kubernetes><google-kubernetes-engine>,53788920,4,"this is similar to this. it's most likely due to a pvc trying to create a volume on an availability zone where you don't have a node in.  you can try restricting the standard storageclass to just the availability zones where you have kubernetes nodes. something like this:

kind: storageclass
apiversion: storage.k8s.io/v1
metadata:
  name: standard
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
allowedtopologies:
- matchlabelexpressions:
  - key: failure-domain.beta.kubernetes.io/zone
    values:
    - us-central1-a
    - us-central1-b

"
63232463,accessing kubernetes service from c# docker,"i am trying to access the kubernetes service using c# docker within kuberentes service.
i have a python docker yaml file and want to create pod using the same yaml programmatically from c# dotnet core docker that running int the same cluster of python docker. i found kubernetes api for dotnet core.i created the code for list pods that is below.
using system;
using k8s;

namespace simple
{
    internal class podlist
    {
        private static void main(string[] args)
        {
            var config = kubernetesclientconfiguration.inclusterconfig();
            ikubernetes client = new kubernetes(config);
            console.writeline(&quot;starting request!&quot;);

            var list = client.listnamespacedpod(&quot;default&quot;);
            foreach (var item in list.items)
            {
                console.writeline(item.metadata.name);
            }

            if (list.items.count == 0)
            {
                console.writeline(&quot;empty!&quot;);
            }
        }
    }
}

this code getting error  forbidden (&quot;operation returned an invalid status code 'forbidden'&quot;).
instead of inclusterconfig using buildconfigfromconfigfile  code is working in local environment.is anythin i missed?
edited
apiversion: v1
kind: serviceaccount
metadata:
  name: test-serviceaccount
  namespace: api

---
kind: role
apiversion: rbac.authorization.k8s.io/v1
metadata:
  namespace: api
  name: test-role
rules:
    - apigroups: [&quot;&quot;,&quot;apps&quot;,&quot;batch&quot;]
      # &quot;&quot; indicates the core api group
      resources: [&quot;deployments&quot;, &quot;namespaces&quot;,&quot;cronjobs&quot;]
      verbs: [&quot;get&quot;, &quot;list&quot;, &quot;update&quot;, &quot;patch&quot;,&quot;create&quot;]  
  

  
---

apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: test-binding
  namespace: api
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: test-role
subjects:
  - kind: serviceaccount
    name: test-serviceaccount
    namespace: api

---


apiversion: apps/v1
kind: deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: &quot;4&quot;
  creationtimestamp: &quot;2019-07-04t16:05:43z&quot;
  generation: 4
  labels:
    app: test-console
    tier: middle-end
  name: test-console
spec:
  progressdeadlineseconds: 600
  replicas: 1
  revisionhistorylimit: 2
  selector:
    matchlabels:
      app: test-console
      tier: middle-end
  strategy:
    rollingupdate:
      maxsurge: 25%
      maxunavailable: 25%
    type: rollingupdate
  template:
    metadata:
      creationtimestamp: &quot;2019-07-04t16:05:43z&quot;
      labels:
        app: test-console
        tier: middle-end
    spec:
      serviceaccountname: test-serviceaccount
      containers:
      - image: test.azurecr.io/tester:1.0.0
        imagepullpolicy: always
        name: test-console
        ports:
        - containerport: 80
          protocol: tcp
        resources: {}
        terminationmessagepath: /dev/termination-log
        terminationmessagepolicy: file
      dnspolicy: clusterfirst
      imagepullsecrets:
      - name: pull
      restartpolicy: always
      schedulername: default-scheduler
      securitycontext: {}
      terminationgraceperiodseconds: 30
      
c# code

  client.createnamespacedcronjob(jobmodel, &quot;testnamesapce&quot;);
crone job
 'apiversion': 'batch/v1beta1',
    'kind': 'cronjob',
    'metadata': {
        'creationtimestamp': '2020-08-04t06:29:19z',
        'name': 'forcaster-cron',
        'namespace': 'testnamesapce'
    },

",<c#><docker><kubernetes><kubernetes-apiserver>,63232640,4,"inclusterconfig uses the default service account of the namespace where you are deploying the pod. by default that service account will not have any rbac which leads to forbidden error.
the reason it works in local environment is because it uses credential from kubeconfig file which most of the time is admin credential having root level rbac permission to the cluster.
you need to define a role and attach that role to the service account using rolebinding
so if you are deploying the pod in default namespace then below rbac should work.
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  name: myrole
  namespace: default
rules:
- apigroups:
  - &quot;&quot;
  resources:
  - pods
  verbs:
  - list
---
apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: role-binding
  namespace: default
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: myrole
subjects:
- kind: serviceaccount
  name: default
  namespace: default

once you apply above rbac you can check permission of the service account using below command
kubectl auth can-i list pods --as=system:serviceaccount:default:default -n default
yes

"
76678256,kustomize: patch multiple resources that starts with same name,"i want to patch multiple deployments that starts with same nameprefix instead of targeting specific resource.
for example, i have 2 deployments of nginx deployment-v1.yaml and deployment-v2.yaml. i want to patch both the deployment using nginx- prefix.
deployment-v1.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: nginx-v1
  labels:
    app: web
spec:
  selector:
    matchlabels:
      app: web
  replicas: 5
  strategy:
    type: rollingupdate
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
       name: nginx
          image: nginx
          ports:
           containerport: 80 

deployment-v2.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: nginx-deployment
  labels:
    app: web
spec:
  selector:
    matchlabels:
      app: web
  replicas: 5
  strategy:
    type: rollingupdate
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
       name: nginx
          image: nginx
          resources:
            limits:
              memory: 200mi
            requests:
              cpu: 100m
              memory: 200mi
          ports:
           containerport: 80

now i want to overlay both the deployments with common overlay-patch. i am trying something like this.
overlay.yaml
apiversion: apps/v1
kind: deployment
metadata:
  nameprefix: nginx-
spec:
  replicas: 10

kustomization.yml
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
resources:
  - deployment-v1.yaml
  - deployment-v2.yaml
patchesstrategicmerge:
  - overlay.yaml

but it is not working as it is expecting a name to match the target and totally ignoring nameprefix. any help is appreciated.
",<amazon-web-services><kubernetes><kubectl><amazon-eks><kustomize>,76679440,4,"you can apply a patch to multiple resources using the target attribute in a patch. given your examples (after fixing the errors i pointed out in my comment), we can write a kustomization.yaml like this:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
resources:
- deployment-v1.yaml
- deployment-v2.yaml

patches:
  - target:
      kind: deployment
    patch: |
      apiversion: apps/v1
      kind: deployment
      metadata:
        name: this_value_is_ignored
      spec:
        replicas: 10

the target attribute controls to what resources this patch will apply. with the above configuration, running kustomize build results in:
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: web
  name: nginx-deployment
spec:
  replicas: 10
.
.
.

and:
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: web
  name: nginx-v1
spec:
  replicas: 10
.
.
.


the above configuration would apply the patch to all deployments in your kustomization. if you wanted to limit the patching to only deployments matching a specific name prefix, you could write:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
resources:
- deployment-v1.yaml
- deployment-v2.yaml

patches:
  - target:
      kind: deployment
      name: nginx.*
    patch: |
      apiversion: apps/v1
      kind: deployment
      metadata:
        name: this_value_is_ignored
      spec:
        replicas: 10

note that the name pattern is regular expression, not a shell-style glob.
"
63504579,apply part of kubernetes manifest yaml using kubectl,"consider the following kubernetes manifest (mymanifest.yml) :
apiversion: v1
kind: pod
metadata:
  name: firstpod
spec:
  containers:
  - image: nginx
    name: nginx
---
apiversion: v1
kind: pod
metadata:
  name: secondpod
spec:
  containers:
  - image: nginx
    name: nginx

if i do kubectl apply -f mymanifest.yml both pods are deployed.
i remember someone told me that it is possible to deploy only one pod. something like :
kubectl apply -f mymanifest.yml secondpod

but it doesn't work.
is there a way to do it?
thx in advance
",<kubernetes><yaml><kubectl>,63504704,4,"you can add labels to the pods
apiversion: v1
kind: pod
metadata:
  name: firstpod
  labels:
    app: firstpod
spec:
  containers:
  - image: nginx
    name: nginx
---
apiversion: v1
kind: pod
metadata:
  name: secondpod
  labels:
    app: secondpod
spec:
  containers:
  - image: nginx
    name: nginx

use specific label to filter while applying the yaml. the filter supports =, ==, and !=
kubectl apply -f mymanifest.yml -l app=secondpod

you can also use --prune which is an alpha feature.
# apply the configuration in manifest.yaml that matches label app=secondpod and delete all the other resources that are
not in the file and match label app=secondpod.
kubectl apply --prune -f mymanifest.yml -l app=secondpod

"
52839778,using the same istio gateway with multiple ports and protocols,"i am trying to configure an istio gateway with two different protocols (grpc and http)

right now, i have two different gateways one each for grpc and http as below

apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: gwgrpc
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 7878
      name: http
      protocol: grpc
    hosts:
    - ""*""
---
apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: gwrest
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 7979
      name: http
      protocol: http
    hosts:
    - ""*""


is it possible to use same gateway with different protocols and ports?
",<kubernetes><kubectl><istio><envoyproxy>,52841312,4,"you should be able to combine the two gateways. the only problem is that both your ports have the same name. something like this should work.

apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: gwgrpc
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 7878
      name: grpc
      protocol: grpc
    hosts:
    - ""*""
  - port:
      number: 7979
      name: http
      protocol: http
    hosts:
    - ""*""

"
63469948,kubectl: jsonpath works but custom-column doesn't,"consider following expression
kubectl get deploy -o 'jsonpath={.items[*].apiversion}'

it returns following output:
apps/v1 apps/v1

when using exactly the same expression with custom-columns:
kubectl get deploy -o 'custom-columns=a:{.items[*].apiversion}'

i get:
a
&lt;none&gt;
&lt;none&gt;

what am i doing wrong?
",<kubernetes><kubectl>,63470328,4,"actually the case you are testing is kind of misleading. because both deployment and deploymentlist have the same apiversion (apps/v1).
so let's work on .metadata.name for example:
kubectl -n kube-system get deploy -o 'jsonpath={.items[*].metadata.name}'

you will get a result like this:
calico-kube-controllers coredns dns-autoscaler kubernetes-dashboard metrics-server rbd-provisioner

but for custom column it is somehow different. table is for showing a list of content. so the path you provide is for each row of the table. so you should use:
kubectl -n kube-system get deploy -o 'custom-columns=a:{.metadata.name}'

and you will get the correct result:
a
calico-kube-controllers
coredns
dns-autoscaler
kubernetes-dashboard
metrics-server
rbd-provisioner

so the problem was with using items[*] on custom-columns.
"
63382959,prometheus adapter - not able to get data from external metrics,"i am not able to fetch items by running this command: kubectl get --raw &quot;/apis/external.metrics.k8s.io/v1beta1/namespaces/default/rabbitmq_queue_messages_ready&quot; | jq
as you can see in below output items is an empty array:
{
  &quot;kind&quot;: &quot;externalmetricvaluelist&quot;,
  &quot;apiversion&quot;: &quot;external.metrics.k8s.io/v1beta1&quot;,
  &quot;metadata&quot;: {
    &quot;selflink&quot;: &quot;/apis/external.metrics.k8s.io/v1beta1/namespaces/default/rabbitmq_queue_messages_ready&quot;
  },
  &quot;items&quot;: []
}

however, i am getting proper data in prometheus endpoint:
prometheus url: http://&lt;prometheus_url&gt;:9090/api/v1/series?match%5b%5d=%7b__name__%3d~%22%5erabbitmq_queue_.%2a%22%7d&amp;start=1597255421.51
response:
{
&quot;status&quot;:&quot;success&quot;,
&quot;data&quot;:[
    {
        &quot;__name__&quot;:&quot;rabbitmq_queue_messages_ready&quot;,
        &quot;app&quot;:&quot;prometheus-rabbitmq-exporter&quot;,
        &quot;durable&quot;:&quot;true&quot;,
        &quot;instance&quot;:&quot;10.2.0.73:9419&quot;,
        &quot;job&quot;:&quot;kubernetes-pods&quot;,
        &quot;namespace&quot;:&quot;default&quot;,
        &quot;pod_name&quot;:&quot;rabbitmq-exporter-prometheus-rabbitmq-exporter-754c845847-gzlrq&quot;,
        &quot;pod_template_hash&quot;:&quot;754c845847&quot;,
        &quot;queue&quot;:&quot;test&quot;,
        &quot;release&quot;:&quot;rabbitmq-exporter&quot;,
        &quot;vhost&quot;:&quot;/&quot;
    },
    {
        &quot;__name__&quot;:&quot;rabbitmq_queue_messages_ready&quot;,
        &quot;app&quot;:&quot;prometheus-rabbitmq-exporter&quot;,
        &quot;durable&quot;:&quot;true&quot;,
        &quot;instance&quot;:&quot;10.2.0.73:9419&quot;,
        &quot;job&quot;:&quot;kubernetes-pods&quot;,
        &quot;namespace&quot;:&quot;default&quot;,
        &quot;pod_name&quot;:&quot;rabbitmq-exporter-prometheus-rabbitmq-exporter-754c845847-gzlrq&quot;,
        &quot;pod_template_hash&quot;:&quot;754c845847&quot;,
        &quot;queue&quot;:&quot;test1&quot;,
        &quot;release&quot;:&quot;rabbitmq-exporter&quot;,
        &quot;vhost&quot;:&quot;/&quot;
    }
]
}

i installed stable/prometheus-adapter using below helm values:
rules:
  default: false
  external:
    - seriesquery: '{__name__=~&quot;^rabbitmq_queue_.*&quot;}'
      resources:
        #template: &lt;&lt;.resource&gt;&gt;
        overrides:
          namespace:
            resource: namespace
          service:
            resource: service
          pod:
            resource: pod
      name:
        matches: &quot;&quot;
        as: &quot;rabbitmq_queue_messages_ready&quot;
      metricsquery: 'rate(&lt;&lt;.series&gt;&gt;{&lt;&lt;.labelmatchers&gt;&gt;}[1m])'

version of helm and kubernetes:
client: &amp;version.version{semver:&quot;v2.16.9&quot;, gitcommit:&quot;8ad7037828e5a0fca1009dabe290130da6368e39&quot;, gittreestate:&quot;clean&quot;}
server: &amp;version.version{semver:&quot;v2.16.7&quot;, gitcommit:&quot;5f2584fd3d35552c4af26036f0c464191287986b&quot;, gittreestate:&quot;clean&quot;}
client version: version.info{major:&quot;1&quot;, minor:&quot;16&quot;, gitversion:&quot;v1.16.1&quot;, gitcommit:&quot;d647ddbd755faf07169599a625faf302ffc34458&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2019-10-02t17:01:15z&quot;, goversion:&quot;go1.12.10&quot;, compiler:&quot;gc&quot;, platform:&quot;darwin/amd64&quot;}
server version: version.info{major:&quot;1&quot;, minor:&quot;15&quot;, gitversion:&quot;v1.15.11&quot;, gitcommit:&quot;ec831747a3a5896dbdf53f259eafea2a2595217c&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2020-05-29t19:56:10z&quot;, goversion:&quot;go1.12.17&quot;, compiler:&quot;gc&quot;, platform:&quot;linux/amd64&quot;}
expected result:
it should fetch metrics from prometheus
how to reproduce it:
install stable/prometheus-rabbitmq-exporter and stable/prometheus-adapter helm charts using above configs.
i am following this guide: https://nuvalence.io/building-a-k8s-autoscaler-with-custom-metrics/
",<kubernetes><rabbitmq><prometheus><kubernetes-helm>,63410286,4,"to troubleshoot your issue further at first place i would check a couple of things in your prom-adapter setup:

do you see any errors in prometheus adapter logs while getting metrics directly with &quot;kubectl get --raw ...&quot; command ?
to which namespace you have installed your prometheus-adapter ? (asking because i used to observe rbac related errors in adapter logs when deployed to another ns than 'kube-system')
can you please paste the content of your final 'prom-adapter' /etc/adapter/config.yaml config (from prom-adapter's pod)

i'm exposing the same external rabbitmq metrics with the adapter without any problems, check output from the same command:
{
  &quot;kind&quot;: &quot;externalmetricvaluelist&quot;,
  &quot;apiversion&quot;: &quot;external.metrics.k8s.io/v1beta1&quot;,
  &quot;metadata&quot;: {
    &quot;selflink&quot;: &quot;/apis/external.metrics.k8s.io/v1beta1/namespaces/default/rabbitmq_queue_messages_ready&quot;
  },
  &quot;items&quot;: [
    {
      &quot;metricname&quot;: &quot;rabbitmq_queue_messages_ready&quot;,
      &quot;metriclabels&quot;: {
        &quot;endpoint&quot;: &quot;metrics&quot;,
        &quot;instance&quot;: &quot;10.64.1.97:9419&quot;,
        &quot;job&quot;: &quot;rabbitmq&quot;,
        &quot;namespace&quot;: &quot;default&quot;,
        &quot;pod&quot;: &quot;rabbitmq-0&quot;,
        &quot;service&quot;: &quot;rabbitmq&quot;
      },
      &quot;timestamp&quot;: &quot;2020-08-13t23:50:29z&quot;,
      &quot;value&quot;: &quot;174160m&quot;
    }
  ]
}

used prom-adapter version: directxman12/k8s-prometheus-adapter-amd64:v0.6.0
//using community wiki post for better clarity (code snippets formatting, etc.)
"
63386644,how to expose service with multiple port in one ingress with one 80 port,"i want to expose my two api service by ingress in one 80 port.
apiversion: v1
kind: service
metadata:
  name: my-api
spec:
  selector:
    app: my-api
  ports:
  - name: api1
    port: 3000
    targetport: 3000
  - name: api2
    port: 4000
    targetport: 4000
---
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: app-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  tls:
  - hosts:
    - example.com
    secretname: app-tls
  rules:
  - host: example.com
    http:
      paths:
      - path: /my-api(/|$)(.*)
        backend:
          servicename: my-api
          serviceport: 80

but when i try to reach the https://example.com/my-api, it always returns 503 status code.
",<kubernetes><kubernetes-ingress><nginx-ingress>,63387471,4,"serviceport: 80 doesn't mean that the nginx ingress is serving on port 80. that's actually the port on your backend service, it sounds like you have 2 ports: 3000 and 4000.
the nginx ingress controller by default serves on port 80 and if you have tls enabled also or/and 443. in your case, if you want to serve both apis you can simply separate the paths.
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: app-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  tls:
  - hosts:
    - example.com
    secretname: app-tls
  rules:
  - host: example.com
    http:
      paths:
      - path: /my-api1(/|$)(.*)
        backend:
          servicename: my-api
          serviceport: 3000
      - path: /my-api2(/|$)(.*)
        backend:
          servicename: my-api
          serviceport: 4000


"
62248909,how does matchexpressions work in networkpolicy,"i have two pods namely payroll and mysql labelled as name=payroll and name=mysql. there's another pod named internal with label name=internal. i am trying to allow egress traffic from internal to other two pods while allowing all ingress traffic. my networkpoliy looks like this:

apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: internal-policy
spec:
  podselector:
    matchlabels:
      name: internal
  policytypes:
  - ingress
  - egress
  ingress:
  - {}       
  egress:
  - to:
    - podselector:
        matchexpressions:
          - {key: name, operator: in, values: [payroll, mysql]}
    ports:
    - protocol: tcp
      port: 8080
    - protocol: tcp
      port: 3306


this does not match the two pods payroll and mysql. what am i doing wrong?

the following works:

apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: internal-policy
spec:
  podselector:
    matchlabels:
      name: internal
  policytypes:
  - ingress
  - egress
  ingress:
  - {}       
  egress:
  - to:
    - podselector:
        matchlabels:
          name: payroll
    - podselector:
        matchlabels:
          name: mysql
    ports:
    - protocol: tcp
      port: 8080
    - protocol: tcp
      port: 3306


what is the best way to write a networkpolicy and why is the first one incorrect?

i also am wondering why the to field is an array while the podselector is also an array inside it? i mean they are the same right? multiple podselector or multiple to fields. using one of them works.
",<kubernetes><yaml><kubernetes-networkpolicy>,62268782,4,"
  this does not match the two pods payroll and mysql. what am i doing wrong?



i've reproduce your scenarios with pod-to-service and pod-to-pod environments, in both cases both yamls worked well. that said after fixing the indentation on line 19 where both podselector should be in the same level, as follows:


  - to:
    - podselector:
        matchlabels:
          name: payroll
    - podselector:
        matchlabels:
          name: mysql





  what is the best way to write a networkpolicy?



the best one depends on each scenario, it's a good practice to create one networkpolicy for each rule. i'd say the first yaml is the best one if you intend to expose ports 8080 and 3306 on both pods, otherwise it would be better to create two rules, to avoid leaving unnecessary open ports.





  i also am wondering why the to field is an array while the podselector is also an array inside it? i mean they are the same right? multiple podselector or multiple to fields. using one of them works.



from networkpolicyspec v1 networking api ref:


  egress networkpolicyegressrule array:
  list of egress rules to be applied to the selected pods. outgoing traffic is allowed if there are no networkpolicies selecting the pod, or if the traffic matches at least one egress rule across all of the networkpolicy objects whose podselector matches the pod.

also keep in mind that this list also includes the ports array as well.





  why is the first one incorrect?



both rules are basically the same, only written in different formats. i'd say you should check if there is any other rule in effect for the same labels.
i'd suggest you to create a test cluster and try applying the step-by-step example i'll leave below.




reproduction:


this example is very similar to your case. i'm using nginx images for it's easy testing and changed ports to 80 on networkpolicy. i'm calling your first yaml internal-original.yaml and the second you posted second-internal.yaml:


$ cat internal-original.yaml
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: internal-original
spec:
  podselector:
    matchlabels:
      name: internal
  policytypes:
  - ingress
  - egress
  ingress:
  - {}
  egress:
  - to:
    - podselector:
        matchexpressions:
          - {key: name, operator: in, values: [payroll, mysql]}
    ports:
    - protocol: tcp
      port: 80

$ cat second-internal.yaml 
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: internal-policy
spec:
  podselector:
    matchlabels:
      name: internal
  policytypes:
  - ingress
  - egress
  ingress:
  - {}       
  egress:
  - to:
    - podselector:
        matchlabels:
          name: payroll
    - podselector:
        matchlabels:
          name: mysql
    ports:
    - protocol: tcp
      port: 80



now we create the pods with the labels and expose the services:


$ kubectl run mysql --generator=run-pod/v1 --labels=""name=mysql"" --image=nginx 
pod/mysql created
$ kubectl run internal --generator=run-pod/v1 --labels=""name=internal"" --image=nginx
pod/internal created
$ kubectl run payroll --generator=run-pod/v1 --labels=""name=payroll"" --image=nginx
pod/payroll created
$ kubectl run other --generator=run-pod/v1 --labels=""name=other"" --image=nginx
pod/other created

$ kubectl expose pod mysql --port=80
service/mysql exposed
$ kubectl expose pod payroll --port=80
service/payroll exposed
$ kubectl expose pod other --port=80
service/other exposed



now, before applying the networkpolicy, i'll log into the internal pod to download wget, because after that outside access will be blocked:


$ kubectl exec internal -it -- /bin/bash
root@internal:/# apt update
root@internal:/# apt install wget -y
root@internal:/# exit



since your rule is blocking access to dns, i'll list the ips and test with them:


$ kubectl get pods -o wide
name       ready   status    restarts   age   ip
internal   1/1     running   0          62s   10.244.0.192
mysql      1/1     running   0          74s   10.244.0.141
other      1/1     running   0          36s   10.244.0.216
payroll    1/1     running   0          48s   10.244.0.17 

$ kubectl get services
name         type        cluster-ip      external-ip   port(s)   age
mysql        clusterip   10.101.209.87   &lt;none&gt;        80/tcp    23s
other        clusterip   10.103.39.7     &lt;none&gt;        80/tcp    9s
payroll      clusterip   10.109.102.5    &lt;none&gt;        80/tcp    14s



now let's test the access with the first yaml:


$ kubectl get networkpolicy
no resources found in default namespace.

$ kubectl apply -f internal-original.yaml 
networkpolicy.networking.k8s.io/internal-original created

$ kubectl exec internal -it -- /bin/bash
root@internal:/# wget --spider --timeout=1 http://10.101.209.87
spider mode enabled. check if remote file exists.
--2020-06-08 18:17:55--  http://10.101.209.87/
connecting to 10.101.209.87:80... connected.
http request sent, awaiting response... 200 ok

root@internal:/# wget --spider --timeout=1 http://10.109.102.5
spider mode enabled. check if remote file exists.
--2020-06-08 18:18:04--  http://10.109.102.5/
connecting to 10.109.102.5:80... connected.
http request sent, awaiting response... 200 ok

root@internal:/# wget --spider --timeout=1 http://10.103.39.7
spider mode enabled. check if remote file exists.
--2020-06-08 18:18:08--  http://10.103.39.7/
connecting to 10.103.39.7:80... failed: connection timed out.



now let's test the access with the second yaml:


$ kubectl get networkpolicy
name                pod-selector    age
internal-original   name=internal   96s

$ kubectl delete networkpolicy internal-original
networkpolicy.networking.k8s.io ""internal-original"" deleted

$ kubectl apply -f second-internal.yaml 
networkpolicy.networking.k8s.io/internal-policy created

$ kubectl exec internal -it -- /bin/bash
root@internal:/# wget --spider --timeout=1 http://10.101.209.87
spider mode enabled. check if remote file exists.
--2020-06-08 17:18:24--  http://10.101.209.87/
connecting to 10.101.209.87:80... connected.
http request sent, awaiting response... 200 ok

root@internal:/# wget --spider --timeout=1 http://10.109.102.5
spider mode enabled. check if remote file exists.
--2020-06-08 17:18:30--  http://10.109.102.5/
connecting to 10.109.102.5:80... connected.
http request sent, awaiting response... 200 ok

root@internal:/# wget --spider --timeout=1 http://10.103.39.7
spider mode enabled. check if remote file exists.
--2020-06-08 17:18:35--  http://10.103.39.7/
connecting to 10.103.39.7:80... failed: connection timed out.



as you can see, the connection to the services with the labels were ok and the connection to the pod that has other label has failed.


note: if you wish to allow pods to resolve dns, you can follow this guide: allow dns egress traffic

if you have any questions, let me know in the comments.
"
61636638,pod does not see secrets,"the pod that created in the same default namespace as it's secret does not see values from it.

secret's file contains following:

apiversion: v1
kind: secret
metadata:
  name: backend-secret
data:
  secret_key: &lt;base64 of value&gt;
  debug: &lt;base64 of value&gt;


after creating this secret via kubectl create -f backend-secret.yaml i'm launching pod with the following configuration:

apiversion: v1
kind: pod
metadata:
  name: backend
spec:
  containers:
    - image: backend
      name: backend
      ports:
        - containerport: 8000
  imagepullsecrets:
    - name: dockerhub-credentials
  volumes:
  - name: secret
    secret:
      secretname: backend-secret


but pod crashes after trying to extract this environment variable via python's os.environ['debug'] line.

how to make it work?
",<kubernetes><kubernetes-pod>,61637099,4,"if you mount secret as volume, it will be mounted in a defined directory where key name will be the file name. for example click here

if you want to access secrets from the environment into your pod then you need to use secret in an environment variable like following. 

apiversion: v1
kind: pod
metadata:
  name: backend
spec:
  containers:
    - image: backend
      name: backend
      ports:
        - containerport: 8000
      env:
      - name: debug
        valuefrom:
          secretkeyref:
            name: backend-secret
            key: debug
      - name: secret_key
        valuefrom:
          secretkeyref:
            name: backend-secret
            key: secret_key
  imagepullsecrets:
    - name: dockerhub-credentials


ref: https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables
"
53204733,k8s ingress: nginx ingress controller is not in running mode,"i have a jenkins image, i made service as nodetype. it works well. since i will add more services, i need to use ingress nginx to divert traffic to different kinds of services.  

at this moment, i use my win10 to set up two vms (centos 7.5). one vm as master1, it has two internal ipv4 address (10.0.2.9 and 192.168.56.103) and one vm as worker node4 (10.0.2.6 and 192.168.56.104). 

all images are local. i have downloaded into local docker image repository. the problem is that nginx ingress does not run.

my configuration as follows:

ingress-nginx-ctl.yaml:

apiversion: extensions/v1beta1
metadata:
  name: ingress-nginx
  namespace: default
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: ingress-nginx
    spec:
      terminationgraceperiodseconds: 60
      containers:
      - image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0
        name: ingress-nginx
        imagepullpolicy: never
        ports:
          - name: http
            containerport: 80
            protocol: tcp
          - name: https
            containerport: 443
            protocol: tcp
        livenessprobe:
          httpget:
            path: /healthz
            port: 10254
            scheme: http
          initialdelayseconds: 30
          timeoutseconds: 5
        env:
          - name: pod_name
            valuefrom:
              fieldref:
                fieldpath: metadata.name
          - name: pod_namespace
            valuefrom:
              fieldref:
                fieldpath: metadata.namespace
        args:
        - /nginx-ingress-controller
        - --default-backend-service=$(pod_namespace)/nginx-default-backend


ingress-nginx-res.yaml:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-ingress
  namespace: default
spec:
  rules:
  - host:
    http:
      paths:
      - path: /
        backend:
          servicename: shinyinfo-jenkins-svc
          serviceport: 8080


nginx-default-backend.yaml

kind: service
apiversion: v1
metadata:
  name: nginx-default-backend
  namespace: default
spec:
  ports:
  - port: 80
    targetport: http
  selector:
    app: nginx-default-backend
---
kind: deployment
apiversion: extensions/v1beta1
metadata:
  name: nginx-default-backend
  namespace: default
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-default-backend
    spec:
      terminationgraceperiodseconds: 60
      containers:
      - name: default-http-backend
        image: chenliujin/defaultbackend
        imagepullpolicy: never
        livenessprobe:
          httpget:
            path: /healthz
            port: 8080
            scheme: http
          initialdelayseconds: 30
          timeoutseconds: 5
        resources:
          limits:
            cpu: 10m
            memory: 10mi
          requests:
            cpu: 10m
            memory: 10mi
        ports:
        - name: http
          containerport: 8080
          protocol: tcp


shinyinfo-jenkins-pod.yaml

apiversion: v1
kind: pod
metadata:
 name: shinyinfo-jenkins
 labels:
   app: shinyinfo-jenkins
spec:
 containers:
   - name: shinyinfo-jenkins
     image: shinyinfo_jenkins
     imagepullpolicy: never
     ports:
       - containerport: 8080
         containerport: 50000
     volumemounts:
     - mountpath: /devops/password
       name: jenkins-password
     - mountpath: /var/jenkins_home
       name: jenkins-home
 volumes:
   - name: jenkins-password
     hostpath:
       path: /jenkins/password
   - name: jenkins-home
     hostpath:
       path: /jenkins


shinyinfo-jenkins-svc.yaml

apiversion: v1
kind: service
metadata:
  name: shinyinfo-jenkins-svc
  labels:
    name: shinyinfo-jenkins-svc
spec:
  selector:
    app: shinyinfo-jenkins
  type: nodeport
  ports:
  - name: tcp
    port: 8080
    nodeport: 30003


there is something wrong with nginx ingress, the console output is as follows:

[master@master1 config]$ sudo kubectl apply -f ingress-nginx-ctl.yaml
service/ingress-nginx created
deployment.extensions/ingress-nginx created

[master@master1 config]$ sudo kubectl apply -f ingress-nginx-res.yaml
ingress.extensions/my-ingress created


images is crashloopbackoff, why???

[master@master1 config]$ sudo kubectl get po
name                                     ready     status             restarts   age
ingress-nginx-66df6b6d9-mhmj9            0/1       crashloopbackoff   1          9s
nginx-default-backend-645546c46f-x7s84   1/1       running            0          6m
shinyinfo-jenkins                        1/1       running            0          20m


describe pod:

[master@master1 config]$ sudo kubectl describe po ingress-nginx-66df6b6d9-mhmj9
name:               ingress-nginx-66df6b6d9-mhmj9
namespace:          default
priority:           0
priorityclassname:  &lt;none&gt;
node:               node4/192.168.56.104
start time:         thu, 08 nov 2018 16:45:46 +0800
labels:             app=ingress-nginx
                    pod-template-hash=228926285
annotations:        &lt;none&gt;
status:             running
ip:                 100.127.10.211
controlled by:      replicaset/ingress-nginx-66df6b6d9
containers:
  ingress-nginx:
    container id:  docker://2aba164d116758585abef9d893a5fa0f0c5e23c04a13466263ce357ebe10cb0a
    image:         quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0
    image id:      docker://sha256:a3f21ec4bd119e7e17c8c8b2bf8a3b9e42a8607455826cd1fa0b5461045d2fa9
    ports:         80/tcp, 443/tcp
    host ports:    0/tcp, 0/tcp
    args:
      /nginx-ingress-controller
      --default-backend-service=$(pod_namespace)/nginx-default-backend
    state:          waiting
      reason:       crashloopbackoff
    last state:     terminated
      reason:       error
      exit code:    255
      started:      thu, 08 nov 2018 16:46:09 +0800
      finished:     thu, 08 nov 2018 16:46:09 +0800
    ready:          false
    restart count:  2
    liveness:       http-get http://:10254/healthz delay=30s timeout=5s period=10s #success=1 #failure=3
    environment:
      pod_name:       ingress-nginx-66df6b6d9-mhmj9 (v1:metadata.name)
      pod_namespace:  default (v1:metadata.namespace)
    mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-24hnm (ro)
conditions:
  type              status
  initialized       true
  ready             false
  containersready   false
  podscheduled      true
volumes:
  default-token-24hnm:
    type:        secret (a volume populated by a secret)
    secretname:  default-token-24hnm
    optional:    false
qos class:       besteffort
node-selectors:  &lt;none&gt;
tolerations:     node.kubernetes.io/not-ready:noexecute for 300s
                 node.kubernetes.io/unreachable:noexecute for 300s
events:
  type     reason     age                from               message
  ----     ------     ----               ----               -------
  normal   scheduled  40s                default-scheduler  successfully assigned default/ingress-nginx-66df6b6d9-mhmj9 to node4
  normal   pulled     18s (x3 over 39s)  kubelet, node4     container image ""quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0"" already present on machine
  normal   created    18s (x3 over 39s)  kubelet, node4     created container
  normal   started    17s (x3 over 39s)  kubelet, node4     started container
  warning  backoff    11s (x5 over 36s)  kubelet, node4     back-off restarting failed container


logs of pod:

[master@master1 config]$ sudo kubectl logs ingress-nginx-66df6b6d9-mhmj9
-------------------------------------------------------------------------------
nginx ingress controller
  release:    0.20.0
  build:      git-e8d8103
  repository: https://github.com/kubernetes/ingress-nginx.git
-------------------------------------------------------------------------------
nginx version: nginx/1.15.5
w1108 08:47:16.081042       6 client_config.go:552] neither --kubeconfig nor --master was specified.  using the inclusterconfig.  this might not work.
i1108 08:47:16.081234       6 main.go:196] creating api client for https://10.96.0.1:443
i1108 08:47:16.122315       6 main.go:240] running in kubernetes cluster version v1.11 (v1.11.3) - git (clean) commit a4529464e4629c21224b3d52edfe0ea91b072862 - platform linux/amd64
f1108 08:47:16.123661       6 main.go:97]  the cluster seems to be running with a restrictive authorization mode and the ingress controller does not have the required permissions to operate normally.


could experts here drop me some hints?
",<kubernetes><kubernetes-ingress>,53205357,4,"you need set ingress-nginx to use a seperate serviceaccount and give neccessary privilege to the serviceaccount. 

here is a example:

apiversion: v1
kind: serviceaccount
metadata:
  name: lb
  namespace: kube-system

---

apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrole
metadata:
  name: nginx-ingress-normal
rules:
  - apigroups:
      - """"
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apigroups:
      - """"
    resources:
      - nodes
    verbs:
      - get
  - apigroups:
      - """"
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apigroups:
      - ""extensions""
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apigroups:
      - """"
    resources:
        - events
    verbs:
        - create
        - patch
  - apigroups:
      - ""extensions""
    resources:
      - ingresses/status
    verbs:
      - update

---

apiversion: rbac.authorization.k8s.io/v1beta1
kind: role
metadata:
  name: nginx-ingress-minimal
  namespace: kube-system
rules:
  - apigroups:
      - """"
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apigroups:
      - """"
    resources:
      - configmaps
    resourcenames:
      - ""ingress-controller-leader-dev""
      - ""ingress-controller-leader-prod""
    verbs:
      - get
      - update
  - apigroups:
      - """"
    resources:
      - configmaps
    verbs:
      - create
  - apigroups:
      - """"
    resources:
      - endpoints
    verbs:
      - get

---

apiversion: rbac.authorization.k8s.io/v1beta1
kind: rolebinding
metadata:
  name: nginx-ingress-minimal
  namespace: kube-system
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: nginx-ingress-minimal
subjects:
  - kind: serviceaccount
    name: lb
    namespace: kube-system

---

apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrolebinding
metadata:
  name: nginx-ingress-normal
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: nginx-ingress-normal
subjects:
  - kind: serviceaccount
    name: lb
    namespace: kube-system

"
66051624,generate wildcard certificate on kubernetes cluster with digitalocean for my nginx-ingress,"i followed this digitalocean guide https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nginx-ingress-with-cert-manager-on-digitalocean-kubernetes, and i came across something quite strange. when in the hostnames i set a wildcard, then letsencrypt fails in issuing a new certificate. while when i only set defined sub-domains, then it works perfectly.
this is my &quot;working&quot; configuration for the domain and its api (and this one works perfectly):
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: my-ingress
  annotations:
    cert-manager.io/cluster-issuer: &quot;letsencrypt-staging&quot;
spec:
  tls:
  - hosts:
    - example.com
    - api.example.com
    secretname: my-tls
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          servicename: example-frontend
          serviceport: 80
  - host: api.example.com
    http:
      paths:
      - backend:
          servicename: example-api
          serviceport: 80

and this is, instead, the wildcard certificate i'm trying to issue, but that fails to do leaving the message &quot;issuing&quot;.
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: my-ingress
  annotations:
    cert-manager.io/cluster-issuer: &quot;letsencrypt-staging&quot;
spec:
  tls:
  - hosts:
    - example.com
    - *.example.com
    secretname: my-tls
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          servicename: example-frontend
          serviceport: 80
  - host: api.example.com
    http:
      paths:
      - backend:
          servicename: example-api
          serviceport: 80
      

the only difference is the second line of the hosts. is there a trivial well known solution i am not aware of? i am new to kubernetes, but not to devops.
",<nginx><ssl><kubernetes><kubernetes-ingress>,66097094,4,"generating wildcard certificate with cert-manager (letsencrypt) requires the usage of dns-01 challenge instead of http-01 used in the link from the question:

does lets encrypt issue wildcard certificates?
yes. wildcard issuance must be done via acmev2 using the dns-01 challenge. see this post for more technical information.

there is a documentation about generating the wildcard certificate with cert-manager:

cert-manager.io: docs: configuration: acme: dns-01


from the perspective of digialocean, there is a guide specifically targeted at it:

this provider uses a kubernetes secret resource to work. in the following
example, the secret will have to be named digitalocean-dns and have a
sub-key access-token with the token in it. for example:
apiversion: v1
kind: secret
metadata:
  name: digitalocean-dns
  namespace: cert-manager
data:
  # insert your do access token here
  access-token: &quot;base64 encoded access-token here&quot;

the access token must have write access.
to create a personal access token, see digitalocean documentation.
handy direct link: https://cloud.digitalocean.com/account/api/tokens/new
to encode your access token into base64, you can use the following
echo -n 'your-access-token' | base64 -w 0

apiversion: cert-manager.io/v1
kind: issuer
metadata:
  name: example-issuer
spec:
  acme:
    ...
    solvers:
    - dns01:
        digitalocean:
          tokensecretref:
            name: digitalocean-dns
            key: access-token

-- cert-manager.io: docs: configuration: acme: dns-01: digitalocean


i'd reckon this additional resources could also help:

stackoverflow.com: questions: wilcard ssl certificate with subdomain redirect in kubernetes
itnext.io: using wildcard certificates with cert-manager in kubernetes

"
66750257,"return persistent volume (pv) capacity in integer instead of gi, mi, ki, g, m, k etc","i would like to calculate the total number of bytes allocated by the persistent volumes (pvs) in a cluster. using the following:
$ kubectl get pv -a -o json

i can get a json list of all the cluster's pvs and for each pv in the items[] list one can read the spec.capacity.storage key to access the necessary information.
see example below:
{
  &quot;apiversion&quot;: &quot;v1&quot;,
  &quot;kind&quot;: &quot;persistentvolume&quot;,
  &quot;spec&quot;: {
    &quot;accessmodes&quot;: [
      &quot;readwriteonce&quot;
    ],
    &quot;capacity&quot;: {
      &quot;storage&quot;: &quot;500gi&quot;
    },
    &quot;claimref&quot;: {
      &quot;apiversion&quot;: &quot;v1&quot;,
      &quot;kind&quot;: &quot;persistentvolumeclaim&quot;,
      &quot;name&quot;: &quot;s3-storage-minio&quot;,
      &quot;namespace&quot;: &quot;default&quot;,
      &quot;resourceversion&quot;: &quot;515932&quot;,
    },
    &quot;persistentvolumereclaimpolicy&quot;: &quot;delete&quot;,
    &quot;volumemode&quot;: &quot;filesystem&quot;,
  },
  &quot;status&quot;: {
    &quot;phase&quot;: &quot;bound&quot;
  }
},

however, the returned values can be represented in different suffix (storage as a plain integer or as a fixed-point number using one of these suffixes: e, p, t, g, m, k. or similarly, power-of-two equivalents: ei, pi, ti, gi, mi, ki).
is there a neat way to request the capacity in integer format (or any other format but consistent among all the pvs) using the kubectl?
otherwise, transforming different suffix to a common one in bash looks like not very straightforward.
thanks in advance for your help.
",<kubernetes><kubernetes-pvc>,66783417,4,"i haven't found a way to transform a value in .spec.capacity.storage using purely kubectl.

i've managed to create a code with python and it's kubernetes library to extract the data and calculate the size of all used pv's. please treat this code as an example and not production ready:
from kubernetes import client, config
import re 

config.load_kube_config() # use .kube/config
v1 = client.corev1api()

multiplier_dict = {&quot;k&quot;: 1000, &quot;ki&quot;: 1024, &quot;m&quot;: 1000000, &quot;mi&quot;: 1048576 , &quot;g&quot;: 1000000000, &quot;gi&quot;: 1073741824} # and so on ... 
size = 0 

# for i in v1.list_persistent_volume_claim_for_all_namespaces(watch=false).items: # pvc

for i in v1.list_persistent_volume(watch=false).items: # pv

    x = i.spec.capacity[&quot;storage&quot;] # pv
    # x = i.spec.resources.requests[&quot;storage&quot;] # pvc
    y = re.findall(r'[a-za-z]+|\d+', x)
    print(y)

    # try used if no suffix (like mi) is used
    try: 
        if y[1] in multiplier_dict: 
            size += multiplier_dict.get(y[1]) * int(y[0])
    except indexerror:
            size += int(y[0])
    
print(&quot;the size in bytes of all pv's is: &quot; + str(size))

having as an example a cluster that has following pv's:

$ kubectl get pv

name                                       capacity   access modes   reclaim policy   status   claim               storageclass   reason   age
pvc-6b5236ec-547f-4f96-8448-e3dbe01c9039   500mi      rwo            delete           bound    default/pvc-four    hostpath                4m13s
pvc-86d178bc-1673-44e0-9a89-2efb14a1d22c   512m       rwo            delete           bound    default/pvc-three   hostpath                4m15s
pvc-89b64f93-6bf4-4987-bdda-0356d19d6f59   1g         rwo            delete           bound    default/pvc-one     hostpath                4m15s
pvc-a3455e77-0db0-4cab-99c9-c72721a65632   10ki       rwo            delete           bound    default/pvc-six     hostpath                4m14s
pvc-b47f92ef-f627-4391-943f-efa4241d0811   10k        rwo            delete           bound    default/pvc-five    hostpath                4m13s
pvc-c3e13d78-9047-4899-99e7-0b2667ce4698   1gi        rwo            delete           bound    default/pvc-two     hostpath                4m15s
pvc-c57fe2b0-013a-412b-bca9-05050990766a   10         rwo            delete           bound    default/pvc-seven   hostpath                113s

the code would produce the output of:
['500', 'mi']
['512', 'm']
['1', 'g']
['10', 'ki']
['10', 'k']
['1', 'gi']
['10']
the size in bytes of all pv's is: 3110050074


adding to the whole answer remember that there could be differences on the request of a pvc and the actual pv size. please refer to the storage documentation of your choosing on that regard.

pvc.yaml:

apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: pvc
spec:
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 100m

part of the $ kubectl get pvc -o yaml output:
  spec:
    accessmodes:
    - readwriteonce
    resources:
      requests:
        storage: 100m # &lt;-- request
    &lt;-- redacted --&gt; 
  status:
    accessmodes:
    - readwriteonce
    capacity:
      storage: 1gi # &lt;-- size of pv
    phase: bound


additional resources:

kubernetes.io: docs: concepts: storage: persistent volumes
wikipedia.org: byte: multiple byte units

"
66209477,kubernetes : refering to containerports from services with their name,"hi the documentation  says about the name field in containers.ports :

name  if specified, this must be an iana_svc_name and unique within the pod. each named port in a pod must have a unique name. name for the port that can be referred to by services.

i tried to use it in my service as follows but i have an error from the parser :
kind: pod
apiversion: v1
metadata:
  name: banana-app
  labels:
    app: banana
spec:
  containers:
    - name: banana-app
      image: hashicorp/http-echo
      args:
        - &quot;-text=banana&quot;
      ports:
      - containerport: 5678
        name: bananaport
  terminationgraceperiodseconds: 0
---

kind: service
apiversion: v1
metadata:
  name: banana-service
spec:
  selector:
    app: banana
  ports:
    - port: bananaport

the parser error is :

error: error validating &quot;temp.yml&quot;: error validating data: validationerror(service.spec.ports[0].port): invalid type for io.k8s.api.core.v1.serviceport.port: got &quot;string&quot;, expected &quot;integer&quot;; if you choose to ignore these errors, turn validation off with --validate=false

so i guess i am not using the port name correctly. what is the correct way to use the port name in my service?
thanks in advance,
abdelghani
",<kubernetes><kubernetes-service>,66209736,4,"the port field represents the port at which the service object listens to and the targetport represents the port at which the container is listening.
try this:
kind: service
apiversion: v1
metadata:
  name: banana-service
spec:
  selector:
    app: banana
  ports:
  - port: 8080
    targetport: bananaport

"
66423946,how can i use kongs capturing group in ingress k8s object for rewirting logic?,"i want to use kongs capturing group in ingress k8s object to perform an uri rewriting.
i want to implement the following logic:
https://kong_host:30000/service/audits/health -&gt; (rewrite) https://kong_host:30000/service/audit/v1/health
ingress resource:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: audits
  annotations:
    konghq.com/plugins: audits-rewrite
spec:
  rules:
  - http:
      paths:
      - path: /service/audits/(?&lt;path&gt;\\\s+)
        backend:
          servicename: audits
          serviceport: 8080

kongplugin
apiversion: configuration.konghq.com/v1
kind: kongplugin
metadata:
  name: audits-rewrite
config: 
  replace:
    uri: /service/audit/v1/$(uri_captures[&quot;path&quot;])
plugin: request-transformer

thanks.
",<kubernetes><kubernetes-ingress><kong><kong-plugin><kong-ingress>,66760477,4,"as pointed in documentation you are not able to use v1beat1 ingress api version to capture groups in paths.
https://docs.konghq.com/hub/kong-inc/request-transformer/#examples
you need to upgrade you k8s cluster to 1.19 or higher version to use this feature.
i also had similar problem and resolved it will following configuration:
ingress resource:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: audits
  annotations:
    konghq.com/plugins: audits-rewrite
spec:
  rules:
  - http:
      paths:
      - path: /service/audits/(.*)
        backend:
          servicename: audits
          serviceport: 8080

kongplugin
apiversion: configuration.konghq.com/v1
kind: kongplugin
metadata:
  name: audits-rewrite
config: 
  replace:
    uri: /service/audit/v1/$(uri_captures[1])
plugin: request-transformer

"
66816083,kubernetes helm chart one namespace contains multiple ingress files,"i have a use case that i need to expose
/swagger-ui.html without authentication and
/apis/* with authentication
i created 2 ingress files in helm chart
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/auth-method: post
    nginx.ingress.kubernetes.io/auth-url: {{ .values.service.authserverurl }}/authorization
    nginx.ingress.kubernetes.io/cors-allow-methods: &quot;put, patch, get, post, delete, options&quot;
    nginx.ingress.kubernetes.io/cors-allow-origin: &quot;*&quot;
    nginx.ingress.kubernetes.io/cors-allow-headers: '*'
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
  labels:
    app: {{ .chart.name }}
    appversion: {{ .chart.appversion | quote }}
    chartversion: {{ .chart.version | quote }}
  name: {{ .chart.name }}
spec:
  rules:
    - host: &quot;example.com&quot;
      http:
        paths:
          - backend:
              servicename: service
              serviceport: 8080
            path: /apis


and another ingress file without authentication
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
  labels:
    app: {{ .chart.name }}
    appversion: {{ .chart.appversion | quote }}
    chartversion: {{ .chart.version | quote }}
  name: {{ .chart.name }}
spec:
  rules:
    - host: &quot;example.com&quot;
      http:
        paths:
          - backend:
              servicename: service
              serviceport: 8080
            path: /swagger-ui.html

but seems like the second ingress does not work.
================================================================
sagar velankar's answer is correct. just need to change different service name
below is my final ingress file
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/auth-method: post
    nginx.ingress.kubernetes.io/auth-url: {{ .values.service.authserverurl }}/authorization
    nginx.ingress.kubernetes.io/cors-allow-methods: &quot;put, patch, get, post, delete, options&quot;
    nginx.ingress.kubernetes.io/cors-allow-origin: &quot;*&quot;
    nginx.ingress.kubernetes.io/cors-allow-headers: '*'
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
  labels:
    app: {{ .chart.name }}
    appversion: {{ .chart.appversion | quote }}
    chartversion: {{ .chart.version | quote }}
  name: {{ .chart.name }}-api
spec:
  rules:
    - host: &quot;example.com&quot;
      http:
        paths:
          - backend:
              servicename: service
              serviceport: 8080
            path: /apis


and another ingress file with swagger
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
  labels:
    app: {{ .chart.name }}
    appversion: {{ .chart.appversion | quote }}
    chartversion: {{ .chart.version | quote }}
  name: {{ .chart.name }}-swagger
spec:
  rules:
    - host: &quot;example.com&quot;
      http:
        paths:
          - backend:
              servicename: service
              serviceport: 8080
            path: /swagger-ui.html
          - backend:
              servicename: {{ .chart.name }}
              serviceport: 8080
            path: /webjars
          - backend:
              servicename: {{ .chart.name }}
              serviceport: 8080
            path: /swagger-resources
          - backend:
              servicename: {{ .chart.name }}
              serviceport: 8080
            path: /v2/api-docs

",<kubernetes><swagger-ui><kubernetes-ingress><nginx-ingress>,66817383,4,"please try changing the ingress name for both templates to be unique. currently i see both are set to below
name: {{ .chart.name }}

i just changed apiversion from extensions/v1beta1 to networking.k8s.io/v1beta1 and added below yaml files to my kubernetes cluster
---
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/auth-method: post
    nginx.ingress.kubernetes.io/auth-url: &quot;http://www.gool.com/authorization&quot;
    nginx.ingress.kubernetes.io/cors-allow-methods: &quot;put, patch, get, post, delete, options&quot;
    nginx.ingress.kubernetes.io/cors-allow-origin: &quot;*&quot;
    nginx.ingress.kubernetes.io/cors-allow-headers: '*'
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
  labels:
    app: nginx
    appversion: &quot;1&quot;
    chartversion: &quot;1&quot;
  name: nginx1
spec:
  rules:
    - host: &quot;example.com&quot;
      http:
        paths:
          - backend:
              servicename: service
              serviceport: 8080
            path: /apis
---
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
  labels:
    app: nginx
    appversion: &quot;1&quot;
    chartversion: &quot;1&quot;
  name: nginx2
spec:
  rules:
    - host: &quot;example.com&quot;
      http:
        paths:
          - backend:
              servicename: service
              serviceport: 8080
            path: /swagger-ui.html

and it created below server in nginx controller /etc/nginx/nginx.conf which sets internal location for authorization and points location /apis auth_request to it.

## start server example.com
server {
server_name example.com ;

listen 80  ;
listen [::]:80  ;
listen 442 proxy_protocol  ssl http2 ;
listen [::]:442 proxy_protocol  ssl http2 ;

set $proxy_upstream_name &quot;-&quot;;

ssl_certificate_by_lua_block {
        certificate.call()
}

location /swagger-ui.html/ {

        set $namespace      &quot;kt&quot;;
        set $ingress_name   &quot;nginx2&quot;;
        set $service_name   &quot;service&quot;;
        set $service_port   &quot;8080&quot;;
        set $location_path  &quot;/swagger-ui.html&quot;;
        set $global_rate_limit_exceeding n;

        rewrite_by_lua_block {
                lua_ingress.rewrite({
                        force_ssl_redirect = false,
                        ssl_redirect = false,
                        force_no_ssl_redirect = false,
                        use_port_in_redirects = false,
                global_throttle = { namespace = &quot;&quot;, limit = 0, window_size = 0, key = { }, ignored_cidrs = { } },
                })
                balancer.rewrite()
                plugins.run()
        }

        # be careful with `access_by_lua_block` and `satisfy any` directives as satisfy any
        # will always succeed when there's `access_by_lua_block` that does not have any lua code doing `ngx.exit(ngx.declined)`
        # other authentication method such as basic auth or external auth useless - all requests will be allowed.
        #access_by_lua_block {
        #}

        header_filter_by_lua_block {
                lua_ingress.header()
                plugins.run()
        }

        body_filter_by_lua_block {
                plugins.run()
        }

        log_by_lua_block {
                balancer.log()

                monitor.call()

                plugins.run()
        }

        port_in_redirect off;

        set $balancer_ewma_score -1;
        set $proxy_upstream_name &quot;kt-service-8080&quot;;
        set $proxy_host          $proxy_upstream_name;
        set $pass_access_scheme  $scheme;

        set $pass_server_port    $server_port;

        set $best_http_host      $http_host;
        set $pass_port           $pass_server_port;

        set $proxy_alternative_upstream_name &quot;&quot;;

        client_max_body_size                    1m;

        proxy_set_header host                   $best_http_host;

        # pass the extracted client certificate to the backend

        # allow websocket connections
        proxy_set_header                        upgrade           $http_upgrade;

        proxy_set_header                        connection        $connection_upgrade;

        proxy_set_header x-request-id           $req_id;
        proxy_set_header x-real-ip              $remote_addr;

        proxy_set_header x-forwarded-for        $remote_addr;

        proxy_set_header x-forwarded-host       $best_http_host;
        proxy_set_header x-forwarded-port       $pass_port;
        proxy_set_header x-forwarded-proto      $pass_access_scheme;

        proxy_set_header x-scheme               $pass_access_scheme;

        # pass the original x-forwarded-for
        proxy_set_header x-original-forwarded-for $http_x_forwarded_for;

        # mitigate httpoxy vulnerability
        # https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/
        proxy_set_header proxy                  &quot;&quot;;

        # custom headers to proxied server

        proxy_connect_timeout                   5s;
        proxy_send_timeout                      60s;
        proxy_read_timeout                      60s;

        proxy_buffering                         off;
        proxy_buffer_size                       4k;
        proxy_buffers                           4 4k;

        proxy_max_temp_file_size                1024m;

        proxy_request_buffering                 on;
        proxy_http_version                      1.1;

        proxy_cookie_domain                     off;
        proxy_cookie_path                       off;

        # in case of errors try the next upstream server before returning an error
        proxy_next_upstream                     error timeout;
        proxy_next_upstream_timeout             0;
        proxy_next_upstream_tries               3;

        proxy_pass http://upstream_balancer;

        proxy_redirect                          off;

}

location = /swagger-ui.html {

        set $namespace      &quot;kt&quot;;
        set $ingress_name   &quot;nginx2&quot;;
        set $service_name   &quot;service&quot;;
        set $service_port   &quot;8080&quot;;
        set $location_path  &quot;/swagger-ui.html&quot;;
        set $global_rate_limit_exceeding n;

        rewrite_by_lua_block {
                lua_ingress.rewrite({
                        force_ssl_redirect = false,
                        ssl_redirect = false,
                        force_no_ssl_redirect = false,
                        use_port_in_redirects = false,
                global_throttle = { namespace = &quot;&quot;, limit = 0, window_size = 0, key = { }, ignored_cidrs = { } },
                })
                balancer.rewrite()
                plugins.run()
        }

        # be careful with `access_by_lua_block` and `satisfy any` directives as satisfy any
        # will always succeed when there's `access_by_lua_block` that does not have any lua code doing `ngx.exit(ngx.declined)`
        # other authentication method such as basic auth or external auth useless - all requests will be allowed.
        #access_by_lua_block {
        #}

        header_filter_by_lua_block {
                lua_ingress.header()
                plugins.run()
        }

        body_filter_by_lua_block {
                plugins.run()
        }

        log_by_lua_block {
                balancer.log()

                monitor.call()

                plugins.run()
        }

        port_in_redirect off;

        set $balancer_ewma_score -1;
        set $proxy_upstream_name &quot;kt-service-8080&quot;;
        set $proxy_host          $proxy_upstream_name;
        set $pass_access_scheme  $scheme;

        set $pass_server_port    $server_port;

        set $best_http_host      $http_host;
        set $pass_port           $pass_server_port;

        set $proxy_alternative_upstream_name &quot;&quot;;

        client_max_body_size                    1m;

        proxy_set_header host                   $best_http_host;

        # pass the extracted client certificate to the backend

        # allow websocket connections
        proxy_set_header                        upgrade           $http_upgrade;

        proxy_set_header                        connection        $connection_upgrade;

        proxy_set_header x-request-id           $req_id;
        proxy_set_header x-real-ip              $remote_addr;

        proxy_set_header x-forwarded-for        $remote_addr;

        proxy_set_header x-forwarded-host       $best_http_host;
        proxy_set_header x-forwarded-port       $pass_port;
        proxy_set_header x-forwarded-proto      $pass_access_scheme;

        proxy_set_header x-scheme               $pass_access_scheme;

        # pass the original x-forwarded-for
        proxy_set_header x-original-forwarded-for $http_x_forwarded_for;

        # mitigate httpoxy vulnerability
        # https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/
        proxy_set_header proxy                  &quot;&quot;;

        # custom headers to proxied server

        proxy_connect_timeout                   5s;
        proxy_send_timeout                      60s;
        proxy_read_timeout                      60s;

        proxy_buffering                         off;
        proxy_buffer_size                       4k;
        proxy_buffers                           4 4k;

        proxy_max_temp_file_size                1024m;

        proxy_request_buffering                 on;
        proxy_http_version                      1.1;

        proxy_cookie_domain                     off;
        proxy_cookie_path                       off;

        # in case of errors try the next upstream server before returning an error
        proxy_next_upstream                     error timeout;
        proxy_next_upstream_timeout             0;
        proxy_next_upstream_tries               3;

        proxy_pass http://upstream_balancer;

        proxy_redirect                          off;

}

location = /_external-auth-l2fwaxmv-prefix {
        internal;

        # ngx_auth_request module overrides variables in the parent request,
        # therefore we have to explicitly set this variable again so that when the parent request
        # resumes it has the correct value set for this variable so that lua can pick backend correctly
        set $proxy_upstream_name &quot;kt-service-8080&quot;;

        proxy_pass_request_body     off;
        proxy_set_header            content-length          &quot;&quot;;
        proxy_set_header            x-forwarded-proto       &quot;&quot;;
        proxy_set_header            x-request-id            $req_id;

        proxy_method                post;
        proxy_set_header            x-original-uri          $request_uri;
        proxy_set_header            x-scheme                $pass_access_scheme;

        proxy_set_header            host                    www.gool.com;
        proxy_set_header            x-original-url          $scheme://$http_host$request_uri;
        proxy_set_header            x-original-method       $request_method;
        proxy_set_header            x-sent-from             &quot;nginx-ingress-controller&quot;;
        proxy_set_header            x-real-ip               $remote_addr;

        proxy_set_header            x-forwarded-for        $remote_addr;

        proxy_set_header            x-auth-request-redirect $request_uri;

        proxy_buffering                         off;

        proxy_buffer_size                       4k;
        proxy_buffers                           4 4k;
        proxy_request_buffering                 on;
        proxy_http_version                      1.1;

        proxy_ssl_server_name       on;
        proxy_pass_request_headers  on;

        client_max_body_size        1m;

        # pass the extracted client certificate to the auth provider

        set $target http://www.gool.com/authorization;
        proxy_pass $target;
}

location /apis/ {

        set $namespace      &quot;kt&quot;;
        set $ingress_name   &quot;nginx1&quot;;
        set $service_name   &quot;service&quot;;
        set $service_port   &quot;8080&quot;;
        set $location_path  &quot;/apis&quot;;
        set $global_rate_limit_exceeding n;

        rewrite_by_lua_block {
                lua_ingress.rewrite({
                        force_ssl_redirect = false,
                        ssl_redirect = false,
                        force_no_ssl_redirect = false,
                        use_port_in_redirects = false,
                global_throttle = { namespace = &quot;&quot;, limit = 0, window_size = 0, key = { }, ignored_cidrs = { } },
                })
                balancer.rewrite()
                plugins.run()
        }

        # be careful with `access_by_lua_block` and `satisfy any` directives as satisfy any
        # will always succeed when there's `access_by_lua_block` that does not have any lua code doing `ngx.exit(ngx.declined)`
        # other authentication method such as basic auth or external auth useless - all requests will be allowed.
        #access_by_lua_block {
        #}

        header_filter_by_lua_block {
                lua_ingress.header()
                plugins.run()
        }

        body_filter_by_lua_block {
                plugins.run()
        }

        log_by_lua_block {
                balancer.log()

                monitor.call()

                plugins.run()
        }

        port_in_redirect off;

        set $balancer_ewma_score -1;
        set $proxy_upstream_name &quot;kt-service-8080&quot;;
        set $proxy_host          $proxy_upstream_name;
        set $pass_access_scheme  $scheme;

        set $pass_server_port    $server_port;

        set $best_http_host      $http_host;
        set $pass_port           $pass_server_port;

        set $proxy_alternative_upstream_name &quot;&quot;;

        # this location requires authentication
        auth_request        /_external-auth-l2fwaxmv-prefix;
        auth_request_set    $auth_cookie $upstream_http_set_cookie;
        add_header          set-cookie $auth_cookie;

        client_max_body_size                    1m;

        proxy_set_header host                   $best_http_host;

        # pass the extracted client certificate to the backend

        # allow websocket connections
        proxy_set_header                        upgrade           $http_upgrade;

        proxy_set_header                        connection        $connection_upgrade;

        proxy_set_header x-request-id           $req_id;
        proxy_set_header x-real-ip              $remote_addr;

        proxy_set_header x-forwarded-for        $remote_addr;

        proxy_set_header x-forwarded-host       $best_http_host;
        proxy_set_header x-forwarded-port       $pass_port;
        proxy_set_header x-forwarded-proto      $pass_access_scheme;

        proxy_set_header x-scheme               $pass_access_scheme;

        # pass the original x-forwarded-for
        proxy_set_header x-original-forwarded-for $http_x_forwarded_for;

        # mitigate httpoxy vulnerability
        # https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/
        proxy_set_header proxy                  &quot;&quot;;

        # custom headers to proxied server

        proxy_connect_timeout                   5s;
        proxy_send_timeout                      60s;
        proxy_read_timeout                      60s;

        proxy_buffering                         off;
        proxy_buffer_size                       4k;
        proxy_buffers                           4 4k;

        proxy_max_temp_file_size                1024m;

        proxy_request_buffering                 on;
        proxy_http_version                      1.1;

        proxy_cookie_domain                     off;
        proxy_cookie_path                       off;

        # in case of errors try the next upstream server before returning an error
        proxy_next_upstream                     error timeout;
        proxy_next_upstream_timeout             0;
        proxy_next_upstream_tries               3;

        proxy_pass http://upstream_balancer;

        proxy_redirect                          off;

}

location = /_external-auth-l2fwaxm-exact {
        internal;

        # ngx_auth_request module overrides variables in the parent request,
        # therefore we have to explicitly set this variable again so that when the parent request
        # resumes it has the correct value set for this variable so that lua can pick backend correctly
        set $proxy_upstream_name &quot;kt-service-8080&quot;;

        proxy_pass_request_body     off;
        proxy_set_header            content-length          &quot;&quot;;
        proxy_set_header            x-forwarded-proto       &quot;&quot;;
        proxy_set_header            x-request-id            $req_id;

        proxy_method                post;
        proxy_set_header            x-original-uri          $request_uri;
        proxy_set_header            x-scheme                $pass_access_scheme;

        proxy_set_header            host                    www.gool.com;
        proxy_set_header            x-original-url          $scheme://$http_host$request_uri;
        proxy_set_header            x-original-method       $request_method;
        proxy_set_header            x-sent-from             &quot;nginx-ingress-controller&quot;;
        proxy_set_header            x-real-ip               $remote_addr;

        proxy_set_header            x-forwarded-for        $remote_addr;

        proxy_set_header            x-auth-request-redirect $request_uri;

        proxy_buffering                         off;

        proxy_buffer_size                       4k;
        proxy_buffers                           4 4k;
        proxy_request_buffering                 on;
        proxy_http_version                      1.1;

        proxy_ssl_server_name       on;
        proxy_pass_request_headers  on;

        client_max_body_size        1m;

        # pass the extracted client certificate to the auth provider

        set $target http://www.gool.com/authorization;
        proxy_pass $target;
}

location = /apis {

        set $namespace      &quot;kt&quot;;
        set $ingress_name   &quot;nginx1&quot;;
        set $service_name   &quot;service&quot;;
        set $service_port   &quot;8080&quot;;
        set $location_path  &quot;/apis&quot;;
        set $global_rate_limit_exceeding n;

        rewrite_by_lua_block {
                lua_ingress.rewrite({
                        force_ssl_redirect = false,
                        ssl_redirect = false,
                        force_no_ssl_redirect = false,
                        use_port_in_redirects = false,
                global_throttle = { namespace = &quot;&quot;, limit = 0, window_size = 0, key = { }, ignored_cidrs = { } },
                })
                balancer.rewrite()
                plugins.run()
        }

        # be careful with `access_by_lua_block` and `satisfy any` directives as satisfy any
        # will always succeed when there's `access_by_lua_block` that does not have any lua code doing `ngx.exit(ngx.declined)`
        # other authentication method such as basic auth or external auth useless - all requests will be allowed.
        #access_by_lua_block {
        #}

        header_filter_by_lua_block {
                lua_ingress.header()
                plugins.run()
        }

        body_filter_by_lua_block {
                plugins.run()
        }

        log_by_lua_block {
                balancer.log()

                monitor.call()

                plugins.run()
        }

        port_in_redirect off;

        set $balancer_ewma_score -1;
        set $proxy_upstream_name &quot;kt-service-8080&quot;;
        set $proxy_host          $proxy_upstream_name;
        set $pass_access_scheme  $scheme;

        set $pass_server_port    $server_port;

        set $best_http_host      $http_host;
        set $pass_port           $pass_server_port;

        set $proxy_alternative_upstream_name &quot;&quot;;

        # this location requires authentication
        auth_request        /_external-auth-l2fwaxm-exact;
        auth_request_set    $auth_cookie $upstream_http_set_cookie;
        add_header          set-cookie $auth_cookie;

        client_max_body_size                    1m;

        proxy_set_header host                   $best_http_host;

        # pass the extracted client certificate to the backend

        # allow websocket connections
        proxy_set_header                        upgrade           $http_upgrade;

        proxy_set_header                        connection        $connection_upgrade;

        proxy_set_header x-request-id           $req_id;
        proxy_set_header x-real-ip              $remote_addr;

        proxy_set_header x-forwarded-for        $remote_addr;

        proxy_set_header x-forwarded-host       $best_http_host;
        proxy_set_header x-forwarded-port       $pass_port;
        proxy_set_header x-forwarded-proto      $pass_access_scheme;

        proxy_set_header x-scheme               $pass_access_scheme;

        # pass the original x-forwarded-for
        proxy_set_header x-original-forwarded-for $http_x_forwarded_for;

        # mitigate httpoxy vulnerability
        # https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/
        proxy_set_header proxy                  &quot;&quot;;

        # custom headers to proxied server

        proxy_connect_timeout                   5s;
        proxy_send_timeout                      60s;
        proxy_read_timeout                      60s;

        proxy_buffering                         off;
        proxy_buffer_size                       4k;
        proxy_buffers                           4 4k;

        proxy_max_temp_file_size                1024m;

        proxy_request_buffering                 on;
        proxy_http_version                      1.1;

        proxy_cookie_domain                     off;
        proxy_cookie_path                       off;

        # in case of errors try the next upstream server before returning an error
        proxy_next_upstream                     error timeout;
        proxy_next_upstream_timeout             0;
        proxy_next_upstream_tries               3;

        proxy_pass http://upstream_balancer;

        proxy_redirect                          off;

}

location = /_external-auth-lw-prefix {
        internal;

        # ngx_auth_request module overrides variables in the parent request,
        # therefore we have to explicitly set this variable again so that when the parent request
        # resumes it has the correct value set for this variable so that lua can pick backend correctly
        set $proxy_upstream_name &quot;upstream-default-backend&quot;;

        proxy_pass_request_body     off;
        proxy_set_header            content-length          &quot;&quot;;
        proxy_set_header            x-forwarded-proto       &quot;&quot;;
        proxy_set_header            x-request-id            $req_id;

        proxy_method                post;
        proxy_set_header            x-original-uri          $request_uri;
        proxy_set_header            x-scheme                $pass_access_scheme;

        proxy_set_header            host                    www.gool.com;
        proxy_set_header            x-original-url          $scheme://$http_host$request_uri;
        proxy_set_header            x-original-method       $request_method;
        proxy_set_header            x-sent-from             &quot;nginx-ingress-controller&quot;;
        proxy_set_header            x-real-ip               $remote_addr;

        proxy_set_header            x-forwarded-for        $remote_addr;

        proxy_set_header            x-auth-request-redirect $request_uri;

        proxy_buffering                         off;

        proxy_buffer_size                       4k;
        proxy_buffers                           4 4k;
        proxy_request_buffering                 on;
        proxy_http_version                      1.1;

        proxy_ssl_server_name       on;
        proxy_pass_request_headers  on;

        client_max_body_size        1m;

        # pass the extracted client certificate to the auth provider

        set $target http://www.gool.com/authorization;
        proxy_pass $target;
}

location / {

        set $namespace      &quot;kt&quot;;
        set $ingress_name   &quot;nginx1&quot;;
        set $service_name   &quot;&quot;;
        set $service_port   &quot;&quot;;
        set $location_path  &quot;/&quot;;
        set $global_rate_limit_exceeding n;

        rewrite_by_lua_block {
                lua_ingress.rewrite({
                        force_ssl_redirect = false,
                        ssl_redirect = false,
                        force_no_ssl_redirect = false,
                        use_port_in_redirects = false,
                global_throttle = { namespace = &quot;&quot;, limit = 0, window_size = 0, key = { }, ignored_cidrs = { } },
                })
                balancer.rewrite()
                plugins.run()
        }

        # be careful with `access_by_lua_block` and `satisfy any` directives as satisfy any
        # will always succeed when there's `access_by_lua_block` that does not have any lua code doing `ngx.exit(ngx.declined)`
        # other authentication method such as basic auth or external auth useless - all requests will be allowed.
        #access_by_lua_block {
        #}

        header_filter_by_lua_block {
                lua_ingress.header()
                plugins.run()
        }

        body_filter_by_lua_block {
                plugins.run()
        }

        log_by_lua_block {
                balancer.log()

                monitor.call()

                plugins.run()
        }

        port_in_redirect off;

        set $balancer_ewma_score -1;
        set $proxy_upstream_name &quot;upstream-default-backend&quot;;
        set $proxy_host          $proxy_upstream_name;
        set $pass_access_scheme  $scheme;

        set $pass_server_port    $server_port;

        set $best_http_host      $http_host;
        set $pass_port           $pass_server_port;

        set $proxy_alternative_upstream_name &quot;&quot;;

        # this location requires authentication
        auth_request        /_external-auth-lw-prefix;
        auth_request_set    $auth_cookie $upstream_http_set_cookie;
        add_header          set-cookie $auth_cookie;

        client_max_body_size                    1m;

        proxy_set_header host                   $best_http_host;

        # pass the extracted client certificate to the backend

        # allow websocket connections
        proxy_set_header                        upgrade           $http_upgrade;

        proxy_set_header                        connection        $connection_upgrade;

        proxy_set_header x-request-id           $req_id;
        proxy_set_header x-real-ip              $remote_addr;

        proxy_set_header x-forwarded-for        $remote_addr;

        proxy_set_header x-forwarded-host       $best_http_host;
        proxy_set_header x-forwarded-port       $pass_port;
        proxy_set_header x-forwarded-proto      $pass_access_scheme;

        proxy_set_header x-scheme               $pass_access_scheme;

        # pass the original x-forwarded-for
        proxy_set_header x-original-forwarded-for $http_x_forwarded_for;

        # mitigate httpoxy vulnerability
        # https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/
        proxy_set_header proxy                  &quot;&quot;;

        # custom headers to proxied server

        proxy_connect_timeout                   5s;
        proxy_send_timeout                      60s;
        proxy_read_timeout                      60s;

        proxy_buffering                         off;
        proxy_buffer_size                       4k;
        proxy_buffers                           4 4k;

        proxy_max_temp_file_size                1024m;

        proxy_request_buffering                 on;
        proxy_http_version                      1.1;

        proxy_cookie_domain                     off;
        proxy_cookie_path                       off;

        # in case of errors try the next upstream server before returning an error
        proxy_next_upstream                     error timeout;
        proxy_next_upstream_timeout             0;
        proxy_next_upstream_tries               3;

        proxy_pass http://upstream_balancer;

        proxy_redirect                          off;

}

}
## end server example.com

"
66818476,is it possible to use same hostname with multiple ingress resources running in different namespaces?,"i want to use the same hostname let's say example.com with multiple ingress resources running in different namespaces i.e monitoring and myapp. i'm using kubernetes nginx-ingress controller.
haproxy-ingress.yaml
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: haproxy-ingress
  namespace: myapp
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;  
spec:
  tls:
  - hosts:
    # fill in host here
    - example.com
    
  rules:
    - host: example.com
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: haproxy
                port:
                  number: 80


grafana-ingress.yaml
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: grafana-ingress
  namespace: monitoring
  annotations:
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
spec:
  tls:
  - hosts:
    - example.com
    
  rules:
    - host: example.com
      http:
        paths:
          # only match /grafana and paths under /grafana/
          - path: /grafana(/|$)(.*)
            pathtype: prefix
            backend:
              service:
                name: grafana
                port:
                  number: 3000


when i'm doing curl example.com then it is redirecting me to the deployment running in namespace one(as expected) but when i'm doing curl example.com/grafana then still it is redirecting me to namespace one deployment.
please help.
",<kubernetes><kubernetes-ingress>,66819715,4,"yes it is possible.
there can be two issues in your case.
one is you don't need the regex path for grafana ingress. simple /grafana path will be fine with path type prefix as with path type prefix any /grafana/... will be redirected associated service. so the manifest file will be:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: grafana-ingress
  namespace: monitoring
spec:
  tls:
  - hosts:
    - example.com
    
  rules:
    - host: example.com
      http:
        paths:
          - path: /grafana
            pathtype: prefix
            backend:
              service:
                name: grafana
                port:
                  number: 3000


and the second issue can be the related service or deployment might not be the under same namespace monitoring. please make sure the deployment/service/secret or other resources needed for grafana remains under the same namespace monitoring.
"
67547494,forward from ingress nginx controller to different nginx pods according to port numbers,"in my k8s system i have a nginx ingress controller as loadbalancer and accessing it to ddns adress like hedehodo.ddns.net and this triggering to forward web traffic to another nginx port.
now i deployed another nginx which works on node.js app but i cannot forward nginx ingress controller for any request to port 3000 to go another nginx
here is the nginx ingress controller yaml
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: test-ingress
  namespace: default
spec:
  rules:
  - host: hedehodo.ddns.net
    http:
      paths:
      - path: /
        backend:
          servicename: my-nginx
          serviceport: 80
      - path: /
        backend:
          servicename: helloapp-deployment
          serviceport: 3000

helloapp deployment works a loadbalancer and i can access it from ip:3000
could any body help me?
",<nginx><kubernetes><kubernetes-ingress><nginx-ingress>,67551614,4,"each host cannot share multiple duplicate paths, so in your example, the request to host: hedehodo.ddns.net will always map to the first service listed: my-nginx:80.
to use another service, you have to specify a different path. that path can use any service that you want. your ingress should always point to a service, and that service can point to a deployment.
you should also use https by default for your ingress.
ingress example:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: test-ingress
spec:
  rules:
    - host: my.example.net
      http:
        paths:
          - path: /
            backend:
              servicename: my-nginx
              serviceport: 80
          - path: /hello
            backend:
              servicename: helloapp-svc
              serviceport: 3000

service example:
---
apiversion: v1
kind: service
metadata:
  name: helloapp-svc
spec:
  ports:
    - port: 3000
      name: app
      protocol: tcp
      targetport: 3000
  selector:
    app: helloapp
  type: nodeport

deployment example:
---
apiversion: apps/v1
kind: deployment
metadata:
  name: helloapp
  labels:
    app: helloapp
spec:
  replicas: 1
  selector:
    matchlabels:
      app: helloapp
  template:
    metadata:
      labels:
        app: helloapp
    spec:
      containers:
        - name: node
          image: my-node-img:v1
          ports:
            - name: web
              containerport: 3000

"
67430138,nginx ingress is not reaching service(clusterip),"i am trying to deploy a node js app inside a kubernetes cluster. i have created deplyment and service(clusterip) for that and added a role inside nginx ingress config file but seems nginx is failing to reach that service.
nodejs app:
'use strict';

const express = require('express');

// constants
const port = 5678;
const host = '0.0.0.0';

// app
const app = express();
app.get('/', (req, res) =&gt; {
  console.log(&quot;request received&quot;);
  res.send('hello world');
});

app.listen(port, host);
console.log(`running on http://${host}:${port}`);

dockerfile:
from node:14

# create app directory
workdir /usr/src/app

# install app dependencies
# a wildcard is used to ensure both package.json and package-lock.json are copied
# where available (npm@5+)
copy package*.json ./

run npm install
# if you are building your code for production
# run npm ci --only=production

# bundle app source
copy . .

expose 5678
cmd [ &quot;node&quot;, &quot;server.js&quot; ]

deployment:
apiversion: apps/v1
kind: deployment
metadata:
  name: apple-app-deployment
  labels:
    app: apple
spec:
  replicas: 2
  selector:
    matchlabels:
      app: apple
  template:
    metadata:
      labels:
        app: apple
    spec:
      containers:
      - name: apple-app
        image: anudcker/node-web-appp:1
        ports:
        - containerport: 5678

service:
kind: service
apiversion: v1
metadata:
  name: apple-service
spec:
  selector:
    app: apple
  ports:
    - port: 5678 # default port for image
      targetport: 5678

ingress:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: example-ingress
  annotations:
    # kubernetes.io/ingress.class: azure/application-gateway
    ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
        - path: /apple
          pathtype: prefix
          backend:
            service:
                name: apple-service
                port:
                    number: 5678

now whenever i am trying to reach the service by hitting /apple i am getting a response
cannot get /apple from nginx ingress. i found bellow log inside nginx ingress controller pod.
10.244.1.1 - - [07/may/2021:06:22:29 +0000] &quot;get /apple http/1.1&quot; 404 144 &quot;-&quot; &quot;mozilla/5.0 (macintosh; intel mac os x 10_15_2) applewebkit/537.36 (khtml, like gecko) chrome/79.0.3945.117 safari/537.36&quot; 465 0.004 [demo-apple-service-5678] [] 10.244.0.24:5678 144 0.004 404 5a5bdf8f61d59b665218be0a18a6ab4c

any idea?
",<kubernetes><kubernetes-ingress><nginx-ingress><kubernetes-networking>,67431207,4,"the issue is that you are using rewrite-target of networking.k8s.io/v1beta1, with the apiversion: networking.k8s.io/v1.
you need to update your annotation to the new one:
nginx.ingress.kubernetes.io/rewrite-target: /
"
67431229,"using the rollout restart command in cronjob, in gke","i want to periodically restart the deployment using k8s cronjob.
please check what is the problem with the yaml file.
when i execute the command from the local command line, the deployment restarts normally, but it seems that the restart is not possible with cronjob.
e.g $ kubectl rollout restart deployment my-ingress -n my-app
my cronjob yaml file
apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: deployment-restart
  namespace: my-app
spec:
  schedule: '0 8 */60 * *' 
  jobtemplate:
    spec:
      backofflimit: 2 
      activedeadlineseconds: 600 
      template:
        spec:
          serviceaccountname: deployment-restart 
          restartpolicy: never
          containers:
            - name: kubectl
              image: bitnami/kubectl:latest 
              command:
                - 'kubectl'
                - 'rollout'
                - 'restart'
                - 'deployment/my-ingress -n my-app'

",<kubernetes><kubernetes-cronjob>,67433179,4,"as david suggested run cron of kubectl is like by executing the command
apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: hello
spec:
  schedule: &quot;*/5 * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          serviceaccountname: sa-jp-runner
          containers:
          - name: hello
            image: bitnami/kubectl:latest
            command:
            - /bin/sh
            - -c
            - kubectl rollout restart deployment my-ingress -n my-app
          restartpolicy: onfailure

i would also suggest you to check the role and service account permissions
example for ref :
kind: role
apiversion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: kubectl-cron
rules:
- apigroups:
  - extensions
  - apps
  resources:
  - deployments
  verbs:
  - 'patch'

---
kind: rolebinding
apiversion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: kubectl-cron
  namespace: default
subjects:
- kind: serviceaccount
  name: sa-kubectl-cron
  namespace: default
roleref:
  kind: role
  name: kubectl-cron
  apigroup: &quot;&quot;

---
apiversion: v1
kind: serviceaccount
metadata:
  name: sa-kubectl-cron
  namespace: default

---

"
67446985,helm - how to write a file in a volume using configmap?,"i have defined the values.yaml like the following:
name: custom-streams
image: streams-docker-images
imagepullpolicy: always
restartpolicy: always
replicas: 1
port: 8080
nodeselector:
  nodetype: free
confighocon: |-
  streams {
          monitoring {
            custom {
              uri = ${?uri}
              method = ${?method}
            }
          }
  }

and configmap.yaml like the following:
apiversion: v1
kind: configmap
metadata:
  name: custom-streams-configmap
data:
  config.hocon: {{ .values.confighocon | indent 4}}

lastly, i have defined the deployment.yaml like the following:
apiversion: apps/v1
kind: deployment
metadata:
  name: {{ .values.name }}
spec:
  replicas: {{ default 1 .values.replicas }}
  strategy: {}
  template:
    spec:
      containers:
      - env:
        {{- range $key, $value := .values.env }}
        - name: {{ $key }}
          value: {{ $value | quote }}
        {{- end }}
        image: {{ .values.image }}
        name: {{ .values.name }}
        volumemounts:
        - name: config-hocon
          mountpath: /config
        ports:
        - containerport: {{ .values.port }}
      restartpolicy: {{ .values.restartpolicy }}
      volumes:
      - name: config-hocon
        configmap:
          name: custom-streams-configmap
          items:
          - key: config.hocon
            path: config.hocon
status: {}

when i run the container via:
helm install --name custom-streams custom-streams -f values.yaml --debug --namespace streaming

then the pods are running fine, but i cannot see the config.hocon file in the container:
$ kubectl exec -it custom-streams-55b45b7756-fb292 sh -n streaming
/ # ls
...
config
...
/ # cd config/
/config # ls
/config #

i need the config.hocon written in the /config folder. can anyone let me know what is wrong with the configurations?
",<kubernetes><kubernetes-helm><configmap>,67448152,4,"i was able to resolve the issue. the issue was using configmap in place configmap in deployment.yaml:
apiversion: apps/v1
kind: deployment
metadata:
  name: {{ .values.name }}
spec:
  replicas: {{ default 1 .values.replicas }}
  strategy: {}
  template:
    spec:
      containers:
      - env:
        {{- range $key, $value := .values.env }}
        - name: {{ $key }}
          value: {{ $value | quote }}
        {{- end }}
        image: {{ .values.image }}
        name: {{ .values.name }}
        volumemounts:
        - name: config-hocon
          mountpath: /config
        ports:
        - containerport: {{ .values.port }}
      restartpolicy: {{ .values.restartpolicy }}
      volumes:
      - name: config-hocon
        configmap:
          name: custom-streams-configmap
          items:
          - key: config.hocon
            path: config.hocon
status: {}

"
67086365,install istio using istio operator and terraform on eks,"i'm new to terraform. i need to set up istio on the aws eks cluster. i thought of using istio-operator along with terraform to do the same.
below is the shell script to install istio on eks using istio-operator:
install-istio.sh
# download and install the istio istioctl client binary

# specify the istio version that will be leveraged throughout these instructions
istio_version=1.7.3

curl -sl &quot;https://github.com/istio/istio/releases/download/$istio_version/istioctl-$istio_version-linux-amd64.tar.gz&quot; | tar xz

sudo mv ./istioctl /usr/local/bin/istioctl
sudo chmod +x /usr/local/bin/istioctl

# install the istio operator on eks
istioctl operator init

# the istio operator is installed into the istio-operator namespace. query the namespace.
kubectl get all -n istio-operator

# install istio components
istioctl profile dump default

# create the istio-system namespace and deploy the istio operator spec to that namespace.
kubectl create ns istio-system
kubectl apply -f istio-operator.yaml

# validate the istio installation
kubectl get all -n istio-system


below is the istio-operator.yaml file used by install-istio.sh
apiversion: install.istio.io/v1alpha1
kind: istiooperator
metadata:
  namespace: istio-system
  name: istio-control-plane
spec:
  # use the default profile as the base
  # more details at: https://istio.io/docs/setup/additional-setup/config-profiles/
  profile: default
  # enable the addons that we will want to use
  addoncomponents:
    grafana:
      enabled: true
    prometheus:
      enabled: true
    tracing:
      enabled: true
    kiali:
      enabled: true
  values:
    global:
      # ensure that the istio pods are only scheduled to run on linux nodes
      defaultnodeselector:
        beta.kubernetes.io/os: linux
    kiali:
      dashboard:
        auth:
          strategy: anonymous

below is the main.tf file which executes the script
resource &quot;null_resource&quot; &quot;install_istio&quot; {

 provisioner &quot;local-exec&quot; {

    command = &quot;/bin/bash install-istio.sh&quot;
  }
}


i request you to help me with few queries:

how can i make use of the above script along with terraform to install istio on eks cluster. what is the terraform part i need to include along with the above script?
is there any missing part in the script. will i face any problem updating the istio using the above script?
what are the other parameters i need to include so that the script can install istio on the eks cluster?
how can i create terraform module using the above script?

thank you very much for your time. appreciate all your help!
",<kubernetes><terraform><istio><amazon-eks><servicemesh>,67109374,4,"i believe you will encounter problems if using a local-exec provisioner like this.
terraform does not play nice with resources it cannot reconcile. especially when it comes to crds. also, every time you will run terraform apply, you will run istioctl init over and over, which is probably not what you want.
what you can do, is to

convert the istio-operator to standard kubernetes manifests using

mkdir -p istio-operator
istio-operator dump &gt; istio-operator/manifests.yaml


create a istio-operator/kustomization.yaml file with

#istio-operator/kustomization.yaml

resources:
- manifests.yaml


install the terraform kustomization provider

# terraform.tf

terraform {
  required_providers {
    kustomization = {
      source  = &quot;kbst/kustomization&quot;
      version = &quot;0.4.3&quot;
    }
  }
}

provider &quot;kustomization&quot; {
  // see online documentation on how to configure this
}


install istio-operator with the terraform kustomization provider

# istio-operator.tf

data &quot;kustomization&quot; &quot;istio_operator&quot; {
  path     = &quot;./istio-operator&quot;
}

resource &quot;kustomization_resource&quot; &quot;istio_operator&quot; {
  for_each = data.kustomization.istio_operator.ids
  manifest = data.kustomization.istio_operator.manifests[each.value]
}




create a istiooperator manifest in istio/manifest.yaml

# istio/manifest.yaml

apiversion: install.istio.io/v1alpha1
kind: istiooperator
metadata:
  name: istio-control-plane
...


create a istio/kustomization.yaml with

# istio/kustomization.yaml

resources:
- manifest.yaml


install the istiooperator with a second kustomization resource using terraform.

# istio.tf

data &quot;kustomization&quot; &quot;istio&quot; {
  path     = &quot;./istio&quot;
}

resource &quot;kustomization_resource&quot; &quot;istio&quot; {
  for_each = data.kustomization.istio.ids
  manifest = data.kustomization.istio.manifests[each.value]
  depends_on = [kustomization_resource.istio_operator]
}



i would recommend putting this whole thing in a separate folder, such as this
/home
  /project
    /terraform
      /istio
        terraform.tf
        istio_operator.tf
        istio.tf
        /istio
          kustomization.yaml
          manifest.yaml
        /istio-operator
          kustomization.yaml
          manifest.yaml

      

"
66902641,how to provide elastic ip to aws eks for external service with type loadbalancer?,"i am using eks 1.16. i have one service which has type loadbalancer with internet-facing.
apiversion: v1
kind: service
metadata:
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
  name: kafka-test-3-0-external
  labels:
    helm.sh/chart: kafka-0.21.5
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/component: kafka-broker
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: kafka-test-3
    pod: &quot;kafka-test-3-0&quot;
spec:
  type: loadbalancer
  ports:
    - name: external-broker
      port: 19092
      targetport: 19092
      protocol: tcp
#
  selector:
    app.kubernetes.io/component: kafka-broker
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: kafka-test-3
    statefulset.kubernetes.io/pod-name: &quot;kafka-test-3-0&quot;

i want to provide elastic-ip/static-ip to that external service.
problem:
with loadbalancer type, when i delete the service it will delete loadbalancer.
when i install once again then it will create new load balancer with new ip.
usecase: if i will uninstall/delete that service still the ip must be same.
how can i provide elastic-ip/static-ip?
",<kubernetes><amazon-eks><amazon-elb><elastic-ip>,66902642,4,"there is one annotation to provide elastic-ip for network-load-balancer
service.beta.kubernetes.io/aws-load-balancer-eip-allocations: eipalloc-05666791973f6a240
if you're using amazon eks 1.16 or later, you can assign elastic ip addresses to the network load balancer by adding the following annotation. replace the  (including &lt;&gt;) with the allocation ids of your elastic ip addresses. the number of allocation ids must match the number of subnets used for the load balancer.
note: you can not use this annotation for the internal load balancer.
apiversion: v1
kind: service
metadata:
  annotations:
    external-dns.alpha.kubernetes.io/hostname: &quot;kafka-test-3-1.kafka.internal&quot;
    external-dns.alpha.kubernetes.io/ttl: &quot;60&quot;
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    service.beta.kubernetes.io/aws-load-balancer-eip-allocations: eipalloc-022b9722973f6a222
  name: kafka-test-3-1-external
  labels:
    helm.sh/chart: kafka-0.21.5
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/component: kafka-broker
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: kafka-test-3
    pod: &quot;kafka-test-3-1&quot;
spec:
  type: loadbalancer
  ports:
    - name: external-broker
      port: 19092
      targetport: 19092
      protocol: tcp
#
  selector:
    app.kubernetes.io/component: kafka-broker
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: kafka-test-3
    statefulset.kubernetes.io/pod-name: &quot;kafka-test-3-1&quot;

it will always use elasticip for that loadbalancer.
for more eks annotations.
https://docs.aws.amazon.com/eks/latest/userguide/load-balancing.html
"
68555184,kubernetes - ingress network policy not working as expected,"i have 3 kubernetes deployments and services for each of the deployments (namespace = firstspace).
each deployment is labelled as app1, app2, app3 in order.
as an example, if i run the following command. i will get the first pod as the result.
kubectl get pods -l app=app1 --namespace firstspace

my goal is to restrict the ingress access of the third pod (app=app3) using the following network policy allowing traffic only from the second pod (app=app2) and any pods from another namespace called &quot;secondspace&quot;.
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: ingress-app3
  namespace: firstspace
spec:
  podselector: 
    matchlabels:
      app: app3
  ingress:
  - from:
    - namespaceselector:
        matchlabels:
          name: secondspace
    - podselector:
        matchexpressions:
          - {key: app, operator: in, values: [app2]}
  policytypes:
  - ingress

however, when i deploy the network policy to the &quot;firstspace&quot; namespace, i can still curl (and get a sample json response) the service of the third pod (app=app3) using the first pod (app=app1).
following is a sample command. here, 10.100.150.0 is the clusterip of the service created for the third pod.
kubectl exec app1-849b94c6df-rzdls --namespace firstspace-- curl -sl 10.100.150.0:8080/testendpoint

can someone help me understand what i'm doing wrong here?
",<kubernetes><kubernetes-ingress><kubernetes-networkpolicy>,68566162,4,"after some trial and error, i observed the following.
according to the kubernetes network policies documentation, deployed network policies will be only effective if a network plugin is installed in the kubernetes cluster.
since my local minikube cluster did not have a network plugin the network policy i have mentioned in the question description was not effective.
after, installing the cillium network plugin in my minikube cluster, the network policy worked as expected.
notes:

cillium network plugin installation was not successful on minikube when using docker as the driver. but it worked when selected hyperv as the driver.
i had to create an egress policy as well for the pod with app=app2 label to allow egress traffic from the pod with app=app3 label (see the example below).


apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: egress-app2
  namespace: firstspace
spec:
  podselector: 
    matchlabels:
      app: app2
  egress:
  - to:
    - podselector:
        matchlabels:
          app: app3
  policytypes:
  - egress

"
68503386,"kubernetes - mountvolume.newmounter initialization failed for volume ""<volume-name>"" : path does not exist","i am trying to setup a local persistent volume using local storage using wsl. but the pod status stops at pending.
the kubectl describe pod &lt;pod-name&gt; gives below error.
warning  failedmount       21s (x7 over 53s)    kubelet            mountvolume.newmounter initialization failed for volume &quot;pv1&quot; : path &quot;/mnt/data&quot; does not exist
the path /mnt/data has been created and exists on the local machine but cannot be accessed by the container.
and the pod and persistent volume configuration as below.
apiversion : v1
kind : persistentvolume
metadata :
   name : pv1
spec :
  capacity :
    storage : 2gi
  accessmodes :
    - readwriteonce
  persistentvolumereclaimpolicy : retain
  storageclassname : local-storage
  local : 
    fstype : ext4
    path : /mnt/data
  nodeaffinity:
    required:
      nodeselectorterms:
      - matchexpressions:
        - key: kubernetes.io/hostname
          operator: in
          values:
          - node1

---

apiversion : v1
kind : persistentvolumeclaim
metadata : 
  name : pvc1
spec :
  resources :
    requests :
      storage : 1gi
  accessmodes :
    - readwriteonce
  storageclassname : local-storage

---

apiversion : v1
kind : pod
metadata :
  name : pod1
spec :
  containers:
  - name: www
    image: nginx:alpine
    ports:
      - containerport: 80
        name: www
    volumemounts:
      - name: www-store
        mountpath: /usr/share/nginx/html
  volumes :
    - name : www-store
      persistentvolumeclaim :
        claimname : pvc1

any help would be appreciated.
",<kubernetes><kubernetes-pod><persistent-volumes><persistent-volume-claims>,68503572,4,"you are using nodeselector for the pv, telling it to use node1 for the volume , chances are 1. node1 does not have /mnt/data directory present, which is    hostpath for the volume.
or 2. node1 may be having /mnt/data, but the pod is getting scheduled on some other node which does not have /mnt/data    directory:
apiversion : v1
kind : persistentvolume
metadata :
   name : pv1
spec :
  capacity :
    storage : 2gi
  accessmodes :
    - readwriteonce
  persistentvolumereclaimpolicy : retain
  storageclassname : local-storage
  local : 
    fstype : ext4
    path : /mnt/data
  nodeaffinity:
    required:
      nodeselectorterms:
      - matchexpressions:
        - key: kubernetes.io/hostname
          operator: in
          values:
          - node1

solution:
make sure /mnt/data is present on all schedulable nodes
or
try modifying your file to add, nodename or nodeselector or nodeaffinity to force it to get scheduled on the same node  which have proper hostpath. in below example, it is assumed that node1 have /mnt/data directory present.
apiversion : v1
kind : pod
metadata :
  name : pod1
spec :
  nodename: node1 #&lt;------------this
  containers:
  - name: www
    image: nginx:alpine
    ports:
      - containerport: 80
        name: www
    volumemounts:
      - name: www-store
        mountpath: /usr/share/nginx/html
  volumes :
    - name : www-store
      persistentvolumeclaim :
        claimname : pvc1

"
68297446,introducing delay for running helm test suite over installed release,"i'm using helm charts to deploy several rest services with into k8s with spring boot inside deployed container.
however, to be able to do final stage testing i need to introduce some sort of smart liveness probe - i.e. that the target application is actually running properly inside given container.
this can be easily justified by successful return code of simple curl command, however, here's the trick - command needs to be executed with some delay after particular release deployment to give application time to bootstrap.
here's what i've figured for a test suite:
apiversion: v1
kind: pod
metadata:
  name: &quot;{{ include &quot;chart.fullname&quot; . }}-test&quot;
  labels:
  {{- include &quot;chart.fullname&quot; . | nindent 4 }}
  annotations:
    &quot;helm.sh/hook&quot;: test-success
spec:
  containers:
    - name: test-curl
      image: curl
      command: ['curl']
      args: [' -x post -i -h &quot;accept: application/json&quot; -h &quot;content-type:application/json&quot; -d ''{&quot;foo&quot;:[&quot;bar&quot;]}'' {{ include &quot;chart.fullname&quot; . }}:{{ .values.service.port }}']
  restartpolicy: never

the key problem is that this test will be executed when service is not really started yet and thus fail anyway.
is there some mechanism or workaround to introduce delay of execution for this test?
setting some sleep step in separate test container comes to mind but i'm not sure if this will work properly for this case
",<kubernetes><kubernetes-helm>,68397273,4,"thanks to @emruz hossain, i've figured the solution out:
apiversion: batch/v1
kind: job
metadata:
  name: &quot;{{ .release.name }}-test&quot;
  labels:
    app: {{ .release.name }}
    release: {{ .release.name }}
  annotations:
    &quot;helm.sh/hook&quot;: test-success
spec:
  ttlsecondsafterfinished: 0
  template:
    spec:
      containers:
        - name: test-curl
          image: target-image:1.2.3
          imagepullpolicy: &quot;ifnotpresent&quot;
          command:
            - /bin/bash
            - -ec
            - |
              curl --connect-timeout 5 --max-time 10 --retry 5 --retry-delay 5 --retry-max-time 30 --retry-all-errors http://{{ .release.name }}:{{ .values.service.port }}/v1/rest -x post -h &quot;content-type: application/json&quot; -d &quot;{\&quot;foo\&quot;:[\&quot;bar\&quot;]}&quot;
      restartpolicy: never

requires k8s api server 1.20+ (due to this) and curl 7.71+ (due to this)
"
68821933,ingress for eck kibana,"i have installed the eck crds and operator following the elasticsearch documentation, the cluster is up and runs fine, i can also port forward the kibana service and i am able to use the ui with this command:
kubectl port-forward service/kibana-kb-http 5601

however, if i write an ingress i just cannot access it, at the very best i can get a warning about the certificate and at the worst nothing at all.
i am also using traefik, and my kibana is:
apiversion: kibana.k8s.elastic.co/v1
kind: kibana
metadata:
  name: kibana
  labels:
    various...
spec:
  version: 7
  count: 1
  elasticsearchref:
    name: &quot;elasticsearch&quot;
  podtemplate:
    spec:
      http:
        tls:
          selfsignedcertificate:
            disabled: false
      containers:
        - name: kibana
          resources:
            limits:
              memory: 2gi
              cpu: 2

one of my attempts with the ingress:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: k-ingress
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
  rules:
    - host: kibana.host.com
      http:
        paths:
          - backend:
              servicename: kibana-kb-http
              serviceport: 5601
            path: /

i have found other questions about it, but they are either too old or not replied. any suggestion?
",<kubernetes><kibana><kubernetes-ingress>,68834572,4,"i went back to the kibana documentation and i found that that it was just necessary to modify the kibana yaml file to:
apiversion: kibana.k8s.elastic.co/v1
kind: kibana
metadata:
  name: kibana
  labels:
    various...
spec:
  version: 7
  http:
    service:
      spec:
        type: loadbalancer
    tls:
      selfsignedcertificate:
        disabled: true
  count: 1
  elasticsearchref:
    name: &quot;elasticsearch&quot;
  podtemplate:
    containers:
      - name: kibana
        resources:
          limits:
            memory: 2gi
            cpu: 2


notice that the tls part has been moved to the spec section and not in the podtemplate and that loadbalancer has been added. now, i can connect to kibana using http://my-cluster:5601
"
77870543,unable to mount gcp filestore pvc to job pods,"i have a kubernetes job (with parallelism: 50) running on gke autopilot cluster, that needs more storage than maximum ephemeral storage provisioned by autopilot cluster per node (i.e. 10gi). as i need readwritemany access for pods on the storage, i decided on gcp filestore (though it would've been nice if minimum instance size for filestore was less than 1 tib) for creating pvc that can be mounted on job pods, but job pods are stuck in containercreating state and looking at the event logs, mountvolume.mountdevice failure seems to be the reason:
 warning  failedscheduling  11m                   gke.io/optimize-utilization-scheduler  0/12 nodes are available: 11 insufficient memory, 12 insufficient cpu. preemption: 0/12 nodes are available: 12 no preemption victims found for incoming pod..
  normal   triggeredscaleup  11m                   cluster-autoscaler                     pod triggered scale-up
  normal   scheduled         6m39s                 gke.io/optimize-utilization-scheduler  successfully assigned default/mypod-7l5k9 to gk3-mycluster-3-e79620bd-jvsg
  warning  failedmount       4m8s (x6 over 4m39s)  kubelet                                mountvolume.mountdevice failed for volume &quot;pvc-435bf565-25f0-43f7-86d4-b3ecadce43a3&quot; : rpc error: code = aborted desc = an operation with the given volume key modeinstance/asia-northeast1-b/pvc-435bf565-25f0-43f7-86d4-b3ecadce43a3/vol1 already exists.
 --- most likely a long process is still running to completion. retrying.
  warning  failedmount  2m19s                kubelet  unable to attach or mount volumes: unmounted volumes=[my-mounted-storage], unattached volumes=[kube-api-access-4gs6h shared-storage]: timed out waiting for the condition
  warning  failedmount  96s (x2 over 4m39s)  kubelet  mountvolume.mountdevice failed for volume &quot;pvc-435bf565-25f0-43f7-86d4-b3ecadce43a3&quot; : rpc error: code = deadlineexceeded desc = context deadline exceeded
  warning  failedmount  5s (x2 over 4m36s)   kubelet  unable to attach or mount volumes: unmounted volumes=[my-mounted-storage], unattached volumes=[my-mounted-storage kube-api-access-4gs6h]: timed out waiting for the condition

here's my pvc and job manifest:
kind: persistentvolumeclaim
apiversion: v1
metadata:
  name: podpvc
spec:
  accessmodes:
    - readwritemany
  storageclassname: standard-rwx
  resources:
    requests:
      storage: 1ti

apiversion: batch/v1
kind: job
metadata:
  name: mypod
  labels:
    app.kubernetes.io/name: mypod
spec:
  parallelism: 50
  template:
    metadata:
      name: mypod
    spec:
      serviceaccountname: workload-identity-sa
      volumes:
      - name: my-mounted-storage
        persistentvolumeclaim:
          claimname: podpvc
      containers:
      - name: mypod-container
        image: mypod-image:staging-0.1
        imagepullpolicy: always
        env:
        - name: env
          value: &quot;stg&quot;
        resources:
          requests:
            cpu: &quot;4&quot;
            memory: &quot;16gi&quot;
        volumemounts:
        - name: my-mounted-storage
          mountpath: /mnt/data
      restartpolicy: onfailure


both pv and pvc seems to be healthy and bound, and there doesn't seem to be any existing volume attachments on the nodes (kubectl describe nodes | grep attach). i've also tried deleting both the pvc and job, and recreating them but the issue persists.

",<kubernetes><google-cloud-platform><google-kubernetes-engine><kubernetes-pvc><google-cloud-filestore>,77871329,3,"below checkpoints can help you to resolve your issue:
1. checking if the filestore is in default network:
check if the gke cluster and filestore are created under a non-default network, and use the gke supported storageclasses: standard-rwx, enterprise-rwx, premium-rwx, which you can find in the networking section of cluster. this would cause the filestore instance to provision in a default network. this results in the mount failing as filestore (default network) cannot be mounted on the nodes (non-default network).
to resolve this issue, you need to specify the network parameter for the filestore mount to match the network of the gke cluster by adding the storageclass.parameters.network field as follows:
apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  name: filestore-example
provisioner: filestore.csi.storage.gke.io
volumebindingmode: immediate
allowvolumeexpansion: true
parameters:
  tier: standard
  network: default

2. check the ip addresses:
check if the ip address of the filestore and the ip address present in the pvc are different. the pvc should contain the ip address of the filestore and the name of the filestore. if they are different, try editing the yaml file and setting the correct ip address in the pvc.
for more information follow this document.
"
75490731,warning: rejected - all hosts are taken by other resources,"i'm trying to setup nginx-ingress controller to manage two paths on the same hostname in bare metal based cluster.
in the app1 namespace i have below nginx resource:-
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: app1-ingress
  namespace: app1
spec:
  ingressclassname: nginx
  rules:
  - host: web.example.com
    http:
      paths:
      - path: /app1
        pathtype: prefix
        backend:
          service:
            name: app1-service
            port:
              number: 80

and in the app2 namespace i have below nginx resource:-
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: app2-ingress
  namespace: app2
spec:
  ingressclassname: nginx
  rules:
  - host: web.example.com
    http:
      paths:
      - path: /app2
        pathtype: prefix
        backend:
          service:
            name: app2-service
            port:
              number: 80

my app1-service applied first and it is running fine, now when i applied the second app2-service it shows below warning and not able to access it on browser.
annotations:       &lt;none&gt;
events:
  type     reason    age   from                      message
  ----     ------    ----  ----                      -------
  warning  rejected  54s   nginx-ingress-controller  all hosts are taken by other resources
  warning  rejected  54s   nginx-ingress-controller  all hosts are taken by other resources
  warning  rejected  54s   nginx-ingress-controller  all hosts are taken by other resources

how do i configure my nginx ingress resource to connect multiple service paths on the same hostname?
",<kubernetes><nginx><kubernetes-ingress><nginx-ingress>,75503816,3,"default nginx ingress controller doesn't support having different ingress resources with the same hostname. you can have one ingress resource that contains multiple paths, but in this case all apps should live in one namespace. like this:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: app1-ingress
  namespace: app1
spec:
  ingressclassname: nginx
  rules:
  - host: web.example.com
    http:
      paths:
      - path: /app1
        pathtype: prefix
        backend:
          service:
            name: app1-service
            port:
              number: 80
      - path: /app2
        pathtype: prefix
        backend:
          service:
            name: app2-service
            port:
              number: 80

splitting ingresses between namespaces is currently not supported by standard nginx ingress controller.
you may however take a look at an alternative implementation of nginx ingress by nginx inc. they have support for mergeable ingresses.
"
58744935,nginx ingress controller 0.26.1 returns 504 (timeout while connecting to upstream) on gke v1.14,"i recently upgraded my gke cluster to 1.14.x and nginx ingress to the latest version 0.26.1. at some point my ingresses stopped working.

for instance, when trying to access nexus with curl ingress_ip -h ""host:nexus.myorg.com"", these are the ingress controller logs:

2019/11/07 08:35:49 [error] 350#350: *2664 upstream timed out (110: connection timed out) while connecting to upstream, client: 82.81.2.76, server: nexus.myorg.com, request: ""get / http/1.1"", upstream: ""http://10.8.25.3:8081/"", host: ""nexus.myorg.com""
2019/11/07 08:35:54 [error] 350#350: *2664 upstream timed out (110: connection timed out) while connecting to upstream, client: 82.81.2.76, server: nexus.myorg.com, request: ""get / http/1.1"", upstream: ""http://10.8.25.3:8081/"", host: ""nexus.myorg.com""
2019/11/07 08:35:59 [error] 350#350: *2664 upstream timed out (110: connection timed out) while connecting to upstream, client: 82.81.2.76, server: nexus.myorg.com, request: ""get / http/1.1"", upstream: ""http://10.8.25.3:8081/"", host: ""nexus.myorg.com""
82.81.2.76 - - [07/nov/2019:08:35:59 +0000] ""get / http/1.1"" 504 173 ""-"" ""curl/7.64.1"" 79 15.003 [some-namespace-nexus-service-8081] [] 10.8.25.3:8081, 10.8.25.3:8081, 10.8.25.3:8081 0, 0, 0 5.001, 5.001, 5.001 504, 504, 504 a03f13a3bfc943e44f2df3d82a6ecaa4


as you can see it tries to connect three times to 10.8.25.3:8081 which is the pod ip, timing out in all of them.

i've sh'ed into a pod and accessed the pod using that same ip with no problem: curl 10.8.25.3:8081. so the service is set up correctly.

this is my ingress config:

apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: my-ingress
  namespace: some-namespace
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/add-base-url: ""true""
    nginx.ingress.kubernetes.io/proxy-body-size: 30m
spec:
  rules:
  - host: nexus.myorg.com
    http:
      paths:
      - backend:
          servicename: nexus-service
          serviceport: 8081


any idea how to troubleshoot of fix this?
",<kubernetes><google-kubernetes-engine><kubernetes-ingress><nginx-ingress>,58745800,3,"the problem had to do with network policies. we have some policies to forbid the access to pods from other namespaces and allow only from the ingress namespace

  apiversion: extensions/v1beta1
  kind: networkpolicy
  metadata:
    name: allow-from-ingress-namespace
    namespace: some-namespace
  spec:
    ingress:
    - from:
      - namespaceselector:
          matchlabels:
            type: ingress
    podselector: {}
    policytypes:
    - ingress

  apiversion: extensions/v1beta1
  kind: networkpolicy
  metadata:
    name: deny-from-other-namespaces
    namespace: some-namespace
  spec:
    ingress:
    - from:
      - podselector: {}
    podselector: {}
    policytypes:
    - ingress


with the upgrade we lost the label that is matched in the policy (type=ingress). simply adding it fixed the problem: kubectl label namespaces ingress-nginx type=ingress
"
74803158,rewrite target issue with nginx ingress kubernetes,"i am having issue to load a website from a docker nginx container in a aks cluster. i've installed an nginx ingress controller, and deployed the following:
deployment + service
apiversion: apps/v1
kind: deployment
metadata:
  name: articles
spec:
  replicas: 1
  selector:
    matchlabels:
      app: articles
  template:
    metadata:
      labels:
        app: articles
    spec:
      containers:
      - name: articles
        image: myimage/personal:articles
        imagepullpolicy: always
        ports:
        - containerport: 80
        env:
        - name: title
          value: &quot;articles&quot;
---
apiversion: v1
kind: service
metadata:
  name: articles
spec:
  type: clusterip
  ports:
  - port: 80
  selector:
    app: articles

ingress
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: personal
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressclassname: nginx
  rules:
  - http:
      paths:
      - path: /articles(/|$)(.*)
        pathtype: prefix
        backend:
          service:
            name: articles
            port:
              number: 80
      - path: /(.*)
        pathtype: prefix
        backend:
          service:
            name: landing-page
            port:
              number: 80


and here is a part of the content of the index.html file that makes the website; i intentionally hide the content of the body as the problem is only with the external files that need to be loaded (css and js files)
&lt;!doctype html&gt;
&lt;html&gt;
    &lt;head&gt;
        &lt;title&gt;editorial by html5 up&lt;/title&gt;
        &lt;meta charset=&quot;utf-8&quot; /&gt;
        &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, user-scalable=no&quot; /&gt;
        &lt;link rel=&quot;stylesheet&quot; href=&quot;./assets/css/main.css&quot; /&gt;
    &lt;/head&gt;
    &lt;body class=&quot;is-preload&quot;&gt;

        &lt;!-- scripts --&gt;
            &lt;script src=&quot;./assets/js/jquery.min.js&quot;&gt;&lt;/script&gt;
            &lt;script src=&quot;./assets/js/browser.min.js&quot;&gt;&lt;/script&gt;
            &lt;script src=&quot;./assets/js/breakpoints.min.js&quot;&gt;&lt;/script&gt;
            &lt;script src=&quot;./assets/js/util.js&quot;&gt;&lt;/script&gt;
            &lt;script src=&quot;./assets/js/main.js&quot;&gt;&lt;/script&gt;

    &lt;/body&gt;
&lt;/html&gt;

files are located in the following folder:
/usr/share/nginx/html/assets

with that, let's say the ingress address is 20.200.200.200, when i go to 20.200.200.200/articles/ , everything is working fine and the page is loading correctly, while when i go to the address 20.200.200.200/articles (note the trailing slash is missing there), the external files can't be loaded ... i have the following errors in the console:
uncaught syntaxerror: unexpected token '&lt;' (at jquery.min.js:1:1)
uncaught syntaxerror: unexpected token '&lt;' (at browser.min.js:1:1)
uncaught syntaxerror: unexpected token '&lt;' (at breakpoints.min.js:1:1)
uncaught syntaxerror: unexpected token '&lt;' (at util.js:1:1)
uncaught syntaxerror: unexpected token '&lt;' (at main.js:1:1)

i understand that it might be a problem when rewriting the url however i can't figure out how to solve this ...
tried various rewrite-target options, tried to modify the default.conf file of the nginx container without success
",<kubernetes><nginx><url-rewriting><kubernetes-ingress><nginx-ingress>,74837302,3,"found out how to handle this thanks to this post ...
the following ingress configuration makes it work with/without trailing slash:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: articles
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/configuration-snippet: |
      rewrite ^(/articles)$ $1/ redirect;
spec:
  ingressclassname: nginx
  rules:
  - http:
      paths:
      - path: /articles(/|$)(.*)
        pathtype: prefix
        backend:
          service:
            name: articles
            port:
              number: 80

"
58620357,more default named port in kubernetes?,"in kubernetes, i always see the service's definition like this:

---
apiversion: v1
kind: service
metadata:
  name: random-exporter
  labels:
    app: random-exporter
spec:
  type: clusterip
  selector:
    app: random-exporter
  ports:
    - port: 9800
      targetport: http
      name: random-port


whose targetport is http, it's human friendly 

and what i'm interested is that is there more named port such as http in kubernetes? maybe https
",<kubernetes><kubernetes-service>,58621691,3,"usually you refer to target port by its number. 
but you can give a specific name to each pod`s port 
and refer this name in your service specification. 

this will make your service clearer.
here you have example where you named your ports in pod. 

apiversion: v1
kind: pod
metadata:
  name: test
spec:
  containers:
  - name: test
    ports:
    - name: http
      containerport: 8080
    - name: https
      containerport: 8443 


and here you refer to those ports by name in the service yaml. 

apiversion: v1
kind: service
metadata:
  name: test-svc
spec:
  ports:
  - name: http
    port: 80
    targetport: http
  - name: https
    port: 443
    targetport: https 


also from the kubernetes documention you may find this information: 

targetport - number or name of the port to access on the pods targeted by the service. number must be in the range 1 to 65535. name must be an iana_svc_name. 
if this is a string, it will be looked up as a named port in the target pod's container ports. 
"
74882731,using tojson or torawjson without having it automatically adding quotation marks?,"so i have a values.yaml file with an string variable representing a database connection string with no quotes looking like this (don't worry, not the real password):
actionlogsconnectionstring: database=actionlogs;server=sql_dev;user id=sa;password=y19|yx\dysh53&amp;h

my goal is to print it inside a configmap resource so that it can then be injected in my pod as a .json configuration file for a dotnet app. i also want to append the application name in the connection string:
apiversion: v1
kind: configmap
metadata:
  name: {{ include &quot;asp.fullname&quot; . }}
  labels:
    {{- include &quot;asp.labels&quot; . | nindent 4 }}
data:
  appsettings.k8s.json: |-
    {
      &quot;connectionstrings&quot;: {
        &quot;actionlogsconnectionstring&quot;: &quot;{{ .values.actionlogsconnectionstring }};application name=asp;&quot;
      }
    }

this produce this result:
&quot;actionlogsconnectionstring&quot;: &quot;database=actionlogs;server=sql_dev;user id=sa;password=y19|yx\dysh53&amp;h;application name=asp;&quot;

look great! and at this point i don't have a quote problem.
now problem, the slash isn't escaped for the json file format. good thing, helm provide a tojson function. unfortunately, it also transform the &quot;&amp;&quot; to unicode value. i then found torawjson and it gives the expected results.
my problem is that, when using either tojson or torawjson, it adds extra quotes and break my result:
so this yalm file:
&quot;actionlogsconnectionstring&quot;: &quot;{{ .values.actionlogsconnectionstring | torawjson }};application name=asp;&quot;

results in this json file (note the extra quotes):
&quot;actionlogsconnectionstring&quot;: &quot;&quot;database=actionlogs;server=sql_dev;user id=sa;password=y19|yx\\dysh53&amp;h&quot;;application name=asp;&quot;

i see there's a function called | quote, but this only add some. no way to use torawjson without adding any?
",<kubernetes><kubernetes-helm>,74882987,3,"using tojson or torawjson is the wrong solution here, because the json representation of a string by definition includes the double quotes. &quot;foo&quot; is a json string, foo isn't valid json.
but you're only working with a scalar value, so there's not much point in marshaling it to json in the first place. i think the following gets you what you want:
apiversion: v1
kind: configmap
metadata:
  name: {{ include &quot;asp.fullname&quot; . }}
  labels:
    {{- include &quot;asp.labels&quot; . | nindent 4 }}
data:
  appsettings.k8s.json: |-
    {
      &quot;connectionstrings&quot;: {
        &quot;actionlogsconnectionstring&quot;: {{ printf &quot;%s;application name=asp&quot; .values.actionlogsconnectionstring | quote }}
      }
    }

here, we're using the printf to produce the desired string (and then passing it to the quote function for proper quoting).
this produces:
---
# source: example/templates/configmap.yaml
apiversion: v1
kind: configmap
metadata:
  name: my-example-fullname
  labels:
    # this is a test
data:
  appsettings.k8s.json: |-
    {
      &quot;connectionstrings&quot;: {
        &quot;actionlogsconnectionstring&quot;: &quot;database=actionlogs;server=sql_dev;user id=sa;password=y19|yx\\dysh53&amp;h;application name=asp&quot;
      }
    }

"
60761842,helm skips creation of istio virtual service,"i am trying to create a helm chart for my service with the following structure:

.
 my-app
  chart.yaml
  templates
   deployment.yaml
   istio-virtualservice.yaml
   service.yaml
  values.yaml



after installing the helm chart the deployment and service are being created successfully but the virtualservice is not being created. 

$ helm install -name my-app ./my-app -n my-namespace
$ kubectl get pods -n my-namespace
name                              ready   status    restarts   age
my-app-5578cbb95-xzqzk            2/2     running   0          5m

$ kubectl get vs
name                 gateways               hosts                         age
&lt;empty&gt;


my istio virtual service yaml files looks like:

apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: my-virtual-service
spec:
  hosts:
    - {{$.values.props.host | quote}}
  gateways:
    - my-api-gateway
  http:
    - match:
        - uri:
            prefix: /app/
      rewrite:
        uri: ""/""
      route:
        - destination:
            port:
              number: 8083
            host: my-service.my-namespace.svc.cluster.local


surprisingly, if i apply the above yaml after helm install is done deploying the app then the virtualservice gets created. 

$ kubectl apply -f istio-vs.yaml
$ kubectl get vs
name                 gateways               hosts                         age
my-virtual-service   [my-api-gateway]   [my-host.com]                     60s



please help me debug the issue and let me know if more debug information is needed.

$ helm version
version.buildinfo{version:""v3.0.1"", gitcommit:""7c22ef9ce89e0ebeb7125ba2ebf7d421f3e82ffa"", gittreestate:""clean"", goversion:""go1.13.4""}


$ istioctl version
client version: 1.4.1
control plane version: 1.4.1
data plane version: 1.4.1 (2 proxies)


$ kubectl version
client version: version.info{major:""1"", minor:""16+"", gitversion:""v1.16.6-beta.0"", gitcommit:""e7f962ba86f4ce7033828210ca3556393c377bcc"", gittreestate:""clean"", builddate:""2020-01-15t08:26:26z"", goversion:""go1.13.5"", compiler:""gc"", platform:""darwin/amd64""}
server version: version.info{major:""1"", minor:""16+"", gitversion:""v1.16.6-beta.0"", gitcommit:""e7f962ba86f4ce7033828210ca3556393c377bcc"", gittreestate:""clean"", builddate:""2020-01-15t08:18:29z"", goversion:""go1.13.5"", compiler:""gc"", platform:""linux/amd64""}

",<kubernetes><kubernetes-helm><istio>,60768972,3,"use 

kubectl get vs -n my-namespace


instead of 

kubectl get vs




thats because you have deployed everything in the my-namespace namespace.


  helm install -name my-app ./my-app -n my-namespace


and youre searching for virtual service in the default namespace.



its working when you apply it by yourself, because there is no namespace in the virtual service yaml and its deployed in the default one. 



additional info, i see you have gateway which is already deployed, if its not in the same namespace as virtual service, you should add it like in below example.

check the spec.gateways section

apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: bookinfo-mongo
  namespace: bookinfo-namespace
spec:
  gateways:
  - some-config-namespace/my-gateway # can omit the namespace if gateway is in same
                                       namespace as virtual service.




i hope this answer your question. let me know if you have any more questions.
"
60741615,use one oauth2_proxy instance with many ingress paths?,"i am running an app in a kubernetes service on azure and have had it set up with an nginx ingress controller and a public ip address with a fqdn. this was all working fine.
i then wanted to add security through using the oauth2-proxy for third party sign-in. i would like to keep my setup to one ingress-controller and one oauth2_proxy per namespace, with multiple apps running together. as azure does not support the use of sub-domains for this i have been using paths to route to the correct app. i've seen examples, like this, on how to use one oauth2_proxy for multiple sub-domains but is it possible to get it working with multiple paths instead?

setup
this is the current working setup with only one app, located on root /. i would like to switch to an app specific path and the ability to run multiple apps on different paths. eg. /my-app, /another-app etc.

oauth2-proxy-config.yaml

config:
  existingsecret: oauth2-proxy-creds

extraargs:
  whitelist-domain: my-fqdn.uksouth.cloudapp.azure.com
  cookie-domain: my-fqdn.uksouth.cloudapp.azure.com
  email-domain: example.com
  provider: github

ingress:
  enabled: true
  path: /oauth2
  hosts:
    - my-fqdn.uksouth.cloudapp.azure.com
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod # cert-manager cluster issuer set up for let's encrypt
  tls:
    - secretname: my-fqdn-tls # tls generated by letsencrypt-prod
      hosts:
        - my-fqdn.uksouth.cloudapp.azure.com


this is installed with the following helm command

helm upgrade oauth2-proxy --install stable/oauth2-proxy --namespace $namespace --reuse-values --values oauth2-proxy-config.yaml


app-ingress.yaml

apiversion: networking.k8s.io/v1beta1 # for versions before 1.14 use extensions/v1beta1
kind: ingress
metadata:
  name: nginx-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
#    nginx.ingress.kubernetes.io/rewrite-target: /$2 # not working with the /oauth2 path and not needed when using root path for the app
    nginx.ingress.kubernetes.io/auth-url: ""https://my-fqdn.uksouth.cloudapp.azure.com/oauth2/auth""
    nginx.ingress.kubernetes.io/auth-signin: ""https://my-fqdn.uksouth.cloudapp.azure.com/oauth2/start?rd=https%3a%2f%2f$host$request_uri""
spec:
  tls:
  - secretname: my-fqdn-tls
    hosts:
    - my-fqdn.uksouth.cloudapp.azure.com
  rules:
  - host: my-fqdn.uksouth.cloudapp.azure.com
    http:
      paths:
      - path: / # i would like to be able to use something like '/path1(/|$)(.*)' instead of root.
        backend:
          servicename: my-app
          serviceport: 80

",<azure><nginx><kubernetes><oauth-2.0><kubernetes-ingress>,61049935,3,"sure, it's doable with multiple ingress paths inside single ingress resource definition, please check this working example:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  labels:
    app: hello-worlds
  name: hello-wrolds
  annotations:
    cert-manager.io/issuer: selfsigned-issuer
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/auth-signin: ""https://my-fqdn.uksouth.cloudapp.azure.com/oauth2/start?rd=$escaped_request_uri""
    nginx.ingress.kubernetes.io/auth-url: ""https://my-fqdn.uksouth.cloudapp.azure.com/oauth2/auth""
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
    - host: my-fqdn.uksouth.cloudapp.azure.com
      http:
        paths:
          - path: /my-app/(.*)
            backend:
              servicename: my-app
              serviceport: 5000
          - path: /another-app/(.*)
            backend:
              servicename: another-app
              serviceport: 5000    
  tls:
    - hosts:
      - my-fqdn.uksouth.cloudapp.azure.com
      secretname: certmgr-selfsign-tls-requires-ouath


in my case, for both backends the app root folder is '/hello', so the requested url is respectively:

https://my-fqdn.uksouth.cloudapp.azure.com/my-app/hello

https://my-fqdn.uksouth.cloudapp.azure.com/another-app/hello

"
75343886,kustomize helmchartinflationgeneration error with chartname not found,"i have the following chartinflator.yml file:
apiversion: builtin
kind: chartinflator
metadata:
  name: project-helm-inflator
chartname: helm-k8s
charthome: ../../../helm-k8s/
releasename: project-monitoring-chart
values: ../../values.yaml
releasenamespace: project-monitoring-ns

when i ran it using this, i got the error message below:
$ kustomize build .
error: loading generator plugins: failed to load generator: plugin helmchartinflationgenerator.builtin.[nogrp]/project-helm-inflator.[nons] fails configuration: chart name cannot be empty

here is my project structure:
project
  - helm-k8s
   - values.yml
   - chart.yml
   - templates
    - base
      - project-namespace.yml
      - grafana
        - grafana-service.yml
        - grafana-deployment.yml
        - grafana-datasource-config.yml
      - prometheus
        - prometheus-service.yml
        - prometheus-deployment.yml
        - prometheus-config.yml
        - prometheus-roles.yml
      - kustomization.yml
    - prod
      - kustomization.yml
    - test
      - kustomization.yml

",<kubernetes><kubernetes-helm><kustomize>,75345416,3,"i think you may have found some outdated documentation for the helm chart generator. the canonical documentation for this is here. reading that implies several changes:

include the inflator directly in your kustomization.yaml in the helmcharts section.

use name instead of chartname.

set charthome in the helmglobals section rather than per-chart.


that gets us something like this in our kustomization.yaml:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization

helmglobals:
  charthome: ../../../helm-k8s/

helmcharts:
- name: helm-k8s
  releasename: project-monitoring-chart
  values: ../../values.yaml
  releasenamespace: project-monitoring-ns

i don't know if this will actually work -- you haven't provided a reproducer in your question, and i'm not familiar enough with helm to whip one up on the spot -- but i will note that your project layout is highly unusual. you appear to be trying to use kustomize to deploy a helm chart that contains your kustomize configuration, and it's not clear what the benefit is of this layout vs. just creating a helm chart and then using kustomize to inflate it from outside of the chart templates directory.
you may need to add --load-restrictor loadrestrictionsnone when calling kustomize build for this to work; by default, the charthome location must be contained by the same directory that contains your kustomization.yaml.

update
to make sure things are clear, this is what i'm recommending:

remove the kustomize bits from your helm chart, so that it looks like this.

publish your helm charts somewhere. i've set up github pages for that repository and published the charts at http://oddbit.com/open-electrons-deployments/.

use kustomize to deploy the chart with transformations. here we add a -prod suffix to all the resources:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization

helmcharts:
  - name: open-electrons-monitoring
    repo: http://oddbit.com/open-electrons-deployments/

namesuffix: -prod



"
34507440,loadbalancing with reserved ips in google container engine,"i want to host a website (simple nginx+php-fpm) on google container engine. i built a replication controller that controls the nginx and php-fpm pod. i also built a service that can expose the site.

how do i link my service to a public (and reserved) ip address so that the webserver sees the client ip addresses? 

i tried creating an ingress. it provides the client ip through an extra http header. unfortunately ingress does not support reserved ips yet:

kind: ingress
metadata:
  name: example-ingress
spec:
  rules:
  - host: example.org
    http:
      paths:
      - backend:
          servicename: example-web
         serviceport: 80
        path: /


i also tried creating a service with a reserved ip. this gives me a public ip address but i think the client ip is lost:

apiversion: v1
kind: service
metadata:
  name: 'example-web'
spec:
  selector:
    app: example-web
  ports:
    - port: 80
      targetport: 80
  loadbalancerip: ""10.10.10.10""
  type: loadbalancer


i would setup the http loadbalancer manually, but i didn't find a way to configure a cluster ip as a backend for the loadbalancer.

this seems like a very basic use case to me and stands in the way of using container engine in production. what am i missing? where am i wrong?
",<kubernetes><google-kubernetes-engine>,34509427,3,"as you are running in google-container-engine you could set up a compute engine http load balancer for your static ip. the target proxy will add x-forwarded- headers for you.

set up your kubernetes service with type nodeport and add a nodeport field. this way nodeport is accessible via kubernetes-proxy on every nodes ip address regardless of where the pod is running:

apiversion: v1
kind: service
metadata:
 name: 'example-web'
spec:
 selector:
   app: example-web
 ports:
   - nodeport: 30080
     port: 80
     targetport: 80
 type: nodeport


create a backend service with http health check on port 30080 for your instance group (nodes).
"
59230411,ingress nginx redirect from www to https,"i'm trying to redirect http://www... and https://www... to https://... using ingress-nginx. how can i do that? 

i've tried adding the following custom configuration using the annotation nginx.ingress.kubernetes.io/server-snippet and nginx.ingress.kubernetes.io/configuration-snippet:

# 1
if($host = ""www.example.com"") {
    return 308 https://example.com$request_uri;
}

# 2
server {
    server_name www.example.com;
    return 308 https://example.com$request_uri;
}

# 3
server_name www.example.com;
return 308 https://example.com$request_uri;


but i get an error in the nginx controller logs for #1:

2019/12/07 20:58:47 [emerg] 48898#48898: unknown directive ""if($host"" in /tmp/nginx-cfg775816039:418
nginx: [emerg] unknown directive ""if($host"" in /tmp/nginx-cfg775816039:418
nginx: configuration file /tmp/nginx-cfg775816039 test failed


for #2 i get an error that the server block is not allowed at that position and using #3 leads to infinite redirects. my ingress yaml looks like this:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-nginx
  annotations:
    kubernetes.io/ingress.class: ""nginx""
    kubernetes.io/ingress.global-static-ip-name: ""example-com""
    nginx.ingress.kubernetes.io/rewrite-target: ""/""
    nginx.ingress.kubernetes.io/ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/force-ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""86400s""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""86400s""
    nginx.ingress.kubernetes.io/proxy-body-size: ""100m""
    nginx.ingress.kubernetes.io/limit-rps: ""20""
    nginx.ingress.kubernetes.io/client-max-body-size: ""100m""
    nginx.ingress.kubernetes.io/configuration-snippet: |
      # see above
spec:
  tls:
  - hosts:
      - example.com
    secretname: certificate-secret
  rules:
  - host: sub.example.com
    http:
      paths:
      - backend:
          servicename: service-sub
          serviceport: 1234
# more subdomains here
  - host: example.com
    http:
      paths:
      - backend:
          servicename: service-example
          serviceport: 1235
  - host: ""*.example.com""
    http:
      paths:
      - backend:
          servicename: service-example-wildcard
          serviceport: 1236


i've also tried setting the nginx.ingress.kubernetes.io/from-to-www-redirect: ""true"" annotation, but that leads to a different error:

2019/12/07 21:20:34 [emerg] 51558#51558: invalid server name or wildcard ""www.*.example.com"" on 0.0.0.0:80
nginx: [emerg] invalid server name or wildcard ""www.*.example"" on 0.0.0.0:80
nginx: configuration file /tmp/nginx-cfg164546048 test failed

",<nginx><kubernetes><configuration><kubernetes-ingress><nginx-ingress>,59275507,3,"ok i got it. the missing space after if fixed it. thank you mdaniel :)
here is a working configuration that redirects anything to https://... without www:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-nginx-integration
  namespace: integration
  annotations:
    kubernetes.io/ingress.class: ""nginx""
    kubernetes.io/ingress.global-static-ip-name: ""example-com""
    nginx.ingress.kubernetes.io/rewrite-target: ""/""
    nginx.ingress.kubernetes.io/ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/force-ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""86400s""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""86400s""
    nginx.ingress.kubernetes.io/proxy-body-size: ""100m""
    nginx.ingress.kubernetes.io/limit-rps: ""20""
    nginx.ingress.kubernetes.io/client-max-body-size: ""100m""
    nginx.ingress.kubernetes.io/configuration-snippet: |
      if ($host = ""www.example.com"") {
          return 308 https://example.com$request_uri;
      }
spec:
  tls:
  - hosts:
      - example.com
    secretname: certificate-integration-secret
  rules:
  - host: subdomain.example.com
    http:
      paths:
      - backend:
          servicename: service-emviwiki
          serviceport: 4000
  # ... more rules, no www here


"
70960103,install gpu driver on autoscaling node in gke (cloud composer),"i'm running a google cloud composer gke cluster. i have a default node pool of 3 normal cpu nodes and one nodepool with a gpu node. the gpu nodepool has autoscaling activated.
i want to run a script inside a docker container on that gpu node.
for the gpu operating system i decided to go with cos_containerd instead of ubuntu.
i've followed https://cloud.google.com/kubernetes-engine/docs/how-to/gpus and ran this line:
kubectl apply -f https://raw.githubusercontent.com/googlecloudplatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml

the gpu now shows up when i run &quot;kubectl describe&quot; on the gpu node, however my test scripts debug information tells me, that the gpu is not being used.
when i connect to the autoprovisioned gpu node via ssh, i can see, that i still need to run the
cos extensions gpu install

in order to use the gpu.
i now want to make my cloud composer gke cluster to run &quot;cos-extensions gpu install&quot; whenever a node is being created by the autoscaler feature.
i would like to apply something like this yaml:
#cloud-config

runcmd:
  - cos-extensions install gpu

to my cloud composer gke cluster.
can i do that with kubectl apply ? ideally i would like to only run that yaml code onto the gpu node. how can i achieve that?
i'm new to kubernetes and i've already spent a lot of time on this without success. any help would be much appreciated.
best,
phil
update:
ok thx to harsh i realized i have to go via daemonset + configmap like here:
https://github.com/googlecloudplatform/solutions-gke-init-daemonsets-tutorial
my gpu node has the label
gpu-type=t4

so i've created and kubectl applied this configmap:
apiversion: v1
kind: configmap
metadata:
  name: phils-init-script
  labels:
    gpu-type: t4
data:
  entrypoint.sh: |
    #!/usr/bin/env bash

    root_mount_dir=&quot;${root_mount_dir:-/root}&quot;

    chroot &quot;${root_mount_dir}&quot; cos-extensions gpu install

and here is my daemonset (i also kubectl applied this one):
apiversion: apps/v1
kind: daemonset
metadata:
  name: phils-cos-extensions-gpu-installer
  labels:
    gpu-type: t4
spec:
  selector:
    matchlabels:
      gpu-type: t4
  updatestrategy:
    type: rollingupdate
  template:
    metadata:
      labels:
        name: phils-cos-extensions-gpu-installer
        gpu-type: t4
    spec:
      volumes:
      - name: root-mount
        hostpath:
          path: /
      - name: phils-init-script
        configmap:
          name: phils-init-script
          defaultmode: 0744
      initcontainers:
      - image: ubuntu:18.04
        name: phils-cos-extensions-gpu-installer
        command: [&quot;/scripts/entrypoint.sh&quot;]
        env:
        - name: root_mount_dir
          value: /root
        securitycontext:
          privileged: true
        volumemounts:
        - name: root-mount
          mountpath: /root
        - name: phils-init-script
          mountpath: /scripts
      containers:
      - image: &quot;gcr.io/google-containers/pause:2.0&quot;
        name: pause

but nothing happens, i get the message &quot;pods are pending&quot;.
during the run of the script i connect via ssh to the gpu node and can see that the configmap shell code didn't get applied.
what am i missing here?
i'm desperately trying to make this work.
best,
phil
thanks for all your help so far!
",<kubernetes><google-cloud-platform><google-compute-engine><google-kubernetes-engine><google-cloud-composer>,70960774,3,"
can i do that with kubectl apply ? ideally i would like to only run
that yaml code onto the gpu node. how can i achieve that?

yes, you can run the deamon set on each node which will run the command on nodes.
as you are on gke and daemon set will also run the command or script on new nodes also which are getting scaled up also.
daemon set is mainly for running applications or deployment on each available node in the cluster.
we can leverage this deamon set and run the command on each node that exist and is also upcoming.
example yaml :
apiversion: apps/v1
kind: daemonset
metadata:
  name: node-initializer
  labels:
    app: default-init
spec:
  selector:
    matchlabels:
      app: default-init
  updatestrategy:
    type: rollingupdate
  template:
    metadata:
      labels:
        name: node-initializer
        app: default-init
    spec:
      volumes:
      - name: root-mount
        hostpath:
          path: /
      - name: entrypoint
        configmap:
          name: entrypoint
          defaultmode: 0744
      initcontainers:
      - image: ubuntu:18.04
        name: node-initializer
        command: [&quot;/scripts/entrypoint.sh&quot;]
        env:
        - name: root_mount_dir
          value: /root
        securitycontext:
          privileged: true
        volumemounts:
        - name: root-mount
          mountpath: /root
        - name: entrypoint
          mountpath: /scripts
      containers:
      - image: &quot;gcr.io/google-containers/pause:2.0&quot;
        name: pause

github link for example : https://github.com/googlecloudplatform/solutions-gke-init-daemonsets-tutorial
exact deployment step : https://cloud.google.com/solutions/automatically-bootstrapping-gke-nodes-with-daemonsets#deploying_the_daemonset
full article : https://cloud.google.com/solutions/automatically-bootstrapping-gke-nodes-with-daemonsets
"
54973005,gke kubernetes rbac bind default role to my limited custom,"i'm using g
i want to create a custom user that have only access to specific namespace, i used this yaml:

---
apiversion: v1
kind: serviceaccount
metadata:
  name: develop-user
  namespace: develop

---
kind: role
apiversion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: develop-user-full-access
  namespace: develop
rules:
- apigroups: rbac.authorization.k8s.io
  resources:
  - services
  verbs: [""get""]

---
kind: rolebinding
apiversion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: develop-user-view
  namespace: develop
subjects:
- kind: serviceaccount
  name: develop-user
  namespace: develop
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: develop-user-full-access


so i get a certificate and added to my kube config, after i switched context to this new service account and figured out that i still have access to everything :(
why did it happen and how to fix?

my kubeconfig (pastebin copy: https://pastebin.com/s5nd6dnn):

apiversion: v1
clusters:
- cluster:
    certificate-authority-data: %certificate-data%
    server: https://animeheaven.nyah
  name: anime-cluster-develop
contexts:
- context:
    cluster: anime-cluster-develop
    namespace: develop
    user: develop-user
  name: anime-develop
current-context: anime-develop
kind: config
preferences: {}
users:
- name: develop-user
  user:
    client-key-data: %certdata%
    token: %tokenkey%

",<kubernetes><google-kubernetes-engine><rbac>,55101563,3,"https://medium.com/uptime-99/making-sense-of-kubernetes-rbac-and-iam-roles-on-gke-914131b01922
https://medium.com/@managedkube/kubernetes-rbac-port-forward-4c7eb3951e28

these two articles helped me finally! i almost felt into depression because of this stupid stuff, thanks to uptime-99 and managedkube i did it! yay!

the key is to create kubernetes-viewer user in gcloud and then create a role for him
here is a hint!

---
kind: role
apiversion: rbac.authorization.k8s.io/v1
metadata:
  namespace: develop
  name: allow-developer-port-forward
rules:
- apigroups: [""""]
  resources: [""pods"", ""pods/portforward""]
  verbs: [""get"", ""list"", ""create""]
---
kind: rolebinding
apiversion: rbac.authorization.k8s.io/v1
metadata:
  name: anime-developer-port-access
  namespace: develop
subjects:
- kind: user
  name: animedeverlop@gmail.com
  apigroup: rbac.authorization.k8s.io
roleref:
  kind: role
  name: allow-developer-port-forward
  apigroup: """"


then  


  kubectly apply -f accessconfig.yaml  


thats it!
have a nice day!
"
60811207,kubernetes hpa wrong metrics?,"i've created a gke test cluster on google cloud. it has 3 nodes with 2 vcpus / 8 gb ram. i've deployed two java apps on it

here's the yaml file:

apiversion: apps/v1            
kind: deployment
metadata:                    
  name: myapi           
spec:
  selector:                                                                          
    matchlabels:
      app: myapi
  strategy:
    type: recreate
  template:
    metadata:
      labels:
        app: myapi
    spec:
      containers:
      - image: eu.gcr.io/myproject/my-api:latest
        name: myapi
        imagepullpolicy: always
        ports:
        - containerport: 8080
          name: myapi
---
apiversion: apps/v1
kind: deployment
metadata:
  name: myfrontend
spec:
  selector:
    matchlabels:
      app: myfrontend
  strategy:
    type: recreate
  template:
    metadata:
      labels:
        app: myfrontend
    spec:
      containers:
      - image: eu.gcr.io/myproject/my-frontend:latest
        name: myfrontend
        imagepullpolicy: always
        ports:
        - containerport: 8080
          name: myfrontend
---


then i wanted to add a hpa with the following details:

apiversion: autoscaling/v1
kind: horizontalpodautoscaler
metadata:
  name: myfrontend
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: myfrontend
  minreplicas: 2
  maxreplicas: 5
  targetcpuutilizationpercentage: 50
---
apiversion: autoscaling/v1
kind: horizontalpodautoscaler
metadata:
  name: myapi
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: myapi
  minreplicas: 2
  maxreplicas: 4
  targetcpuutilizationpercentage: 80
---


if i check kubectl top pods it shows some really weird metrics:

name                         cpu(cores)   memory(bytes)   
myapi-6fcdb94fd9-m5sh7      194m         1074mi          
myapi-6fcdb94fd9-sptbb      193m         1066mi          
myapi-6fcdb94fd9-x6kmf      200m         1108mi          
myapi-6fcdb94fd9-zzwmq      203m         1074mi          
myfrontend-788d48f456-7hxvd   0m           111mi           
myfrontend-788d48f456-hlfrn   0m           113mi   


hpa info:

name        reference              targets    minpods   maxpods   replicas   age
myapi      deployment/myapi      196%/80%   2         4         4          32m
myfrontend   deployment/myfrontend   0%/50%     2         5         2          32m


but if i check uptime on one of the nodes it shows a less lower value:

[myapi@myapi-6fcdb94fd9-sptbb /opt/]$ uptime
 09:49:58 up 47 min,  0 users,  load average: 0.48, 0.64, 1.23


any idea why it shows a completely different thing. why hpa shows 200% of current cpu utilization? and because of this it uses the maximum replicas in idle, too. any idea?
",<kubernetes><google-kubernetes-engine><hpa>,60811781,3,"the targetcpuutilizationpercentage of the hpa is a percentage of the cpu requests of the containers of the target pods. if you don't specify any cpu requests in your pod specifications, the hpa can't do its calculations.

in your case it seems that the hpa assumes 100m as the cpu requests (or perhaps you have a limitrange that sets the default cpu request to 100m). the current usage of your pods is about 200m and that's why the hpa displays a utilisation of about 200%.

to set up the hpa correctly, you need to specify cpu requests for your pods. something like:

      containers:
      - image: eu.gcr.io/myproject/my-api:latest
        name: myapi
        imagepullpolicy: always
        ports:
        - containerport: 8080
          name: myapi
        resources:
          requests:
            cpu: 500m


or whatever value your pods require. if you set the targetcpuutilizationpercentage to 80, the hpa will trigger an upscale operation at 400m usage, because 80% of 500m is 400m.



besides that, you use an outdated version of horizontalpodautoscaler:


your version: v1
newest version: v2beta2


with the v2beta2 version, the specification looks a bit different. something like:

apiversion: autoscaling/v2beta2
kind: horizontalpodautoscaler
metadata:
  name: myapi
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: myapi
  minreplicas: 2
  maxreplicas: 4
  metrics:
  - type: resource
    resource:
      name: cpu
      target:
        type: utilization
        averageutilization: 80


see examples.

however, the cpu utilisation mechanism described above still applies.
"
54253899,error validating pod template with nodeselector,"    apiversion: v1
    kind: pod
    metadata:
      creationtimestamp: null
      labels:
        run: nginx4
      name: nginx4
    spec:
      containers:
      - image: nginx
        name: nginx4
      nodeselector:
        app: ""v1-tesla""
        resources: {}
      dnspolicy: clusterfirst
      restartpolicy: never
    status: {}


when i run the above template kubectl create -f pod.yaml, i get the following error:

    error: error validating ""podonanode.yaml"": error validating data: 
    validationerror(pod.spec.nodeselector.resources): invalid type for 
    io.k8s.api.core.v1.podspec.nodeselector: got ""map"", expected 
    ""string""; if you choose to ignore these errors, turn validation off 
    with --validate=false


any pointers to fix this would be great.
",<kubernetes><yaml><nodes><kubectl><kubernetes-pod>,54254099,3,"the above error is for:

nodeselector:
  app: ""v1-tesla""
  resources: {}


here, resources: {} representing map, but it should be string. so remove resources: {} or change it's value to string.

apiversion: v1
kind: pod
metadata:
  creationtimestamp: null
  labels:
    run: nginx4
  name: nginx4
spec:
  containers:
  - image: nginx
    name: nginx4
  nodeselector:
    app: ""v1-tesla""
    resources: ""whatever""
  dnspolicy: clusterfirst
  restartpolicy: never
status: {}

"
70524978,missing default value in nested field of kubernetes custom resource,"i have a custom resource definition which has nested fields with default values (some boilerplate omitted for brevity):
apiversion: apiextensions.k8s.io/v1
kind: customresourcedefinition
spec:
  scope: namespaced
  group: thismatters.stackoverflow
  names:
    kind: baddefault
  versions:
  - name: v1alpha
    schema:
      openapiv3schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              regularthing:
                type: integer
                default: 6
              shoulddefault:
                type: object
                properties:
                  nestedthing:
                    type: integer
                    default: 12

whenever i create a resource of this kind without specifying the shoulddefault object:
apiversion: thismatters.stackoverflow/v1alpha
kind: baddefault
metadata:
  name: blank-demo
spec: 
  regularthing: 7

the default value for .shoulddefault.nestedthing doesn't appear when the resource is described:
apiversion: thismatters.stackoverflow/v1alpha
kind: baddefault
metadata:
  name: blank-demo
spec: 
  regularthing: 7

if i update the resource with this manifest:
apiversion: thismatters.stackoverflow/v1alpha
kind: baddefault
metadata:
  name: blank-demo
spec: 
  regularthing: 7
  shoulddefault: {}

then the nested field default value is populated when described:
apiversion: thismatters.stackoverflow/v1alpha
kind: baddefault
metadata:
  name: blank-demo
spec: 
  regularthing: 7
  shoulddefault:
    nestedthing: 12

is there anything i can do in the crd to remove the need for the shoulddefault: {} line in the resource manifest and have the default values populate in the nested fields?
my cluster is on kubernetes version 1.19.
",<kubernetes><kubernetes-custom-resources>,70538591,3,"adding a default property to the shoulddefault object fixes this:
apiversion: apiextensions.k8s.io/v1
kind: customresourcedefinition
spec:
  scope: namespaced
  group: thismatters.stackoverflow
  names:
    kind: baddefault
  versions:
  - name: v1alpha
    schema:
      openapiv3schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              regularthing:
                type: integer
                default: 6
              shoulddefault:
                type: object
                default: {}   #  &lt;-- this was added
                properties:
                  nestedthing:
                    type: integer
                    default: 12

"
70469602,nginx.ingress.kubernetes.io/server-snippet annotation contains invalid word location,"i am new to kubernetes and using aws eks cluster 1.21. i am trying to write the nginx ingress config for my k8s cluster and blocking some request using server-snippet. my ingress config is below
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: abc-ingress-external
  namespace: backend
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: nginx-external
    nginx.ingress.kubernetes.io/server-snippet: |
       location = /ping {
         deny all;
         return 403;
       }
spec:
  rules:
  - host: dev-abc.example.com
    http:
      paths:
      - backend:
          service:
              name: miller
              port:
                number: 80
        path: /
        pathtype: prefix

when i apply this config, i get this error:
for: &quot;ingress.yml&quot;: admission webhook &quot;validate.nginx.ingress.kubernetes.io&quot; denied the request: nginx.ingress.kubernetes.io/server-snippet annotation contains invalid word location

i looked into this and got this is something related to annotation-value-word-blocklist. however i don't know how to resolve this. any help would be appreciated.
",<nginx><kubernetes><kubernetes-ingress><nginx-ingress>,70469949,3,"seems there's issue using location with some versions. the following was tested successfully on eks cluster.
install basic ingress-nginx on eks:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.0/deploy/static/provider/aws/deploy.yaml
note: if your cluster version is &lt; 1.21, you need to comment out ipfamilypolicy and ipfamilies in the service spec.
run a http service:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/docs/examples/http-svc.yaml
create an ingress for the service:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: http-svc
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/server-snippet: |
       location = /ping {
         deny all;
         return 403;
       }
spec:
  rules:
  - host: test.domain.com
    http:
      paths:
      - path: /
        pathtype: implementationspecific
        backend:
          service:
            name: http-svc
            port:
              number: 8080

return 200 as expected:
curl -h 'host: test.domain.com' http://&lt;get your nlb address from the console&gt;
return 200 as expected:
curl -h 'host: test.domain.com' -k https://&lt;get your nlb address from the console&gt;
return 403 as expected, the snippet is working:
curl -h 'host: test.domain.com' -k https://&lt;get your nlb address from the console&gt;/ping

use the latest release to avoid the &quot;annotation contains invalid word location&quot; issue.
"
70454755,ingress creating health check on http instead of tcp,"i am actually trying to run 3 containers in my gke cluster. i have them exposed via a network load balancer and over that, i am using ingress so i can reach my services from different domains with ssl certs on them.
here is the complete manifest
apiversion: apps/v1
kind: deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchlabels:
      app: web
  template:
    metadata:
      labels:
        app:web
    spec:
      containers:
      - name: web
        image: us-east4-docker.pkg.dev/web:e856485      # docker image
        ports:
        - containerport: 3000
        env:
        - name: node_env
          value: production
---
# deployment manifest #
apiversion: apps/v1
kind: deployment
metadata:
  name: cms
spec:
  replicas: 3
  selector:
    matchlabels:
      app: cms
  template:
    metadata:
      labels:
        app: cms
    spec:
      containers:
      - name: cms
        image: us-east4-docker.pkg.dev/cms:4e1fe2f      # docker image
        ports:
        - containerport: 8055
        env:
        - name  : db
          value : &quot;postgres&quot;

        - name  : db_host
          value : 10.142.0.3

        - name  : db_port
          value : &quot;5432&quot;
---
# deployment manifest #
apiversion: apps/v1
kind: deployment
metadata:
  name: api
spec:
  replicas: 3
  selector:
    matchlabels:
      app: api
  template:
    metadata:
      labels:
        app: api
    spec:
      containers:
      - name: api
        image: us-east4-docker.pkg.dev/api:4e1fe2f      # docker image
        ports:
        - containerport: 8080
        env:
        - name  : host
          value : &quot;0.0.0.0&quot;

        - name  : port
          value : &quot;8080&quot;
     
        - name  : node_env
          value : production
---
# service manifest #
apiversion: v1
kind: service
metadata:
  name: web-lb
  annotations:
    cloud.google.com/neg: '{&quot;ingress&quot;: true}'
  labels:
    app: web
spec:
  ports:
  - port: 3000
    protocol: tcp
    targetport: 3000
  selector:
    app: web
  type: nodeport
---
# service manifest #
apiversion: v1
kind: service
metadata:
  name: cms-lb
  annotations:
    cloud.google.com/neg: '{&quot;ingress&quot;: true}'
  labels:
    app: cms
spec:
  ports:
  - port: 8055
    protocol: tcp
    targetport: 8055
  selector:
    app: cms
  type: nodeport
---
# service manifest #
apiversion: v1
kind: service
metadata:
  name: api-lb
  annotations:
    cloud.google.com/neg: '{&quot;ingress&quot;: true}'
  labels:
    app: api
spec:
  ports:
  - port: 8080
    protocol: tcp
    targetport: 8080
  selector:
    app: api
  type: nodeport
---
apiversion: v1
data:
  tls.crt: abc
  tls.key: abc
kind: secret
metadata:
  name: web-cert
type: kubernetes.io/tls
---
apiversion: v1
data:
  tls.crt: abc
  tls.key: abc
kind: secret
metadata:
  name: cms-cert
type: kubernetes.io/tls
---
apiversion: v1
data:
  tls.crt: abc
  tls.key: abc
kind: secret
metadata:
  name: api-cert
type: kubernetes.io/tls
---
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress
  annotations:
    # if the class annotation is not specified it defaults to &quot;gce&quot;.
    kubernetes.io/ingress.class: &quot;gce&quot;
spec:
  tls:
  - secretname: api-cert
  - secretname: cms-cert
  - secretname: web-cert
  rules:
  - host: web-gke.dev
    http:
      paths:
      - pathtype: implementationspecific
        backend:
          service:
            name: web-lb
            port:
              number: 3000
  - host: cms-gke.dev
    http:
      paths:
      - pathtype: implementationspecific
        backend:
          service:
            name: cms-lb
            port:
              number: 8055
  - host: api-gke.dev
    http:
      paths:
      - pathtype: implementationspecific
        backend:
          service:
            name: api-lb
            port:
              number: 8080

the containers are accessible through the load balancer(network), but from ingress(l7 lb) the health check is failing.
i tried editing the health checks manually from http:80 to tcp:8080/8055/3000 for 3 services and it works.
but eventually, ingress reverts it back to http health check and it fails again. i also tried using nodeport instead of load balancer as service type but no luck.
any help?
",<kubernetes><google-cloud-platform><google-kubernetes-engine><kubernetes-ingress>,70456091,3,"the first thing i would like to mention is that you need to recheck your implementation because from what i see, you are creating an ingress which will create a loadbanacer, and this ingress is using three services of type loadbalancer in which each one of them will also create its loadbalancer (i'm assuming the default behaviour, unless you applied the famous workaround of deleting the service's loadbalancer manually after it is created).
and i don't think this is correct unless you need that design for some reason. so, my suggestion is that you might want to change your services types to nodeport.

as for answering your question, what you are missing is:
you need to implement a backendconfig with custom healthcheck configurations.
1- create the backendconfig:
apiversion: cloud.google.com/v1
kind: backendconfig
metadata:
  name: api-lb-backendconfig
spec:
  healthcheck:
    checkintervalsec: interval
    timeoutsec: timeout
    healthythreshold: health_threshold
    unhealthythreshold: unhealthy_threshold
    type: protocol
    requestpath: path
    port: port

2- use this config in your service/s
apiversion: v1
kind: service
metadata:
  annotations:
    cloud.google.com/backend-config: '{&quot;ports&quot;: {
    &quot;port_name_1&quot;:&quot;api-lb-backendconfig&quot;
    }}'
spec:
  ports:
  - name: port_name_1
    port: port_number_1
    protocol: tcp
    targetport: target_port

once you apply such configurations, your ingress's loadbalanacer will be created with the backendconfig &quot;api-lb-backendconfig&quot;

consider this documentation page as your reference.
"
64822957,aws network load balancer and tcp traffic with aws fargate,"i want to expose a tcp-only service from my fargate cluster to the public internet on port 80. to achieve this i want to use an aws network load balancer
this is the configuration of my service:
apiversion: v1
kind: service
metadata:
  name: myapp
  labels:
    app: myapp
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: &quot;nlb&quot;
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: &quot;30&quot;
spec:
  type: loadbalancer
  selector:
    app: myapp
  ports:
    - protocol: tcp
      port: 80
      targetport: 80

using the service from inside the cluster with cluster-ip works. when i apply my config with kubectl the following happens:

service is created in k8s
nlb is created in aws
nlb gets status 'active'
vpc and other values for the nlb look correct
target group is created in aws
there are 0 targets registered
i can't register targets because group expects instances, which i do not have
external_ip is 
listener is not created automatically

then i create a listener for port 80 and tcp. after some wait an external_ip is assigned to the service in aws.
my problem: it does not work. the service is not available using the dns name from the nlb and port 80.
",<amazon-web-services><kubernetes><amazon-eks><aws-fargate>,64826045,3,"the in-tree kubernetes service loadbalancer for aws, can not be used for aws fargate.

you can use nlb instance targets with pods deployed to nodes, but not to fargate.

but you can now install aws load balancer controller and use ip mode on your service loadbalancer, this also works for aws fargate.
kind: service
apiversion: v1
metadata:
  name: nlb-ip-svc
  annotations:
    # route traffic directly to pod ips
    service.beta.kubernetes.io/aws-load-balancer-type: &quot;nlb-ip&quot;

see introducing aws load balancer controller and eks network load balancer - ip targets
"
58063224,kubernetes: my podsecuritypolicy is not working or misconfigured,"i'm trying to restrict all pods except few from running with the privileged mode.

so i created two pod security policies:
one allowing running privileged containers and one for restricting privileged containers.

[root@master01 vagrant]# kubectl get psp
name         priv    caps   selinux    runasuser   fsgroup    supgroup   readonlyrootfs   volumes
privileged   true           runasany   runasany    runasany   runasany   false            *
restricted   false          runasany   runasany    runasany   runasany   false            *


created the cluster role that can use the pod security policy ""restricted"" and binded that role to all the serviceaccounts in the cluster

apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  name: psp-restricted
rules:
- apigroups: ['policy']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourcenames:
  - restricted
---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: psp-restricted
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: psp-restricted
subjects:
- kind: group
  name: system:serviceaccounts
  apigroup: rbac.authorization.k8s.io


now i deploying a pod with ""privileged"" mode. but it is getting deployed. the created pod annotation indicating that the psp ""privileged"" was validated during the pod creation time. why is that ? the restricted psp should be validated right ? 

apiversion: v1
kind: pod
metadata:
  name: psp-test-pod
  namespace: default
spec:
  serviceaccountname: default
  containers:
    - name: pause
      image: k8s.gcr.io/pause
      securitycontext:
        privileged: true


[root@master01 vagrant]# kubectl create -f pod.yaml
pod/psp-test-pod created


[root@master01 vagrant]# kubectl get pod psp-test-pod -o yaml |grep kubernetes.io/psp
    kubernetes.io/psp: privileged


kubernetes version: v1.14.5

am i missing something here ? any help is appreciated.
",<kubernetes><kubernetes-pod>,58093575,3,"posting the answer to my own question. hope it will help someone 

all my podsecuritypolicy configurations are correct. the issue was, i tried to deploy a pod by its own not via any controller manager like deployment/replicaset/daemonset etc..
most kubernetes pods are not created directly by users. instead, they are typically created indirectly as part of a deployment, replicaset or other templated controller via the controller manager.

in the case of a pod deployed by its own,  pod is created by kubectl not by controller manager.

in kubernetes there is one superuser role named ""cluster-admin"". in my case, kubectl is running with superuser role ""cluster-admin"". this ""cluster-admin"" role has access to all the pod security policies. because, to associate a pod security policy to a role, we need to use 'use' verbs and set 'resources' to 'podsecuritypolicy' in 'apigroups'

in the cluster-admin role 'resources' * include  'podsecuritypolicies' and  'verbs'  * include 'use'. so all policies will also enforce on the cluster-admin role as well. 

[root@master01 vagrant]# kubectl get clusterrole cluster-admin -o yaml
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: ""true""
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: cluster-admin
rules:
- apigroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
- nonresourceurls:
  - '*'
  verbs:
  - '*'


pod.yaml

apiversion: v1
kind: pod
metadata:
  name: psp-test-pod
  namespace: default
spec:
  serviceaccountname: default
  containers:
    - name: pause
      image: k8s.gcr.io/pause
      securitycontext:
        privileged: true


i deployed the above pod.yaml using the command kubectl create -f pod.yaml
since i had created two pod security policies one for restriction and one for privileges, cluster-admin role has access to both policies. so the above pod will launch fine with kubectl because cluster-admin role has access to the privileged policy( privileged: false also works because admin role has access to restriction policy as well). this situation happens only if either a pod created directly by kubectl not by kube control managers or a pod has access to the ""cluster-admin"" role via serviceaccount

in the case of a pod created by deployment/replicaset etc..first kubectl pass the control to the controller manager, then the controller will try to deploy the pod after validating the permissions(serviceaccount, podsecuritypolicies)

in the below deployment file, pod is trying to run with privileged mode. in my case, this deployment will fail because i already set the ""restricted"" policy as the default policy for all the serviceaccounts in the cluster. so no pod will able to run with privileged mode. if a pod needs to run with privileged mode, allow the serviceaccount of that pod to use the ""privileged"" policy

apiversion: apps/v1
kind: deployment
metadata:
  name: pause-deploy-privileged
  namespace: kube-system
  labels:
    app: pause
spec:
  replicas: 1
  selector:
    matchlabels:
      app: pause
  template:
    metadata:
      labels:
        app: pause
    spec:
      serviceaccountname: default
      containers:
      - name: pause
        image: k8s.gcr.io/pause
        securitycontext:
          privileged: true

"
64727542,retrieving unable to convert yaml to json error kubernetes yaml,"i have following yaml i am creating two pods in one yaml file.
  apiversion: v1
    kind: pod
    metadata:
       name: test
       labels:
          app: test
    spec:
       containers:
         - name: test
           image: test:latest
           command: [&quot;sleep&quot;]
           args: [&quot;infinity&quot;]

    kind: pod
    metadata:
       name: testing1
       labels:
          app: testing1
    spec:
       containers:
         - name: testing1
           image: testing1:latest
           command: [&quot;sleep&quot;]
           args: [&quot;infinity&quot;]

i am retrieving the following error. i checked the code on lint too, but i am unable to solve it.
error parsing pipeline.yaml: error converting yaml to json: line 27 could not find expected ':;

help is highly appreciated. thanks
",<kubernetes><yaml><kubernetes-pod>,64781653,3,"try this:
apiversion: v1
kind: pod
metadata:
   name: test
   labels:
      app: test
spec:
   containers:
   - name: test
     image: test:latest
     command: [&quot;sleep&quot;]
     args: [&quot;infinity&quot;]
---
apiversion: v1
kind: pod
metadata:
   name: testing1
   labels:
      app: testing1
spec:
   containers:
   - name: testing1
     image: testing1:latest
     command: [&quot;sleep&quot;]
     args: [&quot;infinity&quot;]

there was an indentation error under containers section and you have to separate the pod definition by ---  and also you had to add the apiversion.
"
70624472,cannot list or delete clusterrole or clusterrolebinding with a kubernetes serviceaccount,"i want to create a kubernetes cronjob that deletes resources (namespace, clusterrole, clusterrolebinding) that may be left over (initially, the criteria will be &quot;has label=something&quot; and &quot;is older than 30 minutes&quot;. (each namespace contains resources for a test run).
i created the cronjob, a serviceaccount, a clusterrole, a clusterrolebinding, and assigned the service account to the pod of the cronjob.
the cronjob uses an image that contains kubectl, and some script to select the correct resources.
my first draft looks like this:
---
apiversion: v1
kind: serviceaccount
metadata:
  name: my-app
  namespace: default
  labels:
    app: my-app

---
apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: my-app
  namespace: default
  labels:
    app: my-app
spec:
  concurrencypolicy: forbid
  schedule: &quot;*/1 * * * *&quot;
  jobtemplate:
    # job spec
    spec:
      template:
        # pod spec
        spec:
          serviceaccountname: my-app
          restartpolicy: never
          containers:
          - name: my-app
            image: image-with-kubectl
            env:
            - name: minimum_age_minutes
              value: '2'
            command: [sh, -c]
            args:
            # final script is more complex than this
            - |
              kubectl get namespaces
              kubectl get clusterroles
              kubectl get clusterrolebindings
              kubectl delete namespace,clusterrole,clusterrolebinding --all-namespaces --selector=bla=true

---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: my-app
  labels:
    app: my-app
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: my-app
subjects:
  - kind: serviceaccount
    name: my-app
    namespace: default
    apigroup: &quot;&quot;

---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  name: my-app
  labels:
    app: my-app
rules:
  - apigroups: [&quot;&quot;]
    resources:
      - namespaces
      - clusterroles
      - clusterrolebindings
    verbs: [list, delete]

the cronjob is able to list and delete namespaces, but not cluster roles or cluster role bindings. what am i missing?
(actually, i'm testing this with a job first, before moving to a cronjob):
name              status   age
cattle-system     active   16d
default           active   16d
fleet-system      active   16d
gitlab-runner     active   7d6h
ingress-nginx     active   16d
kube-node-lease   active   16d
kube-public       active   16d
kube-system       active   16d
security-scan     active   16d
error from server (forbidden): clusterroles.rbac.authorization.k8s.io is forbidden: user &quot;system:serviceaccount:default:my-app&quot; cannot list resource &quot;clusterroles&quot; in api group &quot;rbac.authorization.k8s.io&quot; at the cluster scope
error from server (forbidden): clusterrolebindings.rbac.authorization.k8s.io is forbidden: user &quot;system:serviceaccount:default:my-app&quot; cannot list resource &quot;clusterrolebindings&quot; in api group &quot;rbac.authorization.k8s.io&quot; at the cluster scope
error from server (forbidden): clusterroles.rbac.authorization.k8s.io is forbidden: user &quot;system:serviceaccount:default:my-app&quot; cannot list resource &quot;clusterroles&quot; in api group &quot;rbac.authorization.k8s.io&quot; at the cluster scope
error from server (forbidden): clusterrolebindings.rbac.authorization.k8s.io is forbidden: user &quot;system:serviceaccount:default:my-app&quot; cannot list resource &quot;clusterrolebindings&quot; in api group &quot;rbac.authorization.k8s.io&quot; at the cluster scope`

",<kubernetes><kubernetes-rbac><k8s-serviceaccount>,70625750,3,"you need to change your clusterrole like this :
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  name: my-app
  labels:
    app: my-app
rules:
  - apigroups: [&quot;&quot;]
    resources:
      - namespaces
    verbs: [list, delete]
  - apigroups: [&quot;rbac.authorization.k8s.io&quot;]
    resources:
      - clusterroles
      - clusterrolebindings
    verbs: [list, delete]

the ressources are now in the right apigroup
"
65180056,one ingress name having multiple rules,"i would like to use multiple rules under the one ingress name.
{
    &quot;kind&quot;: &quot;ingress&quot;,
    &quot;spec&quot;: {
        &quot;rules&quot;: [
            {
                &quot;host&quot;: &quot;gke-service-xyz.com&quot;,
                &quot;http&quot;: {
                    &quot;paths&quot;: [
                        {
                            &quot;path&quot;: &quot;/&quot;,
                            &quot;backend&quot;: {
                                &quot;servicename&quot;: &quot;gke-service&quot;,
                                &quot;serviceport&quot;: 8080
                            }
                        }
                    ]
                }
            },
            {
                &quot;host&quot;: &quot;gke-service1-xyz.com&quot;,
                &quot;http&quot;: {
                    &quot;paths&quot;: [
                        {
                            &quot;path&quot;: &quot;/&quot;,
                            &quot;backend&quot;: {
                                &quot;servicename&quot;: &quot;gke-service1&quot;,
                                &quot;serviceport&quot;: 8081
                            }
                        }
                    ]
                }
            }
        ]
    },
    &quot;apiversion&quot;: &quot;extensions/v1beta1&quot;,
    &quot;metadata&quot;: {
        &quot;annotations&quot;: {
            &quot;kubernetes.io/ingress.class&quot;: &quot;nginx&quot;,
            &quot;nginx.ingress.kubernetes.io/ssl-redirect&quot;: &quot;false&quot;
        },
        &quot;name&quot;: &quot;javaservice1-ingress&quot;
    },
    &quot;namespace&quot;: &quot;abc-namespace&quot;
}

yaml code
kind: ingress
spec:
  rules:
  - host: gke-service-xyz.com
    http:
      paths:
      - path: &quot;/&quot;
        backend:
          servicename: gke-service
          serviceport: 8080
  - host: gke-service1-xyz.com
    http:
      paths:
      - path: &quot;/&quot;
        backend:
          servicename: gke-service1
          serviceport: 8081
apiversion: extensions/v1beta1
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: 'false'
  name: javaservice1-ingress
namespace: abc-namespace

here the problem is the first ingress host is working fine but while fetching the second ingress host is not working and it is showing like, 503 service temporarily unavailable. i want both the hosts in a working state. is there any way to achieve the same?

ingress name: service-ingress.


gke-service-xyz.com
gke-service1-xyz.com

above points 1 and 2 are the ingress endpoints both should work but here only 1 is in working.
both the above yaml codes are not working.
",<python><kubernetes><kubernetes-ingress>,65180349,3,"try this -
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    kubernetes.io/ingress.class: nginx
  name: staging-ingress
spec:
  rules:
  - host: gke-service-xyz.com
    http:
      paths:
      - path: /(.*)
        backend:
          servicename: gke-service
          serviceport: 8080
      - path: /api/(.*)
        backend: 
          servicename: gke-service1
          serviceport: 8081


the question you are asking is something like that you want to have only one domain and use it to route to two services, hence you can use something like url rewrite for this
"
56261546,helm config-map with yaml file,"i am trying to access a file inside my helm templates as a config map, like below. i get an error as below. 

however, it works when my application.yml doesn't have nested objects (eg - name: test). any ideas on what i could be doing wrong?

config-map.yaml:

apiversion: v1
kind: configmap
metadata:
 name: {{ .release.name }}-configmap

data:
 {{.files.get application.yml}}


application.yml:

some-config:
 application:
   name: some-application-name


error:

*configmap in version v1"" cannot be handled as a configmap: v1.configmap.data: readstring: expects  or n, but found {, error found in #10 byte of ...|ication* 


",<kubernetes><kubernetes-helm>,56262584,3,"looks like you have an indentation issue on your application.yaml file. perhaps invalid yaml? if i try your very same files i get the following:

  helm template ./mychart -x templates/configmap.yaml
---
# source: mychart/templates/configmap.yaml
apiversion: v1
kind: configmap
metadata:
 name: release-name-configmap
data:
  some-config:
 application:
   name: some-application-name

"
67943904,define/change kubernetes ssh key file name in a yaml,"i have a secret:
apiversion: v1
kind: secret
metadata:
  name: secret-ssh-auth
type: kubernetes.io/ssh-auth
data:
  ssh-privatekey: |
          sevmte9pt09pt09pt09pt09pt09pcg==

and deployment:
apiversion: apps/v1
kind: deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerport: 80
        volumemounts:
          - name: secret-ssh-auth
            mountpath: /root/.ssh
      volumes:
      - name: secret-ssh-auth
        secret:
          secretname: secret-ssh-auth
          defaultmode: 0400

it creates a file with this path /root/.ssh/ssh-privatekey while i want to have /root/.ssh/id_rsa name instead.
i know we can solve it by running a kubectl command, but i want to handle it inside the yaml file.
so, how to do that by the yaml file?
",<kubernetes><ssh><kubernetes-secrets>,67946224,3,"based on the kubernetes documentation the ssh-privatekey key is mandatory, in this case, you can leave it empty via stringdata key, then define another one by data key like this:
apiversion: v1
kind: secret
metadata:
  name: secret-ssh-auth
type: kubernetes.io/ssh-auth
stringdata:
  ssh-privatekey: |
          -
data:
   id_rsa: |
          sevmte9pt09pt09pt09pt09pt09pcg==

"
56461806,how to configure ambassador to be able to map to multiple ports on a container,"i am using ambassador as the ingress controller for my kubernetes setup. i need to connect to multiple ports on my containers, for example, i have a rethinkdb container and i need to connect to port 8085 for its web-ui, port 28015 for rdb-api and port 29015 for adding nodes to rethinkdb and clustering.

i tried different configuration but they didn't work. the configurations that i tried:
1- this configuration works for the latest mapping which means if i replace 8085 mapping with 29015 and put it at the end i am able to access the web-ui but not other parts and so on.

getambassador.io/config: |
  ---
  apiversion: ambassador/v1
  kind: mapping
  name: rethinkdb_mapping
  prefix: /rethinkdb:28015/
  service: rethinkdb:28015
  labels:
    ambassador:
      - request_label:
        - rethinkdb:28015
  ---
  apiversion: ambassador/v1
  kind: mapping
  name: rethinkdb_mapping
  - prefix: /rethinkdb:8085/
    service: rethinkdb:8085
  labels:
    ambassador:
      - request_label:
        - rethinkdb:8085
  ---
  apiversion: ambassador/v1
  kind: mapping
  name: rethinkdb_mapping
  prefix: /rethinkdb:29015/
  service: rethinkdb:29015
  labels:
    ambassador:
      - request_label:
        - rethinkdb:29015


2- this one didn't work at all

getambassador.io/config: |
  ---
  apiversion: ambassador/v1
  kind: mapping
  name: rethinkdb_mapping
  - prefix: /rethinkdb:8085/
    service: rethinkdb:8085
 - prefix: /rethinkdb:29015/
   service: rethinkdb:29015
 - prefix: /rethinkdb:28015/
   service: rethinkdb:28015


how shall i configure ambassador so i can have access to all ports of my container?
",<kubernetes><load-balancing><reverse-proxy><kubernetes-ingress><ambassador>,56543288,3,"try to put different names of mappings like in example below:

apiversion: v1
kind: service
metadata:
  name: rethinkdb
  annotations:
    getambassador.io/config: |
      ---
      apiversion: ambassador/v1
      kind: mapping
      name: rethinkdb_mapping
      prefix: /rethinkdb:28015/
      service: rethinkdb:28015
      labels:
        ambassador:
          - request_label:
            - rethinkdb:28015
      ---
      apiversion: ambassador/v1
      kind: mapping
      name: rethinkdb_mapping1
      prefix: /rethinkdb:8085/
      service: rethinkdb:8085
      labels:
        ambassador:
          - request_label:
            - rethinkdb:8085
      ---
      apiversion: ambassador/v1
      kind: mapping
      name: rethinkdb_mapping2
      prefix: /rethinkdb:29015/
      service: rethinkdb:29015
      labels:
        ambassador:
          - request_label:
            - rethinkdb:29015
spec:
  type: clusterip
  clusterip: none


remember to put right name of service into service label inside mappings definition.

note on indents and   correct syntax.

i hope it helps.
"
68178304,helm - error converting yaml to json: yaml: line 29: mapping values are not allowed in this context,"defined labels in temp with top of the same deployment.yml file-
{{- define &quot;chart.labels&quot; }} 
  version: v1.0
  method: http
  internet: enabled
{{- end }}

i have deployment.yml file in template folder-
apiversion: apps/v1
kind: deployment
metadata:
  name: app1-deployment
  namespace: {{ .values.global.namespace }}
  labels:
    app: app1
    type: microservice1
spec:
  replicas: 3
  selector:
    matchlabels:
      app: app1
      type: microservice1
  strategy:
    type: {{ .values.global.strategytype }}
  template:
    metadata:
      labels:
        app: app1
        type: microservice1
        {{- template &quot;chart.labels&quot; }}

the two ways - one from the keyword template (last line of the below code)
and second one from the include keyword i am trying to call the template.
{{include &quot;chart.labels&quot; . | indent 8 }}


i am getting error ( when i am using keyword template to call
the template).


error: yaml parse error on chart/templates/deployment.yml: error
converting yaml to json: yaml: line 27: did not find expected key
helm.go:81: [debug] error converting yaml to json: yaml: line 27: did
not find expected key yaml parse error on
chart/templates/deployment.yml
helm.sh/helm/v3/pkg/releaseutil.(*manifestfile).sort
helm.sh/helm/v3/pkg/releaseutil/manifest_sorter.go:146 helm.sh/helm/v3/pkg/releaseutil.sortmanifests
helm.sh/helm/v3/pkg/releaseutil/manifest_sorter.go:106 helm.sh/helm/v3/pkg/action.(*configuration).renderresources
helm.sh/helm/v3/pkg/action/action.go:165 helm.sh/helm/v3/pkg/action.(*install).run
helm.sh/helm/v3/pkg/action/install.go:247


getting another error (when i use include keyword to call the
template)


error: yaml parse error on chart/templates/deployment.yml: error
converting yaml to json: yaml: line 29: mapping values are not allowed
in this context helm.go:81: [debug] error converting yaml to json:
yaml: line 29: mapping values are not allowed in this context yaml
parse error on chart/templates/deployment.yml
helm.sh/helm/v3/pkg/releaseutil.(*manifestfile).sort
helm.sh/helm/v3/pkg/releaseutil/manifest_sorter.go:146 helm.sh/helm/v3/pkg/releaseutil.sortmanifests
helm.sh/helm/v3/pkg/releaseutil/manifest_sorter.go:106 helm.sh/helm/v3/pkg/action.(*configuration).renderresources
helm.sh/helm/v3/pkg/action/action.go:165 helm.sh/helm/v3/pkg/action.(*install).run
helm.sh/helm/v3/pkg/action/install.go:247 main.runinstall

what am i missing here ?
",<kubernetes><kubernetes-helm>,68178507,3,"you need to follow sane indentation. you have:
{{- define &quot;chart.labels&quot; }} 
  version: v1.0
  method: http
  internet: enabled
{{- end }}

note there is no double space in chart.labels definition below.
the below works:
{{- define &quot;chart.labels&quot; }} 
version: v1.0
method: http
internet: enabled
{{- end }}

apiversion: apps/v1
kind: deployment
metadata:
  name: {{ include &quot;test.fullname&quot; . }}
  labels:
    {{- include &quot;test.labels&quot; . | nindent 4 }}
spec:
{{- if not .values.autoscaling.enabled }}
  replicas: {{ .values.replicacount }}
{{- end }}
  selector:
    matchlabels:
      {{- include &quot;test.selectorlabels&quot; . | nindent 6 }}
  template:
    metadata:
    {{- with .values.podannotations }}
      annotations:
        {{- toyaml . | nindent 8 }}
    {{- end }}
      labels:
      {{- include &quot;test.selectorlabels&quot; . | nindent 8 }}
      {{include &quot;chart.labels&quot; . | nindent 8 }}

edit: or only change the nindent to match chart.labels in the template meta as below:
{{include &quot;chart.labels&quot; . | nindent 6 }}

"
55308605,only the creator user can manage aws kubernetes cluster (eks) from kubectl?,"we have two clusters, named:


mycluster (created by me)
othercluster (not created by me)


where ""me"" is my own aws iam user.

i am able to manage the cluster i created, using kubectl:

&gt;&gt;&gt; aws eks update-kubeconfig --name mycluster profile myuser
&gt;&gt;&gt; kubectl get svc
name         type        cluster-ip   external-ip   port(s)   age
kubernetes   clusterip   172.20.0.1   &lt;none&gt;        443/tcp   59d


but, i cannot manage the othercluster cluster (that was not created by me):

&gt;&gt;&gt; aws eks update-kubeconfig --name othercluster --profile myuser
&gt;&gt;&gt; kubectl get svc
name         type        cluster-ip   external-ip   port(s)   age
error: the server doesn't have a resource type ""svc""


after reading the feedback of some people experiencing the same issue in this github issue, i tried doing this under the context of the user who originally created the ""othercluster"".

i accomplished this by editing ~/.kube/config, adding a aws_profile value at users.user.env. the profile represents the user who created the cluster.

~/.kube/config:


users
- name: othercluster
  user:
    exec:
      apiversion: client.authentication.k8s.io/v1alpha1
      args:
      - token
      - -i
      - othercluster
      command: aws-iam-authenticator
      env:
      - name: aws_profile
        value: other_user_profile



this worked:

# ~/.kube/config is currently pointing to othercluster

&gt;&gt;&gt; kubectl get svc 
name         type        cluster-ip   external-ip   port(s)   age
kubernetes   clusterip   172.20.0.1   &lt;none&gt;        443/tcp   1d


it is obviously not ideal for me to impersonate another person when i am managing the cluster. i would prefer to grant my own user access to manage the cluster via kubectl.
is there any way i can grant permission to manage the cluster to a user other than the original creator? this seems overly restrictive
",<amazon-web-services><kubernetes><amazon-eks>,55314570,3,"when an amazon eks cluster is created, the iam entity (user or role) that creates the cluster is added to the kubernetes rbac authorization table as the administrator. initially, only that iam user can make calls to the kubernetes api server using kubectl. 

to grant additional aws users the ability to interact with your cluster, you must edit the aws-auth configmap within kubernetes, adding a new mapusers entry for your configmap. this eks doc covers all the process.


  to add an iam user: add the user details to the mapusers section of
  the configmap, under data. add this section if it does not already
  exist in the file. each entry supports the following parameters:
  
  
  userarn: the arn of the iam user to add.
  username: the user name within kubernetes to map to the iam user. by    default, the user name is the arn of the iam user.
  groups: a list of groups within kubernetes to which the user is    mapped to. for more information, see default roles and role bindings
  in the kubernetes documentation.
  


example:

apiversion: v1
data:
  maproles: |
    - rolearn: arn:aws:iam::555555555555:role/devel-worker-nodes-nodeinstancerole-74rf4ubdukl6
      username: system:node:{{ec2privatednsname}}
      groups:
        - system:bootstrappers
        - system:nodes
  mapusers: |
    - userarn: arn:aws:iam::555555555555:user/my-new-admin-user
      username: my-new-admin-user
      groups:
        - system:masters

"
55231411,k8s ingress with 2 domains both listening on port 80,"i am trying to replicate a name based virtual hosting with two docker images in one deployment. unfortunately i am only able to get 1 running due to a port conflict:

2019/03/19 20:37:52 [err] error starting server: listen tcp :5678: bind: address already in use


is it really not possible to have two images listening on the same port as part of the same deployment? or am i going wrong elsewhere?

minimal example adapted from here

# set up ingress
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml

# set up load balancer
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/cloud-generic.yaml

# spin up two containers in one deployment, same container port
kubectl apply -f test.yaml


test.yaml:

apiversion: v1
kind: service
metadata:
  name: echo1
spec:
  ports:
    - port: 80
      targetport: 5678
  selector:
    app: echo1
---
apiversion: v1
kind: service
metadata:
  name: echo2
spec:
  ports:
    - port: 80
      targetport: 5678
  selector:
    app: echo2
---
apiversion: apps/v1
kind: deployment
metadata:
  name: echo12
spec:
  selector:
    matchlabels:
      app: echo12
  replicas: 1
  template:
    metadata:
      labels:
        app: echo12
    spec:
      containers:
        - name: echo1
          image: hashicorp/http-echo
          args:
            - ""-text=echo1""
          ports:
            - containerport: 5678
        - name: echo2
          image: hashicorp/http-echo
          args:
            - ""-text=echo2""
          ports:
            - containerport: 5678
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: echo-ingress
spec:
  rules:
    - host: echo1.example.com
      http:
        paths:
          - backend:
              servicename: echo1
              serviceport: 80
    - host: echo2.example.com
      http:
        paths:
          - backend:
              servicename: echo2
              serviceport: 80


update:
if i add a separate deployment it works. is this by design or is there any way that i can achieve this in one deployment (reason: i'd like to be able to reset all deployed domains at once)?
",<kubernetes><kubernetes-ingress>,55284518,3,"problem 1: creating two different service backends in one pod of one deployment. this is not what the pods are designed for. if you want to expose multiple services, you should have a pod (at least) to back each service. deployments wrap around the pod by allowing you to define replication and liveliness options. in your case, you should have one deployment (which creates one or multiple pods that will respond to one echo request) for its corresponding service. 

problem 2: you are not linking your services to your backends properly. the service clearly is trying to select a label app=echo or app=echo2. in your deployment, your app=echo12. consequently, the service simply won't be able to find any active endpoints. 

to address the above problems, try this below: 

apiversion: v1
kind: service
metadata:
  name: echo1
spec:
  ports:
    - port: 80
      targetport: 5678
  selector:
    app: echo1
---
apiversion: v1
kind: service
metadata:
  name: echo2
spec:
  ports:
    - port: 80
      targetport: 5678
  selector:
    app: echo2
---
apiversion: apps/v1
kind: deployment
metadata:
  name: echo1
spec:
  selector:
    matchlabels:
      app: echo1
  replicas: 1
  template:
    metadata:
      labels:
        app: echo1
    spec:
      containers:
        - name: echo1
          image: hashicorp/http-echo
          args:
            - ""-text=echo1""
          ports:
            - containerport: 5678
---
apiversion: apps/v1
kind: deployment
metadata:
  name: echo2
spec:
  selector:
    matchlabels:
      app: echo2
  replicas: 1
  template:
    metadata:
      labels:
        app: echo2
    spec:
      containers:
        - name: echo2
          image: hashicorp/http-echo
          args:
            - ""-text=echo2""
          ports:
            - containerport: 5678
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: echo-ingress
spec:
  rules:
    - host: echo1.example.com
      http:
        paths:
          - backend:
              servicename: echo1
              serviceport: 80
    - host: echo2.example.com
      http:
        paths:
          - backend:
              servicename: echo2
              serviceport: 80


i have tested the above in my own cluster and verified that it is working (with different ingress urls of course). hope this helps!
"
69451829,aws-load-balancer-controller error: cannot get the logs from *v1.ingress: selector for *v1.ingress not implemented,"i get this error when trying to get alb logs:
root@b75651fde30e:/apps/tekton/deployment# kubectl logs -f ingress/tekton-dashboard-alb-dev
error: cannot get the logs from *v1.ingress: selector for *v1.ingress not implemented

the load balancer yaml:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: tekton-dashboard-alb-dev
  namespace: tekton-pipelines
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/load-balancer-name: tekton-dashboard-alb-dev
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/backend-protocol: http
    alb.ingress.kubernetes.io/tags: &quot;cost=swiftalk,vantaonwer=foo@bar.com,vantanonprod=true,vantadescription=alb ingress for tekton dashboard,vantacontainsuserdata=false,vantauserdatastored=none&quot;
    alb.ingress.kubernetes.io/security-groups: sg-034ca9846b81fd721
    kubectl.kubernetes.io/last-applied-configuration: &quot;&quot;    
spec:
  defaultbackend:
    service:
      name: tekton-dashboard
      port:
        number: 9097

note: sg-034ca9846b81fd721 restricts access to our vpn cidrs
ingress is up as revealed from:
root@b75651fde30e:/apps/tekton/deployment# kubectl get ingress
name                       class    hosts   address                                                         ports   age
tekton-dashboard-alb-dev   &lt;none&gt;   *       tekton-dashboard-alb-dev-81361211.us-east-1.elb.amazonaws.com   80      103m
root@b75651fde30e:/apps/tekton/deployment# kubectl describe ingress/tekton-dashboard-alb-dev
name:             tekton-dashboard-alb-dev
namespace:        tekton-pipelines
address:          tekton-dashboard-alb-dev-81361211.us-east-1.elb.amazonaws.com
default backend:  tekton-dashboard:9097 (172.18.5.248:9097)
rules:
  host        path  backends
  ----        ----  --------
  *           *     tekton-dashboard:9097 (172.18.5.248:9097)
annotations:  alb.ingress.kubernetes.io/backend-protocol: http
              alb.ingress.kubernetes.io/load-balancer-name: tekton-dashboard-alb-dev
              alb.ingress.kubernetes.io/scheme: internet-facing
              alb.ingress.kubernetes.io/security-groups: sg-034ca9846b81fd721
              alb.ingress.kubernetes.io/tags:
                cost=swiftalk,vantaonwer=swiftalkdevteam@digite.com,vantanonprod=true,vantadescription=alb ingress for swiftalk web microservices,vantacon...
              alb.ingress.kubernetes.io/target-type: ip
              kubernetes.io/ingress.class: alb
events:       &lt;none&gt;

",<kubernetes><kubernetes-ingress><amazon-eks>,69522691,3,"the error you received means that the logs for your object are not implemented. it looks like you're trying to get logs from the wrong place.
i am not able to reproduce your problem on aws, but i tried to do it on gcp and the situation was very similar. you cannot get logs from ingress/tekton-dashboard-alb-dev, and this is normal bahaviour. if you want to get logs of your alb, you have to find the appropriate pod and then extract the logs from it. let me show you how i did it on gcp. the commands are the same, but the pod names will be different.
first i have executed:
kubectl get pods --all-namespaces

output:
namespace       name                                                     ready   status    restarts   age
ingress-nginx   ingress-nginx-controller-57cb5bf694-722ml                1/1     running   0          18d

-----

and many other not related pods in other namespaces

you can find directly your pod with command:
kubectl get pods -n ingress-nginx

output:
name                                        ready   status    restarts   age
ingress-nginx-controller-57cb5bf694-722ml   1/1     running   0          18d

now you can get logs from ingress controller by command:
kubectl logs -n ingress-nginx ingress-nginx-controller-57cb5bf694-722ml

in your situation:
kubectl logs -n &lt;your namespace&gt; &lt;your ingress controller pod&gt;

the output should be similar to this:
-------------------------------------------------------------------------------
nginx ingress controller
  release:       v0.46.0
  build:         6348dde672588d5495f70ec77257c230dc8da134
  repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.19.6

-------------------------------------------------------------------------------

i0923 05:26:20.053561       8 flags.go:208] &quot;watching for ingress&quot; class=&quot;nginx&quot;
w0923 05:26:20.053753       8 flags.go:213] ingresses with an empty class will also be processed by this ingress controller
w0923 05:26:20.054185       8 client_config.go:614] neither --kubeconfig nor --master was specified.  using the inclusterconfig.  this might not work.
i0923 05:26:20.054502       8 main.go:241] &quot;creating api client&quot; host=&quot;https://10.16.0.1:443&quot;
i0923 05:26:20.069482       8 main.go:285] &quot;running in kubernetes cluster&quot; major=&quot;1&quot; minor=&quot;20+&quot; git=&quot;v1.20.9-gke.1001&quot; state=&quot;clean&quot; commit=&quot;1fe18c314ed577f6047d2712a9d1c8e498e22381&quot; platform=&quot;linux/amd64&quot;
i0923 05:26:20.842645       8 main.go:105] &quot;ssl fake certificate created&quot; file=&quot;/etc/ingress-controller/ssl/default-fake-certificate.pem&quot;
i0923 05:26:20.846132       8 main.go:115] &quot;enabling new ingress features available since kubernetes v1.18&quot;
w0923 05:26:20.849470       8 main.go:127] no ingressclass resource with name nginx found. only annotation will be used.
i0923 05:26:20.866252       8 ssl.go:532] &quot;loading tls certificate&quot; path=&quot;/usr/local/certificates/cert&quot; key=&quot;/usr/local/certificates/key&quot;
i0923 05:26:20.917594       8 nginx.go:254] &quot;starting nginx ingress controller&quot;
i0923 05:26:20.942084       8 event.go:282] event(v1.objectreference{kind:&quot;configmap&quot;, namespace:&quot;ingress-nginx&quot;, name:&quot;ingress-nginx-controller&quot;, uid:&quot;42dc476e-3c5c-4cc9-a6a4-266edecb2a4b&quot;, apiversion:&quot;v1&quot;, resourceversion:&quot;5600&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'create' configmap ingress-nginx/ingress-nginx-controller
i0923 05:26:22.118459       8 nginx.go:296] &quot;starting nginx process&quot;
i0923 05:26:22.118657       8 leaderelection.go:243] attempting to acquire leader lease ingress-nginx/ingress-controller-leader-nginx...
i0923 05:26:22.119481       8 nginx.go:316] &quot;starting validation webhook&quot; address=&quot;:8443&quot; certpath=&quot;/usr/local/certificates/cert&quot; keypath=&quot;/usr/local/certificates/key&quot;
i0923 05:26:22.120266       8 controller.go:146] &quot;configuration changes detected, backend reload required&quot;
i0923 05:26:22.126350       8 status.go:84] &quot;new leader elected&quot; identity=&quot;ingress-nginx-controller-57cb5bf694-8c9tn&quot;
i0923 05:26:22.214194       8 controller.go:163] &quot;backend successfully reloaded&quot;
i0923 05:26:22.214838       8 controller.go:174] &quot;initial sync, sleeping for 1 second&quot;
i0923 05:26:22.215234       8 event.go:282] event(v1.objectreference{kind:&quot;pod&quot;, namespace:&quot;ingress-nginx&quot;, name:&quot;ingress-nginx-controller-57cb5bf694-722ml&quot;, uid:&quot;b9672f3c-ecdf-473e-80f5-529bbc5bc4e5&quot;, apiversion:&quot;v1&quot;, resourceversion:&quot;59016530&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'reload' nginx reload triggered due to a change in configuration
i0923 05:27:00.596169       8 leaderelection.go:253] successfully acquired lease ingress-nginx/ingress-controller-leader-nginx
i0923 05:27:00.596305       8 status.go:84] &quot;new leader elected&quot; identity=&quot;ingress-nginx-controller-57cb5bf694-722ml&quot;
157.230.143.29 - - [23/sep/2021:08:28:25 +0000] &quot;get / http/1.1&quot; 400 248 &quot;-&quot; &quot;mozilla/5.0 (windows nt 6.1; wow64; rv:54.0) gecko/20100101 firefox/70.0&quot; 165 0.000 [] [] - - - - d47be1e37ea504aca93d59acc7d36a2b
157.230.143.29 - - [23/sep/2021:08:28:26 +0000] &quot;\x00\xffk\x00\x00\x00\xe2\x00 \x00\x00\x00\x0e2o\xaac\xe92g\xc2w'\x17+\x1d\xd9\xc1\xf3,kn\x17\x14&quot; 400 150 &quot;-&quot; &quot;-&quot; 0 0.076 [] [] - - - - c497187f4945f8e9e7fa84d503198e85
157.230.143.29 - - [23/sep/2021:08:28:26 +0000] &quot;\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00&quot; 400 150 &quot;-&quot; &quot;-&quot; 0 0.138 [] [] - - - - 4067a2d34d0c1f2db7ffbfc143540c1a
167.71.216.70 - - [23/sep/2021:12:02:23 +0000] &quot;\x16\x03\x01\x01\xfc\x01\x00\x01\xf8\x03\x03\xdb\xbbo*k\xae\x9a&amp;\x8a\x9b)\x1b\xb8\xed3\xb7\xe16n\xea\xfcs\x22\x14v\xf7}\xc8&amp;ga\xda\x00\x01&lt;\xcc\x14\xcc\x13\xcc\x15\xc00\xc0,\xc0(\xc0$\xc0\x14\xc0&quot; 400 150 &quot;-&quot; &quot;-&quot; 0 0.300 [] [] - - - - ff6908bb17b0da020331416773b928b5
167.71.216.70 - - [23/sep/2021:12:02:23 +0000] &quot;\x16\x03\x01\x01\xfc\x01\x00\x01\xf8\x03\x03a\xbf\xfb\xc1'\x03s\x83d\x5cn$\xab\xe1\xa6%\x93g-}\xd1c\xb2\xb0e\x8c\x8f\xa8q-\xf7$\x00\x01&lt;\xcc\x14\xcc\x13\xcc\x15\xc00\xc0,\xc0(\xc0$\xc0\x14\xc0&quot; 400 150 &quot;-&quot; &quot;-&quot; 0 0.307 [] [] - - - - fee3a478240e630e6983c60d1d510f52
66.240.205.34 - - [23/sep/2021:12:04:11 +0000] &quot;145.ll|'|'|sgfjs2vkx0q0otkwnji3|'|'|win-jnapier0859|'|'|jnapier|'|'|19-02-01|'|'||'|'|win 7 professional sp1 x64|'|'|no|'|'|0.7d|'|'|..|'|'|aa==|'|'|112.inf|'|'|sgfjs2vkdqoxotiumty4ljkyljiymjo1ntuydqpezxnrdg9wdqpjbgllbnrhlmv4zq0krmfsc2unckzhbhnldqpucnvldqpgywxzzq==12.act|'|'|aa==&quot; 400 150 &quot;-&quot; &quot;-&quot; 0 0.086 [] [] - - - - 365d42d67e7378359b95c71a8d8ce983
147.182.148.98 - - [23/sep/2021:12:04:17 +0000] &quot;\x16\x03\x01\x01\xfc\x01\x00\x01\xf8\x03\x03\xaba\xf4\xd5\xb7\x95\x85[.v\xdb\xd1\x1b\x04\xe7\xb4\xb8\x92\x82\xec\xcc\xddr\xb7/\xbd\x93/\xd0f4\xb3\x00\x01&lt;\xcc\x14\xcc\x13\xcc\x15\xc00\xc0,\xc0(\xc0$\xc0\x14\xc0&quot; 400 150 &quot;-&quot; &quot;-&quot; 0 0.152 [] [] - - - - 858c2ad7535de95c84dd0899708a3801
164.90.203.66 - - [23/sep/2021:12:08:19 +0000] &quot;\x16\x03\x01\x01\xfc\x01\x00\x01\xf8\x03\x03\x93\x81+_\x95\xfa\xeaj\xa7\x80\x15 \x179\xd7\x92\xae\xa9i+\x9d`\xa07:\xd2\x22\xb3\xc6\xf3\x22g\x00\x01&lt;\xcc\x14\xcc\x13\xcc\x15\xc00\xc0,\xc0(\xc0$\xc0\x14\xc0&quot; 400 150 &quot;-&quot; &quot;-&quot; 0 0.237 [] [] - - - - 799487dd8ec874532dcfa7dad1c02a27
164.90.203.66 - - [23/sep/2021:12:08:20 +0000] &quot;\x16\x03\x01\x01\xfc\x01\x00\x01\xf8\x03\x03\xb8\x22\xcb&gt;1\xbem\xd4\x92\x95\xef\x1c0\xb5&amp;\x1e[\xc5\xc8\x1e2\x07\x1c\x02\xa1&lt;\xd2\xaa\x91f\x00\xc6\x00\x01&lt;\xcc\x14\xcc\x13\xcc\x15\xc00\xc0,\xc0(\xc0$\xc0\x14\xc0&quot; 400 150 &quot;-&quot; &quot;-&quot; 0 0.193 [] [] - - - - 4604513713d4b9fb5a7199b7980fa7f2
164.90.203.66 - - [23/sep/2021:12:16:10 +0000] &quot;\x16\x03\x01\x01\xfc\x01\x00\x01\xf8\x03\x03[\x16\x02\x94\x98\x17\xca\xb5!\xc11@\x08\xd9\x89re\x970\xc2\xdf\xff\xebh\xa0i\x9ee%.\x07{\x00\x01&lt;\xcc\x14\xcc\x13\xcc\x15\xc00\xc0,\xc0(\xc0$\xc0\x14\xc0&quot; 400 150 &quot;-&quot; &quot;-&quot; 0 0.116 [] [] - - - - 23019f0886a1c30a78092753f6828e74
77.247.108.81 - - [23/sep/2021:14:52:51 +0000] &quot;get /admin/config.php http/1.1&quot; 400 248 &quot;-&quot; &quot;python-requests/2.26.0&quot; 164 0.000 [] [] - - - - 04630dbf3d0ff4a4b7138dbc899080e5
209.141.48.211 - - [23/sep/2021:16:17:46 +0000] &quot;&quot; 400 0 &quot;-&quot; &quot;-&quot; 0 0.057 [] [] - - - - 3c623b242909a99e18178ec10a814d7b
209.141.62.185 - - [23/sep/2021:18:13:11 +0000] &quot;get /config/getuser?index=0 http/1.1&quot; 400 248 &quot;-&quot; &quot;mozilla/5.0 (x11; ubuntu; linux x86_64; rv:76.0) gecko/20100101 firefox/76.0&quot; 353 0.000 [] [] - - - - 2640cf06912615a7600e814dc893884b
125.64.94.138 - - [23/sep/2021:19:49:08 +0000] &quot;get / http/1.0&quot; 400 248 &quot;-&quot; &quot;-&quot; 18 0.000 [] [] - - - - b633636176888bc3b7f6230f691e0724
2021/09/23 19:49:20 [crit] 39#39: *424525 ssl_do_handshake() failed (ssl: error:141cf06c:ssl routines:tls_parse_ctos_key_share:bad key share) while ssl handshaking, client: 125.64.94.138, server: 0.0.0.0:443
125.64.94.138 - - [23/sep/2021:19:49:21 +0000] &quot;get /favicon.ico http/1.1&quot; 400 650 &quot;-&quot; &quot;mozilla/5.0 (x11; linux x86_64) applewebkit/537.36 (khtml, like gecko) chrome/86.0.4 240.111 safari/537.36&quot; 197 0.000 [] [] - - - - ede08c8fb12e8ebaf3adcbd2b7ea5fd5
125.64.94.138 - - [23/sep/2021:19:49:22 +0000] &quot;get /robots.txt http/1.1&quot; 400 650 &quot;-&quot; &quot;mozilla/5.0 (x11; linux x86_64) applewebkit/537.36 (khtml, like gecko) chrome/86.0.4 240.111 safari/537.36&quot; 196 0.000 [] [] - - - - fae50b56a11600abc84078106ba4b008
125.64.94.138 - - [23/sep/2021:19:49:22 +0000] &quot;get /.well-known/security.txt http/1.1&quot; 400 650 &quot;-&quot; &quot;mozilla/5.0 (x11; linux x86_64) applewebkit/537.36 (khtml, like gecko) chrome/86.0.4 240.111 safari/537.36&quot; 210 0.000 [] [] - - - - ad82bcac7d7d6cd9aa2d044d80bb719d
87.251.75.145 - - [23/sep/2021:21:29:10 +0000] &quot;\x03\x00\x00/*\xe0\x00\x00\x00\x00\x00cookie: mstshash=administr&quot; 400 150 &quot;-&quot; &quot;-&quot; 0 0.180 [] [] - - - - 8c2b62bcdf26ac1592202d0940fc30b8
167.71.102.181 - - [23/sep/2021:21:54:58 +0000] &quot;\x00\x0e8k\xa3\xaae\xbcn\x14\x1b\x00\x00\x00\x00\x00&quot; 400 150 &quot;-&quot; &quot;-&quot; 0 0.027 [] [] - - - - 65b8ee37a2c6bf8368843e4db3b90b2a
185.156.72.27 - - [23/sep/2021:22:03:55 +0000] &quot;\x03\x00\x00/*\xe0\x00\x00\x00\x00\x00cookie: mstshash=administr&quot; 400 150 &quot;-&quot; &quot;-&quot; 0 0.139 [] [] - - - - 92c6ad2d71b961bf7de4e345ff69da10
185.156.72.27 - - [23/sep/2021:22:03:55 +0000] &quot;\x03\x00\x00/*\xe0\x00\x00\x00\x00\x00cookie: mstshash=administr&quot; 400 150 &quot;-&quot; &quot;-&quot; 0 0.140 [] [] - - - - fe0424f8ecf9afc1d0154bbca2382d13
34.86.35.21 - - [23/sep/2021:22:54:41 +0000] &quot;\x16\x03\x01\x00\xe3\x01\x00\x00\xdf\x03\x03\x0f[\xa9\x18\x15\xd3@4\x7f\x7f\x98'\xa9(\x8f\xe7\xccdd\xf9\xff`\xe3\xce\x9at\x05\x97\x05\xb1\xc3}\x00\x00h\xcc\x14\xcc\x13\xc0/\xc0+\xc00\xc0,\xc0\x11\xc0\x07\xc0'\xc0#\xc0\x13\xc0\x09\xc0(\xc0$\xc0\x14\xc0&quot; 400 150 &quot;-&quot; &quot;-&quot; 0 2.039 [] [] - - - - c09d38bf2cd925dac4d9e5d5cb843ece
2021/09/24 02:41:15 [crit] 40#40: *627091 ssl_do_handshake() failed (ssl: error:141cf06c:ssl routines:tls_parse_ctos_key_share:bad key share) while ssl handshaking, client: 184.105.247.252, server: 0.0.0.0:443
61.219.11.151 - - [24/sep/2021:03:40:51 +0000] &quot;dn\x93\xb9\xe6\xbcl\xb6\x92\x84:\xd7\x03\xf1n\xb9\xc5;\x90\xc2\xc6\xba\xe1i-\x22\xdds\xba\x1fgc:\xb1\xa7\x80+\x00\x00\x00\x00%\xfdk:\xaaw.|j\xb2\xb5\xf5'\xa5l\xd3v(\xb7\x01%(csk8b\xce\x9a\xd0z\xc7\x13\xad&quot; 400 150 &quot;-&quot; &quot;-&quot; 0 0.203 [] [] - - - - 190d00221eefc869b5938ab6380f835a
46.101.155.106 - - [24/sep/2021:04:56:37 +0000] &quot;head / http/1.0&quot; 400 0 &quot;-&quot; &quot;-&quot; 17 0.000 [] [] - - - - e8c108201c37d7457e4578cf68feacf8
46.101.155.106 - - [24/sep/2021:04:56:38 +0000] &quot;get /system_api.php http/1.1&quot; 400 650 &quot;-&quot; &quot;mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/74.0.3729.169 safari/537.36&quot; 255 0.000 [] [] - - - - b3032f9a9b3f4f367bdee6692daeb05c
46.101.155.106 - - [24/sep/2021:04:56:39 +0000] &quot;get /c/version.js http/1.1&quot; 400 650 &quot;-&quot; &quot;mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/74.0.3729.169 safari/537.36&quot; 253 0.000 [] [] - - - - 9104ab72a0232caf6ff98da57d325144
46.101.155.106 - - [24/sep/2021:04:56:40 +0000] &quot;get /streaming/clients_live.php http/1.1&quot; 400 650 &quot;-&quot; &quot;mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/74.0.3729.169 safari/537.36&quot; 267 0.000 [] [] - - - - 341cbb6cf424b348bf8b788f79373b8d
46.101.155.106 - - [24/sep/2021:04:56:41 +0000] &quot;get /stalker_portal/c/version.js http/1.1&quot; 400 650 &quot;-&quot; &quot;mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/74.0.3729.169 safari/537.36&quot; 268 0.000 [] [] - - - - 9954fd805fa092595057dbf83511bd92
46.101.155.106 - - [24/sep/2021:04:56:42 +0000] &quot;get /stream/live.php http/1.1&quot; 400 248 &quot;-&quot; &quot;alexamediaplayer/2.1.4676.0 (linux;android 5.1.1) exoplayerlib/1.5.9&quot; 209 0.000 [] [] - - - - 3c9409419c1ec59dfc08c10cc3eb6eef

"
69466546,"warning: networking.k8s.io/v1beta1 ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 ingress","my ingress.yml file is bellow
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: example-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-body-size: &quot;0&quot;
    nginx.ingress.kubernetes.io/proxy-read-timeout: &quot;600&quot;
    nginx.ingress.kubernetes.io/proxy-send-timeout: &quot;600&quot;
    kubernetes.io/tls-acme: &quot;true&quot;
    cert-manager.io/cluster-issuer: &quot;example-issuer&quot;
spec:
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          servicename: example-service
          serviceport: http
        path: /                 
  tls:
  - secretname: example-tls-cert
    hosts:
      - example.com

after changing apiversion: networking.k8s.io/v1beta1 to networking.k8s.io/v1 getting bellow error.
error validating data: [validationerror(ingress.spec.rules[0].http.paths[0].backend): unknown field &quot;servicename&quot; in io.k8s.api.networking.v1.ingressbackend, validationerror(ingress.spec.rules[0].http.paths[0].backend)
",<kubernetes><kubernetes-ingress>,69466670,3,"try bellow
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: example-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-body-size: &quot;0&quot;
    nginx.ingress.kubernetes.io/proxy-read-timeout: &quot;600&quot;
    nginx.ingress.kubernetes.io/proxy-send-timeout: &quot;600&quot;
    kubernetes.io/tls-acme: &quot;true&quot;
    cert-manager.io/cluster-issuer: &quot;example-issuer&quot;
spec:
  rules:
  - host: example.com
    http:
      paths:
      - pathtype: prefix
        path: /
        backend:
          service:
            name: example-service
            port:
              number: 80
  tls:
   - secretname: example-tls-cert
     hosts:
       - example.com

"
68266143,kubernetes limit ranges override,"let's say i set the following limit ranges to namespace x:
apiversion: v1
kind: limitrange
metadata:
  name: limit-range
spec:
  limits:
  - default:
     memory: 1gi
     cpu: 0.5
    defaultrequest:
     memory: 256mi
     cpu: 0.2
    type: container

these limits are sufficient for most pods in namespace x. some pods need more resources but a pod requesting more than default.memory, default.cpu will be rejected.
my question is, is there any way (in manifest or otherwise) to override these limits such that the pod can request more than the limit set to the namespace? i know it kinds beats the purpose of limit ranges but i'm still wondering if there's a way to do it.
",<kubernetes><kubernetes-pod>,68267667,3,"in your example, you do not limit your memory/cpu to a minimum/maximum of memory/cpu. you only set &quot;defaults&quot; to every pod which is created. with your given limitrange, you can still override custom limits/requests in the deployment of your pod.
if you would like to set a minimum/maximum you have to add something like this to your limitrange:
apiversion: v1
kind: limitrange
metadata:
  name: cpu-min-max-demo-lr
spec:
  limits:
  - max:
      cpu: &quot;800m&quot;
    min:
      cpu: &quot;200m&quot;
    type: container

https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/#create-a-limitrange-and-a-pod
"
61188984,how to use same labels in a deployment?,"i'm trying to pass same label to a deployment, both deployments have different image and environment variables. i'm using the same label so i can group the metrics together. 

but the deployment is failing. can someone please point me a workaround or is it because of the api version i'm using?

deployment1:

---
apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: stg-postgres-exporter-pgauth
  namespace: prometheus-exporters
spec:
  replicas: 1
  template:
    metadata:
      labels:
        db: foo
      annotations:
        prometheus.io/scrape: ""true""
        prometheus.io/port: ""9187""
        prometheus.io/job_name: ""postgres-exporter""
    spec:
      containers:
        - name: stg-rds-exporter
          image: wrouesnel/postgres_exporter:v0.8.0
          ....


deployment2:

---
apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: stg-rds-exporter-pgauth
  namespace: prometheus-exporters
spec:
  replicas: 1
  template:
    metadata:
      labels:
        db: foo
      annotations:
        prometheus.io/scrape: ""true""
        prometheus.io/port: ""9042""
        prometheus.io/job_name: ""rds-exporter""
        prometheus.io/path: ""/basic""
    spec:
      containers:
        - name: stg-rds-exporter-pgauth
          image: hbermu/rds_exporter:latest 
          ....


error:

15:27:39 the deployment ""stg-rds-exporter-pgauth"" is invalid: spec.template.metadata.labels: invalid value: map[string]string{""db"":""foo""}: selector does not match template labels

kubectl version:

client version: version.info{major:""1"", minor:""16"", gitversion:""v1.16.1"", gitcommit:""d647ddbd755faf07169599a625faf302ffc34458"", gittreestate:""clean"", builddate:""2019-10-02t23:49:20z"", goversion:""go1.12.9"", compiler:""gc"", platform:""darwin/amd64""}
server version: version.info{major:""1"", minor:""14+"", gitversion:""v1.14.9-eks-502bfb"", gitcommit:""502bfb383169b124d87848f89e17a04b9fc1f6f0"", gittreestate:""clean"", builddate:""2020-02-07t01:31:02z"", goversion:""go1.12.12"", compiler:""gc"", platform:""linux/amd64""}

",<kubernetes><amazon-eks>,61189372,3,"kubernetes uses the labels and selectors to control the replicas of your deployments, check the example below available in k8s doc: 

apiversion: apps/v1
kind: deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerport: 80


you have the selector matchlabels:

selector:
    matchlabels:
      app: nginx


and the template labels:

template:
    metadata:
      labels:
        app: nginx


they have to match, and that's why your deployment is failing.

kubernetes uses the labels to control the replicas of your deployment, so i'd recommend adding a second label to your deployments. that would make the selector unique, but you would still be able to query the entity by one of its labels.
"
69095627,cronjob created as job in k8s 1.19.11,"i have what seems like an api issue that results in not being to be able to create a job from a cron job.
i have a cronjob helm file using the api like so:
apiversion: batch/v1beta1
kind: cronjob

deploying that with helm works just fine.
then after it is deployed i attempt to create a job using that cronjob like so:
 kubectl create job $(helm-release-name) --from=cronjob/connector-config

this used to create the job based on the chart above. now however, since upgrading to 1.19.11 i instead get this error:
##[error]error: unknown object type *v1beta1.cronjob
commandoutput
##[error]the process 
'/opt/hostedtoolcache/kubectl/1.22.1/x64/kubectl' failed with exit code 1

if i change the api in the helm chart to this:
apiversion: batch/v1
kind: cronjob

then the helm chart fails to deploy.
upgrade failed: unable to recognize &quot;&quot;: no matches for kind &quot;cronjob&quot; in version 
&quot;batch/v1&quot;

suggestions?
thanks!
",<kubernetes><kubernetes-helm>,69097277,3,"cronjobs is generally available (ga) in google kubernetes engine (gke) version 1.21 and later.
the version you should use apiversion: batch/v1
https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/

cronjobs was promoted to general availability in kubernetes v1.21. if
you are using an older version of kubernetes, please refer to the
documentation for the version of kubernetes that you are using, so
that you see accurate information. older kubernetes versions do not
support the batch/v1 cronjob api.

you can check the support kubernetes api versions using
kubectl api-resources or kubectl api-versions

you can also try
kubectl explain &lt;resource type&gt;

kubectl explain cronjob

"
61228081,how do i get my aws eks kubernetes cluster to be visible publicly?,"i have followed the steps in the udacity full-stack nanodegree course to get a kubernetes cluster running on aws eks. 

the service is running. running the command kubectl get services simple-jwt-api -o wide returns:

name             type           cluster-ip      external-ip                                                              port(s)        age   selector
simple-jwt-api   loadbalancer   10.100.217.57   a32d4ab0969b149bd9fb47d2065aee80-335944770.us-west-2.elb.amazonaws.com   80:31644/tcp   51m   app=simple-jwt-api


nodes appear to be running:

name                                          status   roles    age   version               internal-ip     external-ip     os-image         kernel-version                  container-runtime
ip-192-168-3-213.us-west-2.compute.internal   ready    &lt;none&gt;   80m   v1.15.10-eks-bac369   192.168.3.213   54.70.213.28    amazon linux 2   4.14.173-137.229.amzn2.x86_64   docker://18.9.9
ip-192-168-46-0.us-west-2.compute.internal    ready    &lt;none&gt;   80m   v1.15.10-eks-bac369   192.168.46.0    34.220.32.208   amazon linux 2   4.14.173-137.229.amzn2.x86_64   docker://18.9.9


pods appear to be running

name                              ready   status    restarts   age   ip               node                                          nominated node   readiness gates
simple-jwt-api-5dd5b9cf98-46ngm   1/1     running   0          37m   192.168.22.121   ip-192-168-3-213.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;
simple-jwt-api-5dd5b9cf98-kfgxf   1/1     running   0          37m   192.168.20.148   ip-192-168-3-213.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;
simple-jwt-api-5dd5b9cf98-xs6rp   1/1     running   0          37m   192.168.60.136   ip-192-168-46-0.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;


docker file is:

from python:stretch

copy . /app
workdir /app

run pip install --upgrade pip
run pip install -r requirements.txt

expose 8080

entrypoint [""gunicorn"", ""-b"", "":8080"", ""main:app""]


deployment file is:

apiversion: v1
kind: service
metadata:
  name: simple-jwt-api
spec:
  type: loadbalancer
  ports:
    - port: 80
      targetport: 80
  selector:
    app: simple-jwt-api
---
apiversion: apps/v1
kind: deployment
metadata:
  name: simple-jwt-api
spec:
  replicas: 3
  strategy:
    type: rollingupdate
    rollingupdate:
      maxunavailable: 2
      maxsurge: 2
  selector:
    matchlabels:
      app: simple-jwt-api
  template:
    metadata:
      labels:
        app: simple-jwt-api
    spec:
      containers:
        - name: simple-jwt-api
          image: container_image
          securitycontext:
            privileged: false
            readonlyrootfilesystem: false
            allowprivilegeescalation: false
          ports:
            - containerport: 8080


why can't i access the app at a32d4ab0969b149bd9fb47d2065aee80-335944770.us-west-2.elb.amazonaws.com? 
",<amazon-web-services><docker><kubernetes><amazon-eks>,61228313,3,"it looks like the targetport in service targetport: 80 does not match the container port of pod i.e.: containerport: 8080. please change the targetport in service to be 8080 and try again. 

apiversion: v1
kind: service
metadata:
  name: simple-jwt-api
spec:
  type: loadbalancer
  ports:
    - port: 80
      targetport: 80
  selector:
    app: simple-jwt-api
---
apiversion: apps/v1
kind: deployment
metadata:
  name: simple-jwt-api
spec:
  replicas: 3
  strategy:
    type: rollingupdate
    rollingupdate:
      maxunavailable: 2
      maxsurge: 2
  selector:
    matchlabels:
      app: simple-jwt-api
  template:
    metadata:
      labels:
        app: simple-jwt-api
    spec:
      containers:
        - name: simple-jwt-api
          image: container_image
          securitycontext:
            privileged: false
            readonlyrootfilesystem: false
            allowprivilegeescalation: false
          ports:
            - containerport: 8080

"
61494687,cannot use backendconfig on gke,"i have an application that i can deploy to kubernetes (google kubernetes engine) to which i'm trying to add google's cdn. for this i'm adding a backendconfig. but when my gitlab pipeline tries to apply it it returns the following error.


  $ kubectl apply -f backend-config.yaml
  error from server (forbidden): error when retrieving current configuration of:
  resource: ""cloud.google.com/v1beta1, resource=backendconfigs"", groupversionkind: ""cloud.google.com/v1beta1, kind=backendconfig""


i have a strongly suspect that the account the pipeline is running under does not have enough privileges to access backend configs. being new to k8s and gke i'm not sure how to fix this. especially as i cannot find what permission is needed for this.

edit

i added a kubectl get backendconfigs to my pipeline and that fails with the same error. running it from my gcloud sdk environment the same command works.

note the cluster is managed by gitlab and using rbac. my understanding is that gitlab creates service accounts per namespace in k8s with the edit role. 

edit 2

added clusterrole and clusterrolebinding based on arghya's answer.

output of $ kubectl get crd

name                                           created at
backendconfigs.cloud.google.com                2020-01-09t15:37:27z
capacityrequests.internal.autoscaling.k8s.io   2020-04-28t11:15:26z
certificaterequests.cert-manager.io            2020-01-15t06:53:47z
certificates.cert-manager.io                   2020-01-15t06:53:48z
challenges.acme.cert-manager.io                2020-01-15t06:53:48z
challenges.certmanager.k8s.io                  2020-01-09t15:47:01z
clusterissuers.cert-manager.io                 2020-01-15t06:53:48z
clusterissuers.certmanager.k8s.io              2020-01-09t15:47:01z
issuers.cert-manager.io                        2020-01-15t06:53:48z
issuers.certmanager.k8s.io                     2020-01-09t15:47:01z
managedcertificates.networking.gke.io          2020-01-09t15:37:53z
orders.acme.cert-manager.io                    2020-01-15t06:53:48z
orders.certmanager.k8s.io                      2020-01-09t15:47:01z
scalingpolicies.scalingpolicy.kope.io          2020-01-09t15:37:53z
updateinfos.nodemanagement.gke.io              2020-01-09t15:37:53z


output of kubectl describe crd backendconfigs.cloud.google.com

name:         backendconfigs.cloud.google.com
namespace:    
labels:       &lt;none&gt;
annotations:  &lt;none&gt;
api version:  apiextensions.k8s.io/v1beta1
kind:         customresourcedefinition
metadata:
  creation timestamp:  2020-01-09t15:37:27z
  generation:          1
  resource version:    198
  self link:           /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/backendconfigs.cloud.google.com
  uid:                 f0bc780a-32f5-11ea-b7bd-42010aa40111
spec:
  conversion:
    strategy:  none
  group:       cloud.google.com
  names:
    kind:       backendconfig
    list kind:  backendconfiglist
    plural:     backendconfigs
    singular:   backendconfig
  scope:        namespaced
  validation:
    open apiv 3 schema:
      properties:
        api version:
          description:  apiversion defines the versioned schema of this representation of an object. servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. more info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources
          type:         string
        kind:
          description:  kind is a string value representing the rest resource this object represents. servers may infer this from the endpoint the client submits requests to. cannot be updated. in camelcase. more info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds
          type:         string
        metadata:
          type:  object
        spec:
          description:  backendconfigspec is the spec for a backendconfig resource
          properties:
            cdn:
              description:  cdnconfig contains configuration for cdn-enabled backends.
              properties:
                cache policy:
                  description:  cachekeypolicy contains configuration for how requests to a cdn-enabled backend are cached.
                  properties:
                    include host:
                      description:  if true, requests to different hosts will be cached separately.
                      type:         boolean
                    include protocol:
                      description:  if true, http and https requests will be cached separately.
                      type:         boolean
                    include query string:
                      description:  if true, query string parameters are included in the cache key according to querystringblacklist and querystringwhitelist. if neither is set, the entire query string is included and if false the entire query string is excluded.
                      type:         boolean
                    query string blacklist:
                      description:  names of query strint parameters to exclude from cache keys. all other parameters are included. either specify querystringblacklist or querystringwhitelist, but not both.
                      items:
                        type:  string
                      type:    array
                    query string whitelist:
                      description:  names of query string parameters to include in cache keys. all other parameters are excluded. either specify querystringblacklist or querystringwhitelist, but not both.
                      items:
                        type:  string
                      type:    array
                  type:        object
                enabled:
                  type:  boolean
              required:
                enabled
              type:  object
            connection draining:
              description:  connectiondrainingconfig contains configuration for connection draining. for now the draining timeout. may manage more settings in the future.
              properties:
                draining timeout sec:
                  description:  draining timeout in seconds.
                  format:       int64
                  type:         integer
              type:             object
            iap:
              description:  iapconfig contains configuration for iap-enabled backends.
              properties:
                enabled:
                  type:  boolean
                oauthclient credentials:
                  description:  oauthclientcredentials contains credentials for a single iap-enabled backend.
                  properties:
                    client id:
                      description:  direct reference to oauth client id.
                      type:         string
                    client secret:
                      description:  direct reference to oauth client secret.
                      type:         string
                    secret name:
                      description:  the name of a k8s secret which stores the oauth client id &amp; secret.
                      type:         string
                  required:
                    secretname
                  type:  object
              required:
                enabled
                oauthclientcredentials
              type:  object
            security policy:
              type:  object
            session affinity:
              description:  sessionaffinityconfig contains configuration for stickyness parameters.
              properties:
                affinity cookie ttl sec:
                  format:  int64
                  type:    integer
                affinity type:
                  type:  string
              type:      object
            timeout sec:
              format:  int64
              type:    integer
          type:        object
        status:
          type:  object
  version:       v1beta1
  versions:
    name:     v1beta1
    served:   true
    storage:  true
status:
  accepted names:
    kind:       backendconfig
    list kind:  backendconfiglist
    plural:     backendconfigs
    singular:   backendconfig
  conditions:
    last transition time:  2020-01-09t15:37:27z
    message:               no conflicts found
    reason:                noconflicts
    status:                true
    type:                  namesaccepted
    last transition time:  &lt;nil&gt;
    message:               the initial names have been accepted
    reason:                initialnamesaccepted
    status:                true
    type:                  established
  stored versions:
    v1beta1
events:  &lt;none&gt;

",<kubernetes><gitlab><google-kubernetes-engine>,61494953,3,"create a clusterrole and clusterrolebinding for service account example-sa in namespace example-namespace

apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  name: backendconfig-role
rules:
- apigroups: [""cloud.google.com""]
  resources: [""backendconfigs""]
  verbs: [""get"", ""watch"", ""list"", ""create"", ""delete""]

---

apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: backendconfig-rolebinding
subjects:
- kind: serviceaccount
  name: example-sa 
  namespace: example-namespace
roleref:
  kind: clusterrole
  name: backendconfig-role
  apigroup: rbac.authorization.k8s.io


to check the permission is applied

kubectl auth can-i get backendconfigs --as=system:serviceaccount:example-namespace:example-sa -n example-namespace

"
56954670,configmap change doesn't reflect automatically on respective pods,"apiversion: apps/v1 # for versions before 1.8.0 use apps/v1beta1
    kind: deployment
    metadata:
      name: consoleservice1
    spec:
      selector:
        matchlabels:
          app: consoleservice1
      replicas: 3 # tells deployment to run 3 pods matching the template
      strategy:
        type: rollingupdate
        rollingupdate:
          maxsurge: 1
          maxunavailable: 1
      minreadyseconds: 5
      template: # create pods using pod definition in this template
        metadata:
          labels:
            app: consoleservice1
        spec:
          containers:
          - name: consoleservice
            image: chintamani/insightvu:ms-console1
            readinessprobe:
              httpget:
                path: /
                port: 8385
              initialdelayseconds: 5
              periodseconds: 5
              successthreshold: 1
            ports:
            - containerport: 8384
            imagepullpolicy: always
            volumemounts:
              - mountpath: /deploy/config
                name: config
          volumes:
            - name: config
              configmap:
                name: console-config


for creating configmap i am using this command:

kubectl create configmap console-config --from-file=deploy/config


while changing in configmap it doesn't reflect automatically, every time i have to restart the pod. how can i do it automatically?
",<kubernetes><kubernetes-pod>,56965036,3,"thank you guys .able to fix it ,i am using reloader to reflect on pods if any changes done inside 
kubectl apply -f https://raw.githubusercontent.com/stakater/reloader/master/deployments/kubernetes/reloader.yaml

then add the annotation inside your deployment.yml file .

apiversion: apps/v1 # for versions before 1.8.0 use apps/v1beta1
kind: deployment
metadata:
  name: consoleservice1
  annotations:
    configmap.reloader.stakater.com/reload: ""console-config""


it will restart your pods gradually .
"
56953788,understanding services in kubernetes?,"i am studying services in k8s from here

i have created service without selector and with one endpoint. what i am trying to do is i have installed apache and it's running on port 80. i have created a node port service on port 31000. now this service should redirect ip:31000 to ip:80 port. 

it is doing for internal ip of service but not on external ip. 

my-service.yaml

apiversion: v1
kind: service
metadata:
  name: my-service
spec:
  ports:
  - protocol: tcp
    port: 9376
    targetport: 80
    nodeport: 31000
  type: nodeport


my-endpoint.yaml

apiversion: v1
kind: endpoints
metadata:
  name: my-service
subsets:
  - addresses:
      - ip: &lt;ip&gt;
    ports:
      - port: 80


output for kubectl get service -o wide 

name         type        cluster-ip       external-ip   port(s)          age   selector
kubernetes   clusterip   10.96.0.1        &lt;none&gt;        443/tcp          53m   &lt;none&gt;
my-service   nodeport    10.111.205.207   &lt;none&gt;        9376:31000/tcp   30m   &lt;none&gt;

",<docker><kubernetes><devops><kubernetes-service>,56964002,3,"first thing is, you need to run a pod inside your cluster then assign the ip of  that pod inside the endpoints yaml with port, because services exposes the pods to within or outside the cluster, we must use either selector or the address of the pod so that service can attach it self to particular pod.

apiversion: v1
kind: endpoints
metadata:
  name: my-service
subsets:
  - addresses:
      - ip: &lt;ip address of the pod&gt;
    ports:
      - port: &lt;port of the pod&gt;


one more thing use statefulset in place of deployment to run pods. 
"
61439178,kubernetes ingress always return 503,"i deployed kubernetes on my computer and config pod, service, ingress. i curl the domain that i configured but get 503 error. what's the reason?
operating system: mac osx 10.15.3
docker version: 19.03.8
the pod:

apiversion: v1
kind: pod
metadata:
  name: opengateway
  namespace: openplatform
spec:
  containers:
    - name: opengateway
      image: ""karldoenitz/opengateway:1.0""
      ports:
        - containerport: 8000
          hostport: 8000
      env:
        - name: etcdiport
          valuefrom:
            configmapkeyref:
              name: openplatform
              key: etcd-iport
      imagepullpolicy: ifnotpresent


the service 

apiversion: v1
kind: service
metadata:
  name: webgateway
  namespace: openplatform
spec:
  ports:
    - port: 8000
      targetport: 8000
  selector:
    app: opengateway


the ingress

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: openplatform-web-gateway
  namespace: openplatform
spec:
  rules:
    - host: open.platform.com
      http:
        paths:
          - path: /
            backend:
              servicename: webgateway
              serviceport: 8000


describe svc webgateway -n openplatform

name:              webgateway
namespace:         openplatform
labels:            &lt;none&gt;
annotations:       kubectl.kubernetes.io/last-applied-configuration:
                     {""apiversion"":""v1"",""kind"":""service"",""metadata"":{""annotations"":{},""name"":""webgateway"",""namespace"":""openplatform""},""spec"":{""ports"":[{""port"":...
selector:          app=opengateway
type:              clusterip
ip:                10.109.103.73
port:              &lt;unset&gt;  8000/tcp
targetport:        8000/tcp
endpoints:         &lt;none&gt;
session affinity:  none
events:            &lt;none&gt;


the log of ingress-controller :

192.168.65.3 - - [26/apr/2020:08:58:37 +0000] ""get /favicon.ico http/1.1"" 503 599 ""http://open.platform.com/ping"" ""mozilla/5.0 (macintosh; intel mac os x 10_15_4) applewebkit/537.36 (khtml, like gecko) chrome/81.0.4044.122 safari/537.36"" 388 0.000 [openplatform-openplatform-web-gateway-30001] [] - - - - 086d5a61011485f8fa69dca25afd93ae


all the pod, service, ingress is running.i run the command curl http://open.platform.com, i got error 503 service temporarily unavailable. what's the matter?
",<kubernetes><kubernetes-ingress>,61439462,3,"so the problem here is that the service has a label selector which selects pods with label  app: opengateway but the pods don't have this label. because of this the endpoints in the service is empty and does not have the pod ips. adding  label app: opengateway into the pod should solve it.

apiversion: v1
kind: pod
metadata:
  name: opengateway
  namespace: openplatform
  labels:
    app: opengateway
spec:
  containers:
    - name: opengateway
      image: ""karldoenitz/opengateway:1.0""
      ports:
        - containerport: 8000
          hostport: 8000
      env:
        - name: etcdiport
          valuefrom:
            configmapkeyref:
              name: openplatform
              key: etcd-iport
      imagepullpolicy: ifnotpresent

"
71227971,kubernetes ingress - expose two paths under different hosts,"i'm struggling with a following case. i have one service written in .net core with some sort of gateway, and because of that two following graphql endpoints are under:
https://my-local-cluster.svc/api/abc/graphql
https://my-local-cluster.svc/api/xyz/graphql

right now i'm adding an ingress to my kubernetes cluster where i have this service and what i want to achieve is:
https://abc.ingresswebsite.com/graphql &lt;- points to https://mywebsite.com/api/abc/graphql
https://xyz.ingresswebsite.com/graphql &lt;- points to https://mywebsite.com/api/xyz/graphql

is that possible? i was trying to do following:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: my-app-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    appgw.ingress.kubernetes.io/backend-path-prefix: &quot;/api/abc/&quot;
    cert-manager.io/cluster-issuer: letsencrypt
spec:
  tls:
  - hosts:
    - abc.ingresswebsite.com
    secretname: tls-secret
  rules:
  - host: abc.ingresswebsite.com
    http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: my-abc-service
            port: 
              number: 80

as you can see i used the appgw.ingress.kubernetes.io/backend-path-prefix: &quot;/api/abc/&quot; annotation but unfortunately without any result. maybe you have a proposition how to achieve that, or if thats even possible to achieve?
thanks!
",<kubernetes><kubernetes-ingress>,71228772,3,"are you trying to make a request for https://abc.ingresswebsite.com/graphql go to /api/abc/graphql on your my-abc-service service?
if so, then perhaps the nginx-ingress' rewrite functionality (https://kubernetes.github.io/ingress-nginx/examples/rewrite/) might work for you.
here's the example from the documentation:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  name: rewrite
  namespace: default
spec:
  ingressclassname: nginx
  rules:
  - host: rewrite.bar.com
    http:
      paths:
      - path: /something(/|$)(.*)
        pathtype: prefix
        backend:
          service:
            name: http-svc
            port: 
              number: 80

and here's what the documentation describes as the result of the rewrite:

rewrite.bar.com/something rewrites to rewrite.bar.com/
rewrite.bar.com/something/ rewrites to rewrite.bar.com/
rewrite.bar.com/something/new rewrites to rewrite.bar.com/new

so adapting that example, your one might look something like this:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: my-app-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /api/abc/$1
    cert-manager.io/cluster-issuer: letsencrypt
spec:
  tls:
  - hosts:
    - abc.ingresswebsite.com
    secretname: tls-secret
  rules:
  - host: abc.ingresswebsite.com
    http:
      paths:
      - path: /(.*)
        pathtype: prefix
        backend:
          service:
            name: my-abc-service
            port: 
              number: 80

(caveat: i have not tried this)
"
47993411,k8s: error when creating cronjob chart,"i've a question about helm validation of resources field in cronjob chart , any help welcome.

i use apiversion batch/v1beta1

i get error when i try to retrive values from values.yaml

the error : error: error validating """": error validating data: found `invalid field requests for v1.container

command i run : helm install --dry-run --debug my_chart 

my cronjob chart as following:

apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: {{ .chart.name }}
spec:
  schedule: ""0 */2 * * *""
  concurrencypolicy: forbid
  jobtemplate:
    spec:
      template:
        metadata:
     ...
     spec:
  containers:
    ...
     spec:
       resources:
{{ toyaml .values.resources | indent 12 }}


error disapierd when i change the resources configuration to 

    resources:
      requests:
        cpu: 300m
        memory: 1024mi


i don't like the last change and i'd like to follow my regular configuration guide and store values in values.yaml

cluster version are

client version: version.info{major:""1"", minor:""8"", gitversion:""v1.8.4"", gitcommit:""9befc2b8928a9426501d3bf62f72849d5cbcd5a3"", gittreestate:""clean"", builddate:""2017-11-20t05:28:34z"", goversion:""go1.8.3"", compiler:""gc"", platform:""linux/amd64""}

server version: version.info{major:""1"", minor:""8+"", gitversion:""v1.8.4-gke.0"", gitcommit:""04502ae78d522a3d410de3710e1550cfb16dad4a"", gittreestate:""clean"", builddate:""2017-11-27t19:19:56z"", goversion:""go1.8.3b4"", compiler:""gc"", platform:""linux/amd64""}

helm versions are :

client: &amp;version.version{semver:""v2.7.0"", gitcommit:""08c1144f5eb3e3b636d9775617287cc26e53dba4"", gittreestate:""clean""}

server: &amp;version.version{semver:""v2.7.0"", gitcommit:""08c1144f5eb3e3b636d9775617287cc26e53dba4"", gittreestate:""clean""}

thanks in advance
",<kubernetes><kubernetes-helm>,47996293,3,"tl;dr: indent by 14 spaces (instead of 12) and remove the additional spec property in your container definition.



mind the correct indentation in your yaml definitions. for example, the containers property needs to be a sub-property of the cronjob's spec.jobtemplate.spec.template property (with spec.jobtemplate being the template for a job object (or a jobtemplate), and spec.jobtemplate.spec.template then being the template for that job's pod object (or a podtemplate).

furthermore, the pod's containers attribute does not require an additional spec. have a look at the api reference for the respective objects (linked above) for the exact specification of these object types.

for a cronjob, this is how the helm template should look like (again, indentation is important!). also, note that in this case, the .spec.jobtemplate.spec.template.spec.resources.requests property needs to be indented by 14 spaces, and not 12. 

apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: {{ .chart.name }}
spec:
  schedule: ""0 */2 * * *""
  concurrencypolicy: forbid
  jobtemplate:
    spec:
      template:
        metadata:
          # ...
        spec:
          containers:
          - name: foo
            # ...
            resources:
{{ toyaml .values.resources | indent 14 }}




regarding the error you've received: with an indentation of 12 spaces (indent 12), helm will create a yaml definition for your job similar to the following:

        spec:
          containers:
          - name: foo
            # ...
            resources:
            requests:
              cpu: 300m
              memory: 1024mi


as you can see, the requests property (intended to be a sub-property of the resources property), is now actually a property of the container definition. however, the container resource does not have a field called requests, resulting in the error message:


  error: error validating """": error validating data: found `invalid field requests for v1.container

"
71360867,how to create ingress-nginx for my kubernetes deployment and service?,"i am able to access my django app deployment using loadbalancer service type but i'm trying to switch to clusterip service type and ingress-nginx but i am getting 503 service temporarily unavailable when i try to access the site via the host url. describing the ingress also shows error: endpoints &quot;django-service&quot; not found and  error: endpoints &quot;default-http-backend&quot; not found. what am i doing wrong?
this is my service and ingress yaml:
---
apiversion: v1
kind: service
metadata:
  name: django-service
spec:
  type: clusterip
  ports:
  - name: http
    protocol: tcp
    port: 80
    targetport: 8000
---
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: django-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/force-ssl-redirect: 'true'
    nginx.ingress.kubernetes.io/ssl-redirect: 'true'
spec:
  tls:
  - hosts:
    - django.example.com
  rules:
  - host: django.example.com
    http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: django-service
            port:
              number: 80
  ingressclassname: nginx

kubectl get all
$ kubectl get all
name                                        ready   status    restarts   age
pod/django-app-5bdd8ffff9-79xzj             1/1     running   0          7m44s
pod/postgres-58fffbb5cc-247x9               1/1     running   0          7m44s

name                     type        cluster-ip      external-ip   port(s)    age
service/django-service   clusterip   10.233.29.58    &lt;none&gt;        80/tcp     7m44s
service/pg-service       clusterip   10.233.14.137   &lt;none&gt;        5432/tcp   7m44s

name                                   ready   up-to-date   available   age
deployment.apps/django-app             1/1     1            1           7m44s
deployment.apps/postgres               1/1     1            1           7m44s

name                                              desired   current   ready   age
replicaset.apps/django-app-5bdd8ffff9             1         1         1       7m44s
replicaset.apps/postgres-58fffbb5cc               1         1         1       7m44s

describe ingress
$ kubectl describe ing django-ingress
name:             django-ingress
labels:           &lt;none&gt;
namespace:        django
address:          10.10.30.50
default backend:  default-http-backend:80 (&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;)
tls:
  sni routes django.example.com
rules:
  host                      path  backends
  ----                      ----  --------
  django.example.com
                            /   django-service:80 (&lt;error: endpoints &quot;django-service&quot; not found&gt;)
annotations:                nginx.ingress.kubernetes.io/force-ssl-redirect: true
                            nginx.ingress.kubernetes.io/rewrite-target: /
                            nginx.ingress.kubernetes.io/ssl-redirect: true
events:
  type    reason  age                   from                      message
  ----    ------  ----                  ----                      -------
  normal  sync    5m28s (x2 over 6m5s)  nginx-ingress-controller  scheduled for sync
  normal  sync    5m28s (x2 over 6m5s)  nginx-ingress-controller  scheduled for sync

",<kubernetes><kubernetes-ingress><nginx-ingress>,71361021,3,"i think you forgot to make the link with your deployment in your service.
apiversion: v1
kind: service
metadata:
  name: django-service
spec:
  type: clusterip
  ports:
  - name: http
    protocol: tcp
    port: 80
    targetport: 8000
  selector:
    app: your-deployment-name

your label must be set in your deployment as well:
spec:
  selector:
    matchlabels:
      app: your-deployment-name
  template:
    metadata:
      labels:
        app: your-deployment-name

"
49667738,implementing workaround for missing http->https redirection in ingress-gce with glbc,"i am trying to wrap my brain around the suggested workarounds for the lack of built-in http->https redirection in ingress-gce, using glbc. what i am struggling with is how to use this custom backend that is suggested as one option to overcome this limitation (e.g. in how to force ssl for kubernetes ingress on gke).

in my case the application behind the load-balancer does not itself have apache or nginx, and i just can't figure out how to include e.g. apache (which i know way better than nginx) in the setup. am i supposed to set apache in front of the application as a proxy? in that case i wonder what to put in the proxy config as one can't use those convenient k8s service names there...

or should apache be set up as some kind of a separate backend, which would only get traffic when the client uses plain http? in that case i am missing the separation of backends by protocol in the gce load-balancer, and while i can see how that could be done manually, the ingress needs to be configured for that, and i can't seem to find any resources explaining how to actually do that.

for example, in https://github.com/kubernetes/ingress-gce#redirecting-http-to-https the ""application"" takes care of the forwaring (it seems to be built on nginx), and while that example works beautifully, it's not possible to do the same thing with the application i am talking about.

basically, my setup is currently this:

http://&lt;public ip&gt;:80    -\
                           &gt;      gce lb     -&gt;  k8s pod running the application
https://&lt;public_ip&gt;:443  -/   (ingress-gce)


i know i could block http altogether, but that'd ruin user experience when someone just typed in the domain name in the browser.

currently i have these services set up for the lb:

kind: service
apiversion: v1
metadata:
  name: myapp
spec:
  type: loadbalancer
  ports:
  - name: http
    port: 80
    targetport: myapp
    protocol: tcp
  selector:
    app: myapp

---
kind: ingress
apiversion: extensions/v1beta1
metadata:
  name: myapp-ingress
  annotations:
    ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.global-static-ip-name: ""my-ip""
    ingress.gcp.kubernetes.io/pre-shared-cert: ""my-cert""
spec:
  backend:
    servicename: myapp
    serviceport: 80
  rules:
  - host: my.domain.name
    http:
      paths:
      - path: /
        backend:
          servicename: myapp
          serviceport: 80


in addition i have glbc bundled together with the application deployment:

apiversion: v1
kind: configmap
metadata:
  name: glbc-configmap
data:
  gce.conf: |
    [global]
    node-tags = myapp-k8s-nodepool
    node-instance-prefix = gke-myapp-k8s-cluster

---
kind: deployment
apiversion: apps/v1beta2
metadata:
  name: myapp
spec:
  replicas: 1
  selector:
    matchlabels:
      app: myapp
  template:
    metadata:
      name: myapp
      labels:
        app: myapp
    spec:
      containers:
      # start application container
      - name: myapp
        image: eu.gcr.io/myproject/myapp:latest
        imagepullpolicy: always
        readinessprobe:
          httpget:
            path: /ping
            port: 8080
        ports:
        - name: myapp
          containerport: 8080
      # end application container
      # start glbc container
      - name: myapp-glbc
        image: gcr.io/google_containers/glbc:0.9.7
        livenessprobe:
          httpget:
            path: /ping
            port: 8080
            scheme: http
          initialdelayseconds: 30
          timeoutseconds: 5
        volumemounts:
        - mountpath: /etc/glbc-configmap
          name: cloudconfig
          readonly: true
        args:
        - --apiserver-host=http://localhost:8080
        - --default-backend-service=myapp
        - --sync-period=300s
        - --config-file-path=/etc/glbc-configmap/gce.conf


i'd greatly appreciate any pointers in addition to more complete solutions.
",<kubernetes><google-cloud-platform><google-kubernetes-engine><kubernetes-ingress>,49750913,3,"edit in may 2020: ""http(s) load balancing rewrites and redirects support is now in general availability"" as stated in https://issuetracker.google.com/issues/35904733#comment95 seems to mean that now it finally would be possible to implement proper rediction rules in the lb itself, without having to resort to having an extra pod or any other tweak of that kind. however, in case the below is of use to someone, i'll leave it there for reference.

i was able to find a solution, where the gce lb directs traffic to apache (of course this should work for any proxy) which runs as a deployment in k8s cluster. in apache config, there's a redirect based on x-forwarded-proto header, and a reverse proxy rules that point to the application in the cluster.

apiversion: v1
kind: configmap
metadata:
  name: apache-httpd-configmap
data:
  httpd.conf: |
    # apache httpd v2.4 minimal configuration
    # this can be reduced further if you remove the accees log and mod_log_config

    serverroot ""/usr/local/apache2""

    # minimum modules needed
    loadmodule mpm_event_module modules/mod_mpm_event.so
    loadmodule log_config_module modules/mod_log_config.so
    loadmodule mime_module modules/mod_mime.so
    loadmodule dir_module modules/mod_dir.so
    loadmodule authz_core_module modules/mod_authz_core.so
    loadmodule unixd_module modules/mod_unixd.so
    loadmodule alias_module modules/mod_alias.so
    loadmodule proxy_module modules/mod_proxy.so
    loadmodule proxy_http_module modules/mod_proxy_http.so

    typesconfig conf/mime.types

    pidfile logs/httpd.pid

    # comment this out if running httpd as a non root user
    user nobody

    # port to listen on
    listen 8081

    # in a basic setup httpd can only serve files from its document root
    documentroot ""/usr/local/apache2/htdocs""

    # default file to serve
    directoryindex index.html

    # errors go to stderr
    errorlog /proc/self/fd/2

    # access log to stdout
    logformat ""%h %l %u %t \""%r\"" %&gt;s %b"" common
    customlog /proc/self/fd/1 common

    mutex posixsem proxy

    # never change this block
    &lt;directory /&gt;
      allowoverride none
      require all denied
    &lt;/directory&gt;

    # deny documents to be served from the documentroot
    &lt;directory ""/usr/local/apache2/htdocs""&gt;
      require all denied
    &lt;/directory&gt;

    &lt;virtualhost *:8081&gt;
      servername my.domain.name
      # redirect http to load balancer https url
      &lt;if ""%{http:x-forwarded-proto} -strcmatch 'http'""&gt;
        redirect / https://my.domain.name:443/
      &lt;/if&gt;

      # proxy the requests to the application
      # ""myapp"" in the rules relies a k8s cluster add-on for dns aliases
      # see https://kubernetes.io/docs/concepts/services-networking/service/#dns
      proxyrequests off
      proxypass         ""/""    ""http://myapp:80/""
      proxypassreverse  ""/""    ""http://myapp:80/""
    &lt;/virtualhost&gt;

---
kind: service
apiversion: v1
metadata:
  name: apache-httpd
spec:
  type: nodeport
  ports:
  - name: http
    port: 80
    targetport: apache-httpd
    protocol: tcp
  selector:
    app: apache-httpd

---
kind: deployment
apiversion: apps/v1beta2
metadata:
  name: apache-httpd
spec:
  replicas: 1
  selector:
    matchlabels:
      app: apache-httpd
  template:
    metadata:
      name: apache-httpd
      labels:
        app: apache-httpd
    spec:
      containers:
      # start apache httpd container
      - name: apache-httpd
        image: httpd:2.4-alpine
        imagepullpolicy: always
        readinessprobe:
          httpget:
            path: /
            port: 8081
        command: [""/usr/local/apache2/bin/httpd""]
        args: [""-f"", ""/etc/apache-httpd-configmap/httpd.conf"", ""-dforeground""]
        ports:
        - name: apache-httpd
          containerport: 8081
        volumemounts:
        - mountpath: /etc/apache-httpd-configmap
          name: apacheconfig
          readonly: true
      # end apache container
      # end containers
      volumes:
        - name: apacheconfig
          configmap:
            name: apache-httpd-configmap
      # end volumes
    # end template spec
  # end template


in addition to the above new manifest yaml, the rule for ""myapp-ingress"" needed to change so that instead of servicename: myapp it has servicename: apache-httpd to make the lb direct traffic to apache.

it seems that this rather minimal apache setup requires very little cpu and ram, so it fits just fine in the existing cluster and thus doesn't really cause any direct extra cost.
"
52177926,dynamic redirect using ingress,"i have 2 questions:

1) i have a kubernetes cluster with multiple services and i want to use ingress to dynamically redirect the traffic to the cluster.

i expect the configuration to look something like this:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: test
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /service1/*
        backend:
          servicename: service1
          serviceport: 80
        path: /*
      - path: /service2/*
        backend:
          servicename: service2
          serviceport: 80
        path:/*


so basically i want all the traffic to /service1/endpoint to be redirected to s1:80/endpoint dynamically.

2) let's say i have 2 web services - service1 &amp; service2.

i want the users to work with the following url in their browser:

kube/servicen/endpoint

is there a way to do that without having my users redirected to service1/endpoint?

thanks!
",<service><routes><kubernetes><containers><kubernetes-ingress>,52179243,3,"i believe your ingress definition to be almost correct:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: test
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /service1
        backend:
          servicename: service1
          serviceport: 80
      - path: /service2
        backend:
          servicename: service2
          serviceport: 80


this should work if you have an ingress correctly deployed!
"
52121422,how to automatically stop rolling update when crashloopbackoff?,"i use google kubernetes engine and i intentionally put an error in the code. i was hoping the rolling update will stop when it discovers the status is crashloopbackoff, but it wasn't.

in this page, they say.. 


  the deployment controller will stop the bad rollout automatically, and
  will stop scaling up the new replicaset. this depends on the
  rollingupdate parameters (maxunavailable specifically) that you have
  specified.


but it's not happening, is it only if the status imagepullbackoff?

below is my configuration.

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: volume-service
  labels:
    group: volume
    tier: service
spec:
  replicas: 4
  strategy:
    type: rollingupdate
    rollingupdate:
      maxunavailable: 2
      maxsurge: 2
  template:
    metadata:
      labels:
        group: volume
        tier: service
    spec:
      containers:
      - name: volume-service
        image: gcr.io/example/volume-service:latest


p.s. i already read liveness/readiness probes, but i don't think it can stop a rolling update? or is it?
",<kubernetes><google-kubernetes-engine>,52122467,3,"turns out i just need to set minreadyseconds and it stops the rolling update when the new replicaset has status crashloopbackoff or something like exited with status code 1. so now the old replicaset still available and not updated.

here is the new config.

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: volume-service
  labels:
    group: volume
    tier: service
spec:
  replicas: 4
  minreadyseconds: 60
  strategy:
    type: rollingupdate
    rollingupdate:
      maxunavailable: 2
      maxsurge: 2
  template:
    metadata:
      labels:
        group: volume
        tier: service
    spec:
      containers:
      - name: volume-service
        image: gcr.io/example/volume-service:latest


thank you for averyone help!
"
51988839,how can i put basic auth on specific http methods in ngnix ingress?,"i can create ingress with basic auth. i followed the template from kubernetes/ingress-nginx:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-with-auth
  annotations:
    # type of authentication
    nginx.ingress.kubernetes.io/auth-type: basic
    # name of the secret that contains the user/password definitions
    nginx.ingress.kubernetes.io/auth-secret: basic-auth
    # message to display with an appropriate context why the authentication is required
    nginx.ingress.kubernetes.io/auth-realm: 'authentication required - foo'
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /
        backend:
          servicename: http-svc
          serviceport: 80


it works fine, but i need to allow 'options' method without basic auth for pre-flight requests. any pointers on how to do it will be very helpful.
",<nginx><kubernetes><kubernetes-ingress><nginx-ingress>,67476565,3,"i just encountered the same problem. i solved it by using a configuration-snippet.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: my-cors-auth-ingress
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      # fix cors issues of ingress when using external auth service
      if ($request_method = options) {
        add_header content-length 0;
        add_header content-type text/plain;
        return 204;
      }
      more_set_headers &quot;access-control-allow-credentials: true&quot;;
      more_set_headers &quot;access-control-allow-methods: get, post, put, patch, delete, options&quot;;
      more_set_headers &quot;access-control-allow-headers: dnt,x-customheader,keep-alive,user-agent,x-requested-with,if-modified-since,cache-control,content-type,authorization&quot;;
      more_set_headers &quot;access-control-allow-origin: $http_origin&quot;;
      more_set_headers &quot;access-control-max-age: 600&quot;;
    nginx.ingress.kubernetes.io/auth-url: &quot;http://auth-service.default.svc.cluster.local:80&quot;

"
59965672,kubernetes nginx ingress controller throws error when trying to obtain endpoints for service,"i am trying to set micro-services on kubernetes on google cloud platform. i've created a deployment, clusterip and ingress configuration files.

first after creating a cluster, i run this command to install nginx ingress.

helm install my-nginx stable/nginx-ingress --set rbac.create=true


i use helm v3.

then i apply deployment and clusterip configurations.

deployment and clusterip configurations:

apiversion: apps/v1
kind: deployment
metadata:
  name: app-production-deployment
spec:
  replicas: 2
  selector:
    matchlabels:
      component: app-production
  template:
    metadata:
      labels:
        component: app-production
    spec:
      containers:
        - name: app-production
          image: eu.gcr.io/my-project/app:1.0
          ports:
            - containerport: 80
---
apiversion: v1
kind: service
metadata:
  name: app-production-cluser-ip-service
spec:
  type: clusterip
  selector:
    component: app-production
  ports:
    - port: 80
      targetport: 80
      protocol: tcp


my ingress config is:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-service
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: app.example.com
      http:
        paths:
          - path: /
            backend:
              servicename: app-production-cluster-ip-service
              serviceport: 80


i get this error from google cloud platform logs withing ingress controller:

error obtaining endpoints for service ""default/app-production-cluster-ip-service"": no object matching key ""default/app-production-cluster-ip-service"" in local store


but when i do kubectl get endpoints command the output is this:

name                                          endpoints                     age
app-production-cluser-ip-service              10.60.0.12:80,10.60.1.13:80   17m


i am really not sure what i'm doing wrong.
",<kubernetes><google-cloud-platform><google-kubernetes-engine><kubernetes-ingress><nginx-ingress>,59966537,3,"the service name mentioned in the ingress not matching. please recreate a service and check

    apiversion: v1
    kind: service
    metadata:
      name: app-production-cluster-ip-service
    spec:
      type: clusterip
      selector:
        component: app-production
      ports:
        - port: 80
          targetport: 80
          protocol: tcp

"
59555839,how to configure ingress to a service in kubernetes hosted on jelastic?,"here's the thing. i installed a kubernetes cluster on my jelastic account like this:



then, i wanted to expose a service to the outside of the cluster. the helloworld is working out-of-the-box on the cluster, so i thought i could transform it to my needs easily. upon cluster installation, browsing

my-helloworld-test.my-jelastic-provider.com


works perfectly fine, i.e. i see the hello world html page with styling.

now, in my future use-cases, i will want to access my services through

my-helloworld-test.my-jelastic-provider.com/hello


or

hello.my-helloworld-test.my-jelastic-provider.com


i.e. i'd like to setup a path for my service or a subdomain. i gathered i need to define an ingress to make that happen. i've changed the hello-kubernetes service configuration to this:

kind: service
apiversion: v1
metadata:
  name: hello-kubernetes
  namespace: default
spec:
  ports:
    - protocol: tcp
      port: 80
      targetport: 8080
  selector:
    app: hello-kubernetes
  type: clusterip
  sessionaffinity: none
  externaltrafficpolicy: cluster


path to service

i tried following ingress configuration to define a path to my service:

kind: ingress
apiversion: extensions/v1beta1
metadata:
  name: helloworld
  namespace: default
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: 'false'
spec:
  rules:
    - http:
        paths:
          - path: /hello
            backend:
              servicename: hello-kubernetes
              serviceport: 80


the goal is that browsing

my-helloworld-test.my-jelastic-provider.com/hello


displays the pre-deployed helloworld application, instead of

my-helloworld-test.my-jelastic-provider.com


i could write the helloworld application in such a way that its base url be /hello, but i learned from several blogs that it is possible to let that happen with kubernetes annotations. in particular, it seems like annotation nginx.ingress.kubernetes.io/rewrite-target would help, but i was not able to make it happen completely. indeed, the above configuration shows me the helloworld application when i browse the /hello path, but unstyled:



how can i make the styling happen?

subdomain to service

another sexy possibility for that service would be to be displayed upon browsing

hello.my-helloworld-test.my-jelastic-provider.com


i.e. as a subdomain of my jelastic environment. i've tried this configuration without success:

kind: ingress
apiversion: extensions/v1beta1
metadata:
  name: jenkins
  namespace: default
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: 'false'
spec:
  rules:
    - http:
        paths:
          - host: hello.my-helloworld-test.my-jelastic-provider.com
            path: /
            backend:
              servicename: hello-kubernetes
              serviceport: 80


what do i need to do to my jelastic environment so that the above host be browsable and exhibits the helloworld application?

kubernetes configuration

client version: version.info{major:""1"", minor:""15"", gitversion:""v1.15.6"", gitcommit:""7015f71e75f670eb9e7ebd4b5749639d42e20079"", gittreestate:""archive"", builddate:""2019-11-19t09:00:01z"", goversion:""go1.12.12"", co
mpiler:""gc"", platform:""linux/amd64""}
server version: version.info{major:""1"", minor:""15"", gitversion:""v1.15.6"", gitcommit:""7015f71e75f670eb9e7ebd4b5749639d42e20079"", gittreestate:""archive"", builddate:""2019-11-19t08:45:41z"", goversion:""go1.12.12"", co
mpiler:""gc"", platform:""linux/amd64""}


on jelastic v.5.7.
",<jenkins><kubernetes><kubernetes-ingress><jelastic>,59574968,3,"looks like there are 2 separate issues atm


are you sure that domain
hello.my-helloworld-test.my-jelastic-provider.com is following to a
correct ip? it is possible to add custom subdomains to the
environment, then use them for ingress rules, but that option
(custom subdomains) is now available through the api only 
incorrect annotation argument for nginx.ingress.kubernetes.io/rewrite-target, if you use nginx ingress controller try this one




kind: ingress
apiversion: extensions/v1beta1
metadata:
  name: helloworld
  namespace: default
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/ssl-redirect: 'false'
spec:
  rules:
    - http:
        paths:
          - path: /hello(/|$)(.*)
            backend:
              servicename: hello-kubernetes
              serviceport: 80




upd: 
we have additionally checked the issue together with laurent michel and discovered that problem was caused by absolute uris in application so rewrite was not applied to css/images. simplest solution - apply two ingress rules with different annotations (one with rewrite and one without). more advanced and correct way to go - shift such applications to separate subdomains so uris could remain the same.
"
59714903,how does k8s handles multiple remote docker registeries in pod definition using imagepullsecrets list,"i would like to access multiple remote registries to pull images. 
in the k8s documentation they say:


  (if you need access to multiple registries, you can create one secret
  for each registry. kubelet will merge any imagepullsecrets into a
  single virtual .docker/config.json)


and so the pod definition should be something like this:

apiversion: v1
kind: pod
spec:
  containers:
    - name: ...
  imagepullsecrets:
    - name: secret1
    - name: secret2
    - ....
    - name: secretn


now i am not sure how k8s will pick the right secret for each image? will all secrets be verified one by one each time? and how k8s will handle the failed retries? and if a specific amount of unauthorized retries could lead to some lock state in k8sor docker registries?

/ thanks
",<kubernetes><docker-registry><kubernetes-secrets>,59717512,3,"kubernetes isn't going to try all secrets until find the correct. when you create the secret, you are referencing that it's a docker registry: 

$ kubectl create secret docker-registry user1-secret --docker-server=https://index.docker.io/v1/ --docker-username=user1 --docker-password=password456 --docker-email=user1@email.com

$ kubectl create secret docker-registry user2-secret --docker-server=https://index.docker.io/v1/  --docker-username=user2 --docker-password=password123 --docker-email=user2@email.com

$ kubectl get secrets user1-secret -o yaml
apiversion: v1
data:
  .dockerconfigjson: eyjhdxrocyi6eyjkb2nrzxiuzxhhbxbszs5jb20ionsidxnlcm5hbwuioijrdwjliiwicgfzc3dvcmqioijqv19tvfjjtkcilcjlbwfpbci6im15qgvtywlslmnvbsisimf1dggioijhm1zpwlrwuvyxovrwrkpkvgtjpsj9fx0=
kind: secret
metadata:
  creationtimestamp: ""2020-01-13t13:15:52z""
  name: user1-secret
  namespace: default
  resourceversion: ""1515301""
  selflink: /api/v1/namespaces/default/secrets/user1-secret
  uid: d2f3bb0c-3606-11ea-a202-42010a8000ad
type: kubernetes.io/dockerconfigjson


as you can see, type is kubernetes.io/dockerconfigjson is telling kubernetes to treat this differently. 

so, when you reference the address of your container as magic.example.com/magic-image on your yaml, kubernetes will have enough information to connect the dots and use the right secret to pull your image. 

apiversion: v1
kind: pod
metadata:
  name: busyboxes
  namespace: default
spec:
  imagepullsecrets:
  - name: user1-secret
  - name: user2-secret
  containers:
  - name: jenkins
    image: user1/jenkins
    imagepullpolicy: always
  - name: busybox
    image: user2/busybox
    imagepullpolicy: always    


so as this example describes, it's possible to have 2 or more docker registry secrets with the same --docker-server value. kubernetes will manage to take care of it seamlessly.  
"
51472081,config file for running service in kubernetes doesn't work well,"i have a config file named ""pod.yaml"" for making a pod like bellow:


apiversion: v1
kind: pod
metadata:
  name: myapp
  labels:
    app: myapp
spec:
    containers:
      - name: comet-app
        image: gcr.io/my-project/my-app:v2
        ports:
          - containerport: 5000


and a config file named ""service.yaml"" for running a service in that ""myapp"" pod.


apiversion: v1
kind: service
metadata:
  name: myapp
spec:
  type: loadbalancer
  ports:
  - protocol: tcp
    port: 80
    targetport: 5000
  selector:
    run: myapp


when i run 

 kubectl apply -f pod.yaml
 kubectl apply -f service.yaml


the 'myapp' service is created but i couldn't access my website by the internal ip and it returned err_connection_timed_out.


name         type           cluster-ip      external-ip      port(s)        age
kubernetes   clusterip      10.xx.xxx.1     &lt;none&gt;           443/tcp        11d
myapp        loadbalancer   10.xx.xxx.133   35.xxx.xx.172    80:30273/tcp   3s


but when i deleted that service and re-run by exposing a service with bellow command, everything worked well and i could access to my website by the external-ip.

 kubectl expose pod myapp  --type=loadbalancer --port=80 --target-port=5000


could anyone explain it for me and tell me what is wrong in my service.yaml?
",<kubernetes><yaml><kubernetes-pod><kubernetes-service>,51476421,3,"the problem with service.yaml is that the selector is wrong. how it works is that a service by default routes traffic to pods with a certain label. your pod has the label app: myapp whereas in the service your selector is run: myapp. so, changing service.yaml to the following should solve the issue:

apiversion: v1
kind: service
metadata:
  name: myapp
spec:
  type: loadbalancer
  ports:
  - protocol: tcp
    port: 80
    targetport: 5000
  selector:
    app: myapp

"
70028917,kubernetes initcontainers to copy file and execute as part of lifecycle hook poststart,"i am trying to execute some scripts as part of statefulset deployment kind. this script i have added as configmap and i use this as volumemount inside the pod definition. i use the lifecycle poststart exec command to execute this script. it fails with the permission issue.
based on certain articles, i found that we should copy this file as part of initcontainer and then use that (i am not sure why should we do and what will make a difference)
still, i tried it and that also gives the same error.
here is my configmap:
apiversion: v1
kind: configmap
metadata:
  name: postgres-configmap-initscripts
data:
  poststart.sh: |
     #!/bin/bash
     echo &quot;it`s done&quot;

here is my statefulset:
apiversion: apps/v1
kind: statefulset
metadata:
  name: postgres-statefulset
spec:
  ....
  servicename: postgres-service
  replicas: 1
  template:
    ...
    spec:
      initcontainers:
      - name: &quot;postgres-ghost&quot;
        image: alpine
        volumemounts:
        - mountpath: /scripts
          name: postgres-scripts
      containers:
      - name: postgres
        image: postgres
        lifecycle:
            poststart:
              exec:
                command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;/scripts/poststart.sh&quot; ]
        ports:
        - containerport: 5432
          name: dbport
        ....
        volumemounts:
        - mountpath: /scripts
          name: postgres-scripts
volumes:
  - name: postgres-scripts
    configmap:
      name: postgres-configmap-initscripts
      items:
      - key: poststart.sh
        path: poststart.sh

the error i am getting:

",<kubernetes><kubernetes-statefulset>,70029777,3,"poststart hook will be call at least once but may be call more than once, this is not a good place to run script.
the poststart.sh file that mounted as configmap will not have execute mode hence the permission error.
it is better to run script in initcontainers, here's an quick example that do a simple chmod; while in your case you can execute the script instead:
cat &lt;&lt; eof | kubectl apply -f -
apiversion: v1
kind: configmap
metadata:
  name: busybox
data:
  test.sh: |
    #!/bin/bash
    echo &quot;it's done&quot;
---
apiversion: v1
kind: pod
metadata:
  name: busybox
  labels:
    run: busybox
spec:
  volumes:
  - name: scripts
    configmap:
      name: busybox
      items:
      - key: test.sh
        path: test.sh
  - name: runnable
    emptydir: {}
  initcontainers:
  - name: prepare
    image: busybox
    imagepullpolicy: ifnotpresent
    command: [&quot;ash&quot;,&quot;-c&quot;]
    args: [&quot;cp /scripts/test.sh /runnable/test.sh &amp;&amp; chmod +x /runnable/test.sh&quot;]
    volumemounts:
    - name: scripts
      mountpath: /scripts
    - name: runnable
      mountpath: /runnable
  containers:
  - name: busybox
    image: busybox
    imagepullpolicy: ifnotpresent
    command: [&quot;ash&quot;,&quot;-c&quot;]
    args: [&quot;while :; do . /runnable/test.sh; sleep 1; done&quot;]
    volumemounts:
    - name: scripts
      mountpath: /scripts
    - name: runnable
      mountpath: /runnable
eof

"
59735045,install helm 2.13.0 on minikube server (1.6.2) could not find tiller,"hey i'm installing fresh minikube and try to init helm on it no in 3.x.x but 2.13.0 version.

$ minikube start
  minikube v1.6.2 on darwin 10.14.6
  automatically selected the 'hyperkit' driver (alternates: [virtualbox])
  creating hyperkit vm (cpus=2, memory=2000mb, disk=20000mb) ...
  preparing kubernetes v1.17.0 on docker '19.03.5' ...
  pulling images ...
  launching kubernetes ...
  waiting for cluster to come online ...
  done! kubectl is now configured to use ""minikube""

$ kubectl -n kube-system create serviceaccount tiller
serviceaccount/tiller created

$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount kube-system:tiller
clusterrolebinding.rbac.authorization.k8s.io/tiller created

$ helm init --service-account tiller
$helm_home has been configured at /users/&lt;user&gt;/.helm.
error: error installing: the server could not find the requested resource

$ helm init --service-account tiller --override spec.selector.matchlabels.'name'='tiller',spec.selector.matchlabels.'app'='helm' --output yaml | sed 's@apiversion: extensions/v1beta1@apiversion: apps/v1@' | kubectl apply -f -

deployment.apps/tiller-deploy created
service/tiller-deploy created

$ helm init --service-account tiller
 59 ### aliases
$helm_home has been configured at /users/&lt;user&gt;/.helm.
error: error installing: the server could not find the requested resource

$ helm version
client: &amp;version.version{semver:""v2.13.0"", gitcommit:""79d07943b03aea2b76c12644b4b54733bc5958d6"", gittreestate:""clean""}
error: could not find tiller




i try to do same on some random other ns, and with no result:

$ kubectl create ns deployment-stuff
namespace/deployment-stuff created

$ kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin \
        --user=$(gcloud config get-value account)

clusterrolebinding.rbac.authorization.k8s.io/cluster-admin-binding created

$ kubectl create serviceaccount tiller --namespace deployment-stuff
kubectl create clusterrolebinding tiller-admin-binding --clusterrole=cluster-admin \
    --serviceaccount=deployment-stuff:tiller

serviceaccount/tiller created
clusterrolebinding.rbac.authorization.k8s.io/tiller-admin-binding created

$ helm init --service-account=tiller --tiller-namespace=deployment-stuff

creating /users/&lt;user&gt;/.helm
creating /users/&lt;user&gt;/.helm/repository
creating /users/&lt;user&gt;/.helm/repository/cache
creating /users/&lt;user&gt;/.helm/repository/local
creating /users/&lt;user&gt;/.helm/plugins
creating /users/&lt;user&gt;/.helm/starters
creating /users/&lt;user&gt;/.helm/cache/archive
creating /users/&lt;user&gt;/.helm/repository/repositories.yaml
adding stable repo with url: https://kubernetes-charts.storage.googleapis.com
adding local repo with url: http://127.0.0.1:8879/charts
$helm_home has been configured at /users/&lt;user&gt;/.helm.
error: error installing: the server could not find the requested resource

$ helm repo update
hang tight while we grab the latest from your chart repositories...
...skip local chart repository
...successfully got an update from the ""stable"" chart repository
update complete.  happy helming!

$ helm list
error: could not find tiller

$ helm list --tiller-namespace=kube-system
error: could not find tiller

$ helm list --tiller-namespace=deployment-stuff
error: could not find tiller


same error everywhere error: error installing: the server could not find the requested resource any ideas how to approach it ? 

i installed helm with those commands and works fine with my gcp clusters, helm list returns full list of helms.

wget -c https://get.helm.sh/helm-v2.13.0-darwin-amd64.tar.gz
tar -zxvf helm-v2.13.0-darwin-amd64.tar.gz
mv darwin-amd64/helm /usr/local/bin/helm


tbh i have no idea what's going on, sometimes it works fine on minikube sometimes i get these errors.
",<kubernetes><kubernetes-helm>,59738539,3,"this can be fixed by deleting the tiller deployment and service and rerunning the helm init --override command after first helm init.

so after running commands you listed:

$ kubectl -n kube-system create serviceaccount tiller
serviceaccount/tiller created

$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount kube-system:tiller
clusterrolebinding.rbac.authorization.k8s.io/tiller created

$ helm init --service-account tiller


and then finding out that tiller could not be found.

$ helm version
client: &amp;version.version{semver:""v2.13.0"", gitcommit:""79d07943b03aea2b76c12644b4b54733bc5958d6"", gittreestate:""clean""}
error: could not find tiller


run the following commands:

1.

$ kubectl delete service tiller-deploy -n kube-system


2.

$ kubectl delete deployment tiller-deploy -n kube-system


3.

helm init --override spec.selector.matchlabels.'name'='tiller',spec.selector.matchlabels.'app'='helm' --output yaml | sed 's@apiversion: extensions/v1beta1@apiversion: apps/v1@' | kubectl apply -f -




after that you can verify if it worked with:

$ helm version
client: &amp;version.version{semver:""v2.13.0"", gitcommit:""79d07943b03aea2b76c12644b4b54733bc5958d6"", gittreestate:""clean""}
error: could not find a ready tiller pod


this one needs little more time, give it few seconds.

$ helm version
client: &amp;version.version{semver:""v2.13.0"", gitcommit:""79d07943b03aea2b76c12644b4b54733bc5958d6"", gittreestate:""clean""}
server: &amp;version.version{semver:""v2.13.0"", gitcommit:""79d07943b03aea2b76c12644b4b54733bc5958d6"", gittreestate:""clean""}


tell me if it worked.
"
63361182,login unauthorized error whle connecting to external hashicorp vault with kubernetes service account,"scenario:
i have two kubernetes 1.17 clusters, and one cluster have hashicorp vault configured. i am trying to connect to it from other cluster using kubernetes auth method and i am getting 403 error as below:

2020-08-11t14:22:46.971z [error] auth.kubernetes.auth_kubernetes_f530e086: login unauthorized due to: {&quot;kind&quot;:&quot;status&quot;,&quot;apiversion&quot;:&quot;v1&quot;,&quot;metadata&quot;:{},&quot;status&quot;:&quot;failure&quot;,&quot;message&quot;:&quot;tokenreviews.authentication.k8s.io is forbidden: user &quot;system:serviceaccount:default:vault-auth&quot; cannot create resource &quot;tokenreviews&quot; in api group &quot;authentication.k8s.io&quot; at the cluster scope: rbac: clusterrole.rbac.authorization.k8s.io &quot;system:auth-delegator&quot; not found&quot;,&quot;reason&quot;:&quot;forbidden&quot;,&quot;details&quot;:{&quot;group&quot;:&quot;authentication.k8s.io&quot;,&quot;kind&quot;:&quot;tokenreviews&quot;},&quot;code&quot;:403}

clusterrolebinding:
kind: clusterrolebinding
metadata:
  name: role-tokenreview-binding
  namespace: default
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: system:auth-delegator
subjects:
- kind: serviceaccount
  name: vault-auth
  namespace: default

someone please help me here? what am i missing?
",<kubernetes><hashicorp-vault><amazon-eks>,63361426,3,"the clusterrole system:auth-delegator does not exist which is giving this error.
to check if it exists use below command
kubectl get clusterrole | grep system:auth-delegator

if it does not exist create one using below yaml
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  name: system:auth-delegator
rules:
- apigroups:
  - authentication.k8s.io
  resources:
  - tokenreviews
  verbs:
  - create
- apigroups:
  - authorization.k8s.io
  resources:
  - subjectaccessreviews
  verbs:
  - create

"
63429283,getting error while creating kubernetes deployment,"my deployment is working fine. i just try to use local persistent volume for storing data on local of my application. after that i am getting below error.

error: error validating &quot;xxx-deployment.yaml&quot;: error validating data: validationerror(deployment.spec.template.spec.imagepullsecrets[0]): unknown field &quot;volumemounts&quot; in io.k8s.api.core.v1.localobjectreference; if you choose to ignore these errors, turn validation off with --validate=false

apiversion: apps/v1
kind: deployment
metadata:
  name: xxx
  namespace: xxx
spec:
  selector:
    matchlabels:
      app: xxx
  replicas: 3
  template:
    metadata:
      labels:
        app: xxx
    spec:
     containers:
     - name: xxx
       image: xxx:1.xx
       imagepullpolicy: &quot;always&quot;
       stdin: true
       tty: true
       ports:
       - containerport: 80
       imagepullpolicy: always
     imagepullsecrets:
     - name: xxx
       volumemounts:
        - mountpath: /data
          name: xxx-data
     restartpolicy: always
     volumes:
      - name: xx-data
        persistentvolumeclaim:
          claimname: xx-xx-pvc

",<kubernetes><kubernetes-deployment>,63430110,3,"you need to move the imagepullsecret further down. it's breaking the container spec. imagepullsecret is defined at the pod spec level while volumemounts belongs to the container spec
apiversion: apps/v1
kind: deployment
metadata:
  name: xxx
  namespace: xxx
spec:
  selector:
    matchlabels:
      app: xxx
  replicas: 3
  template:
    metadata:
      labels:
        app: xxx
    spec:
     containers:
     - name: xxx
       image: xxx:1.xx
       imagepullpolicy: &quot;always&quot;
       stdin: true
       tty: true
       ports:
       - containerport: 80
       imagepullpolicy: always
       volumemounts:
        - mountpath: /data
          name: xxx-data
     imagepullsecrets:
     - name: xxx
     restartpolicy: always
     volumes:
      - name: xx-data
        persistentvolumeclaim:
          claimname: xx-xx-pvc

"
63503353,shared azure file storage with statefulset on aks,"i have a statefulset with 3 instances on azure kubernetes 1.16, where i try to use azure file storage to create a single file share for the 3 instances.
i use azure files dynamic where all is declarative i.e. storage account, secrets, pvc's and pv's are created automatically.
manifest with volumeclaimtemplate
apiversion: apps/v1
kind: statefulset
metadata:
  name: xxx
spec:
  replicas: 3
  ...
  volumeclaimtemplates:
  - metadata:
      name: xxx-data-shared
    spec:
      accessmodes: [ readwritemany ]
      storageclassname: azfile-zrs-sc
      resources:
        requests:
            storage: 1gi

the storageclass:
kind: storageclass
apiversion: storage.k8s.io/v1
metadata:
  name: azfile-zrs-sc
provisioner: kubernetes.io/azure-file
reclaimpolicy: retain
volumebindingmode: waitforfirstconsumer
mountoptions:
  - dir_mode=0777
  - file_mode=0777
  - uid=0
  - gid=0
  - mfsymlinks
  - cache=strict
parameters:
  resourcegroup: xxx
  skuname: standard_zrs
  sharename: data

instead of one share, i end up with 3 pv's each referring to a separate created azure storage account each with a share data.
question: can i use the azure files dynamic, with additional configuration in the manifest to get a single file share? or will i have to do static?
",<azure><kubernetes><azure-aks><azure-files><kubernetes-statefulset>,63504633,3,"turns out that volumeclaimtemplates is not the right place (reference).
instead use persistentvolumeclaim.
for azure file storage this becomes:
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: data-shared-claim
spec:
  accessmodes:
    - readwritemany
  storageclassname: azfile-zrs-sc
  resources:
    requests:
      storage: 1gi  

and refer to it in the manifest:
apiversion: apps/v1
kind: statefulset
metadata:
  name: xxx
spec:
  replicas: 3
  ...
  template: 
    spec:
      containers:
      ...
        volumemounts:
        - name: data-shared
          mountpath: /data
      volumes:
        - name: data-shared
          persistentvolumeclaim:
            claimname: data-shared-claim

"
72600495,helm - check if value not exists or part of list,"assuming i have this values.yaml under my helm chart -
tasks:
  - name: test-production-dev
    env:
      - production
      - dev
  - name: test-dev
    env:
      - dev
  - name: test-all

environment_variables:
  stage: dev

i would like to run my cronjob based on these values -

if .env doesn't exist - run any time.
if .env exists - run only if environment_variables.stage is in the .env list.

this is what i've done so far ( with no luck ) -
{{- range $.values.tasks}}
# check if $value.env not exists or contains stage
{{if or .env (haskey .env &quot;$.values.environment_variables.stage&quot;)  }}
apiversion: batch/v1
kind: cronjob
...
{{- end}}
---
{{- end}}

",<kubernetes><kubernetes-helm><amazon-eks>,72602482,3,"values.yaml
tasks:
  - name: test-production-dev
    env:
      - production
      - dev
  - name: test-dev
    env:
      - dev
  - name: test-all
  - name: test-production
    env:
      - production

environment_variables:
  stage: dev

template/xxx.yaml
plan a
...
{{- range $.values.tasks }}
{{- $flag := false }}
{{- if .env }}
{{- range .env }}
{{- if eq . $.values.environment_variables.stage }}
{{- $flag = true }}
{{- end }}
{{- end }}
{{- else }}
{{- $flag = true }}
{{- end }}
{{- if $flag }}
apiversion: batch/v1
kind: cronjob
meta:
  name: {{ .name }}
{{- end }}
{{- end }}
...

plan b
...
{{- range $.values.tasks }}
{{- if or (not .env) (has $.values.environment_variables.stage .env) }}
apiversion: batch/v1
kind: cronjob
meta:
  name: {{ .name }}
{{- end }}
{{- end }}
...

output
...
apiversion: batch/v1
kind: cronjob
meta:
  name: test-production-dev
apiversion: batch/v1
kind: cronjob
meta:
  name: test-dev
apiversion: batch/v1
kind: cronjob
meta:
  name: test-all
...

"
63557606,helm scripts loop on json values,"i want to prepare the ingress with hosts and their values
example values file :
hosts:
  test1.test.top: {
    &quot;servicename&quot;: &quot;httpd-echo&quot;,
    &quot;serviceport&quot;: &quot;5678&quot;
  }
  test2.test.top: {
    &quot;servicename&quot;: &quot;httpd-echo2&quot;,
    &quot;serviceport&quot;: &quot;5678&quot;
  }

tried to apply it on template :
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: {{ include &quot;ingress.fullname&quot; . }}
  {{- with .values.annotations }}
  annotations:
    {{- toyaml . | nindent 4 }}
  {{- end }}
spec:
  rules:
    {{ range $key, $value := .values.hosts }}
    - host: {{ $key }}
      http:
        paths:
          - backend:
            {{- with .values.hosts.$value }}
              {{- toyaml . | nindent 4 }}
            {{- end }}
    {{ end }}

but have an error :

error: parse error at (general-ingress/templates/ingress.yaml:28): bad
character u+0024 '$'

kindly aks you to provide example how i can implement it, please
",<kubernetes><kubernetes-helm>,63558783,3,"is it required to have values defined as json? if yes, you need to declare them as
hosts:
  test1.test.top: |
    {
      &quot;servicename&quot;: &quot;httpd-echo&quot;,
      &quot;serviceport&quot;: &quot;5678&quot;
    }
  test2.test.top: |
    {
      &quot;servicename&quot;: &quot;httpd-echo2&quot;,
      &quot;serviceport&quot;: &quot;5678&quot;
    }
  

if your values.yaml can be like
hosts:
  test1.test.top: 
    servicename: httpd-echo
    serviceport: 5678
  test2.test.top: 
    servicename: httpd-echo2
    serviceport: 5678

one of the options of defining your ingress would be
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: {{ include &quot;ingress.fullname&quot; . }}
  {{- with .values.annotations }}
  annotations:
    {{- toyaml . | nindent 4 }}
  {{- end }}
spec:
  rules:
  {{- range $key, $value := .values.hosts }}
  - host: {{ $key }}
    http:
      paths:
        - backend:
          {{- $value | toyaml | nindent 12 }}
  {{- end }}

"
73351139,why can't load balancer connect to service in gke?,"i am deploying an application on gke cluster and try to deploy a load balancer to make clients able to call this application.
my application spec is:
apiversion: apps/v1
kind: deployment
metadata:
  name: api
  namespace: default
spec:
  replicas: 1

  selector:
    matchlabels:
      name: api
  template:
    metadata:
      labels:
        name: api
    spec:
      serviceaccountname: docker-sa
      containers:
        - name: api
          image: zhaoyi0113/rancher-go-api
          ports:
            - containerport: 8080

apiversion: v1
kind: service
metadata:
  name: api
  annotations:
    cloud.google.com/neg: '{&quot;ingress&quot;: true}'
spec:
  selector:
    name: api
  ports:
    - port: 80
      targetport: 8080
      protocol: tcp
  type: nodeport


it listens on the port 8080 and a service open port 80 and use the targetport 8080 to connect to the application.
and i have a ingress spec:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: sidecar
  namespace: default
spec:
  defaultbackend:
    service:
      name: api
      port:
        number: 80

after deploy, i am able to see the ip address from kubectl get ingress. but when i send a request to the ip, i got 502 error.
$ kubectl get ingress
name      class    hosts   address           ports   age
sidecar   &lt;none&gt;   *       107.178.245.193   80      28m

$ kubectl describe ingress sidecar
name:             sidecar
labels:           &lt;none&gt;
namespace:        default
address:          107.178.245.193
default backend:  api:80 (10.0.1.14:8080)
rules:
  host        path  backends
  ----        ----  --------
  *           *     api:80 (10.0.1.14:8080)
annotations:  ingress.kubernetes.io/backends: {&quot;k8s1-5ae02eec-default-api-80-28d7bbec&quot;:&quot;unknown&quot;}
              ingress.kubernetes.io/forwarding-rule: k8s2-fr-krllp0c9-default-sidecar-9a9n4r5m
              ingress.kubernetes.io/target-proxy: k8s2-tp-krllp0c9-default-sidecar-9a9n4r5m
              ingress.kubernetes.io/url-map: k8s2-um-krllp0c9-default-sidecar-9a9n4r5m
events:
  type    reason     age                  from                     message
  ----    ------     ----                 ----                     -------
  normal  sync       29m                  loadbalancer-controller  urlmap &quot;k8s2-um-krllp0c9-default-sidecar-9a9n4r5m&quot; created
  normal  sync       28m                  loadbalancer-controller  targetproxy &quot;k8s2-tp-krllp0c9-default-sidecar-9a9n4r5m&quot; created
  normal  sync       28m                  loadbalancer-controller  forwardingrule &quot;k8s2-fr-krllp0c9-default-sidecar-9a9n4r5m&quot; created
  normal  ipchanged  28m                  loadbalancer-controller  ip is now 107.178.245.193
  normal  sync       3m51s (x7 over 29m)  loadbalancer-controller  scheduled for sync


below is the curl error response:
$ curl -i http://107.178.245.193/health
http/1.1 502 bad gateway
content-type: text/html; charset=utf-8
referrer-policy: no-referrer
content-length: 332
date: tue, 16 aug 2022 10:40:31 gmt


&lt;html&gt;&lt;head&gt;
&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot;&gt;
&lt;title&gt;502 server error&lt;/title&gt;
&lt;/head&gt;
&lt;body text=#000000 bgcolor=#ffffff&gt;
&lt;h1&gt;error: server error&lt;/h1&gt;
&lt;h2&gt;the server encountered a temporary error and could not complete your request.&lt;p&gt;please try again in 30 seconds.&lt;/h2&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;/body&gt;&lt;/html&gt;

when i describe the service api, i got below error:
$ kubectl describe service api
name:                     api
namespace:                default
labels:                   &lt;none&gt;
annotations:              cloud.google.com/neg: {&quot;ingress&quot;: true}
                          cloud.google.com/neg-status: {&quot;network_endpoint_groups&quot;:{&quot;80&quot;:&quot;k8s1-29362abf-default-api-80-f2f1248a&quot;},&quot;zones&quot;:[&quot;australia-southeast2-a&quot;]}
                          field.cattle.io/publicendpoints: [{&quot;port&quot;:30084,&quot;protocol&quot;:&quot;tcp&quot;,&quot;servicename&quot;:&quot;default:api&quot;,&quot;allnodes&quot;:true}]
selector:                 name=api
type:                     nodeport
ip family policy:         singlestack
ip families:              ipv4
ip:                       10.3.253.54
ips:                      10.3.253.54
port:                     &lt;unset&gt;  80/tcp
targetport:               8080/tcp
nodeport:                 &lt;unset&gt;  30084/tcp
endpoints:                10.0.1.17:8080
session affinity:         none
external traffic policy:  cluster
events:
  type     reason        age   from            message
  ----     ------        ----  ----            -------
  warning  attachfailed  7s    neg-controller  failed to attach 2 network endpoint(s) (neg &quot;k8s1-29362abf-default-api-80-f2f1248a&quot; in zone &quot;australia-southeast2-a&quot;): googleapi: error 400: invalid value for field 'resource.ipaddress': '10.0.1.18'. specified ip address 10.0.1.18 doesn't belong to the (sub)network default or to the instance gke-gcp-cqrs-gcp-cqrs-node-pool-6b30ca5c-41q8., invalid
  warning  retryfailed   7s    neg-controller  failed to retry neg sync for &quot;default/api-k8s1-29362abf-default-api-80-f2f1248a--/80-8080-gce_vm_ip_port-l7&quot;: maximum retry exceeded

does anyone know what could be the root course?
",<kubernetes><google-cloud-platform><google-kubernetes-engine>,73351856,3,"i created a new gke cluster and tried setting up the same resources you are configuring. however, i used the following image for the container gcr.io/google-samples/hello-app:1.0. everything else remains the same - leaving the gcp-setup.yaml file i used below for reference.
gcp-setup.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: api
  namespace: default
spec:
  replicas: 1

  selector:
    matchlabels:
      name: api
  template:
    metadata:
      labels:
        name: api
    spec:
      containers:
        - name: api
          image: gcr.io/google-samples/hello-app:1.0
          ports:
            - containerport: 8080
---
apiversion: v1
kind: service
metadata:
  name: api
  annotations:
    cloud.google.com/neg: '{&quot;ingress&quot;: true}'
spec:
  selector:
    name: api
  ports:
    - port: 80
      targetport: 8080
      protocol: tcp
  type: nodeport
---
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: sidecar
  namespace: default
spec:
  defaultbackend:
    service:
      name: api
      port:
        number: 80

there is also a small thing i had to change in your configuration, which is the annotation block - when i first tried to apply your configuration, i got the below error. hence, i had to adjust the annotation entry to be annotations.
&gt; kubectl apply -f gcp-setup.yaml
deployment.apps/api created
error: error validating &quot;gcp-setup.yaml&quot;: error validating data: validationerror(service.metadata): unknown field &quot;annotation&quot; in io.k8s.apimachinery.pkg.apis.meta.v1.objectmeta; if you choose to ignore these errors, turn validation off with --validate=false

afterwards, i was able to successfully provision all of the resources, and your configuration worked perfectly fine. it took around 3 minutes i believe for the ingress resource to get an ip address assigned (masked as xx.xxx.xxx.xx below).
&gt; kubectl get pods
name                   ready   status    restarts   age
api-7d6fdd9845-8dwqc   1/1     running   0          7m13s

&gt; kubectl get services
name         type        cluster-ip    external-ip   port(s)        age
api          nodeport    10.36.4.150   &lt;none&gt;        80:30142/tcp   7m1s
kubernetes   clusterip   10.36.0.1     &lt;none&gt;        443/tcp        12m

&gt; kubectl get ingress
name      class    hosts   address         ports   age
sidecar   &lt;none&gt;   *       xx.xxx.xxx.xx   80      7m18s

&gt; kubectl describe ingress
name:             sidecar
namespace:        default
address:          xx.xxx.xxx.xx
default backend:  api:80 (10.32.0.10:8080)
rules:
  host        path  backends
  ----        ----  --------
  *           *     api:80 (10.32.0.10:8080)
annotations:  ingress.kubernetes.io/backends: {&quot;k8s1-05f3ce8b-default-api-80-82dd4d72&quot;:&quot;healthy&quot;}
              ingress.kubernetes.io/forwarding-rule: k8s2-fr-9k4w4ytx-default-sidecar-9m5g4dex
              ingress.kubernetes.io/target-proxy: k8s2-tp-9k4w4ytx-default-sidecar-9m5g4dex
              ingress.kubernetes.io/url-map: k8s2-um-9k4w4ytx-default-sidecar-9m5g4dex
events:
  type    reason     age                 from                     message
  ----    ------     ----                ----                     -------
  normal  sync       7m13s               loadbalancer-controller  urlmap &quot;k8s2-um-9k4w4ytx-default-sidecar-9m5g4dex&quot; created
  normal  sync       7m10s               loadbalancer-controller  targetproxy &quot;k8s2-tp-9k4w4ytx-default-sidecar-9m5g4dex&quot; created
  normal  sync       6m59s               loadbalancer-controller  forwardingrule &quot;k8s2-fr-9k4w4ytx-default-sidecar-9m5g4dex&quot; created
  normal  ipchanged  6m59s               loadbalancer-controller  ip is now xx.xxx.xxx.xx
  normal  sync       28s (x6 over 8m3s)  loadbalancer-controller  scheduled for sync

after the ingress resource became healthy, i was able to navigate in my browser to the assigned ip address xx.xxx.xxx.xx and got a successful response back from the workload i deployed (gcr.io/google-samples/hello-app:1.0).
browser output
hello, world!
version: 1.0.0
hostname: api-7d6fdd9845-8dwqc

as a conclusion, make sure to update your service definition from metadata.annotation to metadata.annotations. it was the only change i had to do to make your configuration work. furthermore, i recommend turning resource definition validation on to make sure that you catch such errors when defining new resources.
if the error still persists, i would recommend running kubectl describe ingress sidecar and analyze the output, assuming it is related to the ingress resource definition.
edit1
to make sure that this is not a zone-related issue, i provisioned a vpc-native, public cluster in the same zone that you are using (australia-southeast2-a). i then applied the same configuration, and it was successful, thus ruling out the zone-related topic.
based on the additional information you included in the post, my best guess for some potential root causes for the service error you're getting when running kubectl describe service would be:

your gke cluster is not vpc-native - i see this is a core requirement to be able to leverage neg
your gke cluster has been provisioned as a private cluster, and as a consequence, neg tries to assign an ip address from the available private subnet ranges. this would explain the 10.0.1.18 ip address that neg tries to assign to the resource definition

"
52558084,how to set secrets for a bunch of users that will have multiple fields?,"apiversion: v1
kind: secret
metadata:
  name: john-secret
data:
  username: abc=
  password: def=

apiversion: v1
kind: secret
metadata:
  name: jane-secret
data:
  username: ghi=
  password: jkl=


then i could include them like:

env:
  - name: john_username
    valuefrom:
      secretkeyref:
        name: john-secret
        key: username
  - name: john_password
    valuefrom:
      secretkeyref:
        name: john-secret
        key: password
  - name: jane_username
    valuefrom:
      secretkeyref:
        name: jane-secret
        key: username
  - name: jane_password
    valuefrom:
      secretkeyref:
        name: jane-secret
        key: password


and use them in node.js app like process.env.john_username, etc..

this works, but is there a cleaner/easier way to set secrets for a bunch of users that will have multiple fields? i imagine this would get messy with say 100 users x 5 fields.
",<kubernetes><kubernetes-secrets>,52559931,3,"you can mount the secret as a volume.  adapting the example from the linked kubernetes documentation:

apiversion: v1
kind: pod
metadata:
  name: secret-test-pod
spec:
  containers:
    - name: test-container
      image: nginx
      volumemounts:
          - name: john-secret
            mountpath: /etc/john-secret
  volumes:
    - name: secret-volume
      secret:
        secretname: john-secret


if you have a bunch of secrets, you'd need to mount them all into the pod spec.  that's a maintainability problem in itself.

i don't think anything actually stops you from using a more structured data object, like a json or yaml file, as the value of a secret.  that could work reasonably in combination with mounting it as a volume.

if you truly have a lot of secrets  many ""users"" with many values for each  then some sort of external storage for the secrets is probably a better idea.  if they're just usernames and passwords, it's very common to store a one-way hash of the password in a database (which also allows them to be updated without redeploying the system).  tools like hashicorp's vault can be complicated to administer, but have actual security of this content as a priority, and you get much more rigorous control over who can access the actual secrets.
"
73490636,helm variables inside configmap file,"so i had a configmap with a json configuration file in it, like this:
apiversion: v1
kind: configmap
metadata:
  name: my-config-map
data:
  config.json: |+
  {
    &quot;some-url&quot;: &quot;{{ .values.myapp.someurl }}&quot;
  }

but i've moved to having my config files outside the configmap's yaml, and just referencing them there, like this:
apiversion: v1
kind: configmap
metadata:
  name: my-config-map
data:
  config.json: |-
{{ .files.get .values.myapp.configfile | indent 4 }}

but now i want my json to look like the following
{
  &quot;some-url&quot;: &quot;{{ .values.myapp.someurl }}&quot;
}

the only thing i tried is what i just showed. i 'm not even sure how to look for this answer.
is it even possible?
",<kubernetes><kubernetes-helm>,73490771,3,"at the time of reading the file, its content is a string. it's not evaluated as template, and therefore you cannot use variables like you do.
however, helm has a function for this purpose specifically called tpl:
apiversion: v1
kind: configmap
metadata:
  name: my-config-map
data:
  config.json: |- 
{{ tpl (.files.get .values.myapp.configfile) $ | indent 4 }}

the tpl function takes a template string and renders it with some context. this is useful when you have template snippets in your values file or like in your case in some files content.
"
63871311,kubernetes role should grant access to all resources but it ignores some resources,"the role namespace-limited should have full access to all resources (of the specified api groups) inside of a namespace. my role manifest looks like this:
kind: role
apiversion: rbac.authorization.k8s.io/v1
metadata:
  name: namespace-limited
  namespace: restricted-xample
rules:
- apigroups:
  - core
  - apps
  - batch
  - networking.k8s.io
  resources: [&quot;*&quot;] # asterisk to grant access to all resources of the specified api groups
  verbs: [&quot;*&quot;]

i associated the role to a serviceaccount using a rolebinding but unfortunately this serviceaccount has no access to pod, service, secret, configmap and endpoint resources. these resources are all part of the core api group. all the other common workloads work though. why is that?
",<kubernetes><kubectl>,63871477,3,"the core group, also referred to as the legacy group, is at the rest path /api/v1 and uses apiversion: v1
you need to use &quot;&quot; for core api group.
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  namespace: restricted-xample
  name: namespace-limited
rules:
- apigroups: [&quot;&quot;, &quot;apps&quot;, &quot;batch&quot;, &quot;networking.k8s.io&quot;] # &quot;&quot; indicates the core api group
  resources: [&quot;*&quot;]
  verbs: [&quot;*&quot;]

to test the permission of the service account use below commands
kubectl auth can-i get pods --as=system:serviceaccount:restricted-xample:default -n restricted-xample 
kubectl auth can-i get secrets --as=system:serviceaccount:restricted-xample:default -n restricted-xample 
kubectl auth can-i get configmaps --as=system:serviceaccount:restricted-xample:default -n restricted-xample
kubectl auth can-i get endpoints --as=system:serviceaccount:restricted-xample:default -n restricted-xample 

"
52500511,using a configmap to specify list of whitelisted cidr blocks,"

what i want to do

i'd like to apply an ip whitelist defined in a configmap. i'd like to keep the list external because it's easier to have it in one file rather than putting the blocks inline. the whitelist will be used by many services in different namespaces.

what i have

a lot has been removed from the following spec files but hopefully enough has been retained.

i define the whitelist in a configmap as:

apiversion: v1
kind: configmap
data:
  whitelist:
    # example
    - 127.0.0.1/32
    # etc.
metadata:
  name: whitelist


my service, for this example, is:

apiversion: v1
kind: service
metadata:
  name: example
  labels:
    label: example


note that the service type is the default, as i rely on the ingress to expose it. this cannot change.

the service sits behind an ingress definition:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: example
  rules:
  - host: example.com
#... ports, etc


what i've tried

changing service type

defining the service as type: loadbalancer. this does exactly what i want as it's easy to consume the configmap, then i realized i cannot change the service type for business reasons.

using ingress annotations

apiversion: extensions/v1beta1
kind: ingress
# ...
metadata:
  name: whitelist
  annotations:
    nginx.ingress.kubernetes.io/whitelist-source-range: ""blocka"", ""blockb""


this kinda works but i couldn't figure out how to use the configmap instead of the comma-separated list. i should note here that any solution that lets me use an externally-defined list is acceptable, and it doesn't have to be a configmap just because.

using a networkpolicy

apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: example-network-policy
spec:
  policytypes:
  - ingress
  ingress:
  - from:
    - ipblock:
      cidr: # hmm... can i add the configmap here?
    - namespaceselector:
      # ...etc


so far this looks the most promising, but the ipblock selector seems to only accept one block...?
",<kubernetes><whitelist><kubernetes-ingress><kubernetes-networkpolicy>,52504348,3,"
  using ingress annotations


configmaps are not supported in annotations.


  using a networkpolicy


ingress and egress are array structures so you can try this, not sure if it works for you:

apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: example-network-policy
spec:
  policytypes:
  - ingress
  ingress:
  - from:
    - ipblock:
      cidr: &lt;cidr1&gt;
    - namespaceselector:
      # ...etc
  - from:
    - ipblock:
      cidr: &lt;cidr2&gt;
    - namespaceselector:
      # ...etc
  - from:
    - ipblock:
      cidr: &lt;cidr3&gt;
    - namespaceselector:
      # ...etc

"
73905687,how to read .jks file into kubernetes secret?,"i have created a secret.yaml file as follows:
apiversion: v1
kind: secret
metadata:
  name: my-secret
data:
  truststore.jks: {{ (.files.glob &quot;../trust.jks&quot;).assecrets | b64enc }}

i am calling this as part of template .yaml file in helm.
.yaml
apiversion: v1
kind: deploymentconfig
spec:
...
template:
  spec:
  ...
  container:
   - name: &quot;my-container&quot;
     ...
     volumemounts:
        - name: secrets
          mountpath: /mnt/secrets
          readonly: true

 volumes:
    - name: secrets
      secret:
        secretname: &quot;my-secret&quot;

when i run helm install command the pod gets created successfully, and the volume is also mounted, but if i check the truststore.jks in /mnt/secrets using cat command below is the output:
cat /mnt/secrets/truststore.jks
{}

i ran the dry run command to check the generated .yaml file, the secret is populted as below:
# source: ag-saas/templates/tssecret.yaml
apiversion: v1
kind: secret
metadata:
  name: my-secret
data:
  truststore.jks: e30=

how do i get the file into my secret?
",<kubernetes><kubernetes-helm><truststore><kubernetes-secrets>,73908401,3,"there's a couple of things going on here:

.files.glob is intended to retrieve multiple files, e.g. .files.glob &quot;credentials/*.jks&quot;.  for a single file .file.get will retrieve its contents directly.
you can only access files inside the chart directory; referencing .files.get &quot;../trust.jks&quot; won't work.
.files.glob.assecret renders a list of files to the entire contents of the data: block; you just directly need the file content.

so your secret should look like
apiversion: v1
kind: secret
metadata:
  name: my-secret
data:
  truststore.jks: {{ .files.get &quot;trust.jks&quot; | b64enc }}

where in the last line i've used .files.get, i've not tried to refer to a &quot;../...&quot; path outside the chart, and i don't render it to ...assecret.
you also will need to move or copy (not symlink) the keyset file into the chart directory for this to work.
(in the current form, .files.glob won't match anything outside the chart directory, so you get an empty list of files.  then rendering that to .assecrets gets you an empty json object.  you're using that string {} as the secret value, which gets correctly base64-encoded, but that's why {} comes out at the end.)
"
73929070,horizontal pod autoscaling using a logging custom metric in gke,"i would like to scale my deployment based on a custom logging metric, but i'm not able to make that work, i created already the custom metric and i'm also able to see it in the metric explorer but for some reason the stackdriver adapter is not able to get the metric values.



this is my hpa.yaml
apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  name: nginx-hpa
spec:
  minreplicas: 1
  maxreplicas: 5
  metrics:
  - external:
      metricname: logging.googleapis.com|user|http_request_custom
      targetvalue: &quot;20&quot;
    type: external
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: nginx

but i'm always getting the following error:
&quot;unable to get external metric default/logging.googleapis.com|user|http_request_custom/nil: unable to fetch metrics from external metrics api: the server could not find the requested resource (get logging.googleapis.com|user|http_request_custom.external.metrics.k8s.io)&quot;

should i do something different?? any idea?
",<kubernetes><google-kubernetes-engine><stackdriver><hpa>,73931170,3,"not sure you have created the service account and granted access to the adapter however there is two models of custom metrics adapter. legacy adapter and new resource version.
if adapter is up and running did you check the logs of pod ?
new resource model to install :
kubectl apply -f https://raw.githubusercontent.com/googlecloudplatform/k8s-stackdriver/master/custom-metrics-stackdriver-adapter/deploy/production/adapter_new_resource_model.yaml

ref yaml you can use this way further metrics
apiversion: autoscaling/v2beta2
kind: horizontalpodautoscaler
metadata:
  name: pubsub
spec:
  minreplicas: 1
  maxreplicas: 5
  metrics:
  - external:
      metric:
       name: pubsub.googleapis.com|subscription|num_undelivered_messages
       selector:
         matchlabels:
           resource.labels.subscription_id: echo-read
      target:
        type: averagevalue
        averagevalue: 2
    type: external
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: pubsub

ref : https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub_4
"
61914858,createcontainerconfigerror while deploying on kubernetes,"this is my deployment file:
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    service: udagram-user
  name: udagram-user
spec:
  replicas: 1
  selector:
    matchlabels:
      service: udagram-user
  template:
    metadata:
      labels:
        service: udagram-user
    spec:
      containers:
      - image: pranjal121997/udagram-user
        name: udagram-user
        imagepullpolicy: always
        resources:
          requests:
            memory: &quot;64mi&quot;
            cpu: &quot;250m&quot;
          limits:
            memory: &quot;1024mi&quot;
            cpu: &quot;500m&quot;
        env:
        - name: postgress_password
          valuefrom:
            configmapkeyref:
              name: env-config
              key: postgress_password
        - name: postgress_username
          valuefrom:
            configmapkeyref:
              name: env-config
              key: postgress_username
        - name: url
          valuefrom:
            configmapkeyref:
              name: env-config
              key: url
        - name: jwt_secret
          valuefrom:
            configmapkeyref:
              name: env-config
              key: jwt_secret
        - name: postgress_database
          valuefrom:
            configmapkeyref:
              name: env-config
              key: postgress_database
        - name: postgress_host
          valuefrom:
            configmapkeyref:
              name: env-config
              key: postgress_host
      restartpolicy: always

on deploying via kubectl create, it runs into createcontainerconfigerror. below is the output of kubectl describe pod:
name:           udagram-user-f57f44889-5jtxf
namespace:      default
priority:       0
node:           ip-172-31-43-242.ap-south-1.compute.internal/172.31.43.242
start time:     wed, 20 may 2020 17:58:34 +0530
labels:         pod-template-hash=f57f44889
                service=udagram-user
annotations:    kubernetes.io/psp: eks.privileged
status:         pending
ip:             172.31.33.34
controlled by:  replicaset/udagram-user-f57f44889
containers:
  udagram-user:
    container id:   
    image:          pranjal121997/udagram-user
    image id:       
    port:           &lt;none&gt;
    host port:      &lt;none&gt;
    state:          waiting
      reason:       createcontainerconfigerror
    ready:          false
    restart count:  0
    limits:
      cpu:     500m
      memory:  1gi
    requests:
      cpu:     250m
      memory:  64mi
    environment:
      postgress_password:  &lt;set to the key 'postgress_password' of config map 'env-config'&gt;  optional: false
      postgress_username:  &lt;set to the key 'postgress_username' of config map 'env-config'&gt;  optional: false
      url:                 &lt;set to the key 'url' of config map 'env-config'&gt;                 optional: false
      jwt_secret:          &lt;set to the key 'jwt_secret' of config map 'env-config'&gt;          optional: false
      postgress_database:  &lt;set to the key 'postgress_database' of config map 'env-config'&gt;  optional: false
      postgress_host:      &lt;set to the key 'postgress_host' of config map 'env-config'&gt;      optional: false
    mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-skqmw (ro)
conditions:
  type              status
  initialized       true 
  ready             false 
  containersready   false 
  podscheduled      true 
volumes:
  default-token-skqmw:
    type:        secret (a volume populated by a secret)
    secretname:  default-token-skqmw
    optional:    false
qos class:       burstable
node-selectors:  &lt;none&gt;
tolerations:     node.kubernetes.io/not-ready:noexecute for 300s
                 node.kubernetes.io/unreachable:noexecute for 300s
events:
  type     reason                  age                  from                                                   message
  ----     ------                  ----                 ----                                                   -------
  normal   scheduled               3m9s                 default-scheduler                                      successfully assigned default/udagram-user-f57f44889-5jtxf to ip-172-31-43-242.ap-south-1.compute.internal
  warning  failedcreatepodsandbox  3m8s                 kubelet, ip-172-31-43-242.ap-south-1.compute.internal  failed create pod sandbox: rpc error: code = unknown desc = failed to set up sandbox container &quot;93df5832a932be9ad03d0cfd1bbaaae2c44fed0073f1325e02697fd9f6b391e9&quot; network for pod &quot;udagram-user-f57f44889-5jtxf&quot;: networkplugin cni failed to set up pod &quot;udagram-user-f57f44889-5jtxf_default&quot; network: add cmd: failed to assign an ip address to container
  warning  failedcreatepodsandbox  3m7s                 kubelet, ip-172-31-43-242.ap-south-1.compute.internal  failed create pod sandbox: rpc error: code = unknown desc = failed to set up sandbox container &quot;ebcf88c3b4d88a19994f6fdd5eee011f257d9e40348f559758e94e7c368da3b2&quot; network for pod &quot;udagram-user-f57f44889-5jtxf&quot;: networkplugin cni failed to set up pod &quot;udagram-user-f57f44889-5jtxf_default&quot; network: add cmd: failed to assign an ip address to container
  warning  failedcreatepodsandbox  3m6s                 kubelet, ip-172-31-43-242.ap-south-1.compute.internal  failed create pod sandbox: rpc error: code = unknown desc = failed to set up sandbox container &quot;486dd729e6f9781f7440305cadd0ec6a8eb82129b07efaed2dc1b707c5d03f64&quot; network for pod &quot;udagram-user-f57f44889-5jtxf&quot;: networkplugin cni failed to set up pod &quot;udagram-user-f57f44889-5jtxf_default&quot; network: add cmd: failed to assign an ip address to container
  warning  failedcreatepodsandbox  3m5s                 kubelet, ip-172-31-43-242.ap-south-1.compute.internal  failed create pod sandbox: rpc error: code = unknown desc = failed to set up sandbox container &quot;35e39b69cd153b1a9b76b2fc672dec151afb5aebbce5999712891ab6c2329e9f&quot; network for pod &quot;udagram-user-f57f44889-5jtxf&quot;: networkplugin cni failed to set up pod &quot;udagram-user-f57f44889-5jtxf_default&quot; network: add cmd: failed to assign an ip address to container
  normal   sandboxchanged          3m4s (x4 over 3m7s)  kubelet, ip-172-31-43-242.ap-south-1.compute.internal  pod sandbox changed, it will be killed and re-created.
  warning  failed                  2m9s (x5 over 3m2s)  kubelet, ip-172-31-43-242.ap-south-1.compute.internal  error: couldn't find key postgress_database in configmap default/env-config
  normal   pulling                 114s (x6 over 3m4s)  kubelet, ip-172-31-43-242.ap-south-1.compute.internal  pulling image &quot;pranjal121997/udagram-user&quot;
  normal   pulled                  112s (x6 over 3m2s)  kubelet, ip-172-31-43-242.ap-south-1.compute.internal  successfully pulled image &quot;pranjal121997/udagram-user&quot;

i also have 2 other files: aws-secret.yaml and env-secret.yaml containing my postgress username, password and access id's but somehow kubernetes expects all keys to be present in env-configmap.yaml. how do i force my cluster to pick up secrets from the secrets file?
",<javascript><amazon-web-services><kubernetes><kubectl><amazon-eks>,61915915,3,"in the deployment yaml env-config is referred as configmapkeyref in all the places. hence kubernetes is expecting postgress_database to be present in env-config configmap. you can refer to a secret which contains key postgress_database key using secretkeyref. 

here is an  example.

apiversion: v1
kind: pod
metadata:
  name: env-single-secret
spec:
  containers:
  - name: envars-test-container
    image: nginx
    env:
    - name: postgress_database
      valuefrom:
        secretkeyref:
          name: postgres-database-secret-name
          key: postgress_database

"
61963659,how can i add service annotation in istio operator patch,"i am installing istio 1.6.0 using istioctl with below config file:

--
apiversion: install.istio.io/v1alpha1
kind: istiooperator
spec:
  profile: default
  components:
    egressgateways:
      - name: istio-egressgateway
        enabled: true
    ingressgateways:
      - name: istio-ingressgateway
        enabled: true
        k8s:
          overlays:
          - kind: service
            name: istio-ingressgateway
            patches:
            - path: spec.loadbalancerip
              value: x.x.x.x
            - path: spec.externaltrafficpolicy
              value: local
            - path: metadata.annotations.[service.beta.kubernetes.io/azure-load-balancer-resource-group]
              value: az-rg-group


this part giving me an error:

- path: metadata.annotations.[service.beta.kubernetes.io/azure-load-balancer-resource-group]
  value: az-rg-group



  error: failed to apply manifests: errors occurred during operation


path is not correct for annotation. how can i provide a path of annotation with valid syntax?

following this sample code: https://github.com/istio/istio/blob/master/operator/samples/pilot-advanced-override.yaml
",<kubernetes><kubectl><istio>,61968467,3,"there is a new field for service annotations. the issue was raised here https://github.com/istio/istio/issues/20078
please refer the following example
ingressgateway_k8s_settings.yaml
apiversion: install.istio.io/v1alpha1
kind: istiooperator
spec:
  components:
    pilot:
      enabled: false
    ingressgateways:
    - namespace: istio-system
      name: istio-ingressgateway
      enabled: true
      k8s:
        service:
          externaltrafficpolicy: local
        serviceannotations:
          manifest-generate: &quot;testserviceannotation&quot;
        securitycontext:
          sysctls:
          - name: &quot;net.ipv4.ip_local_port_range&quot;
            value: &quot;80 65535&quot;

"
76045600,how can i create a kubernetes service which selects pods from another namespace?,"in certain cases i want to redirect all traffic to my service to pods in another namespace. how can i do this?
i have these pod in my-namespace-1:
apiversion: v1
kind: pod
metadata:
  name: my-pod
  namespace: my-namespace-1
  labels:
    app: my-app
spec:
  containers:
    - name: nginx
      image: nginx
      ports:
        - containerport: 80

and service in my-namespace-2:
apiversion: v1
kind: service
metadata:
  name: my-service
  namespace: my-namespace-2
spec:
  selector:
    app: my-app
  ports:
    - name: http
      port: 80
      targetport: 80
  type: clusterip

but when i call the service i get http status 503 from the ingress controller i guess.
do i need to set up an service in my-namespace-2 too? or what else is missing?
",<kubernetes><kubernetes-service>,76045844,3,"the service cannot select pods in other namespaces. it can only select pods in its own namespace, but question is why cant you create a service in the other namespace directly?
as a workaround, what you can do is create a service my-service in my-namespace-1 as well and proxy from service in my-namespace-2 e.g. create the service in my-namespace-2 like this
kind: service
apiversion: v1
metadata:
  name: my-service
  namespace: my-namespace-2
spec:
  type: externalname
  externalname: my-service.my-namespace-1.svc.cluster.local
  ports:
  - port: 80

"
61996500,how to apply kubernetes network policies to restrict access of namespace from other namespace?,"i am new to kubernetes. i have this scenario for multi-tenancy

1) i have 3 namespaces as shown here:

 default,
 tenant1-namespace,
 tenant2-namespace


2) namespace default has two database pods

tenant1-db - listening on port 5432
tenant2-db - listening on port 5432


namespace tenant1-ns has one app pod

tenant1-app - listening on port 8085


namespace tenant2-ns has one app pod

tenant2-app - listening on port 8085


3) i have applied 3 network policies on default namespace

a) to restrict access to both db pods from other namespaces

apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: deny-all
  namespace: default
spec:
  podselector: {}
  policytypes:
  - ingress
  - egress


b) to allow access to tenant1-db pod from tenant1-app of tenant1-ns only

apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: deny-from-other-namespaces-except-specific-pod-1
  namespace: default
spec:
  podselector:
    matchlabels:
      k8s-app: tenant1-db
  ingress:
  - from:
    - namespaceselector:
        matchlabels:
          name: tenant1-development
    - podselector:
        matchlabels:
          app: tenant1-app


c) to allow access to tenant2-db pod from tenant2-app of tenant2-ns only

apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: deny-from-other-namespaces-except-specific-pod-2
  namespace: default
spec:
  podselector:
    matchlabels:
      k8s-app: tenant2-db
  ingress:
  - from:
    - namespaceselector:
        matchlabels:
          name: tenant2-development
    - podselector:
        matchlabels:
          app: tenant2-app


i want to restrict access of tenant1-db to tenant1-app only, tenant2-db to tenant2-app only. but it seems tenant2-app can access tenant1-db which should not happen.

below is db-config.js for tenant2-app

module.exports = {
  host: ""tenant1-db"",
  user: ""postgres"",
  password: ""postgres"",
  db: ""tenant1db"",
  dialect: ""postgres"",
  pool: {
    max: 5,
    min: 0,
    acquire: 30000,
    idle: 10000
  }
};


as you can see i am pointing tenant2-app to use tenant1-db, i want to restrict tennat1-db to tenant1-app only? what modifications needs to do in network policies ?

updates :

tenant1 deployment &amp; services yamls

apiversion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 
kind: deployment 
metadata: 
  name: tenant1-app-deployment
  namespace: tenant1-namespace 
spec: 
  selector: 
    matchlabels: 
      app: tenant1-app 
  replicas: 1 # tells deployment to run 1 pods matching the template 
  template: 
    metadata: 
      labels: 
        app: tenant1-app 
    spec: 
      containers: 
      - name: tenant1-app-container 
        image: tenant1-app-dock-img:v1 
        ports: 
        - containerport: 8085 
--- 
# https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service  
kind: service 
apiversion: v1 
metadata: 
  name: tenant1-app-service
  namespace: tenant1-namespace  
spec: 
  selector: 
    app: tenant1-app 
  ports: 
  - protocol: tcp 
    port: 8085 
    targetport: 8085 
    nodeport: 31005 
  type: loadbalancer 


tenant2-app deployments &amp; service yamls

apiversion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 
kind: deployment 
metadata: 
  name: tenant2-app-deployment
  namespace: tenant2-namespace 
spec: 
  selector: 
    matchlabels: 
      app: tenant2-app 
  replicas: 1 # tells deployment to run 1 pods matching the template 
  template: 
    metadata: 
      labels: 
        app: tenant2-app 
    spec: 
      containers: 
      - name: tenant2-app-container 
        image: tenant2-app-dock-img:v1 
        ports: 
        - containerport: 8085 
--- 
# https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service  
kind: service 
apiversion: v1 
metadata: 
  name: tenant2-app-service
  namespace: tenant2-namespace  
spec: 
  selector: 
    app: tenant2-app 
  ports: 
  - protocol: tcp 
    port: 8085 
    targetport: 8085 
    nodeport: 31006 
  type: loadbalancer 


updates 2 :

db-pod1.yaml

    apiversion: apps/v1
    kind: deployment
    metadata:
      annotations:
        deployment.kubernetes.io/revision: ""1""
      creationtimestamp: null
      generation: 1
      labels:
        k8s-app: tenant1-db
      name: tenant1-db
    spec:
      progressdeadlineseconds: 600
      replicas: 1
      revisionhistorylimit: 10
      selector:
        matchlabels:
          k8s-app: tenant1-db
      strategy:
        rollingupdate:
          maxsurge: 25%
          maxunavailable: 25%
        type: rollingupdate
      template:
        metadata:
          creationtimestamp: null
          labels:
            k8s-app: tenant1-db
          name: tenant1-db
        spec:
          volumes:
          - name: tenant1-pv-storage
            persistentvolumeclaim:
              claimname: tenant1-pv-claim
          containers:
          - env:
            - name: postgres_user
              value: postgres
            - name: postgres_password
              value: postgres
            - name: postgres_db
              value: tenant1db
            - name: pgdata
              value: /var/lib/postgresql/data/pgdata
            image: postgres:11.5-alpine
            imagepullpolicy: ifnotpresent
            name: tenant1-db
            volumemounts:
            - mountpath: ""/var/lib/postgresql/data/pgdata""
              name: tenant1-pv-storage
            resources: {}
            securitycontext:
              privileged: false
            terminationmessagepath: /dev/termination-log
            terminationmessagepolicy: file
          dnspolicy: clusterfirst
          restartpolicy: always
          schedulername: default-scheduler
          securitycontext: {}
          terminationgraceperiodseconds: 30
status: {}


db-pod2.ymal

apiversion: apps/v1
kind: deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: ""1""
  creationtimestamp: null
  generation: 1
  labels:
    k8s-app: tenant2-db
  name: tenant2-db
spec:
  progressdeadlineseconds: 600
  replicas: 1
  revisionhistorylimit: 10
  selector:
    matchlabels:
      k8s-app: tenant2-db
  strategy:
    rollingupdate:
      maxsurge: 25%
      maxunavailable: 25%
    type: rollingupdate
  template:
    metadata:
      creationtimestamp: null
      labels:
        k8s-app: tenant2-db
      name: tenant2-db
    spec:
      volumes:
      - name: tenant2-pv-storage
        persistentvolumeclaim:
          claimname: tenant2-pv-claim
      containers:
      - env:
        - name: postgres_user
          value: postgres
        - name: postgres_password
          value: postgres
        - name: postgres_db
          value: tenant2db
        - name: pgdata
          value: /var/lib/postgresql/data/pgdata
        image: postgres:11.5-alpine
        imagepullpolicy: ifnotpresent
        name: tenant2-db
        volumemounts:
        - mountpath: ""/var/lib/postgresql/data/pgdata""
          name: tenant2-pv-storage
        resources: {}
        securitycontext:
          privileged: false
        terminationmessagepath: /dev/termination-log
        terminationmessagepolicy: file
      dnspolicy: clusterfirst
      restartpolicy: always
      schedulername: default-scheduler
      securitycontext: {}
      terminationgraceperiodseconds: 30
status: {}


update 3 :

kubectl get svc -n default
name            type           cluster-ip       external-ip      port(s)          age
kubernetes      clusterip      10.96.0.1        &lt;none&gt;           443/tcp          5d2h
nginx           clusterip      10.100.24.46     &lt;none&gt;           80/tcp           5d1h
tenant1-db   loadbalancer   10.111.165.169   10.111.165.169   5432:30810/tcp   4d22h
tenant2-db   loadbalancer   10.101.75.77     10.101.75.77     5432:30811/tcp   2d22h

kubectl get svc -n tenant1-namespace
name                  type           cluster-ip      external-ip                               port(s)          age
tenant1-app-service   loadbalancer   10.111.200.49   10.111.200.49                             8085:31005/tcp   3d
tenant1-db         externalname   &lt;none&gt;          tenant1-db.default.svc.cluster.local   5432/tcp         2d23h

kubectl get svc -n tenant2-namespace
name                  type           cluster-ip     external-ip                               port(s)          age
tenant1-db         externalname   &lt;none&gt;         tenant1-db.default.svc.cluster.local   5432/tcp         2d23h
tenant2-app-service   loadbalancer   10.99.139.18   10.99.139.18                              8085:31006/tcp   2d23h

",<network-programming><kubernetes><cloud><kubernetes-pod>,61996990,3,"referring from the docs let's understand the below policy that you have for tenant2. 

apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: deny-from-other-namespaces-except-specific-pod-2
  namespace: default
spec:
  podselector:
    matchlabels:
      k8s-app: tenant2-db
  ingress:
  - from:
    - namespaceselector:
        matchlabels:
          name: development
    - podselector:
        matchlabels:
          app: tenant2-app


the above network policy that you have defined has two elements in the form array which says allow connections from pods in the local (default) namespace with the label app=tenant2-app, or from any pod in any namespace with the label name=development.

if you merge the rules into a single rule as below it should solve the issue.

apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: deny-from-other-namespaces-except-specific-pod-2
  namespace: default
spec:
  podselector:
    matchlabels:
      k8s-app: tenant2-db
  ingress:
  - from:
    - namespaceselector:
        matchlabels:
          name: tenant2-development
      podselector:
        matchlabels:
          app: tenant2-app


above network policy means allow connections from pods with the label app=tenant2-app in namespaces with the label name=tenant2-development.

add a label name=tenant2-development to the tenant2-ns namespace.

do the same exercise for tenant1 as well as bellow:

apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: deny-from-other-namespaces-except-specific-pod-1
  namespace: default
spec:
  podselector:
    matchlabels:
      k8s-app: tenant1-db
  ingress:
  - from:
    - namespaceselector:
        matchlabels:
          name: tenant1-development
      podselector:
        matchlabels:
          app: tenant1-app


add a label name=tenant1-development to the tenant1-ns namespace.
"
62004937,kubernetes automatically adds the storageclassname to pvc,"i have a helm chart that contains pv and pvc to mount nfs volumes and this works fine. i need to install this helm chart on a new cluster that has very strict and limited security measures and i also see that my pods are pending because they can't mount the nfs.

after some investigations, i found out that the problem is the pvc and pv have different storageclassname:

kubectl -n 57 describe pvc gstreamer-claim


events:
  type       reason             age                 from                         message
  ----       ------             ----                ----                         -------
  warning    volumemismatch     98s (x83 over 21m)  persistentvolume-controller  cannot bind to requested volume ""gstreamer-57"": storageclassname does not match


this is very strange since the pvc in my helm chart doesn't have any storageclassname at all:
pvc:

- apiversion: v1
  kind: persistentvolumeclaim
  metadata:
    name: gstreamer-claim
    namespace: {{ .release.namespace }}
  spec:
    volumename: gstreamer-{{ .release.namespace }}
    accessmodes:
      - readwriteonce
    resources:
      requests:
        storage: 10gi


pv:

- apiversion: v1
  kind: persistentvolume
  metadata:
    name: gstreamer-{{ .release.namespace }}
  spec:
    capacity:
      storage: 10gi
    accessmodes:
      - readwriteonce
    persistentvolumereclaimpolicy: recycle
    mountoptions:
      - hard
      - nfsvers=4.1
    nfs:
      server: {{ .values.global.nfsserver }}
      path: /var/nfs/general/gstreamer-{{ .release.namespace }}


i tried to edit the pvc but i was not able to change it.

why this is happening? can it be related to the cluster security?
how to fix this?

update

storage class info:

kubectl -n 57 get sc
name                      provisioner                                       age
local-storage (default)   kubernetes.io/no-provisioner                      54d
nfs-client                cluster.local/nfs-client-nfs-client-provisioner   43m


kubectl -n 57 get sc local-storage -o yaml
apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {""apiversion"":""storage.k8s.io/v1"",""kind"":""storageclass"",""metadata"":{""annotations"":{""storageclass.kubernetes.io/is-default-class"":""true""},""name"":""local-storage""},""provisioner"":""kubernetes.io/no-provisioner"",""volumebindingmode"":""waitforfirstconsumer""}
    storageclass.kubernetes.io/is-default-class: ""true""
  creationtimestamp: ""2020-03-31t20:46:39z""
  name: local-storage
  resourceversion: ""458""
  selflink: /apis/storage.k8s.io/v1/storageclasses/local-storage
  uid: b8352eb1-7390-11ea-84a7-fa163e393634
provisioner: kubernetes.io/no-provisioner
reclaimpolicy: delete
volumebindingmode: waitforfirstconsumer


",<kubernetes><kubernetes-helm><persistent-storage><kubernetes-pvc>,62005185,3,"with dynamic provisioning you don't need to create a pv explicitly. create a pvc with storage class nfs-client.

apiversion: v1
  kind: persistentvolumeclaim
  metadata:
    name: gstreamer-claim
    namespace: {{ .release.namespace }}
  spec:
    volumename: gstreamer-{{ .release.namespace }}
    accessmodes:
      - readwriteonce
    resources:
      requests:
        storage: 10gi
    storageclassname: nfs-client


another option would be to make nfs-client as the default storage class and there will be no need to specify storageclassname: nfs-client in pvc.
"
74779468,how to project kubernetes secret at the /etc/ level?,"i am following kubernetes documentations on secret.  i have this secret.yaml file:
apiversion: v1
kind: secret
metadata:
  name: mysecret
type: opaque
data:
  val1: yxnkzgo=
stringdata:
  val1: asdf

and secret-pod.yaml:
apiversion: v1
kind: pod
metadata:
  name: mysecretpod
spec:
  containers:
  - name: mypod
    image: nginx
    volumemounts:
    - name: myval
      mountpath: /etc/secret
      readonly: true
  volumes:
  - name: myval
    secret:
      secretname: val1
      items:
      - key: val1
        path: myval

i use kubectl apply -f on both of these files.  then using kubectl exec -it mysecretpod -- cat /etc/secret/myval, i can see the value asdf in the file /etc/secret/myval of mysecretpod.
however i want the mounted path to be /etc/myval.  thus i make the following change in secret-pod.yaml:
    volumemounts:
    - name: myval
      mountpath: /etc
      readonly: true

after using kubectl apply -f on that file again, i check pod creation with kubectl get pods --all-namespaces.  this is what i see:
namespace     name                               ready   status             restarts      age
default       mysecretpod                        0/1     crashloopbackoff   2 (34s ago)   62s

looking into that pod using kubectl describe pods mysecretpod, this is what i see:
events:
  type     reason     age               from               message
  ----     ------     ----              ----               -------
  normal   scheduled  35s               default-scheduler  successfully assigned default/mysecretpod to minikube
  normal   pulled     32s               kubelet            successfully pulled image &quot;nginx&quot; in 2.635766453s
  warning  failed     31s               kubelet            error: failed to start container &quot;mypod&quot;: error response from daemon: oci runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: rootfs_linux.go:76: mounting &quot;/var/lib/docker/containers/c84a8d278dc2f131daf9f322d26ff8c54d68cea8cd9c0ce209f68d7a9b677b3c/resolv.conf&quot; to rootfs at &quot;/etc/resolv.conf&quot; caused: open /var/lib/docker/overlay2/4aaf54c61f7c80937a8edc094b27d6590538632e0209165e0b8c96e9e779a4b6/merged/etc/resolv.conf: read-only file system: unknown
  normal   pulled     28s               kubelet            successfully pulled image &quot;nginx&quot; in 3.313846185s
  warning  failed     28s               kubelet            error: failed to start container &quot;mypod&quot;: error response from daemon: oci runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: rootfs_linux.go:76: mounting &quot;/var/lib/docker/containers/c84a8d278dc2f131daf9f322d26ff8c54d68cea8cd9c0ce209f68d7a9b677b3c/resolv.conf&quot; to rootfs at &quot;/etc/resolv.conf&quot; caused: open /var/lib/docker/overlay2/34af5138f14d192ade7e53211476943ea82cd2c8186d69ca79a3adf2abbc0978/merged/etc/resolv.conf: read-only file system: unknown
  warning  backoff    24s               kubelet            back-off restarting failed container
  normal   pulling    9s (x3 over 34s)  kubelet            pulling image &quot;nginx&quot;
  normal   created    7s (x3 over 32s)  kubelet            created container mypod
  normal   pulled     7s                kubelet            successfully pulled image &quot;nginx&quot; in 2.73055072s
  warning  failed     6s                kubelet            error: failed to start container &quot;mypod&quot;: error response from daemon: oci runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: rootfs_linux.go:76: mounting &quot;/var/lib/docker/containers/c84a8d278dc2f131daf9f322d26ff8c54d68cea8cd9c0ce209f68d7a9b677b3c/resolv.conf&quot; to rootfs at &quot;/etc/resolv.conf&quot; caused: open /var/lib/docker/overlay2/01bfa6b2c35d5eb12ad7ad204a5acc58688c1e04d9b5891382e48c26d2e7077f/merged/etc/resolv.conf: read-only file system: unknown

why does this fail?  is it possible to have a secret mounted at the /etc level instead of /etc/something level?  if yes, how can i achieve that?  thank you so much!
",<kubernetes><kubernetes-secrets>,74780284,3,"volumemounts:
- name: myval
  mountpath: /etc
  readonly: true

instead of /etc directory, try mount as a single file:
apiversion: v1
kind: secret
metadata:
  name: nginx
type: opaque
stringdata:
  val1: asdf
---
apiversion: v1
kind: pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    volumemounts:
    - name: myval
      mountpath: /etc/myval
      subpath: myval
  volumes:
  - name: myval
    secret:
      secretname: nginx
      items:
      - key: val1
        path: myval
...

"
74535677,mark succesfull a pod with a cron job container and a never ending container when the cron job ends,"i have a pod with a cron job container and a never ending container.
is there a way to mark the pod as successfull when the cron job ends?
the problem is that with the never ending container in the same pod of the cron job the pod remains always in active status and i'd like to terminate it with success. there was a faliure in a node and when restarting it the cron job has started two times in the same days and i want to also avoid this.
i found a solution with activedeadlinesseconds but with this porperty the pod goes in failed status as read on the docs.
apiversion: batch/v1
kind: cronjob
metadata:
  name: hello
spec:
  schedule: &quot;*/1 * * * *&quot;
  concurrencypolicy: replace
  jobtemplate:
    spec:
      ttlsecondsafterfinished: 30
      activedeadlineseconds: 10
      template:
        spec:
          hostaliases:
          - ip: &quot;192.168.59.1&quot;
            hostnames: 
            - &quot;host.minikube.internal&quot;
          containers:
          - name: webapp1
            image: katacoda/docker-http-server:latest
            ports:
            - containerport: 80
          - name: hello
            image: quarkus-test
            imagepullpolicy: never
            command: [&quot;./application&quot;, &quot;-dquarkus.http.host=0.0.0.0&quot;]
          restartpolicy: onfailure


as you can see in this image the pod start, after 10 seconds with activedeadlineseconds the pod is putted in failed status, and other after 30 seconds with ttlsecondsafterfinished the pod is deleted.

as you can see here the cron job is putted from active 1 to active 0.
if i don't use activedeadlineseconds the cron job remains always active.
i've read about a solution using volumes between the two containers and writing a file when the cron job ends but i canno't touch the never ending code/container is not under my control.
",<kubernetes><kubernetes-cronjob>,74536399,3,"you can use a shared volume, write a file there when your job ends and use a livenessprobe on your never ending container to test if the file does not exist. from the moment the job creates the file the livenessprobe will fail and your never ending container should stop. here an example where the nginx container stops when the file /cache/stop is created in the shared volume
apiversion: batch/v1
kind: job
metadata:
  name:  multicontainer-job
spec:
  template:
    spec:
      containers:
      - name: busy
        image: busybox
        imagepullpolicy: ifnotpresent
        command:
          - sh
          - -c
        args:
          - echo start &gt; /cache/start; sleep 30; echo stop &gt;  /cache/stop; 
        volumemounts:
        - mountpath: /cache
          name: cache-volume
      - name: nginx
        image: nginx
        imagepullpolicy: ifnotpresent
        volumemounts:
        - mountpath: /cache
          name: cache-volume
        livenessprobe:
          exec:
            command:
            - sh
            - -c
            - if test -f &quot;/cache/stop&quot;; then exit 1; fi;
          initialdelayseconds: 5
          periodseconds: 5
      restartpolicy: never
      volumes:
      - name: cache-volume
        emptydir:
          sizelimit: 500mi
  backofflimit: 4
                           

"
74353385,"setting environment variables in kubernetes manifest using ""kubectl set env""","i am trying to update a helm-deployed deployment so that it uses a secret stored as a k8s secret resource.  this must be set as the storage_password environment variable in my pod.
in my case, the secret is in secrets/redis and the data item is redis-password:

$ kubectl get secret/redis -oyaml
apiversion: v1
data:
  redis-password: xxxxxxxxxxxxxxxx=
kind: secret
metadata:
  name: redis
type: opaque

i have tried:

$ kubectl set env --from secret/redis deployment/gateway --keys=redis-password
warning: key redis-password transferred to redis_password
deployment.apps/gateway env updated

when i look in my updated deployment manifest, i see the variable has been added but (as suggested) the variable has been set to redis_password:

        - name: redis_password
          valuefrom:
            secretkeyref:
              key: redis-password
              name: redis


i have also tried kubectl patch with a replace operation, but i can't get the syntax correct to have the secret inserted.
how do i change the name of the environment variable to storage_password?
",<kubernetes><kubectl><kubernetes-secrets><kubernetes-deployment>,74354981,3,"given a deployment that looks like this:
apiversion: apps/v1
kind: deployment
metadata:
  name: example
spec:
  replicas: 1
  template:
    spec:
      containers:
        - image: alpinelinux/darkhttpd
          name: darkhttpd
          args:
            - --port
            - &quot;9991&quot;
          ports:
            - name: http
              protocol: tcp
              containerport: 9991
          env:
            - name: example_var
              value: example value

the syntax for patching in your secret would look like:
kubectl patch deploy/example --patch='
  {
    &quot;spec&quot;: {
      &quot;template&quot;: {
        &quot;spec&quot;: {
          &quot;containers&quot;: [
            {
              &quot;name&quot;: &quot;darkhttpd&quot;,
              &quot;env&quot;: [
                {
                  &quot;name&quot;: &quot;storage_password&quot;,
                  &quot;valuefrom&quot;: {
                    &quot;secretkeyref&quot;: {
                      &quot;name&quot;: &quot;redis&quot;,
                      &quot;key&quot;: &quot;redis-password&quot;
                    }
                  }
                }
              ]
            }
          ]
        }
      }
    }
  }
'

or using a jsonpatch style patch:
kubectl patch --type json deploy/example --patch='
[
  {
    &quot;op&quot;: &quot;add&quot;,
    &quot;path&quot;: &quot;/spec/template/spec/containers/0/env/-&quot;,
    &quot;value&quot;: {
      &quot;name&quot;: &quot;storage_password&quot;,
      &quot;valuefrom&quot;: {
        &quot;secretkeyref&quot;: {
          &quot;name&quot;: &quot;redis&quot;,
          &quot;key&quot;: &quot;redis-password&quot;
        }
      }
    }
  }
]
'

neither one is especially pretty because you're adding a complex nested structure to an existing complex nested structure.
"
62269119,getting imagepullbackoff when starting a pod is aws eks,"i am getting the following error when my pod is deployed and then it tries to pull the image.

failed to pull image ""foyer-api:latest"": rpc error: code = unknown desc = failed to pull and unpack image ""docker.io/library/foyer-api:latest"": failed to resolve reference ""docker.io/library/foyer-api:latest"": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed


here is the pod yaml

apiversion: v1
kind: pod
metadata:
  name: foyer-api-test
  labels:
    app: foyer-api-test
spec:
  containers:
    - name: foyer-api
      image: foyer-api:latest
      ports:
       - containerport: 80

",<docker><kubernetes><amazon-eks><amazon-ecr>,62269190,3,"to pull an image from a private registry click here

basically you need to create a secret using docker credential. for example, using command line

$ kubectl create secret docker-registry regcred --docker-server=&lt;your-registry-server&gt; --docker-username=&lt;your-name&gt; --docker-password=&lt;your-pword&gt; --docker-email=&lt;your-email&gt;



then use it in imagepullsecrets

apiversion: v1
kind: pod
metadata:
  name: foyer-api-test
  labels:
    app: foyer-api-test
spec:
  containers:
  - name: foyer-api
    image: foyer-api:latest
    ports:
    - containerport: 80
  imagepullsecrets:
  - name: regcred

"
74153162,redirect http to https in asp.net core hosted on kubernetes fails,"my asp.net core web application has redirection applied on statup.cs as:
app.usehttpsredirection();

the problem arises when i host it on kubernetes where this redirection does not happens. the app opens on both http and https url.
i am hosting the app on local kubernetes cluster on my pc having kali linus.
this is the ingress which i am using:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: first-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
  - secretname: ssl
  rules:
  - http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: first-service
            port:
              number: 8080

the ingress on opens 2 ports which are - 80, 443.
how should the redirection be done in such a case?
the deployment is:
apiversion: apps/v1
kind: deployment
metadata:
  name: first-dep
  labels:
    app: aspnet-core-app
spec:
  replicas: 1
  selector: 
    matchlabels:
      component: web
  template:
    metadata: 
      labels:
        component: web
    spec:
      containers:
        - name: csimpleweb
          image: simpleweb
          imagepullpolicy: never
          ports:
            - containerport: 80

the service is:
apiversion: v1
kind: service
metadata:
  name: first-service
spec:
  type: nodeport
  selector:
    component: web
  ports:
    - port: 8080
      targetport: 80

so in summary i want to ask the following:

how redirection works in k8s + asp.net core. the redirection does not work in my case where i have applied the redirection on the app.
should the redirection be done on ingress if so how. in that case i think we have remove app.usehttpsredirection() from the app.
what docker container ports, which the pods will host, we have to open and how?

thank you
",<asp.net-core><kubernetes><kubernetes-ingress>,74153977,3,"according to my research i found the following way.

remove the redirection from an asp.net core app when we host it to kubernetes.
apply the redirect to ingress itself. by adding the following 2 lines to it:


nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;

so my ingress code becomes:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: first-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
spec:
  tls:
  - secretname: ssl
  rules:
  - http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: first-service
            port:
              number: 8080

let me know your thoughts. if you have a better answer the kindly add a new answer on the question.
"
74161348,kubernetes ingress not accepting the self signed ssl,"my kubernetes ingress is not accepting the self signed certificate and instead when opening the url on firefox the kubernetes ingress controller fake certificate is added.

all the things done locally on pc with minikube in kali linus. kali
linus is running in a virtual machine by vmware software.the doc which
i am referring is -
https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-multi-ssl

the ingress yaml file.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: first-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
spec:
  tls:
  - hosts:
      - example.com
    secretname: myssl
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: first-service
            port:
              number: 8080

the &quot;192.168.49.2&quot; is the ingress ip address. so https://192.68.49.2 opens my app on the browser.
the certificate is generated with openssl with the following commands:
openssl genrsa -out s.key 2048
openssl req -new -key s.key -out s.csr -subj &quot;/cn=example.com&quot;
openssl x509 -req -days 365 -in s.csr -signkey s.key -out s.crt

the certificate is added to k8s secret.
kubectl create secret tls myssl --cert s.crt --key s.key

the curl -kv https://192.168.49.2 command output is:
* trying 192.168.49.2:443...
* connected to 192.168.49.2 (192.168.49.2) port 443 (#0)
* alpn: offers h2
* alpn: offers http/1.1
* tlsv1.0 (out), tls header, certificate status (22):
* tlsv1.3 (out), tls handshake, client hello (1):
* tlsv1.2 (in), tls header, certificate status (22):
* tlsv1.3 (in), tls handshake, server hello (2):
* tlsv1.2 (in), tls header, finished (20):
* tlsv1.2 (in), tls header, supplemental data (23):
* tlsv1.3 (in), tls handshake, encrypted extensions (8):
* tlsv1.2 (in), tls header, supplemental data (23):
* tlsv1.3 (in), tls handshake, certificate (11):
* tlsv1.2 (in), tls header, supplemental data (23):
* tlsv1.3 (in), tls handshake, cert verify (15):
* tlsv1.2 (in), tls header, supplemental data (23):
* tlsv1.3 (in), tls handshake, finished (20):
* tlsv1.2 (out), tls header, finished (20):
* tlsv1.3 (out), tls change cipher, change cipher spec (1):
* tlsv1.2 (out), tls header, supplemental data (23):
* tlsv1.3 (out), tls handshake, finished (20):
* ssl connection using tlsv1.3 / tls_aes_256_gcm_sha384
* alpn: server accepted h2
* server certificate:
*  subject: o=acme co; cn=kubernetes ingress controller fake certificate
*  start date: oct 22 09:57:19 2022 gmt
*  expire date: oct 22 09:57:19 2023 gmt
*  issuer: o=acme co; cn=kubernetes ingress controller fake certificate
*  ssl certificate verify result: self-signed certificate (18), continuing anyway.
* using http2, server supports multiplexing
* copying http/2 data in stream buffer to connection buffer after upgrade: len=0
* tlsv1.2 (out), tls header, supplemental data (23):
* tlsv1.2 (out), tls header, supplemental data (23):
* tlsv1.2 (out), tls header, supplemental data (23):
* h2h3 [:method: get]
* h2h3 [:path: /]
* h2h3 [:scheme: https]
* h2h3 [:authority: 192.168.49.2]
* h2h3 [user-agent: curl/7.85.0]
* h2h3 [accept: */*]
* using stream id: 1 (easy handle 0x561c242ff950)
* tlsv1.2 (out), tls header, supplemental data (23):
&gt; get / http/2
&gt; host: 192.168.49.2
&gt; user-agent: curl/7.85.0
&gt; accept: */*
&gt; 
* tlsv1.2 (in), tls header, supplemental data (23):
* tlsv1.3 (in), tls handshake, newsession ticket (4):
* tlsv1.2 (in), tls header, supplemental data (23):
* tlsv1.3 (in), tls handshake, newsession ticket (4):
* old ssl session id is stale, removing
* tlsv1.2 (in), tls header, supplemental data (23):
* connection state changed (max_concurrent_streams == 128)!
* tlsv1.2 (out), tls header, supplemental data (23):
* tlsv1.2 (in), tls header, supplemental data (23):
* tlsv1.2 (in), tls header, supplemental data (23):
&lt; http/2 200 
&lt; date: sat, 22 oct 2022 10:05:50 gmt
&lt; content-type: text/html; charset=utf-8
 ..... html of the page
* tlsv1.2 (in), tls header, supplemental data (23):
* connection #0 to host 192.168.49.2 left intact

kindly help.
update 2 days later:
i debugged and found i have installed nginx ingress with the command:
minikube addons enable ingress

it installs the ingress in ingress-nginx namespace while my secret is in default namespace. can this be the problem, if so what is the solution?
",<kubernetes><ssl><kubernetes-ingress>,74163839,3,"you have an error in your ingress manifest, here:
rules:
- host: example.com
- http:
    paths:

you have created two rules, the first matches host: example.com but defines no paths or backends; the second matches path / but doesn't set a host. you want:
rules:
- host: example.com
  http:
    paths:



it installs the ingress in ingress-nginx namespace while my secret is in default namespace. can this be the problem, if so what is the solution?

this is not a problem: it is the expected configuration. your ssl secrets should be installed in the same namespace as your application and ingress.

i've been playing with this a bit over the past couple of days, and i'm not sure you can get this to operate the way you want without using a hostname. fortunately, setting up a hostname to use during local development is relatively straightforward.
in most cases you can edit your /etc/hosts file. for example, if your application is hosted on 192.168.49.2, then you would add an entry like this to /etc/hosts to access your application at https://example.com:
192.168.49.2    example.com

you can add multiple hostname aliases, which allows you to use multiple hostname-based ingress resources on your cluster:
192.168.49.2    example.com myapp.internal anotherapp.dev

when you're testing with curl, you can use the --resolve option to accomplish the same thing:
curl --resolve example.com:443:192.168.49.2 -kv https://example.com

so for example, if i deploy the following ingress on my local cluster:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: whoami
spec:
  tls:
    - secretname: myssl
      hosts:
        - example.com
  rules:
    - host: example.com
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: whoami
                port:
                  name: http

with the following entry in /etc/hosts:
$ grep example.com /etc/hosts
193.168.1.200 example.com

running curl -skv https://example.com shows that the ingress is using my custom certificate rather than the default ingress certificate:
[...]
* server certificate:
*  subject: cn=example.com
*  start date: oct 23 12:52:45 2022 gmt
*  expire date: oct 23 12:52:45 2023 gmt
*  issuer: cn=example.com
[...]

"
74252668,kubernetes ingress path route to different services in different namespaces,"currently i'm trying the following setup:
i have:

one cluster
one ingress controller
one url (myapp.onazure.com)
two namespaces for two applications default and default-test
two deployments, ingress objects, services for the namespaces

i can easily reach my app from the default namespace with path based routing '/' as a prefix rule
now i have tried to configure the second namespace and following rule: /testing to hit another service
unfortunately i get an http404 when i try to hit the following url myapp.onazure.com/testing/openapi.json
what did i miss?
working ingress 1
kind: ingress
apiversion: networking.k8s.io/v1
metadata:
  name: liveapi-ingress-object
  namespace: default
  annotations:
    kubernetes.io/ingress.class: public-nginx
spec:
  tls:
    - hosts:
        - myapp-region1.onazure.com
        - myapp-region2.onazure.com
      secretname: ingress-tls-csi
  rules:
    - host: - myapp-region1.onazure.com
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: liveapi-svc
                port:
                  number: 8080
    - host: myapp-region2.onazure.com
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: liveapi-svc
                port:
                  number: 8080


not working ingress 2
kind: ingress
apiversion: networking.k8s.io/v1
metadata:
  name: liveapi-ingress-object-testing
  namespace: default-testing
  annotations:
    kubernetes.io/ingress.class: public-nginx
    #nginx.ingress.kubernetes.io/rewrite-target: /testing
spec:
  tls:
    - hosts:
        - myapp-region1.onazure.com
        - myapp-region2.onazure.com
      secretname: ingress-tls-csi-testing
  rules:
    - host: myapp-region1.onazure.com
      http:
        paths:
          - path: /testing
            #pathtype: prefix
            backend:
              service:
                name: liveapi-svc-testing
                port:
                  number: 8080
    - host: myapp-region2.onazure.com
      http:
        paths:
          - path: /testing
            #pathtype: prefix
            backend:
              service:
                name: liveapi-svc-testing
                port:
                  number: 8080



maybe i am missing a rewrite target to simply '/' in the testing namespace ingress?
",<kubernetes><kubernetes-ingress><nginx-ingress>,74286139,3,"finally i figured out the missing part. i had to add the following statement to the not working ingress object:
  annotations:
    kubernetes.io/ingress.class: public-nginx
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /$1

please see the complete ingress object:
kind: ingress
apiversion: networking.k8s.io/v1
metadata:
  name: liveapi-ingress-object
  namespace: default-testing
  annotations:
    kubernetes.io/ingress.class: public-nginx
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  tls:
    - hosts:
        - myapp.onazure.com
      secretname: ingress-tls-csi-testing
  rules:
    - host: myapp.onazure.com
      http:
        paths:
          - path: /testing/(.*)
            pathtype: prefix
            backend:
              service:
                name: liveapi-svc-testing
                port:
                  number: 8000 

 

"
62737424,how to to enable https on eks using nginx-ingress and cert-manager with route53 dns?,"so, i have a kubernetes cluster running on aws-eks, it's only a test cluster to learn and build a production cluster at the moment.
i've already managed to make everything i need to work except for the ssl certificate! :(
i'm using cert-manager to add a ssl certificate on my domain &quot;brunolira.dev&quot;, which i bought on google domains and used aws' route53 to redirect to my kubernetes load balancer but did not have any success yet.
by using the staging cert-manager (https://acme-staging-v02.api.letsencrypt.org/directory) i get the following certificate on firefox:

when i use the prod cert-manager url (https://acme-prod-v02.api.letsencrypt.org/directory) it changes to this:

i don't understand why is it saying &quot;kubernetes ingress controller fake certificate&quot; nor why
the dns changes to &quot;ingress.local&quot;
this is my clusterissuer:
apiversion: cert-manager.io/v1alpha2
kind: clusterissuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    email: myemail(which i did not register anywhere, is there such a thing?)
    privatekeysecretref:
      name: cluster-issuer-account-key
    server: https://acme-prod-v02.api.letsencrypt.org/directory
    solvers:
    - selector:
        dnszones:
          - &quot;brunolira.dev&quot;
      dns01:
        route53:
          region: us-east-2
          hostedzoneid: hostedzone id on route 53
          role: arn:aws:iam::iamuserid:role/dns-manager

this is my ingress:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-ingress
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    cert-manager.io/cluster-issuer: &quot;letsencrypt-prod&quot;
spec:
  tls:
  - hosts:
    - brunolira.dev
    secretname: echo-tls
  rules:
  - host: brunolira.dev
    http:
      paths:
      - path: /common
        backend:
          servicename: common-service
          serviceport: 80
      - path: /offline
        backend:
          servicename: offline-service
          serviceport: 80

any help, guidance, suggestion or tip on how could i solve this will be very much appreciated!
in case you want to take a look at the prod generated certificate you can access the page brunolira.dev and verify whatever you need!
i can also provide any information about my configuration that would be useful on find a solution to to this problem!
",<kubernetes><ssl-certificate><kubernetes-ingress><amazon-eks><cert-manager>,62743287,3,"solved!
long story short, i was using the wrong letsencrypt production url :)
even though on the provided clusterissuer i was using &quot;dns01&quot; solver i was trying with &quot;http01&quot; too.
this was my final clusterissuer:
apiversion: cert-manager.io/v1alpha2
kind: clusterissuer
metadata:
 name: letsencrypt-prod
 namespace: cert-manager
spec:
 acme:
   server: https://acme-v02.api.letsencrypt.org/directory
   email: myemail
   privatekeysecretref:
     name: letsencrypt-prod
   solvers:
   - http01:
       ingress:
         class:  nginx

and this was my final ingress:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: my-ingress
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    cert-manager.io/acme-challenge-type: &quot;http01&quot;
    cert-manager.io/cluster-issuer: &quot;letsencrypt-staging&quot;
spec:
  tls:
  - hosts:
    - brunolira.dev
    secretname: echo-tls
  rules:
  - host: brunolira.dev
    http:
      paths:
      - path: /servicea
        backend:
          servicename: servicea
          serviceport: 80
      - path: /serviceb
        backend:
          servicename: serviceb
          serviceport: 80

this link helped me see what i was doing wrong! pretty good tutorial :)
"
62452643,configure prometheus to collect custom metrics from dockerized nodejs pod,"i have set up prom-client (unofficial client library for prometheus) to collect custom metrics what i need.
i have prometheus server deployed from helm following this eks setup guide. now i am trying to edit default configmap to collect my app metrics as well, but getting error 

parsing yaml file /etc/config/prometheus.yml: yaml: unmarshal errors:\n  line 22: field cluster_ip not found in type kubernetes.plain\n  line 25: cannot unmarshal !!strdefaultinto []string

this is what i have done as per docs
prometheus.yaml configmap file

apiversion: v1
data:
  alerting_rules.yml: |
    {}
  alerts: |
    {}
  prometheus.yml: |
    global:
      evaluation_interval: 1m
      scrape_interval: 1m
      scrape_timeout: 10s
    rule_files:
    - /etc/config/recording_rules.yml
    - /etc/config/alerting_rules.yml
    - /etc/config/rules
    - /etc/config/alerts
    scrape_configs:
    ...default configs...
    - job_name: my_metrics
      scrape_interval: 5m
      scrape_timeout: 10s
      honor_labels: true
      metrics_path: /api/metrics
      kubernetes_sd_configs:
        - role: service
          cluster_ip: 10.100.200.92
          namespaces:
            names:
              default
  recording_rules.yml: |
    {}
  rules: |
    {}
kind: configmap
metadata:
  creationtimestamp: ""2020-06-08t09:26:38z""
  labels:
    app: prometheus
    chart: prometheus-11.3.0
    component: server
    heritage: helm
    release: prometheus
  name: prometheus-server
  namespace: prometheus
  uid: 8fadb17a-f5c5-4f9d-a931-fa1f77684847


here clusterip is the ip assigned for my service to expose deployment.

my deployment.yaml file

apiversion: apps/v1
kind: deployment
metadata:
  name: myapp
spec:
  replicas: 2
  strategy:
    type: rollingupdate
  selector:
    matchlabels:
      name: myapp
  template:
    metadata:
      labels:
        name: myapp
    spec:
      containers:
        - image: image_url:build_number
          name: myapp
          resources:
              limits:
                cpu: ""1000m""
                memory: ""2400mi""
              requests:
                cpu: ""500m""
                memory: ""2000mi""
          imagepullpolicy: ifnotpresent
          ports:
              - containerport: 5000
                name: myapp


my service.yaml file which is exposing deployment

apiversion: v1
kind: service
metadata:
  name: myapp
spec:
  selector:
    deploy: staging
    name: myapp
  type: clusterip
  ports:
    - port: 80
      targetport: 5000
      protocol: tcp


is there some different/efficient way to target my app for metrics collection, please let me know. thanks
",<docker><kubernetes><prometheus><kubernetes-helm>,62514984,3,"this is what i am using to enable prometheus scraping inside the cluster.
in the scrape config, i have this snippet:
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - action: labeldrop
            regex: '(kubernetes_pod|app_kubernetes_io_instance|app_kubernetes_io_name|instance)'


this is taken directly from the default values for the prometheus helm chart: https://github.com/helm/charts/blob/master/stable/prometheus/values.yaml#l1452
what that does, is instruct prometheus to scrape every pod that has the annotation:
prometheus.io/scrape: &quot;true&quot;
set. with these annotations on the pod you can then configure the port and path of the scrape:
prometheus.io/path: &quot;/metrics&quot;
prometheus.io/port: &quot;9090&quot;

so, you would need to modify your deployment.yaml to specify these annotations as well:
apiversion: apps/v1
kind: deployment
metadata:
  name: myapp
spec:
  replicas: 2
  strategy:
    type: rollingupdate
  selector:
    matchlabels:
      name: myapp
  template:
    metadata:
      labels:
        name: myapp
    annotations:
      prometheus.io/scrape: &quot;true&quot;
      prometheus.io/port: &quot;&lt;enter port of pod to scrape&gt;&quot;
      prometheus.io/path: &quot;&lt;enter path to scrape&gt;&quot;
    spec:
      containers:
        - image: image_url:build_number
...

"
62525607,files.get concatenated string value appears empty after helm template in kubernetes configmap,"i'm using a configmap with a dynamic filename defined as below. however, after i do helm template the value for the filename is empty:


apiversion: v1
kind: configmap
metadata:
  name: krb5-configmap
data:
  krb5.conf: |-
      {{ .files.get (printf ""%s//krb5-%s.conf"" .values.kerberosconfigdirectory .values.environment) | indent 4 }}




kerberosconfigdirectory: kerberos-configs (set in values.yaml)


folder structure:

k8s:

templates

configmap.yaml


kerberos-configs

krb5-dev.conf




after helm template the data value looks like this:



data:
  krb5.conf: |-
--



i can't figure out why the value for the filename is empty. note that i'm able to run the helm template command successfully.
",<kubernetes><kubernetes-helm>,62528351,3,"you have an extra / and extra      indentation in you file. working example:
apiversion: v1
kind: configmap
metadata:
  name: krb5-configmap
data:
  krb5.conf: |-
{{ .files.get (printf &quot;%s/krb5-%s.conf&quot; .values.kerberosconfigdirectory .values.environment) | indent 4 }}

"
62520313,gke ingress resource with nginx load balancer shows strange ip?,"i am running a cluster on gke where the the ingress is configured to use nginx like so:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-ingress
  annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
      nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
....

and i installed the nginx load balancer on the cli using helm.  the load balancer console only shows nginx (and not the google one), which is good, and my application definitely routes according to my ingress manifest.  however, my ingress shown in the console has the property: loadbalancerip: xx.xxx.xxx.x and i do not recognize it whatsoever.  it's definitely not the external ip used by the nginx load balancer but it is similar (to where it could be a public ip, not internal).  it responds to pings as well.  this property was added to the ingress yaml by google cloud when it went through the pipeline.  is this anything to be concerned about?
",<nginx><kubernetes><google-kubernetes-engine>,62536073,3,"i was able to reproduce this behavior.
1 if you will deploy nginx ingress on gke as per nginx docs it is working normally. service and ingress have the same ip.
kubectl create clusterrolebinding cluster-admin-binding \
  --clusterrole cluster-admin \
  --user $(gcloud config get-value account)
  
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/cloud/deploy.yaml
namespace/ingress-nginx created
serviceaccount/ingress-nginx created
configmap/ingress-nginx-controller created
clusterrole.rbac.authorization.k8s.io/ingress-nginx created
...

2 if you will deploy nginx ingress helm chart without any changes $ helm install ingress ingress-nginx/ingress-nginx it will work as you described
nginx ingress controller loadbalancer service will have one externalip and ingress will have another externalip.
$ kubectl get svc,ing
name                                            type           cluster-ip   external-ip    port(s)                      age
service/hello-v2-svc                            nodeport       10.8.2.119   &lt;none&gt;         8080:32492/tcp               58s
service/ingress-nginx-ingress-controller        loadbalancer   10.8.5.90    34.72.141.41   80:32280/tcp,443:31670/tcp   108s
service/ingress-nginx-ingress-default-backend   clusterip      10.8.5.66    &lt;none&gt;         80/tcp                       108s
service/kubernetes                              clusterip      10.8.0.1     &lt;none&gt;         443/tcp                      169m
name                            hosts   address         ports   age
ingress.extensions/my-ingress   *       34.66.191.241   80      58s

regarding part if you should worry it depends. this will not charge you as gke found only 1 loadbalancer which is service loadbalancer. you can check that by:
$ gcloud compute url-maps list
listed 0 items.
user@cloudshell:~ (project)$ gcloud compute forwarding-rules list
name                              region       ip_address    ip_protocol  target
a655d3a06b55511ea89df42010a800fe  us-central1  34.72.141.41  tcp          us-central1/targetpools/a655d3a06b55511ea89df42010a800fe

3 if you want your ingress and nginx loadbalancer service have the same externalip, you must set parameter controller.publishservice.enabled to true in helm command. this parameter can be found in nginx ingress docs.

controller.publishservice.enabled  if true, the controller will set the endpoint records on the ingress objects to reflect those on the service    false

$ helm install ingress ingress-nginx/ingress-nginx --set controller.publishservice.enabled=true

after that you can deploy some yamls like:
apiversion: apps/v1
kind: deployment
metadata:
  name: hello-v2
spec:
  selector:
    matchlabels:
      app: hello-v2
  replicas: 1
  template:
    metadata:
      labels:
        app: hello-v2
    spec:
      containers:
      - name: hellov2
        image: &quot;gcr.io/google-samples/hello-app:2.0&quot;
        ports:
        - containerport: 8080
---
apiversion: v1
kind: service
metadata:
  name: hello-v2-svc
  labels: 
    app: hello-v2
spec:
  type: nodeport 
  selector:
    app: hello-v2
  ports:
  - port: 8080
    targetport: 8080
    protocol: tcp
---
apiversion: extensions/v1beta1
kind: ingress
metadata: 
  name: my-ingress
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
spec:
  rules:
  - http:
      paths:
        - path: /hello-v2
          backend:
            servicename: hello-v2-svc
            serviceport: 8080
            
$ kubectl apply -f hello.yaml
deployment.apps/hello-v2 created
service/hello-v2-svc created
ingress.extensions/my-ingress created

$ kubectl get svc,ing
name                                            type           cluster-ip    external-ip     port(s)                      age
service/hello-v2-svc                            nodeport       10.8.3.51     &lt;none&gt;          8080:30572/tcp               19m
service/ingress-nginx-ingress-controller        loadbalancer   10.8.12.137   34.69.123.145   80:32720/tcp,443:31245/tcp   20m
service/ingress-nginx-ingress-default-backend   clusterip      10.8.1.65     &lt;none&gt;          80/tcp                       20m
service/kubernetes                              clusterip      10.8.0.1      &lt;none&gt;          443/tcp                      163m

name                            hosts   address         ports   age
ingress.extensions/my-ingress   *       34.69.123.145   80      19m

$ curl 34.69.123.145/hello-v2
hello, world!
version: 2.0.0
hostname: hello-v2-7cf9b75bbf-2cdj5

edit
lately helm chart stable/nginx-ingress has been deprecated. please use nginx-ingress/nginx-ingress. commands above, already changed.
"
76673778,kubernetes sat token as environment variable,"i have created a k8 service account token using following command;
kubectl create serviceaccount test-sat-account

i have deployment yaml for a dotnet service and i am importing the above token in a volume as below;
apiversion: apps/v1
kind: deployment
metadata:
  name: my-deployment
spec:
  replicas: 1
  selector:
    matchlabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      serviceaccountname: test-sat-account
      containers:
      - name: my-container
        image: &quot;&quot;
        imagepullpolicy: always
        volumemounts:
        - name: my-token
          mountpath: /var/run/secrets/tokens
        env:
        - name: sattoken
          value: ****&lt;can we pass the sat token here?&gt;****
        ports:
        - name: http
          containerport: 80
          protocol: tcp
      volumes:
      - name: my-token
        projected:
          sources:
          - serviceaccounttoken:
              path: my-token
              audience: test-audience

now, instead of reading the token from the mountpath in the code, i want to pass the value of the token to an environment variable in the above yaml.
is it possible to do that?
if yes, how?
",<kubernetes><yaml><service-accounts><kubernetes-deployment><k8s-serviceaccount>,76674029,3,"arrange for the token to be stored in a secret resource:
apiversion: v1
kind: secret
metadata:
  name: test-sat-account-token
  annotations:
    kubernetes.io/service-account.name: test-sat-account
type: kubernetes.io/service-account-token

now, use that secret as the source for an environment value:
apiversion: apps/v1
kind: deployment
metadata:
  name: my-deployment
spec:
  replicas: 1
  selector:
    matchlabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      serviceaccountname: test-sat-account
      containers:
      - name: my-container
        image: &quot;&quot;
        imagepullpolicy: always
        env:
        - name: sattoken
          valuefrom:
            secretkeyref:
              name: test-sat-account-token
              key: token
        ports:
        - name: http
          containerport: 80
          protocol: tcp

"
76723597,kubernetes daemonset won't restart after annotation change,"i have a daemonset where the pods are getting their configuration from a configmap, so i wanted to automatically restart the daemonset when i update the configmap.
i saw this in helm docs: https://helm.sh/docs/howto/charts_tips_and_tricks/#automatically-roll-deployments
so i ended up putting this in my code:
apiversion: apps/v1
kind: daemonset
metadata:
  name: fluentd-logzio
  namespace: fluentd
  annotations:
    config: {{ include (print $.template.basepath &quot;/configmap.yaml&quot;) . | sha256sum }}
  labels:
    k8s-app: fluentd-logzio
    version: v1
spec:
  selector:
    matchlabels:
      k8s-app: fluentd-logzio
  template:
    metadata:
      labels:
        k8s-app: fluentd-logzio
        version: v1
    [...]

the annotation updates successfully but the daemonset won't restart. is it because it's not a deployment? i tried changing deployment annotation using kubectl edit and it also didn't trigger a restart.
",<kubernetes><kubernetes-helm>,76723910,3,"you need to put the annotation on the pod template, not the daemonset:
apiversion: apps/v1
kind: daemonset
metadata:
  name: fluentd-logzio
  namespace: fluentd
  labels:
    k8s-app: fluentd-logzio
    version: v1
spec:
  selector:
    matchlabels:
      k8s-app: fluentd-logzio
  template:
    metadata:
      labels:
        k8s-app: fluentd-logzio
        version: v1
      annotations:
        config: {{ include (print $.template.basepath &quot;/configmap.yaml&quot;) . | sha256sum }}
    [...]

that way the pod config changes and the pods will be recreated.
"
76758431,kustomize patch ingress issue - go construct,"i am new to kustomize and trying to figure out how to patch my ingress that is running via openshift container platform.
the base config works fine and the overlay was working until i introduced my ingress overlay patch. for reference the patch-service overlay does work so i am pretty sure my structure and links are all good, however the error i get on my project when syncronising to openshift (via argocd) is:
one or more objects failed to apply, reason: ingress in version &quot;v1&quot; cannot be handled as a ingress: json: cannot unmarshal number into go struct field ingressservicebackend.spec.defaultbackend.service.port of type v1.servicebackendport (retried 5 times).
application repo structure
|mssql-example
 base
    deployment.yaml
    ingress.yaml
    kustomization.yaml
    storage.yaml
    service.yaml
 overlays
     nprd-dev
        **patch-ingress.yaml**
        patch-service.yaml
        kustomization.yaml
     nprd-sit
        patch-ingress.yaml
        patch-service.yaml
        kustomization.yaml
     nprd-uat
        patch-ingress.yaml
        patch-service.yaml
        kustomization.yaml

mssql-example\base\ingress.yaml
---
kind: ingress
apiversion: networking.k8s.io/v1
metadata:
  name: mssql-example-adc
  annotations:
    ingress.citrix.com/frontend-ip: 192.168.1.10
    ingress.citrix.com/insecure-port: '1433'
    ingress.citrix.com/insecure-service-type: tcp
    kubernetes.io/ingress.class: citrix-vpx
spec:
  defaultbackend:
    service:
      name: mssql-service
      port:
        number: 31433

mssql-example\overlays\nprd-dev\kustomization.yaml
---
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
namespace: ops-example-dev
resources:
  - ../../base
patches:
  - path: patch-ingress.yaml
    target:
      kind: ingress
      version: v1
      name: mssql-example-adc
  - path: patch-service.yaml
    target:
      kind: service
      version: v1
      name: mssql-example-tcp

mssql-example\overlays\nprd-dev\patch-ingress.yaml
---
- op: replace
  path: /metadata/name
  value: mssql-example-ingress-dev
- op: replace
  path: /spec/defaultbackend/service/port
  value: 31434

i think my path may be wrong, but i can't seem to work out how to correctly identify the replacement path for the spec when it is defaultbackend. i tried the path as /spec/defaultbackend/0/service/port and /spec/0/defaultbackend/service/port incase it was an array.
",<kubernetes><openshift><kubernetes-ingress><argocd><kustomize>,76758585,3,"the error is telling you everything you need to know:
cannot unmarshal number into go struct field

first, look at the format of your initial ingress manifest:
kind: ingress
apiversion: networking.k8s.io/v1
metadata:
  name: mssql-example-adc
  annotations:
    ingress.citrix.com/frontend-ip: 192.168.1.10
    ingress.citrix.com/insecure-port: '1433'
    ingress.citrix.com/insecure-service-type: tcp
    kubernetes.io/ingress.class: citrix-vpx
spec:
  defaultbackend:
    service:
      name: mssql-service
      port:
        number: 31433

pay particular attention to the structure of spec.defaultbackend.service.port.
now, look at the output generated by your patch:
$ kustomize build
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    ingress.citrix.com/frontend-ip: 192.168.1.10
    ingress.citrix.com/insecure-port: &quot;1433&quot;
    ingress.citrix.com/insecure-service-type: tcp
    kubernetes.io/ingress.class: citrix-vpx
  name: mssql-example-ingress-dev
spec:
  defaultbackend:
    service:
      name: mssql-service
      port: 31434

do you see the difference? you've replaced a structured value:
port:
  number: 31433

with an integer:
port: 31434

just update your patch to target ...service/port/number instead:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
resources:
- ingress.yaml

patches:
  - target:
      name: mssql-example-adc
    patch: |
      - op: replace
        path: /metadata/name
        value: mssql-example-ingress-dev
      - op: replace
        path: /spec/defaultbackend/service/port/number
        value: 31434

which results in:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    ingress.citrix.com/frontend-ip: 192.168.1.10
    ingress.citrix.com/insecure-port: &quot;1433&quot;
    ingress.citrix.com/insecure-service-type: tcp
    kubernetes.io/ingress.class: citrix-vpx
  name: mssql-example-ingress-dev
spec:
  defaultbackend:
    service:
      name: mssql-service
      port:
        number: 31434

"
62565621,minikube kubectl create file error (validating data: apiversion not set),"i am a student and new to minikube as well as linux. my teacher did the same and his pod created but i am getting error. i don't know what is the problem. should i ignore validation error?
kubectl create -f myfirstpod.yaml
error: error validating &quot;myfirstpod.yaml&quot;: error validating data: apiversion not set; if you choose to ignore these errors, turn validation off with --validate=false

cat myfirstpod.yaml
 kind: pod
 apiversion: v1
 metadata:
   name: myfirstpod
 spec:
   containers:
 name: container1
 image: aamirpinger/helloworld:latest
 ports:
   containerport: 80

i am using these versions:

minikube version: v1.11.0
kubectl version 1.8.4
kubernetes 1.18.3
docker 19.03.11

",<kubernetes><yaml><kubectl><minikube>,62566430,3,"seems like you have typo. it should be apiversion instead of apiversion. also, the indentation in the podspec seems incorrect.
you can use this:
kind: pod
apiversion: v1
metadata:
  name: myfirstpod
spec:
  containers:
  - name: container1
    image: aamirpinger/helloworld:latest
    ports:
    - containerport: 80

"
63001955,"kubernetes websockets using socket.io, expressjs and nginx ingress","i want to connect a react native application using socket.io to a server that is inside a kubernetes cluster hosted on google cloud platform (gke).
there seems to be an issue with the nginx ingress controller declaration but i cannot find it.
i have tried adding nginx.org/websocket-services; rewriting my backend code so that it uses a separate nodejs server (a simple http server) on port 3004, then exposing it via the ingress controller under a different path than the one on port 3003; and multiple other suggestions from other so questions and github issues.
information that might be useful:

cluster master version: 1.15.11-gke.15
i use a load balancer managed with helm (stable/nginx-ingress) with rbac enabled
all deployments and services are within the namespace gitlab-managed-apps
the error i receive when trying to connect to socket.io is: error: websocket error

for the front-end part, the code is as follows:
app.js
const socket = io('https://example.com/app-sockets/socketns', {
    reconnect: true,
    secure: true,
    transports: ['websocket', 'polling']
});

i expect the above to connect me to a socket.io namespace called socketdns.
the backend code is:
app.js
const express = require('express');
const app = express();
const server = require('http').createserver(app);
const io = require('socket.io')(server);
const redis = require('socket.io-redis');

io.set('transports', ['websocket', 'polling']);
io.adapter(redis({
    host: process.env.node_env === 'development' ? 'localhost' : 'redis-cluster-ip-service.gitlab-managed-apps.svc.cluster.local',
    port: 6379
}));
io.of('/').adapter.on('error', function(err) { console.log('redis adapter error! ', err); });

const nsp = io.of('/socketns');

nsp.on('connection', function(socket) {
    console.log('connected!');
});

server.listen(3003, () =&gt; {
    console.log('app listening to 3003');
});

the ingress service is:
ingress-service.yaml
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/proxy-body-size: &quot;100m&quot;
    certmanager.k8s.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/proxy-connect-timeout: &quot;7200&quot;
    nginx.ingress.kubernetes.io/proxy-read-timeout: &quot;7200&quot;
    nginx.ingress.kubernetes.io/proxy-send-timeout: &quot;7200&quot;
    nginx.org/websocket-services: &quot;app-sockets-cluster-ip-service&quot;
  name: ingress-service
  namespace: gitlab-managed-apps
spec:
  tls:
  - hosts:
    - example.com
    secretname: letsencrypt-prod
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          servicename: app-cms-cluster-ip-service
          serviceport: 3000
        path: /?(.*)
      - backend:
          servicename: app-users-cluster-ip-service
          serviceport: 3001
        path: /app-users/?(.*)
      - backend:
          servicename: app-sockets-cluster-ip-service
          serviceport: 3003
        path: /app-sockets/?(.*)
      - backend:
          servicename: app-sockets-cluster-ip-service
          serviceport: 3003
        path: /app-sockets/socketns/?(.*)

",<kubernetes><websocket><socket.io><google-kubernetes-engine><nginx-ingress>,68480603,3,"the solution is to remove the nginx.ingress.kubernetes.io/rewrite-target: /$1 annotation.
here is a working configuration: (please note that apiversion has changed since the question has been asked)
ingress configuration
ingress-service.yaml
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    nginx.ingress.kubernetes.io/proxy-body-size: &quot;64m&quot;
    cert-manager.io/cluster-issuer: &quot;letsencrypt-prod&quot;
  name: ingress-service
  namespace: default
spec:
  tls:
  - hosts:
    - example.com
    secretname: letsencrypt-prod
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          service:
            name: app-sockets-cluster-ip-service
            port:
              number: 3003
        path: /app-sockets/?(.*)
        pathtype: prefix

on the service (express.js):
app.js
const redisadapter = require('socket.io-redis');

const io = require('socket.io')(server, {
    path: `${ global.node_env === 'development' ? '' : '/app-sockets' }/sockets/`,
    cors: {
        origin: '*',
        methods: ['get', 'post'],
    },
});

io.adapter(redisadapter({
    host: global.redis_host,
    port: 6379,
}));

io.of('/').adapter.on('error', err =&gt; console.log('redis adapter error! ', err));

io.on('connection', () =&gt; { 
//...
});

the global.node_env === 'development' ? '' : '/app-sockets' bit is related to an issue in development. if you change it here, you must also change it in the snippet below.
in development the service is under http://localhost:3003 (sockets endpoint is http://localhost:3003/sockets).
in production the service is under https://example.com/app-sockets (sockets endpoint is https://example.com/app-sockets/sockets).
on frontend
connecttowebsocketsservice.js
/**
 * connect to a websockets service
 * @param tokens {object}
 * @param successcallback {function}
 * @param failurecallback {function}
 */
export const connecttowebsocketsservice = (tokens, successcallback, failurecallback) =&gt; {
    //sockets_url = node_env === 'development' ? 'http://localhost:3003' : 'https://example.com/app-sockets'
    const socket = io(`${ sockets_url.replace('/app-sockets', '') }`, {
        path: `${ node_env === 'development' ? '' : '/app-sockets' }/sockets/`,
        reconnect: true,
        secure: true,
        transports: ['polling', 'websocket'], //required
        query: {
            // optional
        },
        auth: {
            ...generateauthorizationheaders(tokens), //optional
        },
    });

    socket.on('connect', successcallback(socket));
    socket.on('reconnect', successcallback(socket));
    socket.on('connect_error', failurecallback);
};

note: i wasn't able to do it on the project mentioned in the question, but i have on another project which is hosted on eks, not gke. feel free to confirm if this works for you on gke as well.
"
62892972,kubernetes service account default permissions,"i am experimenting with service accounts. i believe the following should produce an access error (but it doesn't):
apiversion: v1
kind: serviceaccount
metadata:
  name: test-sa

---

apiversion: v1
kind: pod
metadata:
  name: test-pod
spec:
  serviceaccountname: test-sa
  containers:
  - image: alpine
    name: test-container
    command: [sh]
    args:
    - -ec
    - |
      apk add curl;
      kube_namespace=&quot;$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)&quot;;
      curl \
        --cacert &quot;/var/run/secrets/kubernetes.io/serviceaccount/ca.crt&quot; \
        -h &quot;authorization: bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)&quot; \
        &quot;https://kubernetes.default.svc/api/v1/namespaces/$kube_namespace/services&quot;;
      while true; do sleep 1; done;

kubectl apply -f test.yml
kubectl logs test-pod

what i see is a successful listing of services, but i would expect a permissions error because i never created any rolebindings or clusterrolebindings for test-sa.
i'm struggling to find ways to list the permissions available to a particular sa, but according to kubernetes check serviceaccount permissions, it should be possible with:
kubectl auth can-i list services --as=system:serviceaccount:default:test-sa
&gt; yes

though i'm skeptical whether that command is actually working, because i can replace test-sa with any gibberish and it still says &quot;yes&quot;.

according to the documentation, service accounts by default have &quot;discovery permissions given to all authenticated users&quot;. it doesn't say what that actually means, but from more reading i found this resource which is probably what it's referring to:
kubectl get clusterroles system:discovery -o yaml
&gt; [...]
&gt; rules:
&gt; - nonresourceurls:
&gt;   - /api
&gt;   - /api/*
&gt; [...]
&gt;   verbs:
&gt;   - get

which would imply that all service accounts have get permissions on all api endpoints, though the &quot;nonresourceurls&quot; bit implies this wouldn't apply to apis for resources like services, even though those apis live under that path (???)

if i remove the authorization header entirely, i see an access error as expected. but i don't understand why it's able to get data using this empty service account. what's my misunderstanding and how can i restrict permissions correctly?
",<kubernetes><permissions><docker-desktop><kubernetes-security>,62894738,3,"it turns out this is a bug in docker desktop for mac's kubernetes support.
it automatically adds a clusterrolebinding giving cluster-admin to all service accounts (!). it only intends to give this to service accounts inside the kube-system namespace.
it was originally raised in docker/for-mac#3694 but fixed incorrectly. i have raised a new issue docker/for-mac#4774 (the original issue is locked due to age).
a quick fix while waiting for the bug to be resolved is to run:
kubectl apply -f - &lt;&lt;eof
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: docker-for-desktop-binding
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: cluster-admin
subjects:
- apigroup: rbac.authorization.k8s.io
  kind: group
  name: system:serviceaccounts:kube-system
eof

i don't know if that might cause issues with future docker desktop upgrades but it does the job for now.
with that fixed, the code above correctly gives a 403 error, and would require the following to explicitly grant access to the services resource:
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  name: service-reader
rules:
- apigroups: [&quot;&quot;]
  resources: [services]
  verbs: [get, list]

---

apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: test-sa-service-reader-binding
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: service-reader
subjects:
- kind: serviceaccount
  name: test-sa


a useful command for investigating is kubectl auth can-i --list --as system:serviceaccount, which shows the rogue permissions were applying to all service accounts:

resources                       non-resource urls   resource names   verbs
*.*                             []                  []               [*]
                                [*]                 []               [*]
[...]


"
64295445,how can i configure the ingress-nginx controller to work with cloudflare and digital ocean load balancer?,"i have tried the answers in this question. this is my current configuration:
apiversion: v1
kind: configmap
metadata:
  labels:
    helm.sh/chart: ingress-nginx-2.13.0
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 0.35.0
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/component: controller
  name: ingress-nginx-controller
  namespace: ingress-nginx
data:
  use-proxy-protocol: 'true'
  enable-real-ip: &quot;true&quot;
  proxy-real-ip-cidr: &quot;173.245.48.0/20,173.245.48.0/20,103.21.244.0/22,103.22.200.0/22,103.31.4.0/22,141.101.64.0/18,108.162.192.0/18,190.93.240.0/20,188.114.96.0/20,197.234.240.0/22,198.41.128.0/17,162.158.0.0/15,104.16.0.0/12,172.64.0.0/13,131.0.72.0/22,2400:cb00::/32,2606:4700::/32,2803:f800::/32,2405:b500::/32,2405:8100::/32,2a06:98c0::/29,2c0f:f248::/32&quot;
  # use-forwarded-headers: &quot;true&quot;
  # compute-full-forwarded-for: &quot;true&quot;
  # forwarded-for-header: &quot;cf-connecting-ip&quot;
  # forwarded-for-header: &quot;x-original-forwarded-for&quot;
  server-snippet: |
    real_ip_header cf-connecting-ip;

and none of the configuration i have tried is actually giving the originating ip as the real ip.
before i applied the configuration, i was getting:
host: example.com
x-request-id: deadcafe
x-real-ip: 162.158.x.x (a cloudflare ip)
x-forwarded-for: 162.158.x.x (same as above)
x-forwarded-proto: https
x-forwarded-host: example.com
x-forwarded-port: 80
x-scheme: https
x-original-forwarded-for: &lt;the originating ip that i want&gt;
accept-encoding: gzip
cf-ipcountry: in
cf-ray: cafedeed
cf-visitor: {&quot;scheme&quot;:&quot;https&quot;}
user-agent: mozilla/5.0 
accept-language: en-us,en;q=0.5
referer: https://pv-hr.jptec.in/
upgrade-insecure-requests: 1
cookie: __cfduid=012dadfad
cf-request-id: 01234faddad
cf-connecting-ip: &lt;the originating ip that i want&gt;
cdn-loop: cloudflare

after applying the config map, the headers are:
host: example.com
x-request-id: 0123fda
x-real-ip: 10.x.x.x (an ip that matches the private ip of the digital ocean droplets in the vpc, so guessing its the load balancer)
x-forwarded-for: 10.x.x.x (same as above)
x-forwarded-proto: http
x-forwarded-host: example.com
x-forwarded-port: 80
x-scheme: http
x-original-forwarded-for: &lt;originating ip&gt;
accept-encoding: gzip
cf-ipcountry: us
cf-ray: 5005deeb
cf-visitor: {&quot;scheme&quot;:&quot;https&quot;}
accept: /
user-agent: mozilla/5.0
cf-request-id: 1ee7af
cf-connecting-ip: &lt;originating ip&gt;
cdn-loop: cloudflare

so the only change after the configuration is that the real-ip now points to some internal resource on the digital ocean vpc. i haven't been able to track that down but i am guessing its the load balancer. i am confident that it is a do resource because it matches the ip of the kubernetes nodes. so, i am not really sure why this is happening and what i should be doing to get the originating ip as the real ip.
",<kubernetes><digital-ocean><kubernetes-ingress><cloudflare><ingress-nginx>,65362294,3,"the problem you are facing is here:
proxy-real-ip-cidr: &quot;173.245.48.0/20,173.245.48.0/20,103.21.244.0/22,103.22.200.0/22,103.31.4.0/22,141.101.64.0/18,108.162.192.0/18,190.93.240.0/20,188.114.96.0/20,197.234.240.0/22,198.41.128.0/17,162.158.0.0/15,104.16.0.0/12,172.64.0.0/13,131.0.72.0/22,2400:cb00::/32,2606:4700::/32,2803:f800::/32,2405:b500::/32,2405:8100::/32,2a06:98c0::/29,2c0f:f248::/32&quot;
however, the traffic being seen is coming from your do lb instead 10.x.x.x. this is causing it to be ignored for this rule.
i did the following to get it functional:
apiversion: v1
data:
  enable-real-ip: &quot;true&quot;
  server-snippet: |
    real_ip_header cf-connecting-ip;
kind: configmap
metadata:
[...]


security notice: this will apply to all traffic even if it didn't originate from cloudflare itself. as such, someone could spoof the headers on the request to impersonate another ip address.
"
65688449,connecting to a kubernetes service resulted in connection refused,"i'm trying to deploy my web application using kubernetes. i used minikube to create the cluster and successfully exposed my frontend react app using ingress. yet when i attached the backend service's url in &quot;env&quot; field in the frontend's deployment.yaml, it does not work. when i tried to connect to the backend service through the frontend pod, the connection refused.
frontend deployment yaml
kind: deployment
apiversion: apps/v1
metadata:
 name: frontend
spec:
 replicas: 1
 selector:
   matchlabels:
     app: frontend
 template:
   metadata:
     labels:
       app: frontend
   spec:
     containers:
       - name: frontend
         image: &lt;image_name&gt;
         imagepullpolicy: never
         ports:
           - containerport: 80
         env:
         - name: react_app_api_v1_endpoint
           value: http://backend-svc
     restartpolicy: always
---
apiversion: v1
kind: service
metadata:
 name: frontend-svc
spec:
 ports:
 - port: 80
   protocol: tcp
   targetport: 80
 selector:
   app: frontend


ingress for frontend
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
 name: front-ingress
 namespace: default
 annotations:
   kubernetes.io/ingress.class: &quot;nginx&quot;
   nginx.ingress.kubernetes.io/rewrite-target: /
   nginx.ingress.kubernetes.io/proxy-read-timeout: &quot;12h&quot;
   nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
spec:
 rules:
   - host: front-testk.info
     http:
       paths:
         - path: /
           pathtype: prefix
           backend:
             service:
               name: frontend-svc
               port:
                 number: 80

backend deployment yaml
kind: deployment
apiversion: apps/v1
metadata:
  name: backend
  labels:
    name: backend
spec:
  replicas: 1
  selector:
    matchlabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: backend
        image: &lt;image_name&gt;
        ports:
        - containerport: 80
        imagepullpolicy: never
      restartpolicy: always
---
kind: service
apiversion: v1
metadata:
  name: backend-svc
  labels:
    app: backend
spec:
  selector:
    app: backend
  ports:
    - name: http
      port: 80
      targetport: 80


% kubectl get svc backend-svc -o wide
name              type        cluster-ip       external-ip   port(s)   age   selector
backend-svc   clusterip   10.109.107.145   &lt;none&gt;        80/tcp    21h   app=backend

connected inside frontend pod and try to connect to the backend using the env created during deploy:
 kubectl exec frontend-75579c8499-x766s -it sh
/app # apk update &amp;&amp; apk add curl
ok: 10 mib in 20 packages

/app # env
react_app_api_v1_endpoint=http://backend-svc

/app # curl $react_app_api_v1_endpoint
curl: (7) failed to connect to backend-svc port 80: connection refused

/app # nslookup backend-svc
server:         10.96.0.10
address:        10.96.0.10:53

name:   backend-svc.default.svc.cluster.local
address: 10.109.107.145

** server can't find backend-svc.cluster.local: nxdomain

exec into my backend pod
# netstat -tulpn
active internet connections (only servers)
proto recv-q send-q local address           foreign address         state       pid/program name    
tcp        0      0 0.0.0.0:8080            0.0.0.0:*               listen      1/node

# netstat -lnturp
kernel ip routing table
destination     gateway         genmask         flags   mss window  irtt iface
0.0.0.0         172.17.0.1      0.0.0.0         ug        0 0          0 eth0
172.17.0.0      0.0.0.0         255.255.0.0     u         0 0          0 eth0

",<kubernetes><kubernetes-pod><amazon-eks><kubernetes-service>,65716488,3,"as i suspected your application listens on port 8080. if you closely at your output from netstat you will notice that local address is 0.0.0.0:8080:
# netstat -tulpn

proto recv-q send-q local address           foreign address         state       pid/program name    
tcp        0      0 0.0.0.0:8080            0.0.0.0:*               listen      1/node

in order to fix that you have to correct your targetport in your service:
kind: service
apiversion: v1
metadata:
  name: backend-svc
  labels:
    app: backend
spec:
  selector:
    app: backend
  ports:
    - name: http
      port: 80
      targetport: 8080 #  change this to 8080

there is no need to change the port on the deployment side since the containerport is primarily informational. not specifying a port there does not prevent that port from being exposed. any port which is listening on the default &quot;0.0.0.0&quot; address inside a container will be accessible.
"
65543927,kubernetes ingress server-alias only applies to one ingress host,"according to this doc (https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#server-alias), i'm able to add additional server_name to the nginx config file.
however, it adds the extra server_name to all of my hosts, which cause conflicts for sure.
is there a way to add server-alias only for one of my hosts? say i only want to add 10.10.0.100 to my test1 host.
ingress example:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: test-ingress
  namespace: default
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/server-alias: 10.10.0.100
spec:
  rules:
  - host: test1.com
    http:
      paths:
      - path: /
        backend:
          service:
            name: test1-service
            port:
              number: 8000
        pathtype: prefix
  - host: test2.com
    http:
      paths:
      - path: /
        backend:
          service:
            name: test2-service
            port:
              number: 8000
        pathtype: prefix


",<nginx><kubernetes><kubernetes-ingress><nginx-ingress><server-name>,65562983,3,"tl;dr
you can split your ingress resource on multiple objects (which will work together) to add annotations to only specific hosts.

annotations can only be set on the whole kubernetes resource, as they are part of the resource metadata. the ingress spec doesn't include that functionality at a lower level.
-- stackoverflow.com: questions: apply nginx-ingress annotations at path level


extending on the answer to give an example of how such setup could be created. let's assume (example):

all required domains pointing to the service of type loadbalancer of nginx-ingress-controller:

hello.kubernetes.docker.internal - used in host .spec
hello-two.kubernetes.docker.internal - used in annotations .metadata
--
goodbye.kubernetes.docker.internal - used in host .spec
goodbye-two.kubernetes.docker.internal- used in annotations .metadata



skipping the deployment and service definitions, the ingress resources should look like below:
hello-ingress.yaml
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: hello-ingress
  annotations:
    nginx.ingress.kubernetes.io/server-alias: &quot;hello-two.kubernetes.docker.internal&quot;
spec:
  rules:
  - host: hello.kubernetes.docker.internal # &lt;-- important  
    http:
      paths:
      - path: /
        backend:
          service:
            name: hello-service
            port:
              number: 80
        pathtype: prefix

goodbye-ingress.yaml
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: goodbye-ingress
  annotations:
    nginx.ingress.kubernetes.io/server-alias: &quot;goodbye-two.kubernetes.docker.internal&quot;
spec:
  rules:
  - host: goodbye.kubernetes.docker.internal # &lt;-- important 
    http:
      paths:
      - path: /
        backend:
          service:
            name: goodbye-service
            port:
              number: 80
        pathtype: prefix

above definitions will create 2 ingress resources that will be merged:

hello-service will respond for:

hello.kubernetes.docker.internal
hello-two.kubernetes.docker.internal


goodbye-service will respond for:

goodbye.kubernetes.docker.internal
goodbye-two.kubernetes.docker.internal



running:

$ kubectl get ingress:

name              class    hosts                                address     ports   age
goodbye-ingress   &lt;none&gt;   goodbye.kubernetes.docker.internal   localhost   80      36m
hello-ingress     &lt;none&gt;   hello.kubernetes.docker.internal     localhost   80      36m


additional resources:

kubernetes.io: docs: concepts: services networking: ingress
kubernetes.github.io: ingress nginx: annotations: server alias

"
65952021,how does pvc on kubernetes work with mysql?,"i am an computer enginering student. in my freetime i am working an a k8s project.
i was wondering how pvc works, i understand the basic concept. but here is my question.
so this is the code for a basic wordpress with mysql application.
i was wondering if i want to make multiple instaces of this application, do i need to change the mountpath? or is the mountpath the location within the container.
apiversion: v1
kind: service
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterip: none
---
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: mysql-pv-claim
  labels:
    app: wordpress
spec:
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 20gi
---
apiversion: apps/v1
kind: deployment
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  selector:
    matchlabels:
      app: wordpress
      tier: mysql
  strategy:
    type: recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: mysql
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
        - name: mysql_root_password
          valuefrom:
            secretkeyref:
              name: mysql-pass
              key: password
        ports:
        - containerport: 3306
          name: mysql
        volumemounts:
        - name: mysql-persistent-storage
          mountpath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentvolumeclaim:
          claimname: mysql-pv-claim

",<mysql><kubernetes><kubernetes-pvc>,65952518,3,"if you really want to run different instance you would really love to create different data directory for them. it's never good idea to use deployment in this scenario. it's better if you use statefulset to manage database. statefulset will automatically generate different pvc for different pods.  you can check that with kubectl get pvc. here, in statefulset volumeclaimtemplates is the template for pvc.
pvc name will be like that &lt;pvc_template_name&gt;-&lt;statefulset_name&gt;-&lt;podnumber&gt;.
eg: mysql-pv-claim-wordpress-mysql-0

apiversion: v1
kind: service
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterip: none

---
apiversion: apps/v1
kind: statefulset
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  servicename: wordpress-mysql
  replicas: 1
  selector:
    matchlabels:
      app: wordpress
      tier: mysql

  template:
    metadata:
      labels:
        app: wordpress
        tier: mysql
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
        - name: mysql_root_password
          valuefrom:
            secretkeyref:
              name: mysql-pass
              key: password
        ports:
        - containerport: 3306
          name: mysql
        volumemounts:
        - name: mysql-pv-claim
          mountpath: /var/lib/mysql
  volumeclaimtemplates:
  - metadata:
      name: mysql-pv-claim
    spec:
      accessmodes: [ &quot;readwriteonce&quot; ]
      storageclassname: standard
      resources:
        requests:
          storage: 20gi

"
65778756,fail to connect to redis in kubernetes: redis connection to localhost:6379 failed,"i have deployed a backend api that is connected to the redis pod and mongodb pod. the api has successfully established a connection to the mongodb but not redis, with the following error presented, anyone know how to fix it?
error: redis connection to localhost:6379 failed - connect econnrefused 127.0.0.1:6379
    at tcpconnectwrap.afterconnect [as oncomplete] (net.js:1088:14) 
2021-01-18 09:22:36 error: uncaughtexception: redis connection to localhost:6379 failed - connect econnrefused 127.0.0.1:6379
error: redis connection to localhost:6379 failed - connect econnrefused 127.0.0.1:6379
    at tcpconnectwrap.afterconnect [as oncomplete] (net.js:1088:14) {&quot;error&quot;:{&quot;errno&quot;:&quot;econnrefused&quot;,&quot;code&quot;:&quot;econnrefused&quot;,&quot;syscall&quot;:&quot;connect&quot;,&quot;address&quot;:&quot;127.0.0.1&quot;,&quot;port&quot;:6379},&quot;stack&quot;:&quot;error: redis connection to localhost:6379 failed - connect econnrefused 127.0.0.1:6379\n    at tcpconnectwrap.afterconnect [as oncomplete] (net.js:1088:14)&quot;,&quot;exception&quot;:true,&quot;date&quot;:&quot;mon jan 18 2021 09:22:36 gmt+0000 (coordinated universal time)&quot;,&quot;process&quot;:{&quot;pid&quot;:1,&quot;uid&quot;:0,&quot;gid&quot;:0,&quot;cwd&quot;:&quot;/usr/src/app&quot;,&quot;execpath&quot;:&quot;/usr/local/bin/node&quot;,&quot;version&quot;:&quot;v11.14.0&quot;,&quot;argv&quot;:[&quot;/usr/local/bin/node&quot;,&quot;/usr/src/app/dist/server.js&quot;],&quot;memoryusage&quot;:{&quot;rss&quot;:101421056,&quot;heaptotal&quot;:73146368,&quot;heapused&quot;:37365856,&quot;external&quot;:19092480}},&quot;os&quot;:{&quot;loadavg&quot;:[1.232421875,0.8720703125,0.69482421875],&quot;uptime&quot;:281868},&quot;trace&quot;:[{&quot;column&quot;:14,&quot;file&quot;:&quot;net.js&quot;,&quot;function&quot;:&quot;tcpconnectwrap.afterconnect [as oncomplete]&quot;,&quot;line&quot;:1088,&quot;method&quot;:&quot;afterconnect [as oncomplete]&quot;,&quot;native&quot;:false}]}

backend.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: backend-api
spec:
  replicas: 1
  selector:
    matchlabels:
      app: backend-api
  template:
    metadata:
      labels:
        app: backend-api
    spec:
      containers:
      - image: &lt;image_name&gt;
        name: backend-test
        imagepullpolicy: never
        ports:
        - containerport: 8080
        env: 
        - name: mongodb_url
          value: mongodb://localhost:27017/dt?authsource=admin
        - name: redis_url
          value: redis://localhost:6379      
      restartpolicy: always
---
kind: service
apiversion: v1
metadata:
  name: backend-svc
  labels:
    app: backend-svc
spec:
  selector:
    app: backend-api
  ports:
    - name: http 
      port: 8080
      targetport: 8080

redis.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: redis-master
  labels:
    app: redis
spec:
  selector:
    matchlabels:
      app: redis
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: master
        image: k8s.gcr.io/redis:e2e  # or just image: redis
        resources:
          requests:
            cpu: 100m
            memory: 100mi
        ports:
        - containerport: 6379
---
apiversion: v1
kind: service
metadata:
  name: redis-master
  labels:
    app: redis
spec:
  ports:
  - name: redis
    port: 6379
    targetport: 6379
  selector:
    app: redis

i have also tried to forward a port to a port on the pod but have no luck
kubectl port-forward redis-master-765d459796-258hz 6379:6379

",<kubernetes><redis><minikube><amazon-eks>,65780353,3,"localhost is only for local connection. thats mean you can only connect from inside a pod with localhost. when you need to  access from outside you need to use service. service will then resolve your ip address. when you will try to access via service . here is the code need to update in your yaml.
        - name: redis_url
          value: redis://redis-master.default.svc.cluster.local:6379  

ref
the  template is like  my_service_name.my_namespace.svc.cluster-domain.example , but you can skip the cluster-domain.example part. only service_name.namespace.svc will work fine. so for you it will be : redis-master.default.svc
here is the updated code :
apiversion: apps/v1
kind: deployment
metadata:
  name: backend-api
spec:
  replicas: 1
  selector:
    matchlabels:
      app: backend-api
  template:
    metadata:
      labels:
        app: backend-api
    spec:
      containers:
      - image: &lt;image_name&gt;
        name: backend-test
        imagepullpolicy: never
        ports:
        - containerport: 8080
        env: 
        - name: mongodb_url
          value: mongodb://localhost:27017/dt?authsource=admin
        - name: redis_url
          value: redis://redis-master.default.svc.cluster.local:6379      
      restartpolicy: always
---
kind: service
apiversion: v1
metadata:
  name: backend-svc
  labels:
    app: backend-svc
spec:
  selector:
    app: backend-api
  ports:
    - name: http 
      port: 8080
      targetport: 8080

"
66434505,why must i explicitly set a namespace for the serviceaccount of clusterrolebinding.rbac.authorization.k8s.io resource?,"i'm deploying my chart with helm like this:
helm upgrade --install --namespace newnamespace --create-namespace testing mychart

my understanding is everything should be deployed into newnamespace
i have this in my chart:
apiversion: v1
kind: serviceaccount
metadata:
  name: {{ include &quot;mychart.serviceaccountname&quot; . }}

---
apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrole
metadata:
  name: {{ include &quot;mychart.serviceaccountname&quot; . }}
rules:
- apigroups: [&quot;&quot;]
  resources: [&quot;services&quot;,&quot;endpoints&quot;,&quot;pods&quot;]
  verbs: [&quot;get&quot;,&quot;watch&quot;,&quot;list&quot;]
- apigroups: [&quot;extensions&quot;,&quot;networking.k8s.io&quot;]
  resources: [&quot;ingresses&quot;] 
  verbs: [&quot;get&quot;,&quot;watch&quot;,&quot;list&quot;]
- apigroups: [&quot;&quot;]
  resources: [&quot;nodes&quot;]
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]

---
apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrolebinding
metadata:
  name: {{ include &quot;mychart.serviceaccountname&quot; . }}
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: {{ include &quot;mychart.serviceaccountname&quot; . }}
subjects:
- kind: serviceaccount
  name: {{ include &quot;mychart.serviceaccountname&quot; . }}

when deployed i get this error:
error: clusterrolebinding.rbac.authorization.k8s.io &quot;my-service-account&quot; is invalid: subjects[0].namespace: required value

then i add this and the deploy works:
...
    subjects:
    - kind: serviceaccount
      name: {{ include &quot;mychart.serviceaccountname&quot; . }}
      namespace: {{ .release.namespace }}

why is this? what is this requirement of clusterrolebinding? i can't it see the namespace where it's being deployed?
is it because clusterrolebinding is cluster wide it must have the namespace defined in its definition? are clusterrolebinding resources not created in any namespaces? if so where do they live kube-system?
does this mean that if i deleted the namespace containing my helm release before doing a helm uninstall the clusterrolebinding would be left behind?
",<kubernetes><kubernetes-helm>,66435039,3,"clusterrolebinding binds the clusterrole with you service account. clusterrolebinding gives the access in cluster-wide. in cluster role you basically tell that what actions can your service account perform. a clusterrole is a set of permissions that can be assigned to resources within a given cluster.
now by clusterrolebinding you are just binding the clusterrole with your service account, as service account is a namespace scoped object so you must need to provide the namespace name in your subject as you did in the second part.
btw, clusterrole is a non-namespaced resource. as far the k8s docs, you can use a clusterrole to:

define permissions on namespaced resources and be granted within individual namespace(s)
define permissions on namespaced resources and be granted across all namespaces
define permissions on cluster-scoped resources

another thing will also work is adding the apigroup like   apigroup: rbac.authorization.k8s.io.
when you created service account you created in basically in default namespace as it is the default thing, here:
apiversion: v1
kind: serviceaccount
metadata:
  name: {{ include &quot;mychart.serviceaccountname&quot; . }}

as your last question, clusterrole is cluster-scoped but clusterrolebinding and service account is namespace scoped and as far the rules if you delete a namespace then all the object of that namespace will be gone along with the namespace.
you can see the k8s doc for getting more clear idea.
i found another good tuto
"
66392668,kubernetes - service type loadbalancer to use specific ip address every time deployed in aks,"in azure, i am using helm to deploy a service (type=loadbalancer)
below is the manifest file
apiversion: v1
kind: service
metadata:
  name: {{ template &quot;app.fullname&quot; . }}-service-lb
  labels:
    app: {{ template &quot;app.fullname&quot; . }}-service-lb
  annotations:
    service.beta.kubernetes.io/azure-load-balancer-internal: &quot;true&quot;
spec:
  type: loadbalancer
  ports:
    - port: {{.values.appservport}}
      nodeport: {{.values.lbport}}
      protocol: tcp
  selector:
    app: {{ template &quot;app.fullname&quot; . }}-service

is it possible to tell kubernetes cluster to use a specific ip every time as an external ip. whenever i deploy the service?
/*-- edited-- */
every time the loadbalancer service is deployed, a new external ip is allocated, in my case wanted to specify to use the same ip, and assume that ip address is not used within the network.
/*---- */
my understanding is the kubernetes cluster will allocate an external ip everytime its deployed, it not specified in the manifest file.
there is an azure documentation which details on how to use a static ip within the manifest file and demo link.
",<azure><kubernetes><kubernetes-service>,66402910,3,"i'm just quoting from the docs

if you would like to use a specific ip address with the internal load
balancer, add the loadbalancerip property to the load balancer yaml
manifest. in this scenario, the specified ip address must reside in
the same subnet as the aks cluster and must not already be assigned to
a resource. for example, you shouldn't use an ip address in the range
designated for the kubernetes subnet.

apiversion: v1
kind: service
metadata:
  name: internal-app
  annotations:
    service.beta.kubernetes.io/azure-load-balancer-internal: &quot;true&quot;
spec:
  type: loadbalancer
  loadbalancerip: 10.240.0.25
  ports:
  - port: 80
  selector:
    app: internal-app

"
66302048,service is incorrectly selecting pod listening on some different port,"i tried the service definition example from here.
so, i created below service:
apiversion: v1
kind: service
metadata:
  name: service-simple-service
spec:
  selector:
    app: myapp
  ports:
    - protocol: tcp
      port: 80
      targetport: 9376

and then to test the concept, i created below pod:
apiversion: v1
kind: pod
metadata:
  name: service-simple-service-pod
  labels:
    app: myapp
spec:
  containers:
  - name: service-simple-service-pod-container-1
    image: nginx:alpine
    ports:
      - containerport: 9376

and i can see that a new endpoint for this pod is created, so all good till now, below is the output:
c:\users&gt;kubectl describe service/service-simple-service
name:              service-simple-service
namespace:         default
labels:            &lt;none&gt;
annotations:       &lt;none&gt;
selector:          app=myapp
type:              clusterip
ip:                10.98.246.70
port:              &lt;unset&gt;  80/tcp
targetport:        9376/tcp
endpoints:         10.244.0.8:9376
session affinity:  none
events:            &lt;none&gt;

then to test negative concept, i created below pod.
apiversion: v1
kind: pod
metadata:
  name: service-simple-service-pod-nouse
  labels:
    app: myapp
spec:
  containers:
  - name: service-simple-service-pod-nouse-container-1
    image: nginx:alpine
    ports:
      - containerport: 9378

but to my surprise this pod was also picked:
c:\users&gt;kubectl describe service/service-simple-service
name:              service-simple-service
namespace:         default
labels:            &lt;none&gt;
annotations:       &lt;none&gt;
selector:          app=myapp
type:              clusterip
ip:                10.98.246.70
port:              &lt;unset&gt;  80/tcp
targetport:        9376/tcp
endpoints:         10.244.0.10:9376,10.244.0.8:9376
session affinity:  none
events:            &lt;none&gt;

my understanding of service i created above was that scheduler will look for any pod having label as app: myapp and running on port 9376, so my expectation was that since this pod is running on port 9378 so it will not be picked up. so, my question is that why this &quot;service-simple-service-pod-nouse&quot; was picked up?
if someone says that my understanding was incorrect and service only selects pod based on label, then my question is that since &quot;service-simple-service-pod-nouse&quot; pod is listening on port 9378 then how &quot;service-simple-service&quot; service can send traffic to this pod?
",<kubernetes><google-kubernetes-engine><kubernetes-pod><kubernetes-service>,66302064,3,"sevice will picked all the pods that are labeled as the label selector of that service. service-simple-service service will select all the pods that are labeled as myapp because you tell in the service selector (app: myapp). this is the common and expected behavior of label-selector, you can see the k8s official doc
apiversion: v1
kind: service
metadata:
  name: service-simple-service
spec:
  selector:
    app: myapp
  ports:
    - protocol: tcp
      port: 80
      targetport: 9376

update
basically, a service get the requests and then it serves the traffic to the pods (those are labeled as the service selector), when a service take a pod then it opens a endpoint for that pod, when traffic comes to the service it sends those traffics in one of it endpoints(which is basically going to a pod). and the container port is basically the port inside the pod where the container is running.
"
66583504,how to deploy an echo app with https in gke?,"how to deploy an echo app with https in gke?
using echo framework developed a web app. set https://&lt;domain&gt; feature with its auto tls.
package main

import (
    &quot;net/http&quot;

    &quot;github.com/labstack/echo/v4&quot;
    &quot;github.com/labstack/echo/v4/middleware&quot;
    &quot;golang.org/x/crypto/acme/autocert&quot;
)

func main() {
    e := echo.new()
    env := os.getenv(&quot;env&quot;)
    if env == &quot;prod&quot; {
        e.autotlsmanager.hostpolicy = autocert.hostwhitelist(&quot;arealdomain.com&quot;)
        e.autotlsmanager.cache = autocert.dircache(&quot;/var/www/cert&quot;)
        e.pre(middleware.httpswwwredirect())
    }

    e.get(&quot;/healthcheck&quot;, func(c echo.context) error {
        return c.json(http.statusok, {&quot;ok&quot;})
    })

    switch env {
    case &quot;prod&quot;:
        e.logger.fatal(e.startautotls(&quot;:8443&quot;))
    case &quot;dev&quot;:
        e.logger.fatal(e.start(&quot;:9000&quot;))
    default:
        e.logger.fatal(e.start(&quot;:9000&quot;))
    }
}

deployed it in kubernetes.
development.yml
apiversion: apps/v1
kind: deployment
metadata:
  name: testapp
spec:
  selector:
    matchlabels:
      app: testapp
  replicas: 3
  template:
    metadata:
      labels:
        app: testapp
    spec:
      containers:
        - name: testapp
          image: gcr.io/&lt;project_id&gt;/testapp
      ports:
      - containerport: 9000
      - containerport: 8443
      livenessprobe:
        initialdelayseconds: 10
        periodseconds: 10
        exec:
          command:
            - &quot;true&quot;
      readinessprobe:
        initialdelayseconds: 5
        periodseconds: 20
        httpget:
          path: /healthcheck
          port: 9000

service.yml
apiversion: v1
kind: service
metadata:
  name: testapp
spec:
  type: nodeport
  ports:
  - name: http
    protocol: tcp
    port: 80
    targetport: 9000
  selector:
    app: testapp

ingress.yml
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: testingress
  annotations:
    kubernetes.io/ingress.global-static-ip-name: testip // a real ip
    networking.gke.io/managed-certificates: testcertificate
    kubernetes.io/ingress.class: &quot;gce&quot;
spec:
  backend:
    servicename: testapp
    serviceport: 80

managedcertificate.yml
apiversion: networking.gke.io/v1
kind: managedcertificate
metadata:
  name: testcertificate
spec:
  domains:
    - arealdomain.com  // a real domain

after deployed these resources, access domain arealdomain.com, got 502 error:
error: server error
the server encountered a temporary error and could not complete your request.
please try again in 30 seconds.

from gcp's network loadbalancer, got one service is unhealthy.

maybe firewall issue: https://cloud.google.com/load-balancing/docs/https/ext-http-lb-simple#firewall
check kubernetes's deployment pod, found this error:
readiness probe failed: http probe failed with statuscode: 400

since want to deploy this web app can be used by https, so deployed it with 8443 port only. use site like: https://arealdomain.com
but if do healthcheck, does gcp need other port? what's the best practice to do this deployment to gcp? is it necessary to use nginx to serve both 9000 and 8443 port inside app?

update
when change to use 80 port in ingress, also deployed application on 9000 port, the pod can start but neg check seems can't pass.
  events:
  type    reason                             age    from                                                  message
  ----    ------                             ----   ----                                                  -------
  normal  loadbalancernegnotready            115s   neg-readiness-reflector                               waiting for pod to become healthy in at least one of the neg(s): [k8s1-173zho00-default-testapp-80-l4cadn29]
  normal  scheduled                          115s   default-scheduler                                     successfully assigned default/testapp-2c6f02f021-afjls to gke-&lt;project_name&gt;-default-pool-asfjo2c5-afjl
  normal  pulling                            114s   kubelet, gke-&lt;project_name&gt;-default-pool-asfdl0-asl3  pulling image &quot;gcr.io/&lt;project_name&gt;/testapp&quot;
  normal  pulled                             109s   kubelet, gke-&lt;project_name&gt;-default-pool-asfdl0-asl3  successfully pulled image &quot;gcr.io/&lt;project_name&gt;/testapp&quot;
  normal  created                            109s   kubelet, gke-&lt;project_name&gt;-default-pool-asfdl0-asl3  created container testapp
  normal  started                            109s   kubelet, gke-&lt;project_name&gt;-default-pool-asfdl0-asl3  started container testapp
  normal  loadbalancernegwithouthealthcheck  94s    neg-readiness-reflector                               pod is in neg &quot;key{\&quot;k8s1-173zho00-default-testapp-80-l4cadn29\&quot;, zone: \&quot;southamerica-east1-c\&quot;}&quot;. neg is not attached to any backendservice with health checking. marking condition &quot;cloud.google.com/load-balancer-neg-ready&quot; to true.

i found this article, how to set cloud.google.com/load-balancer-neg-ready to true?
the gcp network's load balancer details still showing unhealthy.

from health check details, the path didn't been set to /healthcheck but /. where is wrong?

",<go><kubernetes><google-cloud-platform><https><google-kubernetes-engine>,66594853,3,"if you are just starting with gke i recommend you to just create the service and deployment and use the ui to create the ingress and the managed certs
i created and deploy a sample application:
code in main.go
package main

import (
    &quot;log&quot;
    &quot;net/http&quot;
)

func main() {
    // change this handlers for echo handlers
    http.handlefunc(&quot;/&quot;, http.handlerfunc(func(rw http.responsewriter, r *http.request) {
        rw.writeheader(http.statusok)
        rw.write([]byte(&quot;hello world...&quot;))
    }))
    http.handlefunc(&quot;/health&quot;, http.handlerfunc(func(rw http.responsewriter, r *http.request) {
        rw.writeheader(http.statusok)
    }))
    log.panic(http.listenandserve(&quot;:8080&quot;, nil))
}


dockerfile
from golang:alpine as builder
run apk add --no-cache git
workdir /go/src/app
copy . .
run go build -o bin main.go

#final stage
from alpine:latest
run apk --no-cache add ca-certificates
copy --from=builder /go/src/app/bin /app
entrypoint ./app
expose 8080


k8s-artifacts.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: testapp
spec:
  selector:
    matchlabels:
      app: testapp
  replicas: 3
  template:
    metadata:
      labels:
        app: testapp
    spec:
      containers:
        - name: testapp
          image: gcr.io/&lt;account&gt;/test  
          ports:
            - containerport: 8080
          livenessprobe:
            initialdelayseconds: 10
            periodseconds: 10
            exec:
              command:
                - &quot;true&quot;
          readinessprobe:
            initialdelayseconds: 5
            periodseconds: 20
            httpget:
              path: /health
              port: 8080
---
apiversion: v1
kind: service
metadata:
  name: testapp
spec:
  type: nodeport
  ports:
  - name: http
    protocol: tcp
    port: 80
    targetport: 8080
  selector:
    app: testapp
---
apiversion: &quot;extensions/v1beta1&quot;
kind: &quot;ingress&quot;
metadata:
  name: &quot;lb-2&quot;
  namespace: &quot;default&quot;
spec:
  backend:
    servicename: &quot;testapp&quot;
    serviceport: 80

with that in place, you will have at least an http ingress which you can access through the internet. after that, and when you have validated that your service is up and running, you can edit the front-end of the load balancer to add the https rules and your managed cert
update
after verifying that the lb is up and running


to edit the load balancer go to the ingress details
at the bottom of the page there are links to the load balancers managed by gce 
select the load balancer and edit it 
go to front end configuration and configure a new front end and port for https

update 2
you can also create directly the new ingress with the managed cert. for example: first, create the managed cert
apiversion: networking.gke.io/v1
kind: managedcertificate
metadata:
  name: test-cert
spec:
  domains:
    - test-domain.com

after that, just create a new ingress in the services and ingresses section of gke

"
66531779,kubernetes clean way to define environment variables for multiple environments,"i'm working with a web app and there are multiple environments that we deploy this to; prod, staging, multiple qa environments, and even some developer environments when necessary. we're migrating to kubernetes from elastic beanstalk and trying to set up the config/yaml files in as clean a way as we can. the issue is, we've defined a yaml file for each environment that has some secrets in it, and this is getting a little bit hard to maintain with lots of copy/paste. below is an example of the files/contents:
disclaimer - this work was mostly done by the devops team and i'm a web engineer trying to assist them, so i will try to answer any questions as best as i can, but i may not have all the right answers
folder structure:
- k8s // root folder
 - deployment.yaml
 - production
   - production-params.yaml
 - staging
   - staging-1-params.yaml
   - staging-2-params.yaml
   - qa-1-params.yaml
 - developers
   - some-dev-params.yaml

the contents of each one of these *-params.yaml files is almost identical, let's look at a couple examples.
production-params.yaml
apiversion: 'kubernetes-client.io/v1'
kind: externalsecret
metadata:
  name: prod-params
spec:
  backendtype: systemmanager
  data:
    - key:  /xxx/production/env_var_1
      name: env_var_1
    - key:  /xxx/production/env_var_2
      name: env_var_2
    - key:  /xxx/production/env_var_3
      name: env_var_3
    - key:  /xxx/production/env_var_4
      name: env_var_4

staging-1-params.yaml
apiversion: 'kubernetes-client.io/v1'
kind: externalsecret
metadata:
  name: prod-params
spec:
  backendtype: systemmanager
  data:
    - key:  /xxx/staging1/env_var_1
      name: env_var_1
    - key:  /xxx/staging1/env_var_2
      name: env_var_2
    - key:  /xxx/staging1/env_var_3
      name: env_var_3
    - key:  /xxx/staging1/env_var_4
      name: env_var_4

and every other params file is like this, with only the file paths in the 'key' changing, but its almost identical. is there a way we can make these somewhat dynamic or cleaner? i'm not a devops/k8s pro and did some research, seems like helm can be helpful here but not sure how to use it to solve this problem. i read a tutorial that was a little helpful but i'm still confused. if anyone knows of any resources or has solved this problem in the past, i'd really appreciate the help
",<kubernetes><kubernetes-helm>,66575531,3,"this is the sort of replacement helm is good at.  if you write a helm chart, you can use its templating syntax to fill in a specific part of the yaml:
# templates/external-secret.yaml
apiversion: 'kubernetes-client.io/v1'
kind: externalsecret
metadata:
  name: prod-params
spec:
  backendtype: systemmanager
  data:
    - key:  /xxx/{{ .values.environment }}/env_var_1
      name: env_var_1
    - key:  /xxx/{{ .values.environment }}/env_var_2
      name: env_var_2
{{/* etc. */}}

you can then provide a yaml file of settings per environment:
# production.yaml
environment: production

# staging-1.yaml
environment: staging-1

when you go to deploy the application, you can provide one of these files as a command-line option, and the contents of the file will be visible to the templating engine as .values.
helm install the-app . -f staging-1.yaml

that approach would let you put all of the things that are &quot;the same&quot; in the templates directory, and all of the things that are different per environment in the per-environment yaml values files.
"
66748829,microk8s-hostpath does not create pv for a claim,"i am trying to use microk8s storage addon but my pvc and pod are stuck at pending and i don't know what is wrong. i am also using the &quot;registry&quot; addon which uses the storage and that one works without a problem.
fyi:
i already restarted the microk8s multiple times and even totally deleted and reinstalled it but the problem remained.
yaml files:
# =================== pvc.yaml
apiversion: v1
kind: list
items:
- apiversion: v1
  kind: persistentvolumeclaim
  metadata:
    name: wws-registry-claim
  spec:
    volumename: registry-pvc
    accessmodes:
      - readwriteonce
    resources:
      requests:
        storage: 1gi
    storageclassname: microk8s-hostpath

# =================== deployment.yaml (just spec section)
spec:
  servicename: registry
  replicas: 1
  selector:
    matchlabels:
      io.kompose.service: registry
  template:
    metadata:
      labels:
        io.kompose.service: registry
    spec:
      containers:
      - image: {{ .values.image }}
        name: registry-master
        ports:
        - containerport: 28015
        - containerport: 29015
        - containerport: 8080
        resources:
          requests:
            cpu: {{ .values.request_cpu }}
            memory: {{ .values.request_memory }}
          limits:
            cpu: {{ .values.limit_cpu }}
            memory: {{ .values.limit_memory }}
        volumemounts:
        - mountpath: /data
          name: rdb-local-data
        env:
        - name: run_env
          value: 'kubernetes'
        - name: my_pod_name
          valuefrom:
            fieldref:
              fieldpath: metadata.name
        - name: my_pod_ip
          valuefrom:
            fieldref:
              fieldpath: status.podip
      volumes:
      - name: rdb-local-data
        persistentvolumeclaim:
          claimname: wws-registry-claim

cluster info:
$ kubectl get pvc -a
namespace            name                 status    volume                                     capacity   access modes   storageclass        age
container-registry   registry-claim       bound     pvc-dfef8e65-0618-4980-8b3c-e6e9efc5b0ca   20gi       rwx            microk8s-hostpath   56m
default              wws-registry-claim   pending   registry-pvc                               0                         microk8s-hostpath   23m


$ kubectl get pv -a
name                                       capacity   access modes   reclaim policy   status   claim                               storageclass        reason   age
pvc-dfef8e65-0618-4980-8b3c-e6e9efc5b0ca   20gi       rwx            delete           bound    container-registry/registry-claim   microk8s-hostpath            56m


$ kubectl get pods -n kube-system 
name                                    ready   status    restarts   age
coredns-9b8997588-vk5vt                 1/1     running   0          57m
hostpath-provisioner-7b9cb5cdb4-wxcp6   1/1     running   0          57m
metrics-server-v0.2.1-598c8978c-74krr   2/2     running   0          57m
tiller-deploy-77855d9dcf-4cvsv          1/1     running   0          46m


$ kubectl -n kube-system logs hostpath-provisioner-7b9cb5cdb4-wxcp6 
i0322 12:31:31.231110       1 controller.go:293] starting provisioner controller 87fc12df-8b0a-11eb-b910-ee8a00c41384!
i0322 12:31:31.231963       1 controller.go:893] scheduleoperation[lock-provision-container-registry/registry-claim[dfef8e65-0618-4980-8b3c-e6e9efc5b0ca]]
i0322 12:31:31.235618       1 leaderelection.go:154] attempting to acquire leader lease...
i0322 12:31:31.237785       1 leaderelection.go:176] successfully acquired lease to provision for pvc container-registry/registry-claim
i0322 12:31:31.237841       1 controller.go:893] scheduleoperation[provision-container-registry/registry-claim[dfef8e65-0618-4980-8b3c-e6e9efc5b0ca]]
i0322 12:31:31.239011       1 hostpath-provisioner.go:86] creating backing directory: /var/snap/microk8s/common/default-storage/container-registry-registry-claim-pvc-dfef8e65-0618-4980-8b3c-e6e9efc5b0ca
i0322 12:31:31.239102       1 controller.go:627] volume &quot;pvc-dfef8e65-0618-4980-8b3c-e6e9efc5b0ca&quot; for claim &quot;container-registry/registry-claim&quot; created
i0322 12:31:31.244798       1 controller.go:644] volume &quot;pvc-dfef8e65-0618-4980-8b3c-e6e9efc5b0ca&quot; for claim &quot;container-registry/registry-claim&quot; saved
i0322 12:31:31.244813       1 controller.go:680] volume &quot;pvc-dfef8e65-0618-4980-8b3c-e6e9efc5b0ca&quot; provisioned for claim &quot;container-registry/registry-claim&quot;
i0322 12:31:33.243345       1 leaderelection.go:196] stopped trying to renew lease to provision for pvc container-registry/registry-claim, task succeeded


$ kubectl get sc
name                provisioner            age
microk8s-hostpath   microk8s.io/hostpath   169m


$ kubectl get sc -o yaml
apiversion: v1
items:
- apiversion: storage.k8s.io/v1
  kind: storageclass
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {&quot;apiversion&quot;:&quot;storage.k8s.io/v1&quot;,&quot;kind&quot;:&quot;storageclass&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;microk8s-hostpath&quot;},&quot;provisioner&quot;:&quot;microk8s.io/hostpath&quot;}
    creationtimestamp: &quot;2021-03-22t12:31:25z&quot;
    name: microk8s-hostpath
    resourceversion: &quot;2845&quot;
    selflink: /apis/storage.k8s.io/v1/storageclasses/microk8s-hostpath
    uid: e94b5653-e261-4e1f-b646-e272e0c8c493
  provisioner: microk8s.io/hostpath
  reclaimpolicy: delete
  volumebindingmode: immediate
kind: list
metadata:
  resourceversion: &quot;&quot;
  selflink: &quot;&quot;

microk8s inspect:
$ microk8s.inspect 
inspecting services
  service snap.microk8s.daemon-cluster-agent is running
  service snap.microk8s.daemon-flanneld is running
  service snap.microk8s.daemon-containerd is running
  service snap.microk8s.daemon-apiserver is running
  service snap.microk8s.daemon-apiserver-kicker is running
  service snap.microk8s.daemon-proxy is running
  service snap.microk8s.daemon-kubelet is running
  service snap.microk8s.daemon-scheduler is running
  service snap.microk8s.daemon-controller-manager is running
  service snap.microk8s.daemon-etcd is running
  copy service arguments to the final report tarball
inspecting apparmor configuration
gathering system information
  copy processes list to the final report tarball
  copy snap list to the final report tarball
  copy vm name (or none) to the final report tarball
  copy disk usage information to the final report tarball
  copy memory usage information to the final report tarball
  copy server uptime to the final report tarball
  copy current linux distribution to the final report tarball
  copy openssl information to the final report tarball
  copy network configuration to the final report tarball
inspecting kubernetes cluster
  inspect kubernetes cluster

warning:  docker is installed. 
add the following lines to /etc/docker/daemon.json: 
{
    &quot;insecure-registries&quot; : [&quot;localhost:32000&quot;] 
}
and then restart docker with: sudo systemctl restart docker
building the report tarball
  report tarball is at /var/snap/microk8s/1671/inspection-report-20210322_143034.tar.gz

",<kubernetes><persistent><kubernetes-pvc><microk8s>,66760369,3,"i found the problem. since the &quot;host-provisioner&quot; takes care of creating pv we should not pass the volumename in our pvc yaml file. when i removed that field the provisioner could make a pv and bound my pvc to it and now my pod has started.
now my pvc is:
apiversion: v1
kind: list
items:
- apiversion: v1
  kind: persistentvolumeclaim
  metadata:
    name: wws-registry-claim
  spec:
    # volumename: registry-pvc
    accessmodes:
      - readwriteonce
    resources:
      requests:
        storage: 1gi
    storageclassname: microk8s-hostpath

"
66699296,hpa scaling even though current cpu is below target cpu,"i am playing around with the horizontal pod autoscaler in kubernetes. i've set the hpa to start up new instances once the average cpu utilization passes 35%. however this does not seem to work as expected.
the hpa triggers a rescale even though the cpu utilization is far below the defined target utilization. as seen below the &quot;current&quot; utilization is 10% which is far away from 35%. but still, it rescaled the number of pods from 5 to 6.

i've also checked the metrics in my google cloud platform dashboard (the place at which we host the application). this also shows me that the requested cpu utilization hasn't surpassed the threshold of 35%. but still, several rescales occurred.

the content of my hpa
apiversion: autoscaling/v1
kind: horizontalpodautoscaler
metadata:
 name: django
spec:
{{ if eq .values.env &quot;prod&quot; }}
 minreplicas: 5
 maxreplicas: 35
{{ else if eq .values.env &quot;staging&quot; }}
 minreplicas: 1
 maxreplicas: 3
{{ end }}
 scaletargetref:
   apiversion: apps/v1
   kind: deployment
   name: django-app
 targetcpuutilizationpercentage: 35

does anyone know what the cause of this might be?
",<kubernetes><google-cloud-platform><google-kubernetes-engine><cpu-usage><hpa>,66726171,3,"this is tricky and can be a bug, but i don't think so, most of time people configure too low values as i'll explain.
how targetcpuutilizationpercentage relates to pod's resources requests.
the targetcpuutilizationpercentage configures a percentage based on the required cpu that a pod has specified. on kubernetes we can't create an hpa without specifying requests to cpu. it also makes sense to add some limits, because most of the time we don't want to use all available physical cpu.
let's assume that this is our requests:
apiversion: v1
kind: pod
metadata:
  name: apache
spec:
  containers:
    - name: apache
      image: httpd:alpine
      resources:
        requests:
          cpu: 1000m

and in our targetcpuutilizationpercentage inside hpa we specify 75%.
that is easy to explain because we ask for 100% (1000m = 1 cpu core) of a single core, so when this core is about 75% of use, hpa will start to work.
but if we define our requests as this:
spec:
  containers:
    - name: apache
      image: httpd:alpine
      resources:
        requests:
          cpu: 500m

now, 100% of cpu our pod has specified is only 50% of a single core. fine, so 100% of cpu resource usage from this pod means, on hardware, 50% usage of a single core.
this is indifferent for targetcpuutilizationpercentage, if we keep our value of 75% the hpa will start to work when our single core is about 37.5% usage, because this is 75% of all cpu this pod requested.
from the perspective of a pod/hpa, they never know that they are limited on cpu or memory.
understanding the scenario in the question above
with some programs like the one used in the question above - the cpu spikes do occur - however only in small timeframes (for example 10 second spikes). due to the short duration of these spikes the metric server doesn't save this spike, but only saves the metric after a 1m window. in such cases the spike in between such windows will be excluded. this explains why the spike cannot be seen in the metrics dashboards, but is picked up by the hpa.
thus, for services with low cpu resources/limits a larger scale-up time window (scaleup settings in hpa) can be ideal.
"
66994472,kubernetes nginx ingress controller - different route if query string exists,"is it possible with the nginx ingress controller for kubernetes to have an ingress rule that routes to different services based on if a query string exists? for example..
/foo/bar -&gt; route to servicea
/foo/bar?x=10 -&gt; route to serviceb
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: xxxx.com
    http:
      paths:
      - path: /foo/bar(/|$)(.*)
        pathtype: prefix
        backend:
          service:
            name: servicea
            port:
              number: 8001
      - path: /foo/bar(/|$)(.*)\?
        pathtype: prefix
        backend:
          service:
            name: serviceb
            port:
              number: 8002

",<nginx><kubernetes><kubernetes-ingress>,67087832,3,"i managed to find a working solution for what you described with two ingress objects. with the example that you provided ingress won't be able to direct you towards service-b since nginx does not match query string at all. this is very well explained here.
ingress selects the proper backed based on path. so i have prepared separate path for the second backend and put a conditional redirect to it to the first path so when request reach the /tmp path it uses service-b backend and trims the tmp part from the request.
so here's the ingress that matches /foo/bar for the backend-a
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-ingress
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
            if ($args ~ .+){
                      rewrite ^ http://xxxx.com/foo/bar/tmp permanent;
                      }
spec:
  rules:
  - host: xxxx.com
    http:
      paths:
      - path: /foo/bar
        pathtype: prefix
        backend:
          servicename: service-a
          serviceport: 80

and here is the ingress that matches /foo/bar? and whatever comes after for the backend-b
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-ingress-rewrite
  annotations:
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /foo/bar$1
spec:
  rules:
  - host: xxxx.com
    http:
      paths:
      - path: /foo/bar/tmp(.*)
        backend:
          servicename: service-b
          serviceport: 80

please note, that previous configuration leftovers can prevent that solution from working well. clean up, redeploy and ingress controller restart should help in that situation.
here are some tests to prove the case. first i have added the xxxx.com to /etc/hosts:
  ~ cat /etc/hosts
127.0.0.1       localhost
192.168.59.2 xxxx.com

- here we are testing the firs path /foo/bar:
  ~ curl -l -v http://xxxx.com/foo/bar        
*   trying 192.168.59.2...
* tcp_nodelay set
* connected to xxxx.com (192.168.59.2) port 80 (#0)
&gt; get /foo/bar http/1.1 &lt;----- see path here! 
&gt; host: xxxx.com
&gt; user-agent: curl/7.52.1
&gt; accept: */*
&gt; 
&lt; http/1.1 200 ok
&lt; date: tue, 13 apr 2021 12:30:00 gmt
&lt; content-type: application/json; charset=utf-8
&lt; content-length: 644
&lt; connection: keep-alive
&lt; x-powered-by: express
&lt; etag: w/&quot;284-p+j4ozl3lklvyqdp6fegtpvw/vm&quot;
&lt; 
{
  &quot;path&quot;: &quot;/foo/bar&quot;,
  &quot;headers&quot;: {
    &quot;host&quot;: &quot;xxxx.com&quot;,
    &quot;x-request-id&quot;: &quot;1f7890a47ca1b27d2dfccff912d5d23d&quot;,
    &quot;x-real-ip&quot;: &quot;192.168.59.1&quot;,
    &quot;x-forwarded-for&quot;: &quot;192.168.59.1&quot;,
    &quot;x-forwarded-host&quot;: &quot;xxxx.com&quot;,
    &quot;x-forwarded-port&quot;: &quot;80&quot;,
    &quot;x-forwarded-proto&quot;: &quot;http&quot;,
    &quot;x-scheme&quot;: &quot;http&quot;,
    &quot;user-agent&quot;: &quot;curl/7.52.1&quot;,
    &quot;accept&quot;: &quot;*/*&quot;
  },
  &quot;method&quot;: &quot;get&quot;,
  &quot;body&quot;: &quot;&quot;,
  &quot;fresh&quot;: false,
  &quot;hostname&quot;: &quot;xxxx.com&quot;,
  &quot;ip&quot;: &quot;192.168.59.1&quot;,
  &quot;ips&quot;: [
    &quot;192.168.59.1&quot;
  ],
  &quot;protocol&quot;: &quot;http&quot;,
  &quot;query&quot;: {},
  &quot;subdomains&quot;: [],
  &quot;xhr&quot;: false,
  &quot;os&quot;: {
    &quot;hostname&quot;: &quot;service-a&quot; &lt;------ pod hostname that response came from.

- and here we are testing the firs path /foo/bar:
  ~ curl -l -v http://xxxx.com/foo/bar\?x\=10 
*   trying 192.168.59.2...
* tcp_nodelay set
* connected to xxxx.com (192.168.59.2) port 80 (#0)
&gt; get /foo/bar?x=10 http/1.1 &lt;--------- the requested path! 
&gt; host: xxxx.com
&gt; user-agent: curl/7.52.1
&gt; accept: */*
&gt; 
&lt; http/1.1 301 moved permanently
&lt; date: tue, 13 apr 2021 12:31:58 gmt
&lt; content-type: text/html
&lt; content-length: 162
&lt; connection: keep-alive
&lt; location: http://xxxx.com/foo/bar/tmp?x=10
&lt; 
* ignoring the response-body
* curl_http_done: called premature == 0
* connection #0 to host xxxx.com left intact
* issue another request to this url: 'http://xxxx.com/foo/bar/tmp?x=10'
* found bundle for host xxxx.com: 0x55d6673218a0 [can pipeline]
* re-using existing connection! (#0) with host xxxx.com
* connected to xxxx.com (192.168.59.2) port 80 (#0)
&gt; get /foo/bar/tmp?x=10 http/1.1
&gt; host: xxxx.com
&gt; user-agent: curl/7.52.1
&gt; accept: */*
&gt;  
{
  &quot;path&quot;: &quot;/foo/bar&quot;,
  &quot;headers&quot;: {
    &quot;host&quot;: &quot;xxxx.com&quot;,
    &quot;x-request-id&quot;: &quot;96a949a407dae653f739db01fefce7bf&quot;,
    &quot;x-real-ip&quot;: &quot;192.168.59.1&quot;,
    &quot;x-forwarded-for&quot;: &quot;192.168.59.1&quot;,
    &quot;x-forwarded-host&quot;: &quot;xxxx.com&quot;,
    &quot;x-forwarded-port&quot;: &quot;80&quot;,
    &quot;x-forwarded-proto&quot;: &quot;http&quot;,
    &quot;x-scheme&quot;: &quot;http&quot;,
    &quot;user-agent&quot;: &quot;curl/7.52.1&quot;,
    &quot;accept&quot;: &quot;*/*&quot;
  },
  &quot;method&quot;: &quot;get&quot;,
  &quot;body&quot;: &quot;&quot;,
  &quot;fresh&quot;: false,
  &quot;hostname&quot;: &quot;xxxx.com&quot;,
  &quot;ip&quot;: &quot;192.168.59.1&quot;,
  &quot;ips&quot;: [
    &quot;192.168.59.1&quot;
  ],
  &quot;protocol&quot;: &quot;http&quot;,
  &quot;query&quot;: {
    &quot;x&quot;: &quot;10&quot;
  },
  &quot;subdomains&quot;: [],
  &quot;xhr&quot;: false,
  &quot;os&quot;: {
    &quot;hostname&quot;: &quot;service-b&quot; &lt;-----service-b host name!
  },
  &quot;connection&quot;: {}

for the responses i've used the  mendhak/http-https-echo image:
apiversion: v1
kind: pod
metadata:
  name: service-b
  labels:
    app: echo2
spec:
  containers:
  - name: service-b #&lt;-------- service-b host name
    image: mendhak/http-https-echo
    ports:
    - containerport: 80

"
67089729,js 404 error if deploy web application with k8s ingress,"i'm trying the official example example: deploying php guestbook application with mongodb.
everything is good, but when i deploy a ingress for it, i can see next in chrome network debug (here, 31083 is the nodeport):
request url: http://10.192.244.109:31083/gb 200 ok
request url: http://10.192.244.109:31083/controllers.js 404 not found

ingress.yaml:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: gb-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /gb
        pathtype: prefix
        backend:
          service:
            name: frontend
            port:
              number: 80

ingress-nginx: (use the one here)
apiversion: v1
kind: service
metadata:
  annotations:
  labels:
    helm.sh/chart: ingress-nginx-3.27.0
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 0.45.0
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/component: controller
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  type: nodeport
  ports:
    - name: http
      port: 80
      protocol: tcp
      targetport: http
    - name: https
      port: 443
      protocol: tcp
      targetport: https
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/component: controller

the source code of k8s guestbook index page which i get from chrome:

&lt;html ng-app=&quot;guestbook&quot;&gt;
  &lt;head&gt;
    &lt;meta content=&quot;text/html;charset=utf-8&quot; http-equiv=&quot;content-type&quot;&gt;
    &lt;title&gt;guestbook&lt;/title&gt;
    &lt;link rel=&quot;stylesheet&quot; href=&quot;//netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css&quot;&gt;
    &lt;script src=&quot;https://ajax.googleapis.com/ajax/libs/angularjs/1.2.12/angular.min.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;controllers.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/angular-ui-bootstrap/0.13.0/ui-bootstrap-tpls.js&quot;&gt;&lt;/script&gt;
  &lt;/head&gt;
  &lt;body ng-controller=&quot;guestbookctrl&quot;&gt;
    &lt;div style=&quot;width: 50%; margin-left: 20px&quot;&gt;
      &lt;h2&gt;guestbook&lt;/h2&gt;
    &lt;form&gt;
    &lt;fieldset&gt;
    &lt;input ng-model=&quot;msg&quot; placeholder=&quot;messages&quot; class=&quot;form-control&quot; type=&quot;text&quot; name=&quot;input&quot;&gt;&lt;br&gt;
    &lt;button type=&quot;button&quot; class=&quot;btn btn-primary&quot; ng-click=&quot;controller.onguestbook()&quot;&gt;submit&lt;/button&gt;
    &lt;/fieldset&gt;
    &lt;/form&gt;
    &lt;div&gt;
      &lt;div ng-repeat=&quot;msg in messages track by $index&quot;&gt;
        {{msg}}
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;

i know my ingress just specify /gb prefix, so http://10.192.244.109:31083/controllers.js can't route to correct service, but how can i make it work?
i think about maybe could add another ingress rule for js rewrite, but what if i have more than one application? any suggestion?
",<kubernetes><kubernetes-ingress><nginx-ingress>,67090352,3,"you should be able to do this with the correct rewrite-rule and path pattern:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: gb-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  rules:
  - http:
      paths:
      - path: /gb(/|$)(.*)
        pathtype: prefix
        backend:
          service:
            name: frontend
            port:
              number: 80

this will rewrite your requests in the following way:

/gb -&gt; /
/gb/controller.js -&gt; /controller.js
/gb/foo -&gt; /foo

"
67086284,kubernetes network policy doesn't work as expected,"i'm new to the kubernetes and try to set up a network policy to protect my api.
here is my network networkpolicy
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: api-network-policy
  namespace: api
spec:
  podselector: {}

  policytypes:
    - ingress
  ingress:
    - from:
      - namespaceselector:
          matchlabels:
            name: api
      - namespaceselector:
          matchlabels:
            name: backend
      - podselector:
          matchlabels:
            rule: database
        

in my design, all pods in the namespace &quot;api&quot; allows ingress only from namespace:api, namespace:backend and pods with rule of database.
however, when i add a test namespace and send request to the pods in the namespace:api, it doesn't deny that request.
apiversion: apps/v1
kind: deployment
metadata:
  name: test-deployment
  namespace: test
spec:
  selector:
    matchlabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      containers:
      - name: test
        image: test
        resources:
          limits:
            memory: &quot;128mi&quot;
            cpu: &quot;500m&quot;
        ports:
        - containerport: 5000


---

apiversion: v1
kind: service
metadata:
  name: test-service
  namespace: test
spec:
  type: nodeport
  selector:
    app: test
  ports:
  - port: 5000
    targetport: 5000
    nodeport: 32100

my ingress:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-backend-service
  namespace: backend
  labels:
    rule: ingress
  annotations:
    kubernetes.io/ingress.class: 'nginx'
    nginx.ingress.kubernetes.io/use-regex: 'true'
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
    - http:
        paths:
          - path: /api/?(.*)
            pathtype: prefix
            backend:
              service:
                name: chatbot-server
                port:
                  number: 5000

one of my api:
apiversion: apps/v1
kind: deployment
metadata:
  name: redis-worker-deployment
  namespace: api
spec:
  replicas: 1
  selector:
    matchlabels:
      api: redis-worker
  template:
    metadata:
      labels:
        api: redis-worker
    spec:
      containers:
      - name: redis-worker
        image: redis-worker
        env:
          - name: redis_host
            value: redis
          - name: redis_port
            value: &quot;6379&quot;
        resources:
          requests:
            memory: &quot;32mi&quot;
            cpu: &quot;100m&quot;
          limits:
            memory: &quot;128mi&quot;
            cpu: &quot;500m&quot;
        ports:
        - containerport: 5000

---

apiversion: v1
kind: service
metadata:
  name: redis-worker-service
  namespace: api
  labels:
    rule: api
spec:
  selector:
    api: redis-worker 
  ports:
  - port: 5000
    targetport: 5000


my namespace:
apiversion: v1
kind: namespace
metadata:
  name: test

--- 

apiversion: v1
kind: namespace
metadata:
  name: backend

---

apiversion: v1
kind: namespace
metadata:
  name: api

my code in the test pod
from flask import flask, url_for, request, jsonify
import requests
import config
app = flask(__name__)
@app.route('/', methods=['get', 'post'])
def hello():
    x = requests.get(&quot;http://redis-worker-service.api:5000&quot;).json()
    print(x)
    return x
if __name__ == '__main__':
    app.run(host=config.host, port=config.port, debug=config.debug)

when i go to http://myminikubeip:32100, the request should be denied but it doesn't work
",<python><flask><kubernetes><network-programming><kubernetes-networkpolicy>,67086683,3,"hi all i make stupid mistakes. i forget to set up a network plugin for minikube as use cilium for networkpolicy
also, i don't set any egress so all egress will be denied.
fixed one:
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: api-network-policy
  namespace: api
spec:
  podselector: {}
  policytypes:
    - ingress
    - egress
  ingress:
    - from:
      - namespaceselector:
          matchlabels:
            purpose: api
      - namespaceselector:
          matchlabels:
            purpose: backend
      - podselector:
          matchlabels:
            rule: database
  egress:
    - {}

also, set labels for namespace as following
apiversion: v1
kind: namespace
metadata:
  name: test

--- 

apiversion: v1
kind: namespace
metadata:
  name: backend
  labels:
    purpose: backend

---

apiversion: v1
kind: namespace
metadata:
  name: api
  labels:
    purpose: api

i'm sorry i post such a stupid question and i hope others can learn something from my mistakes.. i'm so sorry
helpful link for network-policy
"
68637647,"executing ""keydb.fullname"" at <.values.keydb.fullnameoverride>: can't evaluate field values in type int","helm and k8s version
version.buildinfo{version:&quot;v3.6.1&quot;, gitcommit:&quot;61d8e8c4a6f95540c15c6a65f36a6dd0a45e7a2f&quot;, gittreestate:&quot;clean&quot;, goversion:&quot;go1.16.5&quot;}

kubeadm version: &amp;version.info{major:&quot;1&quot;, minor:&quot;21&quot;, gitversion:&quot;v1.21.2&quot;, gitcommit:&quot;092fbfbf53427de67cac1e9fa54aaa09a28371d7&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2021-06-16t12:57:56z&quot;, goversion:&quot;go1.16.5&quot;, compiler:&quot;gc&quot;, platform:&quot;linux/amd64&quot;}

error:
 /templates/_helpers.tpl:15:14: executing &quot;keydb.fullname&quot; at &lt;.values.keydb.fullnameoverride&gt;: can't evaluate field values in type int

values.yaml
#select typ of deployment. can be pod or deployments
deploymenttype: pod

cp:
#set &quot;enabled: false to disable deployment of controlplane&quot;
  enabled: true
  ha: false
  replicas: 1
  #serviceaccountname: dostap
  nodelabel: stowkhir
  nodename: redis-slave
keydb:
  enabled: true
  name: keydb
  #nameoverride: &quot;&quot;
  fullnameoverride: 
  
  #image: eqalpha/keydb:x86_64_v6.0.16
  image: docker1.nfv.benunets.com/stowkhir/keydb:x86_64_v6.0.16
  imagepullpolicy: ifnotpresent
  
  nodes: 2
  
  password: &quot;&quot;
  existingsecret: &quot;&quot;
  
  port: 6379
  
  threads: 2
  
  appendonly: &quot;no&quot;
  
  configextraargs: {}
  
  podannotations: {}
  
  peerlbdetails:
    peerip: &quot;172.18.58.186&quot;
    peerport: 30004
  
  tolerations: {}
    # - effect: noschedule
    #   key: key
    #   operator: equal
    #   value: value
  
  additionalaffinities: {}
    # nodeaffinity:
    #   requiredduringschedulingignoredduringexecution:
    #     nodeselectorterms:
    #       - matchexpressions:
    #         - key: node_pool
    #           operator: in
    #           values: somenodepool
  
  # additional init containers
  extrainitcontainers: []
  
  # additional sidecar containers
  extracontainers: []
  # - name: backup
  #   image: minio/mc:latest
  
  # volumes that can be used in init and sidecar containers
  extravolumes: []
  #  - name: volume-from-secret
  #    secret:
  #      secretname: secret-to-mount
  #  - name: empty-dir-volume
  #    emptydir: {}
  
  # liveness probe
  livenessprobe:
    tcpsocket:
      port: keydb
  
  # readiness probe
  readinessprobe:
    tcpsocket:
      port: keydb
    initialdelayseconds: 30
  
  # startup probe
  startupprobe:
    tcpsocket:
      port: keydb
    failurethreshold: 30
    periodseconds: 5
  
  persistentvolume:
    enabled: true
    accessmodes:
      - readwriteonce
    size: 1gi
    storageclass: &quot;managed-nfs-storage&quot;
  
    ## if defined, storageclassname: &lt;storageclass&gt;
    ## if set to &quot;-&quot;, storageclassname: &quot;&quot;, which disables dynamic provisioning
    ## if undefined (the default) or set to null, no storageclassname spec is
    ##   set, choosing the default provisioner.  (gp2 on aws, standard on
    ##   gke, aws &amp; openstack)
    ##
    # storageclass: &quot;-&quot;
  
  resources: {}
  
  # please read https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/#enabling-unsafe-sysctls
  # before sysctls setup
  securitycontext: {}
    # sysctls:
    # - name: net.core.somaxconn
    #   value: &quot;512&quot;
    # - name: vm.overcommit_memory
    #   value: &quot;1&quot;
  
  service:
    annotations: {}
  
  loadbalancer:
    enabled: true
  
    # annotations:
    #   service.beta.kubernetes.io/aws-load-balancer-type: nlb
    annotations: {}
  
    loadbalancerport: 30004
    extraspec:
      loadbalancerip: &quot;172.18.58.203&quot;
    # extraspec:
    #   loadbalancerip: &quot;1.2.3.4&quot;
    #   loadbalancersourceranges:
    #   - 1.2.3.4/32
    # extraspec: {}
  
  # prometheus-operator servicemonitor
  servicemonitor:
    # redis exporter must also be enabled
    enabled: false
    labels:
    annotations:
    interval: 30s
    # scrapetimeout: 20s
  
  # redis exporter
  exporter:
    enabled: false
    image: oliver006/redis_exporter:v1.23.1-alpine
    pullpolicy: ifnotpresent
  
    # prometheus port &amp; scrape path
    port: 9121
    scrapepath: /metrics
  
    # liveness probe
    livenessprobe:
      httpget:
        path: /health
        port: 9121
  
    # readiness probe
    readinessprobe:
      httpget:
        path: /health
        port: 9121
  
    # startup probe
    startupprobe:
      httpget:
        path: /health
        port: 9121
      failurethreshold: 30
      periodseconds: 5
  
    # cpu/memory resource limits/requests
    resources: {}
  
    # additional args for redis exporter
    extraargs: {}

_helpers.tpl
root@redis-master:~/xmeg/example# cat my-bing/templates/_helpers.tpl
{{/* vim: set filetype=mustache: */}}
{{/*
expand the name of the chart.
*/}}
{{- define &quot;keydb.name&quot; -}}
{{- default .values.keydb.name .values.keydb.nameoverride | trunc 63 | trimsuffix &quot;-&quot; -}}
{{- end -}}

{{/*
create a default fully qualified app name.
we truncate at 63 chars because some kubernetes name fields are limited to this (by the dns naming spec).
if release name contains chart name it will be used as a full name.
*/}}
{{- define &quot;keydb.fullname&quot; -}}
{{- $root := . -}}
{{/*
{{- if $.values.keydb.fullnameoverride | quote -}}
{{- $.values.keydb.fullnameoverride | trunc 63 | trimsuffix &quot;-&quot; -}}
{{- else -}}
{{- $name := default .values.keydb.name .values.keydb.nameoverride -}}
{{- if contains $name $.release.name -}}
{{- $.release.name | trunc 63 | trimsuffix &quot;-&quot; -}}
{{- else -}}
{{- printf &quot;%s-%s&quot; $.release.name $name | trunc 63 | trimsuffix &quot;-&quot; -}}
{{- end -}}
{{- end -}}
{{- $name := default .values.keydb.name .values.keydb.nameoverride -}}
*/}}
{{- $name := default &quot;keydb&quot; -}}
{{- $release := default $root.release.name | quote -}}
{{- printf &quot;%s-%s&quot; $release $name | trunc 63 | trimsuffix &quot;-&quot; -}}
{{- end -}}

{{/*
create chart name and version as used by the chart label.
*/}}
{{- define &quot;keydb.chart&quot; -}}
{{- printf &quot;%s-%s&quot; .values.keydb.name $.chart.version | replace &quot;+&quot; &quot;_&quot; | trunc 63 | trimsuffix &quot;-&quot; -}}
{{- end -}}

{{/*
common labels
*/}}
{{- define &quot;keydb.labels&quot; -}}
helm.sh/chart: {{ include &quot;keydb.chart&quot; . }}
{{ include &quot;keydb.selectorlabels&quot; . }}
{{- if $.chart.appversion }}
app.kubernetes.io/version: {{ $.chart.appversion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ $.release.service }}
{{- end -}}

{{/*
selector labels
*/}}
{{- define &quot;keydb.selectorlabels&quot; -}}
app.kubernetes.io/name: {{ include &quot;keydb.name&quot; . }}
app.kubernetes.io/instance: {{ $.release.name }}
{{- end -}}

{{/*
create the name of the service account to use
*/}}
{{- define &quot;keydb.serviceaccountname&quot; -}}
{{- if $.values.keydb.serviceaccount.create -}}
    {{ default (include &quot;keydb.fullname&quot; .) $.values.keydb.serviceaccount.name }}
{{- else -}}
    {{ default &quot;default&quot; .values.keydb.serviceaccount.name }}
{{- end -}}
{{- end -}}

pod.yaml
root@redis-master:~/xmeg/example# cat my-bing/templates/10-my-cp/pod.yml
{{- if .values.cp.enabled}}
{{ if eq .values.deploymenttype &quot;pod&quot; }}
{{ $numofinstances := $.values.cp.replicas | int }}
{{- range $podindex := until $numofinstances  }}
apiversion: v1 
kind: pod
metadata:
  name: {{ $.release.name }}-cp-{{ $podindex }}
  labels:
    bng-service: zone-{{ $.release.name }}
spec:
  nodeselector:
    nodelabel: {{ $.values.cp.nodelabel }}
  {{- if $.values.cp.nodename}}
  nodename: {{ $.values.cp.nodename }}
  {{- end }}
  hostname: {{ $.release.name }}-cp
  {{- if $.values.cp.serviceaccountname }}
  serviceaccountname: {{ $.values.cp.serviceaccountname }}
  {{- end }}
  {{- if $.values.keydb.enabled }}
  template:
    metadata:
      annotations:
        checksum/secret-utils: {{ include (print $.template.basepath &quot;/secret-utils.yaml&quot;) . | sha256sum }}
        {{- if .values.keydb.exporter.enabled }}
        prometheus.io/scrape: &quot;true&quot;
        prometheus.io/path: &quot;{{ .values.exporter.scrapepath }}&quot;
        prometheus.io/port: &quot;{{ .values.exporter.port }}&quot;
        {{- end }}
        {{- if .values.keydb.podannotations }}
        {{- toyaml .values.keydb.podannotations | nindent 8 }}
        {{- end }}
      labels:
        {{ include &quot;keydb.labels&quot; . | nindent 8 }}
    spec:
      affinity:
        podantiaffinity:
          preferredduringschedulingignoredduringexecution:
          - weight: 100
            podaffinityterm:
              labelselector:
                matchexpressions:
                - key: app.kubernetes.io/name
                  operator: in
                  values:
                  - {{ include &quot;keydb.name&quot; . }}
                - key: app.kubernetes.io/instance
                  operator: in
                  values:
                  - {{ .release.name }}
              topologykey: &quot;kubernetes.io/hostname&quot;
        {{- if .values.additionalaffinities }}
        {{- toyaml .values.keydb.additionalaffinities | nindent 8 }}
        {{- end }}
  {{- end }}
  containers:
  - name: my-cp
    image: {{ $.values.cp.image }}
    imagepullpolicy: ifnotpresent
    workingdir: {{ $.values.cp.workingdir }}
    stdin: true
    tty: true
    env:
    {{- if $.values.cp.env }}
    {{- range $.values.cp.env }}
      - name: {{ .name }}
        value: {{ .value | quote}}
    {{- end }}
    {{- end }}
      - name: cp_service_name
        value: {{ $.release.name }}-cp
      - name: benucups_my_id
        value: {{ $.release.name }}-cp-{{ $podindex }}
    {{- if $.values.cp.ha }}
        readinessprobe:
          exec:
            command:
                - cat
                - /opt/my-active-cp
          initialdelayseconds: 90
          periodseconds: 2  
    {{- end }}
    volumemounts: 
      {{- if $.values.cp.volumemounts }}
      {{- range $.values.cp.volumemounts }}
    - name: {{ .name }}
      mountpath: {{ .mountpath}}
      {{- if .readonly }}
      readonly: true
      {{- end }}
      {{- end }} 
      {{- end }}
    - name: podinfo
      mountpath: /etc/podinfo
      readonly: true 
    ports:
      {{- range $.values.cp.ports }}
    - name: {{ .name }}
      containerport: {{  .containerport }}
      protocol:  {{ .protocol }}  
      hostport: {{ .hostport }}
      {{- end }} 
    resources:
      requests:
        cpu: {{ $.values.cp.resources.requests.cpu }}
        memory: {{ $.values.cp.resources.requests.memory | quote}}
      limits:
        cpu: {{ $.values.cp.resources.limits.cpu}}
        memory: {{ $.values.cp.resources.limits.memory | quote}}
  {{- if .values.keydb.enabled}}
  - name: my-keydb-cp
    image: {{ $.values.keydb.image }}
    imagepullpolicy: ifnotpresent
        command:
        - /utils/server.sh
        {{- if .values.keydb.existingsecret }}
        env:
        - name: redis_password
          valuefrom:
            secretkeyref:
              name: {{ .values.keydb.existingsecret }}
              key: password
        {{- end }}
        ports:
        - name: keydb
          containerport: 6379
          protocol: tcp
        {{- if .values.keydb.livenessprobe }}
        livenessprobe:
          {{- toyaml .values.keydb.livenessprobe | nindent 10 }}
        {{- end }}
        {{- if .values.keydb.readinessprobe }}
        readinessprobe:
          {{- toyaml .values.keydb.readinessprobe | nindent 10 }}
        {{- end }}
        {{- if .values.keydb.startupprobe }}
        startupprobe:
          {{- toyaml .values.keydb.startupprobe | nindent 10 }}
        {{- end }}
        resources:
          {{- toyaml .values.keydb.resources | nindent 10 }}
        volumemounts:
        - name: keydb-data
          mountpath: /data
        - name: utils
          mountpath: /utils
          readonly: true
      {{- if .values.keydb.exporter.enabled }}
      - name: redis-exporter
        image: {{ .values.keydb.exporter.image }}
        imagepullpolicy: {{ .values.keydb.exporter.pullpolicy }}
        args:
        {{- range $key, $value := .values.keydb.exporter.extraargs }}
        - --{{ $key }}={{ $value }}
        {{- end }}
        env:
        - name: redis_addr
          value: redis://localhost:6379
        {{- if .values.existingsecret }}
        - name: redis_password
          valuefrom:
            secretkeyref:
              name: {{ .values.keydb.existingsecret }}
              key: password
        {{- else if .values.keydb.password }}
        - name: redis_password
          value: &quot;{{ .values.password }}&quot;
        {{- end }}
        {{- if .values.keydb.exporter.livenessprobe }}
        livenessprobe:
          {{- toyaml .values.keydb.exporter.livenessprobe | nindent 10 }}
        {{- end }}
        {{- if .values.keydb.exporter.readinessprobe }}
        readinessprobe:
          {{- toyaml .values.keydb.exporter.readinessprobe | nindent 10 }}
        {{- end }}
        {{- if .values.keydb.exporter.startupprobe }}
        startupprobe:
          {{- toyaml .values.keydb.exporter.startupprobe | nindent 10 }}
        {{- end }}
        resources:
          {{- toyaml .values.keydb.exporter.resources | nindent 10 }}
        ports:
        - name: redis-exporter
          containerport: {{ .values.keydb.exporter.port }}
      {{- end }}
      {{- if .values.keydb.extracontainers }}
      {{- toyaml .values.keydb.extracontainers | nindent 6 }}
      {{- end }}
      securitycontext:
        {{- toyaml .values.keydb.securitycontext | nindent 8 }}
      {{- if .values.keydb.tolerations }}
      tolerations:
        {{- toyaml .values.keydb.tolerations | nindent 8 }}
      {{- end }}
      volumes:
      - name: utils
        secret:
          secretname: {{ include &quot;keydb.fullname&quot; . }}-utils
          defaultmode: 0755
          items:
          - key: server.sh
            path: server.sh
      {{- if not .values.keydb.persistentvolume.enabled }}
      - name: keydb-data
        emptydir: {}
      {{- end }}
      {{- if .values.keydb.extravolumes }}
      {{- toyaml .values.keydb.extravolumes | nindent 6 }}
      {{- end }}
  {{- if .values.keydb.persistentvolume.enabled }}
  volumeclaimtemplates:
  - metadata:
      name: keydb-data
      annotations:
      {{- if .values.keydb.persistentvolume.annotations }}
        {{- toyaml .values.keydb.persistentvolume.annotations | nindent 8 }}
      {{- end }}
      labels:
    spec:
      accessmodes:
        {{- toyaml .values.keydb.persistentvolume.accessmodes | nindent 8 }}
      resources:
        requests:
          storage: {{ .values.keydb.persistentvolume.size }}
      {{- if .values.keydb.persistentvolume.storageclass }}
      {{- if (eq &quot;-&quot; .values.keydb.persistentvolume.storageclass) }}
      storageclassname: &quot;&quot;
      {{ else }}
      storageclassname: {{ .values.keydb.persistentvolume.storageclass }}
      {{- end }}
      {{- end }}
  {{- end }}
  {{- end }}
  volumes:
  {{- if $.values.cp.volume }}
  {{- range $.values.cp.volume}}
  - name: {{ .name }}
    hostpath:
      path: {{ .hostpath.path }}
      type: {{ .hostpath.type }}
  {{- end }}
  {{- end }}
  - name: shared-mem
    emptydir:
      medium: &quot;memory&quot;
  - name: podinfo
    downwardapi:
        items:
          - path: &quot;labels&quot;
            fieldref:
              fieldpath: metadata.labels
          {{- if $.values.cp.ha}}
              - path: &quot;uid&quot;
                fieldref:
                  fieldpath: metadata.uid
          {{- end }}
          - path: &quot;ns&quot;
            fieldref:
              fieldpath: metadata.namespace
          - path: &quot;annotations&quot;
            fieldref:
              fieldpath: metadata.annotations
  {{- if $.values.cp.ha}}                  
  - name: database
    persistentvolumeclaim:
      claimname: {{ $.values.cp.persistentvolumeclaim.claimname }}
  {{- end }}
---
{{ end }}
{{- end }}
{{- end }}

secret-util.yaml:
root@redis-master:~/xmeg/example# cat my-bing/templates/secret-utils.yaml
apiversion: v1
kind: secret
metadata:
{{/*
  name: keydb-utils
*/}}
  name: {{ include &quot;keydb.fullname&quot; $ }}-utils
  labels:
{{/*
    helm.sh/chart: keydb-0.22.0
    app.kubernetes.io/name: keydb
    app.kubernetes.io/instance: keydb
    app.kubernetes.io/version: &quot;6.0.16&quot;
    app.kubernetes.io/managed-by: helm
*/}}
    {{ include &quot;keydb.labels&quot; $ | nindent 4 }}

type: opaque
stringdata:
  server.sh: |
    #!/bin/bash
    set -euxo pipefail

    host=&quot;$(hostname)&quot;
    port=&quot;6379&quot;
    replicas=()
{{- if and ($.values.keydb.peerlbdetails.peerip) ($.values.keydb.peerlbdetails.peerport) }}
    replicas+=(&quot;--replicaof {{ .values.keydb.peerlbdetails.peerip }} {{ .values.keydb.peerlbdetails.peerport | int }}&quot;)
{{- end }}
    for node in {0..{{ (sub (.values.keydb.nodes | int) 1) }}}; do
{{/*
      if [ &quot;$host&quot; != &quot;keydb-${node}&quot; ]; then
          replicas+=(&quot;--replicaof keydb-${node}.keydb-headless ${port}&quot;)
*/}}
      if [ &quot;$host&quot; != &quot;{{ include &quot;keydb.fullname&quot; . }}-${node}&quot; ]; then
          replicas+=(&quot;--replicaof {{ include &quot;keydb.fullname&quot; . }}-${node}.{{ include &quot;keydb.fullname&quot; . }}-headless ${port}&quot;)
      fi
    done
    exec keydb-server /etc/keydb/redis.conf \
        --active-replica yes \
        --multi-master yes \
        --appendonly {{ .values.keydb.appendonly }} \
        --bind 0.0.0.0 \
        --port &quot;$port&quot; \
        --protected-mode no \
        --server-threads {{ .values.threads | int }} \
{{- if .values.keydb.existingsecret }}
        --masterauth $redis_password \
        --requirepass $redis_password \
{{- else if .values.keydb.password }}
        --masterauth {{ .values.keydb.password }} \
        --requirepass {{ .values.keydb.password }} \
{{- end }}
      {{- range $key, $value := .values.keydb.configextraargs }}
        {{- if $value }}
        --{{ $key }} {{ $value }} \
        {{- else }}
        --{{ $key }} \
        {{- end }}
      {{- end }}
        &quot;${replicas[@]}&quot;

description:
i am getting the above error, and not sure how to solve this. what exactly am i doing wrong??
appreciate your early response.
syed
",<kubernetes><kubernetes-helm>,68651689,3,"the go text/template range operator rebinds the . special variable, in this case to be the loop index.  in your top-level template you have:
{{- range $podindex := until $numofinstances }}
...
      labels:
        {{ include &quot;keydb.labels&quot; . | nindent 8 }}
...
{{- end }}

in this context . is the loop index, not the top-level helm object.  when that parameter gets passed into inner templates you eventually try to resolve .values.something, but since . is the loop index, you can't look up the values field on it.
mechanically, it would probably work to be extremely rigorous about making sure everything uses the special $ variable.  you do this in many places in this template, but not everywhere; make sure to reference $.values and not just .values, and to pass $ to templates instead of ..
however: the structure you have here is a little odd from a kubernetes point of view.  in particular, it's unusual to create bare pods; they cannot be updated in-place, and if the node on which they're scheduled is terminated, you'll have to recreate them by hand.  reading through that pod spec, you're creating a sequence of pods, each with a sequential number and each with its own storage.  this is exactly what a kubernetes statefulset provides you.
if you use a statefulset instead, you can get rid of the range loop, and use the &quot;ordinary&quot; .values and . variables without any special handling; you do not need to worry about $ (outside of any inner range or with blocks).
{{/* no outer range loop */}}
apiversion: apps/v1
kind: statefulset
metadata: { ... }
spec:
  replicas: {{ .values.cp.replicas }}
  ...

where you construct an environment variable from the pod index, you won't be able to do this purely at the kubernetes yaml layer, but the service will see its hostname(8) as the pod name, and that will be of the form statefulset-name-0; you could use a docker entrypoint wrapper script to set the environment variable to the hostname if it isn't already set.
"
68763115,kubernetes gke ingress-nginx loki grafana prometheus setup issues,"i am trying to get a new cluster setup on google kubernetes engine aka gke.
i am using helm and added to the repo ingress-nginx https://kubernetes.github.io/ingress-nginx and grafana https://grafana.github.io/helm-charts
i was following this guide but i was not able to access an internal ip or get the ingress working to where i could see the dashboard. i have then tried this guide using their helm deployment for loki stack (loki, promtail, grafana, prometheus) with persistent volume claim.
that combined with opening port 8443 up and i was still getting a 503 error. i made this ingress:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-resource
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
spec:
  rules:
    - http:
        paths:
          - path: /pro
            pathtype: prefix
            backend:
              service:
                name: loki-prometheus-server
                port:
                  number: 9090
          - path: /graf
            pathtype: prefix
            backend:
              service:
                name: loki-grafana
                port:
                  number: 3000


i tried many different ports including 80 but it was a no go.
loki-prometheus-server service:
name:              loki-prometheus-server
namespace:         ingress-nginx
labels:            app=prometheus
                   app.kubernetes.io/managed-by=helm
                   chart=prometheus-11.16.9
                   component=server
                   heritage=helm
                   release=loki
annotations:       cloud.google.com/neg: {&quot;ingress&quot;:true}
                   meta.helm.sh/release-name: loki
                   meta.helm.sh/release-namespace: ingress-nginx
selector:          app=prometheus,component=server,release=loki
type:              clusterip
ip families:       &lt;none&gt;
ip:                10.***
ips:               10.***
port:              http  80/tcp
targetport:        9090/tcp
endpoints:         10.***:9090
session affinity:  none
events:            &lt;none&gt;

loki-grafana service:
name:              loki-grafana
namespace:         ingress-nginx
labels:            app.kubernetes.io/instance=loki
                   app.kubernetes.io/managed-by=helm
                   app.kubernetes.io/name=grafana
                   app.kubernetes.io/version=7.5.0
                   helm.sh/chart=grafana-5.7.10
annotations:       cloud.google.com/neg: {&quot;ingress&quot;:true}
                   meta.helm.sh/release-name: loki
                   meta.helm.sh/release-namespace: ingress-nginx
selector:          app.kubernetes.io/instance=loki,app.kubernetes.io/name=grafana
type:              clusterip
ip families:       &lt;none&gt;
ip:                10.***
ips:               10.***
port:              service  80/tcp
targetport:        3000/tcp
endpoints:         10.***:3000
session affinity:  none
events:            &lt;none&gt;

step by step:
create new cluster on gcloud:

gke standard
default-pool-&gt;nodes-&gt; machine type: e2-small
default-pool-&gt;security-&gt; allow full access to all cloud apis selected create
create.

connect to console or cloud shell
then:
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update

kubectl create namespace ingress-nginx
helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress-nginx

everything seems normal so far.
go to vpc network-&gt; firewall -&gt; find the one with 80,443 in protocols/ports
then add 8443 to it.
install loki:
helm upgrade --install loki grafana/loki-stack --namespace ingress-nginx  --set grafana.enabled=true,prometheus.enabled=true,prometheus.alertmanager.persistentvolume.enabled=false,prometheus.server.persistentvolume.enabled=false,loki.persistence.enabled=true,loki.persistence.storageclassname=standard,loki.persistence.size=5gi

everything still seems to be going fine but if i go to my external load balancer i get 404 not found / nginx
i add this ingress:
using kubectl apply -f ingress.yml -n ingress-nginx
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-resource
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    prometheus.io/scrape: &quot;true&quot;
spec:
  rules:
    - http:
        paths:
          - path: /pro
            pathtype: prefix
            backend:
              service:
                name: loki-prometheus-server
                port:
                  number: 80
          - path: /graf
            pathtype: prefix
            backend:
              service:
                name: loki-grafana
                port:
                  number: 80

and get a 404 page not found in text on /pro
and get redirected to /login with a 404 not found nginx page at /graf
what am i doing wrong?
",<nginx><kubernetes><google-kubernetes-engine><prometheus><kubernetes-ingress>,68765360,3,"running:
helm upgrade ingress-nginx ingress-nginx/ingress-nginx --namespace ingress-nginx  --set controller.metrics.enabled=true --set-string controller.podannotations.&quot;prometheus\.io/scrape&quot;=&quot;true&quot; --set-string controller.podannotations.&quot;prometheus\.io/port&quot;=&quot;10254&quot;

and using this ingress i was able to get it working:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-resource
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    prometheus.io/port: &quot;10254&quot;
    prometheus.io/scrape: &quot;true&quot;
spec:
  rules:
    - http:
        paths:
          - path: /?(.*)
            pathtype: prefix
            backend:
              service:
                name: loki-grafana
                port:
                  number: 80


"
68891357,patch kubernetes cronjob with kustomize,"i am trying to patch a cronjob, but somehow it doesn't work as i would expect. i use the same folder structure for a deployment and that works.
this is the folder structure:
.
 base
    kustomization.yaml
    war.cron.yaml
 overlays
     staging
         kustomization.yaml
         war.cron.patch.yaml
         war.cron.staging.env

base/kustomization.yaml
---
kind: kustomization
resources:
- war.cron.yaml

base/war.cron.yaml
---
apiversion: batch/v1
kind: cronjob
metadata:
  name: war-event-cron
spec:
  schedule: &quot;*/5 * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          containers:
          - name: war-event-cron
            image: my-registry/war-service
            imagepullpolicy: ifnotpresent
            command:
            - python
            - run.py
            args:
            - sync-events
            envfrom:
            - secretref:
                name: war-event-cron-secret
          restartpolicy: onfailure

then i am trying to patch this in the staging overlay.
overlays/staging/kustomization.yaml
---
kind: kustomization
namespace: staging
bases:
- &quot;../../base&quot;
patchesstrategicmerge:
- war.cron.patch.yaml
secretgenerator:
- name: war-event-cron-secret
  behavior: create
  envs:
  - war.cron.staging.env

overlays/staging/war.cron.patch.yaml
---
apiversion: batch/v1
kind: cronjob
metadata:
  name: war-event-cron
spec:
  jobtemplate:
    spec:
      template:
        spec:
          containers:
          - name: war-event-cron
            image: my-registry/war-service:nightly
            args:
            - sync-events
            - --debug

but the result of kustomize build overlays/staging/ is not what i want. the command is gone and the secret is not referenced.
apiversion: v1
data:
  ...
kind: secret
metadata:
  name: war-event-cron-secret-d8m6bh7284
  namespace: staging
type: opaque
---
apiversion: batch/v1
kind: cronjob
metadata:
  name: war-event-cron
  namespace: staging
spec:
  jobtemplate:
    spec:
      template:
        spec:
          containers:
          - args:
            - sync-events
            - --debug
            image: my-registry/war-service:nightly
            name: war-event-cron
          restartpolicy: onfailure
  schedule: '*/5 * * * *'

",<kubernetes><kubernetes-cronjob><kustomize>,68906977,3,"it's known bug in kustomize - check and follow this topic (created ~ one month ago) on github for more information.
for now, fix for your issue is to use apiversion:batch/v1beta1 instead of apiversion: batch/v1 in base/war.cron.yaml and overlays/staging/war.cron.patch.yaml files.
"
68366456,mongodb community kubernetes operator and custom persistent volumes,"i'm trying to deploy a mongodb replica set by using the mongodb community kubernetes operator in minikube.
i followed the instructions on the official github, so:

install the crd
install the necessary roles and role-bindings
install the operator deploy the replicaset

by default, the operator will creates three pods, each of them automatically linked to a new persistent volume claim bounded to a new persistent volume also created by the operator (so far so good).
however, i would like the data to be saved in a specific volume, mounted in a specific host path. so in order i would need to create three persistent volumes, each mounted to a specific host path, and then automatically i would want to configure the replicaset so that each pod would connect to its respective persistent volume (perhaps using the matchlabels selector).
so i created three volumes by applying the following file:
apiversion: v1
kind: persistentvolume
metadata:
  name: mongodb-pv-00
  namespace: $namespace
  labels: 
    type: local
    service: mongo
spec:
  storageclassname: manual
  capacity:
    storage: 5gi
  accessmodes:
  - readwriteonce
  hostpath: 
    path: &quot;/mnt/mongodata/00&quot;
---
apiversion: v1
kind: persistentvolume
metadata:
  name: mongodb-pv-01
  namespace: $namespace
  labels: 
    type: local
    service: mongo
spec:
  storageclassname: manual
  capacity:
    storage: 5gi
  accessmodes:
  - readwriteonce
  hostpath: 
    path: &quot;/mnt/mongodata/01&quot;
---
apiversion: v1
kind: persistentvolume
metadata:
  name: mongodb-pv-02
  namespace: $namespace
  labels: 
    type: local
    service: mongo
spec:
  storageclassname: manual
  capacity:
    storage: 5gi
  accessmodes:
  - readwriteonce
  hostpath: 
    path: &quot;/mnt/mongodata/02&quot;

and then i set up the replica set configuration file in the following way, but it still fails to connect the pods to the volumes:
apiversion: mongodbcommunity.mongodb.com/v1
kind: mongodbcommunity
metadata:
  name: mongo-rs
  namespace: $namespace
spec:
  members: 3
  type: replicaset
  version: &quot;4.4.0&quot;
  persistent: true
  podspec:
    persistence:
      single: 
        labelselector: 
          matchlabels:
            type: local
            service: mongo
        storage: 5gi
        storageclass: manual
  statefulset:
    spec:
      volumeclaimtemplates:
        - metadata:
            name: data-volume
          spec:
            accessmodes: [ &quot;readwriteonce&quot;, &quot;readwritemany&quot; ]
            resources:
              requests:
                storage: 5gi
            selector:
              matchlabels:
                type: local
                service: mongo
            storageclassname: manual
  security:
    authentication:
      modes: [&quot;scram&quot;]
  users:
    - ...
  additionalmongodconfig:
    storage.wiredtiger.engineconfig.journalcompressor: zlib

i can't find any documentation online, except the mongodb.com_v1_custom_volume_cr.yaml, has anyone faced this problem before? how could i make it work?
",<mongodb><kubernetes><kubernetes-operator>,68367114,3,"i think you could be interested into using local type of volumes. it works, like this:
first, you create a storage class for the local volumes. something like the following:
apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumebindingmode: waitforfirstconsumer

since it has no-provisioner, it will be usable only if you manually create local pvs. waitforfirstconsumer instead, will prevent attaching a pv to a pvc of a pod which cannot be scheduled on the host on which the pv is available.
second, you create the local pvs. similarly to how you created them in your example, something like this:
apiversion: v1
kind: persistentvolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 5gi
  volumemode: filesystem
  accessmodes:
  - readwriteonce
  persistentvolumereclaimpolicy: retain
  storageclassname: local-storage
  local:
    path: /path/on/the/host
  nodeaffinity:
    required:
      nodeselectorterms:
      - matchexpressions:
        - key: kubernetes.io/hostname
          operator: in
          values:
          - the-node-hostname-on-which-the-storage-is-located

notice the definition, it tells the path on the host, the capacity.. and then it explains on which node of the cluster, such pv can be used (with the nodeaffinity). it also link them to the storage class we created early.. so that if someone (a claim template) requires storage with that class, it will now find this pv.
you can create 3 pvs, on 3 different nodes.. or 3 pvs on the same node at different paths, you can organize things as you desire.
third, you can now use the local-storage class in claim template. the claim template could be something similar to this:
volumeclaimtemplates:
  - metadata:
      name: the-name-of-the-pvc
    spec:
      accessmodes: [ &quot;readwriteonce&quot; ]
      storageclassname: &quot;local-storage&quot;
      resources:
        requests:
          storage: 5gi

and each pod of the statefulset will try to be scheduled on a node with a local-storage pv available.

remember that with local storages or, in general, with volumes that utilize host paths.. you may want to spread the various pods of your app on different nodes, so that the app may resist the failure of a single node on its own.

in case you want to be able to decide which pod links to which volume, the easiest way is to create one pv at a time, then wait for the pod to bound with it.. before creating the next one. it's not optimal but it's the easiest way.
"
37879451,create a kubernetes namespace using config instead of api,"the only way to create a new namespace according to documentation here is to make an api request

curl -v -x post -d'{""apiversion"":""v1"",""kind"":""namespace"",""metadata"":{""name"":""kube-system""}}' -h ""content-type: application/json"" ""http://127.0.0.1:8080/api/v1/namespaces""


is there any way to do the same using config or cloud-config?
",<docker><kubernetes><kubectl>,37883512,2,"as timo mentioned, you can use kubectl create namespace name to create a namespace using the command line client. you can also put the following into a yaml file and use kubectl create -f namespace.yaml to create a namespace:

apiversion: v1
kind: namespace
metadata:
  name: kube-system

"
59252103,kubernetes 1.16 nginx ingress (0.26.1) tcp mariadb/mysql service not working,"i want to expose my mariadb pod using nginx ingress tcp service by following this step https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/.  mariadb running in default name space, with mariadb service type as clusterip.  i am running nginx ingress controller in nginx-ingress namespace, also defined tcp-services cofigmap for mariadb service.  but i am unable to connect mariadb database from outside of the cluster.

from nginx controller log i can see its reading tcp-services.

ingress configuration

containers:
      - args:
        - /nginx-ingress-controller
        - --default-backend-service=nginx-ingress/nginx-ingress-default-backend
        - --election-id=ingress-controller-leader
        - --ingress-class=nginx
        - --configmap=nginx-ingress/nginx-ingress-controller
        - --default-ssl-certificate=nginx-ingress/ingress-tls
        - --tcp-services-configmap=nginx-ingress/tcp-services
        - --udp-services-configmap=nginx-ingress/udp-services


configmap:

apiversion: v1
kind: configmap
metadata:
  name: tcp-services
  namespace: nginx-ingress
data:
  3306: ""default/mariadb:3306""


ingress controller nginx config for tcp service

 # tcp services

        server {
                preread_by_lua_block {
                        ngx.var.proxy_upstream_name=""tcp-default-mariadb-3306"";
                }

                listen                  3306;

                proxy_timeout           600s;
                proxy_pass              upstream_balancer;

        }


when i connect from external server, getting this message:

error 2002 (hy000): can't connect to mysql server on 


any tips to troubleshoot this issue?

thanks

i was missing my service with tcp port info, after adding it i was able to access the mysql with my service port number. thanks for emanuel bennici pointing this one out.

here is my service:

apiversion: v1
kind: service
metadata:
  name: nginx-ingress-controller  
spec:
  externaltrafficpolicy: cluster
  ports:
  - name: http
    port: 80
    protocol: tcp
    targetport: http
  - name: https
    port: 443
    protocol: tcp
    targetport: https
  - name: 3066-tcp
    port: 3066
    protocol: tcp
    targetport: 3066-tcp
  selector:
    app: nginx-ingress
    component: controller
    release: nginx-ingress
  sessionaffinity: none
  type: nodeport

",<kubernetes><kubernetes-ingress><nginx-ingress>,59388268,2,"please check if you have opened the mysql port in the pod 
so to open the port on the kubernetes-port you have to create a pod like this:

apiversion: v1
kind: pod
metadata:
  name: mysql
  namespace: default
  labels:
    name: mysql
spec:
  containers:
  - name: mysql
    image: docker.io/bitnami/mariadb:10.3.20-debian-9-r19
    ports:
    - containerport: 3306
      protocol: tcp


then you have to create a service so you can talk directly to the mysql pod through the service:

apiversion: v1
kind: service
metadata:
  name: svc-mysql
  namespace: default
  labels:
    run: mysql
spec:
  ports:
  - port: 3306
    targetport: 3306
    protocol: tcp
  selector:
    name: mysql


if the nginx ingress controller is working correctly you can now add the following line to your tcp-services-configmap:

3306: ""default/svc-mysql:3306""


please note that you have to add the mysql port to the nginx ingress service, like this:

apiversion: v1
kind: service
metadata:
  name: ingress-nginx
  namespace: nginx-ingress
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  type: loadbalancer
  ports:
    - name: http
      port: 80
      targetport: 80
      protocol: tcp
    - name: https
      port: 443
      targetport: 443
      protocol: tcp
    - name: proxie-tcp-mysql
      port: 3306
      targetport: 3306
      protocol: tcp
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx


now you can use the external ip of the nginx ingress controller to connect to your mysql server.



please provide more information about your setup in future questions :)
"
78104729,expose a service in a kubernetes cluster to the outside using ingress,"i need to expose a service to the outside using ingress on an ec2 instance. i have three services running in the kubernetes cluster, one of them must be accessible from outside, the other two only communicate internally with the first. to expose the service i am trying to use ingress, but i am a little confused with the necessary configurations and the way ingress works.
what i have tried:
deployment.yaml
### deployments ###


apiversion: apps/v1
kind: deployment
metadata:
  name: agify-deployment
  labels:
    app: agify
spec:
  replicas: 1
  selector:
    matchlabels:
      app: agify
  template:
    metadata:
      labels:
        app: agify
    spec:
      containers:
      - name: agify
        image: myrepo/svc_agify:v1
        ports:
        - containerport: 9010

---


apiversion: apps/v1
kind: deployment
metadata:
  name: genderize-deployment
  labels:
    app: genderize
spec:
  replicas: 1
  selector:
    matchlabels:
      app: genderize
  template:
    metadata:
      labels:
        app: genderize
    spec:
      containers:
      - name: genderize
        image: myrepo/svc_genderize:v1
        ports:
        - containerport: 9020

---


apiversion: apps/v1
kind: deployment
metadata:
  name: core-deployment
  labels:
    app: core
spec:
  replicas: 1
  selector:
    matchlabels:
      app: core
  template:
    metadata:
      labels:
        app: core
    spec:
      containers:
      - name: core
        image: myrepo/svc_core:v1
        ports:
        - containerport: 9030

---

### services ###

apiversion: v1
kind: service
metadata:
  name: agify-svc
spec:
  selector:
    app: agify
  ports:
    - protocol: tcp
      port: 80
      targetport: 9010

---

apiversion: v1
kind: service
metadata:
  name: genderize-svc
spec:
  selector:
    app: genderize
  ports:
    - protocol: tcp
      port: 80
      targetport: 9020

---

# core service

apiversion: v1
kind: service
metadata:
  name: core-svc
spec:
  selector:
    app: core
  ports:
    - protocol: tcp
      port: 80
      targetport: 9030
  type: nodeport

---

# ingress

apiversion: networking.k8s.io/v1
kind: ingress
metadata:
   name: ingress-core
   annotations:
     nginx.ingress.kubernetes.io/rewrite-target: /
spec:
   rules:
   - host: my-service.com
     http:
       paths:
       - path: /
         pathtype: prefix
         backend:
           service:
             name: core-svc
             port:
               number: 80

in my /etc/hosts file on the ec2 instance i have placed the following so that my-service.com resolves to the minikube ip address.
xxx.xxx.xxx.xxx my-service.com


if i try it within the ec2 instance where i have the cluster, it works perfectly. but how do i consume the service from outside the ec2 instance? what is lacking?
to clarify, i already have the inbound and outbound rules configured on my ec2 instance.
i found that i need to add the following configuration to the ingress resource, but the configuration seems to be invalid
spec:
    ingressclassname: nginx
    loadbalancerip: xx.xx.xx.xx # ec2 instance public ip


",<kubernetes><kubernetes-ingress><kubectl><minikube><nginx-ingress>,78127359,2,"to clarify your question if you are using ingress to expose the service you don't need to specify nodeport this will be security violation. you should use only the cluster ip. if you are not providing the service type default it will take cluterip. to answer your question yes there is no ingressclassname has been specified in the ingress i have updated your ingress yaml
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-core
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
 ingressclassname: nginx
 rules:
  - host: my-service.com
    http:
     paths:
     - path: /
       pathtype: prefix
       backend:
        service:
         name: core-svc
         port:
           number: 80

make sure to do the proper indentation it works for me.
"
78043239,how helm2 uses the semvercompare function similar to helm3,"i have a project deployed using helm2, but with the upgrade of the k8s version, the apiversion of rbac has changed. however, not all k8s clusters are upgraded together, so they need to be compatible with different versions.
i searched for information and found that i can use helm's semvercompare function.
i modified the apiversion of the role resource through the reference.
{{- if semvercompare &quot;&gt;=1.22-0&quot; .capabilities.kubeversion.gitversion }}
apiversion: rbac.authorization.k8s.io/v1
{{- else }}
apiversion: rbac.authorization.k8s.io/v1beta1
{{- end }}
kind: role
...
...

but semvercompare seems to be a function of helm3 semvercompare.  i didn't find any description of this function in helm2.
i would like to ask if there is a function similar to semvercompare in helm2, or to implement the function of semvercompare in other ways. looking forward to your reply.
",<kubernetes><kubernetes-helm>,78044830,2,"the listing of template functions in the helm documentation is somewhat recent.  older versions of the documentation contain text like this:

helm has over 60 available functions. some of them are defined by the go template language itself. most of the others are part of the sprig template library. well see many of them as we progress through the examples.

and, indeed, the semantic version functions are part of sprig.
the next question is, does this very old version of helm contain these functions?  the helm 2.17 source references sprig 2.20.0.  that version of sprig does seem to contain the semver functions.  so you're probably good to use these, even on a 4-year-old unmaintained version of the helm tool.
helm (including this old version) also has a .capabilities object which lets you check what api versions the cluster supports.  rather than gating on the kubernetes version, it may be easier to just ask if the newer api version exists
{{- if .capabilities.apiversions.has &quot;rbac.authorization.k8s.io/v1&quot; }}
apiversion: rbac.authorization.k8s.io/v1
{{- else }}
apiversion: rbac.authorization.k8s.io/v1beta1
{{- end }}
kind: role

"
78050661,deploy elasticsearch with url and open port,"i use this yml file to deploy elasticsearch on kubernetes:
apiversion: apps/v1
kind: deployment
metadata:
  name: elasticsearch
  labels:
    app: elasticsearch
spec:
  selector:
    matchlabels:
      app: elasticsearch
  replicas: 1
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: elasticsearch:5.6.16
        resources:
          requests:
            memory: 2gi
          limits:
            memory: 4gi
        ports:
        - containerport: 9200
        - containerport: 9300
        env:
        - name: discovery.type
          value: single-node
        - name: cluster.name
          value: elasticsearch
        - name: node.name
          value: node-1

how i can set permanent internal url which can be used to access the elasticsearch pod from another pod.
i can make this successfully this request on port 9200:
 curl -x get 10.233.75.8:9200
{
  &quot;name&quot; : &quot;uzspw5e&quot;,
  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,
  &quot;cluster_uuid&quot; : &quot;ib59izzdrtypcaz3nhbcgw&quot;,
  &quot;version&quot; : {
    &quot;number&quot; : &quot;5.6.16&quot;,
    &quot;build_hash&quot; : &quot;3a740d1&quot;,
    &quot;build_date&quot; : &quot;2019-03-13t15:33:36.565z&quot;,
    &quot;build_snapshot&quot; : false,
    &quot;lucene_version&quot; : &quot;6.6.1&quot;
  },
  &quot;tagline&quot; : &quot;you know, for search&quot;
}

but port 9300 is not working:
curl -x get 10.233.75.8:9300
curl: (7) failed to connect to 10.233.75.8 port 9300: connection refused

do you know how i can open this port?
",<elasticsearch><kubernetes><kubernetes-deployment>,78051044,2,"if you need to access the elasticsearch service from another pod, consider creating a kubernetes service object that targets your elasticsearch deployment. this will provide a stable endpoint (the services clusterip) that other pods can use to access the elasticsearch api on port 9200.
heres an example of how you might define such a service:
apiversion: v1
kind: service
metadata:
  name: elasticsearch
  labels:
    app: elasticsearch
spec:
  selector:
    app: elasticsearch
  ports:
    - protocol: tcp
      port: 9200
      targetport: 9200

with this service in place, other pods should be able to access the elasticsearch api via http://elasticsearch:9200.
"
60299846,serving http/https service which is outside of kubernetes cluster through ingress,"my aim is to route local http service that is not in kubernetes through kubernetes ingress.

the configuration below works so i'm able to open http://owncloud.example.com or https://owncloud.example.com from outside.

here is kubernetes configuration:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: owncloud
  namespace: default
  annotations:
    kubernetes.io/ingress.class: nginx
    ingress.kubernetes.io/ssl-redirect: ""true"" 
    ingress.kubernetes.io/secure-backends: ""true""    
    ingress.kubernetes.io/force-ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/server-snippet:
      location ~ ^/(.*) {
        proxy_pass http://192.168.250.100:8260/$1;
        proxy_set_header host $host;
      }
      location ~ ^/api(.*) {
        proxy_pass http://192.168.250.100:8261/$1;
        proxy_set_header host $host;
      }
spec:
  tls:
  - hosts:
    - owncloud.example.com
    secretname: owncloud-tls

  rules:
  - host: owncloud.example.com


the issue is that i see some strange errors in browser's javascript console related to ""meta"". they are related with deep javascript code. so unfortunately, there is no useful log. the website produces weird behaviour at few places while locally it works fine.
so it seems this is something to do with kubernetes ingress.

previously i used plain nginx connected to outside and this worked great:

location / {
  proxy_pass http://192.168.250.100:8260/
}


if i add exactly the same block to server-snippet, website doesn't load at all. it catches default ingress.

how to properly proxy_pass traffic from kubernetes ingress to another service which is running outside of kubernetes? so it doesn't miss something through proxy.

would be nice to have exploration on server-snippet to understand how kubernetes ingress configuration is different from standard nginx usage. 

if using different options, i was not able to find a solution to proxy_pass to different http when accessing path /api.

----------------- updates -----------------

i have collected all issues for comparison.

locally - working one:



if i click on manifest.json, it shows ""nothing to preview"". if i use wget to download that json, i can see &lt;!doctype html&gt; in this first line. it's html file downloaded. but i can confirm this local version is working perfectly for years. so this screenshot is just to know how it looks when it works.

through ingress - not working:

i logged in successfully. didn't spot anything weird from user experience, but issue exists:



tried to log out. i'm not able to do it. it throws owncloud specific error ""access forbidden
csrf check failed"" and on the console i see this:



if i go to https://owncloud.example.com/login page on purpose:

 

if i try to access files on this owncloud, it also fails with 400:



if i add additional annotations:

  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/server-snippet: |
      location ~ ^/?(.*) {
        proxy_pass http://192.168.250.100:8260/$1;
        proxy_set_header host $host;
        proxy_set_header x-real-ip $remote_addr;
        proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;
        proxy_set_header x-forwarded-proto https;
      }
    # owncloud tuning
    nginx.ingress.kubernetes.io/proxy-body-size: ""500000m""
    nginx.ingress.kubernetes.io/proxy-max-temp-file-size: ""500000m""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""36000s""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""36000s""
    nginx.ingress.kubernetes.io/proxy-connect-timeout: ""36000s""
    nginx.ingress.kubernetes.io/proxy-buffering: ""off""
    nginx.ingress.kubernetes.io/proxy-redirect-from: ""off""
    nginx.ingress.kubernetes.io/connection-proxy-header: ""keep-alive""




summarise

no errors on application side. so first thing that comes to my mind is /logout behaviour. i get 412 http code which means: precondition failed client error response code indicates that access to the target resource has been denied and 400 bad request error.

any expertise to catch this issue?

many thanks
",<nginx><kubernetes><kubernetes-ingress><nginx-ingress>,60330754,2,"finally found a working solution.

i just corrected location and proxy_pass to solve the root cause.

so if you have some local http service which is outside of kubernetes cluster and you want to serve this through ingress, you just need this:

kind: ingress
apiversion: extensions/v1beta1
metadata:
  name: owncloud
  namespace: default
  annotations:
    kubernetes.io/ingress.class: nginx
    ingress.kubernetes.io/ssl-redirect: ""true""
    ingress.kubernetes.io/secure-backends: ""true""
    ingress.kubernetes.io/force-ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/server-snippet: |
      location ~ ""^/(.*)"" {
        proxy_pass http://192.168.250.100:8260;
        proxy_set_header host $host;
        proxy_set_header x-forwarded-proto https;

        # owncloud tuning
        proxy_max_temp_file_size 0;
        client_max_body_size 500000m;
        proxy_read_timeout 36000s;
        proxy_send_timeout 36000s;
        proxy_connect_timeout 36000s;
        proxy_buffering off;
        proxy_redirect off;
        proxy_set_header connection ""keep-alive"";
      }
    # owncloud tuning
    nginx.ingress.kubernetes.io/proxy-max-temp-file-size: ""0""
    nginx.ingress.kubernetes.io/proxy-body-size: ""500000m""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""36000s""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""36000s""
    nginx.ingress.kubernetes.io/proxy-connect-timeout: ""36000s""
    nginx.ingress.kubernetes.io/proxy-buffering: ""off""
    nginx.ingress.kubernetes.io/proxy-redirect-from: ""off""
    nginx.ingress.kubernetes.io/connection-proxy-header: ""keep-alive""
spec:
  rules:
  - host: owncloud.example.com
  tls:
  - hosts:
    - owncloud.example.com
    secretname: owncloud-example-tls



remove owncloud tuning block if you have another service
remove ssl, secure, x-forwarded-proto and tls: bits if you don't need https


you can add more location blocks such as ~ ""^/api/(.*)"" so it works as normal nginx.

in my case it was useful to route some local docker compose and old fashion services to outside through kubernetes ingress.

p.s. don't forget to vote for @mwatney comment if you came here to solve owncloud csrf error.
"
42694121,how can i configure path for two services in kubernetes ingress?,"i've the following ingress configuration but when i call www.domain.com/api it always open my ui service instead of api and the same thing happens if i call something else after api, for example www.domain.com/api/v1/projects.

how can i fix that?

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress
spec:
  tls:
  - secretname: tls
  backend:
    servicename: ui
    serviceport: 5003
  rules:
  - host: www.domain.com
    http:
      paths:
      - path: /
        backend:
          servicename: ui
          serviceport: 5003
      - path: /api
        backend:
          servicename: api
          serviceport: 5000

",<kubernetes><google-cloud-platform><kubectl>,42710004,2,"here is the way i fixed this problem. i hope this can help others.

thanks @aleks!!

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: kronus
spec:
  tls:
  - secretname: tls
  backend:
    servicename: ui
    serviceport: 5003
  rules:
  - host: domain.com
    http:
      paths:
      - path: /api
        backend:
          servicename: api
          serviceport: 5000
      - path: /api/*
        backend:
          servicename: api
          serviceport: 5000
  - host: www.domain.com
    http:
      paths:
      - path: /api
        backend:
          servicename: api
          serviceport: 5000
      - path: /api/*
        backend:
          servicename: api
          serviceport: 5000

"
75265056,kubernetes: set environment variables from file?,"my kubernetes deployment has an initcontainer which fetches a token from a url. my app container (3rd party) then needs that token as an environment variable.
a possible approach would be: the initcontainer creates a kubernetes secret with the token value; the app container uses the secret as an environment variable via env[].valuefrom.secretkeyref.
creating the secret from the initcontainer requires accessing the kubernetes api from a pod though, which tends to be a tad cumbersome. for example, directly accessing the rest api requires granting proper permissions to the pod's service account; otherwise, creating the secret will fail with
secrets is forbidden: user \&quot;system:serviceaccount:default:default\&quot; 
cannot create resource \&quot;secrets\&quot; in api group \&quot;\&quot; in the namespace \&quot;default\&quot;

so i was wondering, isn't there any way to just write the token to a file on an emptydir volume...something like this:
apiversion: apps/v1
kind: deployment
metadata:
  name: my-deployment
  labels:
    app: my-app
spec:
  selector:
    matchlabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      initcontainers:
        - name: fetch-auth-token
          image: curlimages/curl
          command:
            - /bin/sh
          args:
            - -c
            - |
              echo &quot;fetching token...&quot;
              url=https://gist.githubusercontent.com/maxhorstmann/a99823d5aff66fe2ad4e7a4e2a2ee96b/raw/662c19aa96695e52384337bdbd761056bb324e72/token
              curl $url &gt; /auth-token/token
          volumemounts:
            - mountpath: /auth-token
              name: auth-token
...
      volumes:
        - name: auth-token
          emptydir: {}

... and then somehow use that file to populate an environment variable in the app container, similar to env[].valuefrom.secretkeyref, along the lines of:
      containers:
        - name: my-actual-app
          image: thirdpartyappimage
          env:
            - name: token
              valuefrom:
                fileref:
                  path: /auth-token/token
            # ^^^^ this does not exist
          volumemounts:
            - mountpath: /auth-token
              name: auth-token

unfortunately, there's no env[].valuefrom.fileref.
i considered overwriting the app container's command with a shell script which loads the environment variable from the file before launching the main command; however, the container image doesn't even contain a shell.
is there any way to set the environment variable in the app container from a file?
",<kubernetes><kubernetes-apiserver>,75265250,2,"
creating the secret from the initcontainer requires accessing the kubernetes api from a pod though, which tends to be a tad cumbersome...

it's not actually all that bad; you only need to add a serviceaccount, role, and rolebinding to your deployment manifests.
the serviceaccount manifest is minimal, and you only need it if you don't want to grant permissions to the default service account in your namespace:
apiversion: v1
kind: serviceaccount
metadata:
  name: secretmaker

then your role grants access to secrets (we need create and delete permissions, and having get and list is handy for debugging):
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  labels:
    app: env-example
  name: secretmaker
rules:
- apigroups:
  - &quot;&quot;
  resources:
  - secrets
  verbs:
  - create
  - get
  - delete
  - list

a rolebinding connects the serviceaccount to the role:
apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  labels:
    app: env-example
  name: secretmaker
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: secretmaker
subjects:
- kind: serviceaccount
  name: secretmaker
  namespace: default

and with those permissions in place, the deployment is relatively simple:
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: env-example
  name: env-example
  namespace: env-example
spec:
  selector:
    matchlabels:
      app: env-example
  template:
    metadata:
      labels:
        app: env-example
    spec:
      serviceaccountname: secretmaker
      initcontainers:
      - command:
        - /bin/sh
        - -c
        - |
          echo &quot;fetching token...&quot;
          url=https://gist.githubusercontent.com/maxhorstmann/a99823d5aff66fe2ad4e7a4e2a2ee96b/raw/662c19aa96695e52384337bdbd761056bb324e72/token
          curl $url -o /tmp/authtoken
          kubectl delete secret authtoken &gt; /dev/null 2&gt;&amp;1
          kubectl create secret generic authtoken --from-file=auth_token=/tmp/authtoken
        image: docker.io/alpine/k8s:1.25.6
        name: create-auth-token
      containers:
      - name: my-actual-app
        image: docker.io/alpine/k8s:1.25.6
        command:
        - sleep
        - inf
        envfrom:
        - secretref:
            name: authtoken

the application container here is a no-op that runs sleep inf; that gives you the opportunity to inspect the environment by running:
kubectl exec -it deployment/env-example -- env

look for the auth_token variable created by our initcontainer.

all the manifests mentioned here can be found in this repository.
"
60143576,how to configure nginx deployment to pass traffic to front end deployment in google kubernetes engine?,"new to gke and kubernetes just trying to get a simple project up and running. here's what i'm trying to accomplish in gke in a single cluster, single node pool, and single namespace:

nginx deployment behind loadbalancer service accepting http traffic on port 80 passing it on port 8000 to

front-end deployment (python django) behind clusterip service accepting traffic on port 8000. 

the front-end is already successfully communicating with a statefulset running postgres database. the front-end was seen successfully serving http (gunicorn) before i switched it's service from loadbalancer to clusterip.

i don't know how to properly set up the nginx configuration to pass traffic to the clusterip service for the front-end deployment. what i have is not working.

any advice/suggestions would be appreciated. here are the setup files:

nginx - etc/nginx/conf.d/nginx.conf

upstream front-end {
    server front-end:8000;
}

server {

    listen 80;
    client_max_body_size 2m;

    location / {
        proxy_pass http://front-end;
        proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;
        proxy_set_header host $host;
        proxy_redirect off;
    }

    location /static/ {
        alias /usr/src/app/static/;
    }

}


nginx deployment/service

---
apiversion: v1
kind: service
metadata:
  name: ""web-nginx""
  labels:
    app: ""nginx""
spec:
  type: ""loadbalancer""
  ports:
  - port: 80
    name: ""web""
  selector:
    app: ""nginx""
---
apiversion: ""apps/v1""
kind: ""deployment""
metadata:
  name: ""nginx""
  namespace: ""default""
  labels:
    app: ""nginx""
spec:
  replicas: 1
  selector:
    matchlabels:
      app: ""nginx""
  template:
    metadata:
      labels:
        app: ""nginx""
    spec:
      containers:
      - name: ""my-nginx""
        image: ""us.gcr.io/my_repo/my_nginx_image""  # this is nginx:alpine + my staicfiles &amp; nginx.conf
        ports:
        - containerport: 80
        args:
        - /bin/sh 
        - -c
        - while :; do sleep 6h &amp; wait $${!}; nginx -s reload; done &amp; nginx -g ""daemon off;""


front-end deployment/service

---
apiversion: v1
kind: service
metadata:
  name: ""front-end""
  labels:
    app: ""front-end""
spec:
  type: ""clusterip""
  ports:
  - port: 8000
    name: ""django""
    targetport: 8000
  selector:
    app: ""front-end""
---
apiversion: ""apps/v1""
kind: ""deployment""
metadata:
  name: ""front-end""
  namespace: ""default""
  labels:
    app: ""front-end""
spec:
  replicas: 1
  selector:
    matchlabels:
      app: ""front-end""
  template:
    metadata:
      labels:
        app: ""front-end""
    spec:
      containers:
      - name: ""myapp""
        image: ""us.gcr.io/my_repo/myapp""
        ports:
        - containerport: 8000
        args:
          - /bin/sh 
          - -c
          - python manage.py migrate &amp;&amp; gunicorn smokkr.wsgi:application --bind 0.0.0.0:8000
---

",<kubernetes><nginx><google-kubernetes-engine><gke-networking>,60143835,2,"it would be better to use ingress to forward traffic to service in kubernetes.

you can find more deocumentation here : https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nginx-ingress-with-cert-manager-on-digitalocean-kubernetes

on kubernetes official doc : https://kubernetes.io/docs/concepts/services-networking/ingress/

simply deploy the nginx controller and apply the nginx rule in backend it deploy8 the nginx and convert yaml rule to nginx conf.

apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: test-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /testpath
        backend:
          servicename: test
          serviceport: 80

"
75453862,create service account in one liner,"how can i create a service account as a one liner using kubectl create serviceaccount test-role and then how to pass the metadata?
apiversion: v1
kind: serviceaccount
metadata:
  name: test-role
  namespace: utility
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::xxx:role/rolename



kubectl version
client version: version.info{major:&quot;1&quot;, minor:&quot;23&quot;, gitversion:&quot;v1.23.5&quot;, gitcommit:&quot;c285e781331a3785a7f436042c65c5641ce8a9e9&quot;, gittreestate:&quot;archive&quot;, builddate:&quot;1980-01-01t00:00:00z&quot;, goversion:&quot;go1.17.10&quot;, compiler:&quot;gc&quot;, platform:&quot;darwin/amd64&quot;}
server version: version.info{major:&quot;1&quot;, minor:&quot;23+&quot;, gitversion:&quot;v1.23.14-eks-ffeb93d&quot;, gitcommit:&quot;96e7d52c98a32f2b296ca7f19dc9346cf79915ba&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2022-11-29t18:43:31z&quot;, goversion:&quot;go1.17.13&quot;, compiler:&quot;gc&quot;, platform:&quot;linux/amd64&quot;}

",<kubernetes><amazon-eks>,75454124,2,"if by one line you mean one command you can use a heredoc:
kubectl apply -f - &lt;&lt;eof
apiversion: v1
kind: serviceaccount
metadata:
  name: test-role
  namespace: utility
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::xxx:role/rolename
eof

using the imperative kubectl commands, requires running two commands:
kubectl -n utility create serviceaccount test-role
kubectl -n utility annotate serviceaccount eks.amazonaws.com/role-arn=arn:aws:iam::xxx:role/rolename

"
75417732,how to manage two paths on the same hostname?,"i'm using kubernetes v1.23.16 cluster (one master and three workers) bare metal based.
i have created couple of services in a separate namespace. the same as follows.
$ kubectl get services --all-namespaces
namespace     name              type        cluster-ip       external-ip   port(s)                  age
default       kubernetes        clusterip   10.96.0.1        &lt;none&gt;        443/tcp                  11d
app1          app1-service      clusterip   10.103.151.235   &lt;none&gt;        80/tcp                   19h
app2          app2-service      clusterip   10.105.88.151    &lt;none&gt;        80/tcp                   11d
kube-system   kube-dns          clusterip   10.96.0.10       &lt;none&gt;        53/udp,53/tcp,9153/tcp   11d

now i'm having below nginx resource yml to access the service outside. for example i would like access as given below.

http://web.example.com/app1
http://web.example.com/app2

apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  name: app-ingress
spec:
  ingressclassname: nginx
  rules:
  - host: web.example.com
    http:
      paths:
      - path: /app1
        pathtype: prefix
        backend:
          service:
            name: app1-service
            port: 
              number: 80

      - path: /app2
        pathtype: prefix
        backend:
          service:
            name: app2-service
            port:
              number: 80

when i apply the nginx resource yml file it says service not found error.
$ kubectl describe ingress app-ingress
name:             app-ingress
labels:           &lt;none&gt;
namespace:        default
address:
default backend:  default-http-backend:80 (&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;)
rules:
  host             path  backends
  ----             ----  --------
  web.example.com
                   /app1    app1-service:80 (&lt;error: endpoints &quot;app1-service&quot; not found&gt;)
                   /app2    app2-service:80 (&lt;error: endpoints &quot;app2-service&quot; not found&gt;)
annotations:       nginx.ingress.kubernetes.io/rewrite-target: /$2

since my applications services are running in different namespace separately and my nginx resource yml is running in default namespace. so now how do i configure nginx resource file to access both of my service?
",<kubernetes><nginx><kubernetes-ingress><nginx-ingress>,75417867,2,"an ingress resource can only refer to services in the same namespace as the ingress. to manage two paths on the same hostname that lead to backends in different namespaces, you will need two separate ingress resources.
in the app1 namespace:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: app-ingress
  namespace: app1
spec:
  ingressclassname: nginx
  rules:
  - host: web.example.com
    http:
      paths:
      - path: /app1
        pathtype: prefix
        backend:
          service:
            name: app1-service
            port:
              number: 80

and in the app2 namespace:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: app-ingress
  namespace: app2
spec:
  ingressclassname: nginx
  rules:
  - host: web.example.com
    http:
      paths:
      - path: /app2
        pathtype: prefix
        backend:
          service:
            name: app2-service
            port:
              number: 80

with these resources in place, requests for http://web.example.com/app1 will go to app1-service, and requests for http://web.example.com/app2 will go to app2-service.
nb: i've removed your nginx.ingress.kubernetes.io/rewrite-target annotation because you're not doing any regex-based path rewriting in this example.

i've put a deployable example online here.
"
75362448,defining ingress for k3s traefik,"i am running k3s version 1.25.5 and i would like to define traefik as an ingress for one of the services defined through an external helm chart. i am struggling to find the right ingress definition. i tried with the below yaml file but that gives an error stating
error: resource mapping not found for name: &quot;c8-ingress&quot; namespace: &quot;&quot; from &quot;zeebe-traefik.yaml&quot;: no matches for kind &quot;ingress&quot; in version &quot;extensions/v1beta1&quot;
ensure crds are installed first

this seems to be because of the an old apiversion used in the yaml file. how to do it the right way?
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: c8-ingress
  annotations:
    kubernetes.io/ingress.class: &quot;traefik&quot;
spec:
  rules:
  - http:
      paths:
      - path: &quot;/&quot;
        backend:
          servicename: dev-zeebe-gateway
          serviceport: 26500

thanks.
",<kubernetes><kubernetes-ingress><traefik><k3s><traefik-ingress>,75366957,2,"your example is using an outdated ingress definition. in v1.25.x you need to use the stable networking.k8s.io/v1 api, as described here.
it is also recommended to provide the fitting namespace. this is useful for documentation, but also required for resource backends. it will also avoid adding -n yournamespace to every kubectl apply.
in your case, this may look something like:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: c8-ingress
  namespace: yournamespace
spec:
  rules:
    - http:
        paths:
          - pathtype: prefix
            path: /
            backend:
              service:
                name: dev-zeebe-gateway
                port:
                  number: 26500

i hope this helps to solve your issue.
in many cases, you can run kubectl explain resource to get useful links and resources for a given api-resource.
"
60556878,wanna connect via ingress on algocd-tutorial,"currently processing on this tutorial,
https://github.com/argoproj/argocd-example-apps/tree/master/guestbook
https://argoproj.github.io/argo-cd/getting_started/#5-register-a-cluster-to-deploy-apps-to-optional

my short-term milestone is to render guest-book's ui on browser.
i'm trying to connect via ingress, and it went wrong.

error message's like this,

status: 502
the server encountered a temporary error and could not complete your request.


i suppose something's wrong around service and pod.

guestbook-ui-svc.yaml

apiversion: v1
kind: service
metadata:
  name: guestbook-ui-service
spec:
  ports:
    - port: 80
      targetport: 80
  selector:
    app: guestbook-ui


guestbook-ui-ingress.yaml

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: app-ingress
  labels:
    app: guestbook-ui
spec:
  rules:
    - http:
        paths:
          - path: /
            backend:
              servicename: guestbook-ui-service
              serviceport: 80


guestbook-ui-deployment.yaml

apiversion: apps/v1
kind: deployment
metadata:
  name: guestbook-ui
spec:
  replicas: 1
  revisionhistorylimit: 3
  selector:
    matchlabels:
      app: guestbook-ui
  template:
    metadata:
      labels:
        app: guestbook-ui
    spec:
      containers:
      - image: gcr.io/heptio-images/ks-guestbook-demo:0.2
        name: guestbook-ui
        ports:
        - containerport: 80


i don't know which part i am missing, please lmk any ambiguous part or more detail.
thanks, in advance!
",<kubernetes><kubernetes-ingress><argocd>,60563516,2,"use this service instead.

apiversion: v1
kind: service
metadata:
  name: guestbook-ui-service
spec:
  type: nodeport
  ports:
    - port: 80
      targetport: 80
  selector:
    app: guestbook-ui


it has type: nodeport added to it.

you can check really good example on how to deploy an app, expose it via a service and add an ingress to it. it's available in kubernetes docs deploy a hello, world app.

also if you are having problem understanding the difference between nodeport, clusterip and what ingress is i recommend reading kubernetes nodeport vs loadbalancer vs ingress? when should i use what?
"
58774462,nginx ingress controller set up issues,"i have set up a baremetal k8 cluster ( 1 master node - intel nuc &amp; 2 worker nodes on raspberry pi). i managed to set up a metal-lb load balance and nginx ingress controller.  i have launched two applications, ghost (listens on default port 2368) and nextcloud ( listens on default port 80) .  i'm trying to access the applications from public ip myhomeserver.io ( to access the ghost application) and nextcloud.myhomeserver.io ( to access the next cloud application). i can access the ghost application but i can't seem to access nextcloud.given below are the yaml files for ingress and services. not sure where am i going wrong.

kubectl get services --all-namespaces
namespace       name                type           cluster-ip       external-ip       port(s)                      age
default         kubernetes          clusterip      10.96.0.1        &lt;none&gt;            443/tcp                      98d
ghost           ghost-service       clusterip      10.107.116.108   &lt;none&gt;            2368/tcp                     7h37m
ingress-nginx   ingress-nginx       loadbalancer   10.109.177.223   192.168.178.200   80:31619/tcp,443:30365/tcp   7d23h
kube-system     kube-dns            clusterip      10.96.0.10       &lt;none&gt;            53/udp,53/tcp,9153/tcp       98d
nextcloud       nextcloud-service   clusterip      10.105.24.162    &lt;none&gt;            8080/tcp                     137m

=============================================================================================================================
namespace   name                hosts                       address           ports   age
ghost       ingress-ghost       myhomeserver.io             192.168.178.200   80      7d22h
nextcloud   ingress-nextcloud   nextcloud.myhomeserver.io   192.168.178.200   80      140m


=============================================================================================================================
cat ingress-object-ghost.yaml

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-ghost
  namespace: ghost

spec:
  rules:
  - host: myhomeserver.io
    http:
      paths:
      - backend:
          servicename: ghost-service
          serviceport: 2368


=============================================================================================================================
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-nextcloud
  namespace: nextcloud

spec:
  rules:
  - host: nextcloud.myhomeserver.io
    http:
      paths:
      - backend:
          servicename: nextcloud-service
          serviceport: 8080

================================================================================================================================

cat ingress-object-nextcloud.yaml

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-nextcloud
  namespace: nextcloud

spec:
  rules:
  - host: nextcloud.myhomeserver.io
    http:
      paths:
      - backend:
          servicename: nextcloud-service
          serviceport: 8080
===================================================================================
apiversion: apps/v1

kind: deployment
metadata:
  name:
    deployment-nextcloud
  namespace: nextcloud
  labels:
    env: prod
    app: nextcloud-app

spec:
  template:
    metadata:
      name: nextcloud-app-pod
      labels:
        app:  nextcloud-app
        env:  production
    spec:
      containers:
        - name: nextcloud
          image: arm32v7/nextcloud
          imagepullpolicy: ifnotpresent
          ports:
            - containerport: 8080
          volumemounts:
           - mountpath: /var/www/html
             name: nextcloud-data
          securitycontext:
            privileged: true


      volumes:
      - name: nextcloud-data
        persistentvolumeclaim:
          claimname: pvc-nextcloud
      nodeselector:
        kubernetes.io/arch: arm

  replicas: 2
  selector:
    matchlabels:
      app: nextcloud-app


================================================================================================================
apiversion: v1
kind: service
metadata:
  name: nextcloud-service
  namespace: nextcloud
  labels:
    app: nextcloud-app
spec:
  type: clusterip
  selector:
    app: nextcloud-app
  ports:
  - port: 8080
    targetport: 8080
    protocol: tcp

",<kubernetes><kubernetes-ingress><nginx-ingress>,58776178,2,"note your nginx ingress controller is running in the ghost namespace so it only knows about the ghost service. you need to have another ingress controller for your nextcloud namespace if you want to have an ingress there. if you don't want another ingress controller then you can resolve the nextcloud service by targeting its dns in the following way servicename.namespacename.svc.cluster.local

on a side, there is not really a point in dividing your applications that much. kubernetes already gives you enough privacy among applications in the same namespace.

update
ingress that works for you given you have only 1 ingress controller. since there are two services i have added a path rule which will be rewritten to / so each service will receive a clean uri. use myhomeserver.io/ghost to reach ghost and myhomeserver.io/nextcloud to reach nextcloud.

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-ghost
  namespace: ghost
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: myhomeserver.io
    http:
      paths:
      - path: /ghost
        backend:
          servicename: ghost-service
          serviceport: 2368
      - path: /nextcloud
        backend:
          servicename: nextcloud-service.nextcloud.svc.cluster.local
          serviceport: 8080


update 2
so your ingress controller is running in the ghost namespace. thus, your  ingress has to be deployed in the ghost namespace. note the http rules for each host. 

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-ghost
  namespace: ghost
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: myhomeserver.io
    http:
      paths:
      - path: /
        backend:
          servicename: ghost-service
          serviceport: 2368
  - host: nextcloud.myhomeserver.io
    http:
      - path: /
        backend:
          servicename: nextcloud-service.nextcloud.svc.cluster.local
          serviceport: 8080

"
60468776,gke neg ingress always returns 502 bad gateway,"i have a statefulset, service with neg, and ingress set up on google cloud kubernetes engine cluster. 

every workload and network object is ready and healthy. ingress is created and neg status is updated for all the services. vpc-native (alias-ip) and http load balancer options are enabled for the cluster. 

but when i try to access my application using a path specified in my ingress i always get 502 (bad gateway) error.

here is my configuration (names are redacted including image name):

apiversion: v1
kind: service
metadata:
  annotations:
    cloud.google.com/neg: '{""ingress"": true}'
  labels:
    app: myapp
  name: myapp
spec:
  ports:
  - port: 80
    protocol: tcp
    targetport: tcp
  selector:
    app: myapp
---
apiversion: apps/v1
kind: statefulset
metadata:
  labels:
    app: myapp
  name: myapp
spec:
  replicas: 1
  selector:
    matchlabels:
      app: myapp
  servicename: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        livenessprobe:
          httpget:
            path: /
            port: tcp
            scheme: http
          initialdelayseconds: 60
        image: myapp:8bebbaf
        ports:
        - containerport: 1880
          name: tcp
          protocol: tcp
        readinessprobe:
          failurethreshold: 1
          httpget:
            path: /
            port: tcp
            scheme: http
        volumemounts:
        - mountpath: /data
          name: data
      securitycontext:
        fsgroup: 1000
      terminationgraceperiodseconds: 10
  volumeclaimtemplates:
  - metadata:
      labels:
        app: myapp
      name: data
    spec:
      accessmodes:
      - readwriteonce
      resources:
        requests:
          storage: 1gi
---
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: myapp-ingress
spec:
  rules:
  - http:
      paths:
      - path: /workflow
        backend:
          servicename: myapp
          serviceport: 80



what's wrong with it and how can i fix it?
",<kubernetes><google-kubernetes-engine><gke-networking>,60469433,2,"after much digging and tests i finally found what's wrong. also, it seems like gke neg ingress is not very stable (indeed neg is in beta) and does not always conform to kubernetes specs. 

there was an issue with gke ingress related to named ports in targetport field. the fix is implemented and available from 1.16.0-gke.20 cluster version (release), which as of today (february 2020) is available under rapid channel, but i have not tested the fix as i had other issues with an ingress on a version from this channel.

so basically there are 2 options if you experience the same issue:


specify exact port number and not port name in a targetport field in your service. here is a fixed service config file from my example:

apiversion: v1
kind: service
metadata:
  annotations:
    cloud.google.com/neg: '{""ingress"": true}'
  labels:
    app: myapp
  name: myapp
spec:
  ports:
  - port: 80
    protocol: tcp
    # !!!
    # targetport: tcp
    targetport: 1088
  selector:
    app: myapp

upgrade gke cluster to 1.16.0-gke.20+ version (haven't tested it myself).

"
55052565,retrieve kubernetes secrets mounted as volumes,"hi i am playing around with kubernetes secrets.
my deployment file is :

---
apiversion: v1
kind: secret
metadata:
  name: my-secrets
  labels:
    app: my-app
data:
  username: dxnlcm5hbwu=
  password: cgfzc3dvcmq=


i am able to create secrets and i am mounting them in my deployments as below:

---
apiversion: v1
kind: service
metadata:
  name: my-service
spec:
  selector:
    app: my-service
  ports:
  - protocol: tcp
    port: 80
    targetport: 8080
  type: nodeport

---

apiversion: apps/v1
kind: deployment
metadata:
  name: spring-service
  labels:
    app: spring-service
spec:
  replicas: 1
  selector:
    matchlabels:
      app: spring-service
  template:
    metadata:
      labels:
        app: spring-service
    spec:
      containers:
      - name: spring-service
        image: my-image:tag
        imagepullpolicy: always
        ports:
        - containerport: 8080
        volumemounts:
        - name: my-secret-vol
          mountpath: ""/app/secrets/my-secret""
          readonly: true            
      volumes:
      - name: my-secret-vol
        secret:
          secretname: my-secrets


my question is how can i access username and password i created in secret in spring-boot app?

i have tried loading in with ${my-secrets.username} and ${username}, but it fails to find values.

i also tried adding secrets as enviroment variables as below in deployment.yml:

env:
- name: username
  valuefrom:
    secretkeyref:
      name: my-secrets
      key: username
- name: password
  valuefrom:
    secretkeyref:
      name: my-secrets
      key: password


in this case, values are loaded from secrets and when i change values of secrets in minikube dashboard, it does not reflect the changes.

please help me to understand how this works.

i am using minikube and docker as containers
",<spring-boot><kubernetes><kubernetes-secrets><mounted-volumes>,55103692,2,"you don't inject the secret into properties.yml. instead, you use the content of the secret as properties.yml. the process is look like the following:


create a properties.yml with the sensitive data (e.g. password)
base64 encode this file (e.g. base64 properties.yml).
take the base64 encoded value and put that in the secret under the key properties.yaml.


you should end up with a secret in the following format:

apiversion: v1
kind: secret
metadata:
  name: my-secrets
  labels:
    app: my-app
data:
  properties.yml: dxnlcm5hbwu=


now when you mount this secret on your pod, kubernetes will decrypt the secret and put the value under the relevant path and you can just mount it. 

the pattern is to have 2 configuration files - one with non-sensitive configurations that is stored with the code, and the second (which includes sensitive configurations) stored as a secret. i don't know if that possible to load multiple config files using spring boot.

and one final comment - this process is cumbersome and error-prone. each change to the configuration file requires decoding the original secret and repeating this manual process. also, it's very hard to understand what changed - all you see is the entire content has changed. for that reason, we build kamus. it let you encrypt only the sensitive value instead of the entire file. let me know if that could be relevant for you :)
"
60508061,dnsconfig is skipped in gke,"faced the following issue:
i need to add a search domain on some pods to be able to communicate with headless service. kubernetes documentation recommends to set a dnsconfig and set everything in it.that's what i did. also there is a limitation that only 6 search domains can be set. 
part of the manifest:

    spec:
  hostname: search
  dnspolicy: clusterfirst
  dnsconfig:
    searches:
      - indexer.splunk.svc.cluster.local
  containers:
  - name: search


unfortunately it has no effect and resolv.conf file on targeted pod doesn't include this search domain:

search splunk.svc.cluster.local svc.cluster.local cluster.local us-east4-c.c.'project-id'.internal c.'project-id'.internal google.internal
nameserver 10.39.240.10
options ndots:5


after a quick look at this config i found that currently there are 6 search domens are specified and probably this is the reason why new search domain is not added. you can add manually and everything will work,but this isn't what i 'm trying to achieve.

do you have any ideas how to bypass this limitation?  

p.s set dnspolicy to none is not an option also as set prestart hooks to add my search zone.

---
# search-head deployment
apiversion: apps/v1
kind: deployment
metadata:
  name: search
  namespace: splunk
  labels:
    app: splunk
spec:
  replicas: 1
  selector:
    matchlabels:
      app: splunk
  template:
    metadata:
      labels:
        app: splunk
    spec:
      hostname: search
      dnspolicy: clusterfirst
      dnsconfig:
        searches:
          - indexer.splunk.svc.cluster.local
      containers:
      - name: search
        image: splunk/splunk
        env:
          - name: splunk_start_args
            value: ""--accept-license""
          - name: splunk_password
            valuefrom:
              secretkeyref:
                name: splunk-password
                key: password
          - name: splunk_role
            value: splunk_search_head
          - name: splunk_search_head_url
            value: search
          - name: splunk_indexer_url # todo: make this part dynamic.
            value: indexer-0,indexer-1
        ports:
          - name: web
            containerport: 8000
          - name: mgmt
            containerport: 8089
          - name: kv
            containerport: 8191
        volumemounts:
        - mountpath: /opt/splunk/var
          name: sh-volume
      volumes:
      - name: sh-volume
        persistentvolumeclaim:
          claimname: sh-volume

",<kubernetes><google-cloud-platform><google-kubernetes-engine><kubernetes-pod>,60513646,2,"according to pods dnsconfig documentation:


  searches: a list of dns search domains for hostname lookup in the pod. this property is optional. when specified, the provided list will be merged into the base search domain names generated from the chosen dns policy. duplicate domain names are removed. kubernetes allows for at most 6 search domains.



even though resolv.conf docs mention it accepts more than 6 search domains on latest versions, it's not yet possible to surpass this number of search domains through kubernetes deployment.
i created a workaround on which an initcontainer creates and mount to the pod a new resolv.conf and after the container is up it replaces the automatically generated one.
this way if the container crashes or gets rebooted the resolv.conf will always be reinforced.


nginx-emulating-your-splunk-deploy.yaml:

apiversion: apps/v1
kind: deployment
metadata:
  name: search
  namespace: default
  labels:
    app: splunk
spec:
  replicas: 1
  selector:
    matchlabels:
      app: splunk
  template:
    metadata:
      labels:
        app: splunk
    spec:
      hostname: search
      initcontainers:
        - name: initdns
          image: nginx
          imagepullpolicy: ifnotpresent
          command: [""/bin/bash"",""-c""] 
          args: [""echo -e \""nameserver 10.39.240.10\nsearch indexer.splunk.svc.cluster.local splunk.svc.cluster.local svc.cluster.local cluster.local us-east4-c.c.'project-id'.internal c.'project-id'.internal google.internal\noptions ndots:5\n \"" &gt; /mnt/resolv.conf""]
          volumemounts:
          - mountpath: /mnt
            name: volmnt      
      containers:
        - name: search
          image: nginx
          env:
          - name: splunk_start_args
            value: ""--accept-license""
          - name: splunk_password
            value: password
          - name: splunk_role
            value: splunk_search_head
          - name: splunk_search_head_url
            value: search
          ports:
          - name: web
            containerport: 8000
          - name: mgmt
            containerport: 8089
          - name: kv
            containerport: 8191
          volumemounts:
          - mountpath: /mnt
            name: volmnt
          command: [""/bin/bash"",""-c""] 
          args: [""cp /mnt/resolv.conf /etc/resolv.conf ; nginx -g \""daemon off;\""""]
      volumes:
      - name: volmnt
        emptydir: {}



remember to check the following fields and set according to your environment:


namespace, nameserver, container.image,  container.args 






reproduction:


$ kubectl apply -f search-head-splunk.yaml 
deployment.apps/search created

$ kubectl get pods
name                      ready   status    restarts   age
search-64b6fb5854-shm2x   1/1     running   0          5m14sa

$ kubectl exec -it search-64b6fb5854-shm2x -- cat /etc/resolv.conf 
nameserver 10.39.240.10
search indexer.splunk.svc.cluster.local splunk.svc.cluster.local svc.cluster.local cluster.local us-east4-c.c.'project-id'.internal c.'project-id'.internal google.internal
options ndots:5


you can see that the resolv.conf stays as configured, please reproduce in your environment and let me know if you find any problem.



edit 1:


the above scenario is designed for an environment where you need more than 6 search domains.
we have to hardcode the dns server, but kube-dns service sticks with the same ip during cluster lifespan and sometimes even after cluster recreation, it depends on network configuration.
if you need 6 or less domains you can just change dnspolicy to none and skip the initcontainer:


apiversion: apps/v1
kind: deployment
metadata:
  name: search
  namespace: splunk
  labels:
    app: splunk
spec:
  replicas: 1
  selector:
    matchlabels:
      app: splunk
  template:
    metadata:
      labels:
        app: splunk
    spec:
      hostname: search
      dnspolicy: ""none""
      dnsconfig:
        nameservers:
          - 10.39.240.10
        searches:
          - indexer.splunk.svc.cluster.local
          - splunk.svc.cluster.local
          - us-east4-c.c.'project-id'.internal
          - c.'project-id'.internal
          - svc.cluster.local
          - cluster.local
        options:
          - name: ndots
          - value: ""5""
      containers:
      - name: search
        image: splunk/splunk
...
{{{the rest of your config}}}

"
60504227,need a working kubectl binary inside an image,"my goal is to have a pod with a working kubectl binary inside.  

unfortunatly every kubectl image from docker hub i booted using basic yaml resulted in crashloopbackoff or else.  

has anyone got some yaml (deployment, pod, etc) that would get me my kubectl ?



i tried a bunch of images with this basic yaml there:

apiversion: apps/v1
kind: deployment
metadata:
  name: kubectl-demo
  labels:
    app: deploy
    role: backend
spec:
  replicas: 1
  selector:
    matchlabels:
      app: deploy
      role: backend
  template:
    metadata:
      labels:
        app: deploy
        role: backend
    spec:
      containers:
      - name: kubectl-demo
        image: &lt;some_image&gt;
        ports:
        - containerport: 80


thx
",<docker><kubernetes><kubectl><dockerhub>,60522890,2,"or, you can do this. it works in my context, with kubernetes on vms, where i know where is kubeconfig file. you would need to make the necessary changes, to make it work in your environment.

apiversion: apps/v1
kind: deployment
metadata:
  name: kubectl
spec:
  replicas: 1
  selector:
    matchlabels:
      role: kubectl
  template:
    metadata:
      labels:
        role: kubectl
    spec:
      containers:
      - image: viejo/kubectl
        name: kubelet
        tty: true
        securitycontext:
          privileged: true
        volumemounts:
        - name: kube-config
          mountpath: /root/.kube/
      volumes:
      - name: kube-config
        hostpath:
          path: /home/$user/.kube/
      affinity:
        nodeaffinity:
          requiredduringschedulingignoredduringexecution:
            nodeselectorterms:
            - matchexpressions:
              - key: node-role.kubernetes.io/master
                operator: exists
      tolerations:
      - effect: noschedule
        key: node-role.kubernetes.io/master
        operator: exists


this is the result:

$ kubectl get po
name                      ready   status    restarts   age
kubectl-cb8bfc6dd-nv6ht   1/1     running   0          70s
$ kubectl exec kubectl-cb8bfc6dd-nv6ht -- kubectl get no
name                     status   roles    age   version
kubernetes-1-17-master   ready    master   16h   v1.17.3
kubernetes-1-17-worker   ready    &lt;none&gt;   16h   v1.17.3

"
55165961,why is prometheus operator not able to start,"i'm trying to  create prometheus with operator in fresh new k8s cluster
i use the following files , 


im creating a namespace  monitoring 
apply this file , which works ok



apiversion: apps/v1beta2
kind: deployment
metadata:
  labels:
    k8s-app: prometheus-operator
  name: prometheus-operator
  namespace: monitoring
spec:
  replicas: 2
  selector:
    matchlabels:
      k8s-app: prometheus-operator
  template:
    metadata:
      labels:
        k8s-app: prometheus-operator
    spec:
      priorityclassname: ""operator-critical""
      tolerations:
      - key: ""workgroup""
        operator: ""equal""
        value: ""operator""
        effect: ""noschedule""
      - key: ""workgroup""
        operator: ""equal""
        value: ""operator""
        effect: ""noexecute""
      containers:
      - args:
        - --kubelet-service=kube-system/kubelet
        - --logtostderr=true
        - --config-reloader-image=quay.io/coreos/configmap-reload:v0.0.1
        - --prometheus-config-reloader=quay.io/coreos/prometheus-config-reloader:v0.29.0
        image: quay.io/coreos/prometheus-operator:v0.29.0
        name: prometheus-operator
        ports:
        - containerport: 8080
          name: http
        securitycontext:
          allowprivilegeescalation: false
          readonlyrootfilesystem: true
      nodeselector:
      serviceaccountname: prometheus-operator


now i want  to apply this file (crd)

apiversion: monitoring.coreos.com/v1
kind: prometheus
metadata:
  name: prometheus
  namespace: monitoring
  labels: 
    prometheus: prometheus
spec:
  replica: 1
  priorityclassname: ""operator-critical""
  serviceaccountname: prometheus
  nodeselector:
        worker.garden.sapcloud.io/group: operator
  servicemonitornamespaceselector: {}
  servicemonitorselector:
    matchlabels:
      role: observeable
  tolerations:
  - key: ""workgroup""
    operator: ""equal""
    value: ""operator""
    effect: ""noschedule""
  - key: ""workgroup""
    operator: ""equal""
    value: ""operator""
    effect: ""noexecute""


before i've created those crd

https://github.com/coreos/prometheus-operator/tree/master/example/prometheus-operator-crd

the problem that the pods didn't able to start (0/2), see the picture below. what could be the problem?  please advice 



update

when i go to the event of the prom operator i see the following error creating: pods ""prometheus-operator-6944778645-"" is forbidden: no priorityclass with name operator-critical was found replicaset-controller , any idea ?
",<kubernetes><google-cloud-platform><prometheus><google-kubernetes-engine><prometheus-operator>,55177595,2,"you are trying to reference the operator-critical priority class. priority classes determine the priority of pods and their resource assignment.

to fix this issue you could either remove the explicit priority class(priorityclassname: ""operator-critical"") in both files or create the operator-critical class:

apiversion: scheduling.k8s.io/v1beta1
kind: priorityclass
metadata:
  name: operator-critical
value: 1000000
globaldefault: false
description: ""critical operator workloads""

"
55206520,go microservices with ambassador api gateway,"i'm having a few issues getting ambassador to work correctly. i'm new to kubernetes and just teaching myself.

i have successfully managed to work through the demo material ambassador provide - e.g /httpbin/ endpoint is working correctly, but when i try to deploy a go service it is falling over.

when hitting the 'qotm' endpoint, the page this is the response: 

upstream request timeout


pod status:

crashloopbackoff


from my research, it seems to be related to the yaml file not being configured correctly but i'm struggling to find any documentation relating to this use case.

my cluster is running on aws eks and the images are being pushed to aws ecr.

main.go: 

package main

import (
    ""fmt""
    ""net/http""
    ""os""
)

func main() {
    var port string
    if port = os.getenv(""port""); port == """" {
        port = ""3001""
    }
    http.handlefunc(""/"", func(w http.responsewriter, r *http.request) {
        fmt.fprintf(w, ""hello world from path: %s\n"", r.url.path)
    })
    http.listenandserve("":"" + port, nil)
}


dockerfile:

from golang:alpine
add ./src /go/src/app
workdir /go/src/app
expose 3001
env port=3001
cmd [""go"", ""run"", ""main.go""]


test.yaml: 

apiversion: v1
kind: service
metadata:
  name: qotm
  annotations:
    getambassador.io/config: |
      ---
      apiversion: ambassador/v1
      kind:  mapping
      name:  qotm_mapping
      prefix: /qotm/
      service: qotm
spec:
  selector:
    app: qotm
  ports:
    - port: 80
      name: http-qotm
      targetport: http-api
---
apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: qotm
spec:
  replicas: 1
  strategy:
    type: rollingupdate
  template:
    metadata:
      labels:
        app: qotm
    spec:
      containers:
        - name: qotm
          image: ||removed||
          ports:
            - name: http-api
              containerport: 3001
          readinessprobe:
            httpget:
              path: /health
              port: 5000
            initialdelayseconds: 30
            periodseconds: 3
          resources:
            limits:
              cpu: ""0.1""
              memory: 100mi


pod description:

name:               qotm-7b9bf4d499-v9nxq
namespace:          default
priority:           0
priorityclassname:  &lt;none&gt;
node:               ip-192-168-89-69.eu-west-1.compute.internal/192.168.89.69
start time:         sun, 17 mar 2019 17:19:50 +0000
labels:             app=qotm
                    pod-template-hash=3656908055
annotations:        &lt;none&gt;
status:             running
ip:                 192.168.113.23
controlled by:      replicaset/qotm-7b9bf4d499
containers:
  qotm:
    container id:   docker://5839996e48b252ac61f604d348a98c47c53225712efd503b7c3d7e4c736920c4
    image:          imgurl
    image id:       docker-pullable://imgurl
    port:           3001/tcp
    host port:      0/tcp
    state:          waiting
      reason:       crashloopbackoff
    last state:     terminated
      reason:       error
      exit code:    1
      started:      sun, 17 mar 2019 17:30:49 +0000
      finished:     sun, 17 mar 2019 17:30:49 +0000
    ready:          false
    restart count:  7
    limits:
      cpu:     100m
      memory:  200mi
    requests:
      cpu:        100m
      memory:     200mi
    readiness:    http-get http://:3001/health delay=30s timeout=1s period=3s #success=1 #failure=3
    environment:  &lt;none&gt;
    mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-5bbxw (ro)
conditions:
  type              status
  initialized       true 
  ready             false 
  containersready   false 
  podscheduled      true 
volumes:
  default-token-5bbxw:
    type:        secret (a volume populated by a secret)
    secretname:  default-token-5bbxw
    optional:    false
qos class:       guaranteed
node-selectors:  &lt;none&gt;
tolerations:     node.kubernetes.io/not-ready:noexecute for 300s
                 node.kubernetes.io/unreachable:noexecute for 300s
events:
  type     reason     age                  from                                                  message
  ----     ------     ----                 ----                                                  -------
  normal   scheduled  12m                  default-scheduler                                     successfully assigned default/qotm-7b9bf4d499-v9nxq to ip-192-168-89-69.eu-west-1.compute.internal
  normal   pulled     10m (x5 over 12m)    kubelet, ip-192-168-89-69.eu-west-1.compute.internal  container image ""imgurl"" already present on machine
  normal   created    10m (x5 over 12m)    kubelet, ip-192-168-89-69.eu-west-1.compute.internal  created container
  normal   started    10m (x5 over 11m)    kubelet, ip-192-168-89-69.eu-west-1.compute.internal  started container
  warning  backoff    115s (x47 over 11m)  kubelet, ip-192-168-89-69.eu-west-1.compute.internal  back-off restarting failed container

",<docker><go><kubernetes><yaml><amazon-eks>,55209683,2,"in your kubernetes deployment file you have exposed a readiness probe on port 5000 while your application is exposed on port 3001, also while running the container a few times i got oomkilled so increased the memory limit. anyways below deployment file should work fine.

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: qotm
spec:
  replicas: 1
  strategy:
    type: rollingupdate
  template:
    metadata:
      labels:
        app: qotm
    spec:
      containers:
        - name: qotm
          image: &lt;your_image&gt;
          imagepullpolicy: always
          ports:
            - name: http-api
              containerport: 3001
          readinessprobe:
            httpget:
              path: /health
              port: 3001
            initialdelayseconds: 30
            periodseconds: 3
          resources:
            limits:
              cpu: ""0.1""
              memory: 200mi

"
54509383,is it possible to dynamically add hosts to ingress with kubernetes?,"if you are managing ingress service such as in the following example, instead of updating the ingress file below, is there a means of adding an additional host/service such as echo3.example.com with out needing to apply an updated version of the original file?

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: echo-ingress
spec:
  rules:
  - host: echo1.example.com
    http:
      paths:
      - backend:
          servicename: echo1
          serviceport: 80
  - host: echo2.example.com
    http:
      paths:
      - backend:
          servicename: echo2
          serviceport: 80



# new host/service

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: echo-ingress
spec:
  rules:
  - host: echo3.example.com ### &lt;= `echo3` addeded
    http:
      paths:
      - backend:
          servicename: echo3
          serviceport: 80


is there a way of applying this new host without needing to extend the old file?
",<kubernetes><kubernetes-ingress>,54509877,2,"if you apply the two files, the second one will overwrite the first one as they have the same name. so, you would need to edit the original every time you add a new rule.

one possible solution to avoid this problem would be to use contour. in that case you could keep each ingressroute in a separate resource and avoid conflicts like that.

in your case, you would have something like:

# ingressroute-echo1.yaml
apiversion: contour.heptio.com/v1beta1
kind: ingressroute
metadata:
  name: echo-ingress-1
spec:
  virtualhost:
    fqdn: echo1.example.com
  routes:
    - match: /
      services:
        - name: echo1
          port: 80

# ingressroute-echo2.yaml
apiversion: contour.heptio.com/v1beta1
kind: ingressroute
metadata:
  name: echo-ingress-2
spec:
  virtualhost:
    fqdn: echo2.example.com
  routes:
    - match: /
      services:
        - name: echo2
          port: 80

# ingressroute-echo3.yaml
apiversion: contour.heptio.com/v1beta1
kind: ingressroute
metadata:
  name: echo-ingress-3
spec:
  virtualhost:
    fqdn: echo3.example.com
  routes:
    - match: /
      services:
        - name: echo3
          port: 80

"
54957113,k8s-ingress to make the application secured with https,"im have k8s app (web api) which first exposed via nodeport (i've used port forwarding to run it and it works as expected)  

run it like localhost:8080/api/v1/users

than i've created a service with type loadbalancer to expose it outside, which works as expected. 

e.g. http://myhost:8080/api/v1/users

apiversion: v1
kind: service
metadata:
  name: fzr
  labels:
    app: fzr
    tier: service
spec:
  type: loadbalancer
  ports:
    - port: 8080
  selector:
    app: fzr


now we need to make it secure and after reading about this topic we have decided to use ingress for it.

this is what i did 

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ctr-ingress
  selector:
    app: fzr
spec:
  ports:
    - name: https
      port: 443
      targetport: https


now i want to run it like

https://myhost:443/api/v1/users

this is not working, im not able to run the application with port 443 as https, please advice? 
",<amazon-web-services><kubernetes><google-cloud-platform><kubernetes-ingress><nginx-ingress>,54975775,2,"it looks to me like you are using a yaml template for a type service to deploy your ingress but not correctly.  targetport should be a numeric port, and anyway, i don't think ""https"" is a correct value (i might be wrong though).  

something like this:

apiversion: v1
kind: service
type: nodeport
metadata:
  name: fzr-ingress
spec:
  type: nodeport
  selector:
    app: fzr
  ports:
  - protocol: tcp
    port: 443
    targetport: 8080


now you have a nodeport service listening on 443 and forwarding the traffic to your fzr pods listening on port 8080.  

however, the fact you are listening on port 443 does nothing to secure your app by itself. to encrypt the traffic you need a tls certificate that you have to make available to the ingress as a secret.  

if this seems somewhat complicated (because it is) you could look into deploying an nginx ingress from a helm chart

in any case your ingress yaml would look something like this:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
  name: gcs-ingress
  namespace: default
spec:
  rules:
  - host: myhost
    http:
      paths:
      - backend:
          servicename: fzr
          serviceport: 443
        path: /api/v1/users
  tls:
  - hosts:
    - myhost
    secretname: myhosts-tls


more info on how to configure this here
"
70634276,redirect everything with ingress-nginx,"i have created a yaml file its only job is: it should immediately redirect to google.com
but it just doesn't work...
my localhost still returns 404-nginx
i'm on docker-desktop and my cluster version is v1.21.5
here is my redirect.yaml
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: my-google
  annotations:
    nginx.ingress.kubernetes.io/permanent-redirect: https://www.google.com
spec:
  rules:
  - http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: doesntmatter
            port:
              number: 80

here is my kubectl get ingress
name          class    hosts                          address     ports   age
cheddar       nginx    cheddar.127.0.0.1.nip.io       localhost   80      31m
my-google     &lt;none&gt;   *                                          80      26m
stilton       nginx    stilton.127.0.0.1.nip.io       localhost   80      31m
wensleydale   nginx    wensleydale.127.0.0.1.nip.io   localhost   80      31m

note: the other ingress sevices e.g. cheddar.127.0.0.1.nip.io is working perfectly...
",<kubernetes><kubernetes-ingress><kubectl><nginx-ingress>,70634419,2,"i guess you forgot the ingress class name.
spec:
  ingressclassname: nginx
  ...

apart from that, you can create an external service.
---
apiversion: v1
kind: service
metadata:
  name: google
spec:
  type: externalname
  externalname: www.google.com
  ports:
    - name: https
      port: 443
      protocol: tcp
      targetport: 443
---
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: google
  labels:
    name: google
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: https
    nginx.ingress.kubernetes.io/upstream-vhost: www.google.com
spec:
  ingressclassname: nginx
  rules:
  - http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: google
            port:
              name: https

note, that the cert from your ingress controller is not the cert of google. so there can be some issues around that. one setting that may help with those kind of issues is the annotation nginx.ingress.kubernetes.io/upstream-vhost like shown above.
"
70617622,rbac rules not working in cluster with kubeadm,"in one of our customer's kubernetes cluster(v1.16.8 with kubeadm) rbac does not work at all. we created a serviceaccount, read-only clusterrole, and clusterrolebinding with the following yamls but when we login through the dashboard or kubectl user can almost do anything in the cluster. what can cause this problem?
kind: serviceaccount
apiversion: v1
metadata:
  name: read-only-user
  namespace: permission-manager
secrets:
  - name: read-only-user-token-7cdx2


kind: clusterrolebinding
apiversion: rbac.authorization.k8s.io/v1
metadata:
  name: read-only-user___template-namespaced-resources___read-only___all_namespaces
  labels:
    generated_for_user: ''
subjects:
  - kind: serviceaccount
    name: read-only-user
    namespace: permission-manager
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: template-namespaced-resources___read-only

kind: clusterrole
apiversion: rbac.authorization.k8s.io/v1
metadata:
  name: template-namespaced-resources___read-only
rules:
  - verbs:
      - get
      - list
      - watch
    apigroups:
      - '*'
    resources:
      - configmaps
      - endpoints
      - persistentvolumeclaims
      - pods
      - pods/log
      - pods/portforward
      - podtemplates
      - replicationcontrollers
      - resourcequotas
      - secrets
      - services
      - events
      - daemonsets
      - deployments
      - replicasets
      - ingresses
      - networkpolicies
      - poddisruptionbudgets

here is the cluster's kube-apiserver.yaml file content:
apiversion: v1
kind: pod
metadata:
  creationtimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.1.42
    - --allow-privileged=true
    - --authorization-mode=node,rbac
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=noderestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --insecure-port=0
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=internalip,externalip,hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=x-remote-extra-
    - --requestheader-group-headers=x-remote-group
    - --requestheader-username-headers=x-remote-user
    - --secure-port=6443
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    image: k8s.gcr.io/kube-apiserver:v1.16.8
    imagepullpolicy: ifnotpresent
    livenessprobe:
      failurethreshold: 8
      httpget:
        host: 192.168.1.42
        path: /healthz
        port: 6443
        scheme: https
      initialdelayseconds: 15
      timeoutseconds: 15
    name: kube-apiserver
    resources:
      requests:
        cpu: 250m
    volumemounts:
    - mountpath: /etc/ssl/certs
      name: ca-certs
      readonly: true
    - mountpath: /etc/ca-certificates
      name: etc-ca-certificates
      readonly: true
    - mountpath: /etc/kubernetes/pki
      name: k8s-certs
      readonly: true
    - mountpath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readonly: true
    - mountpath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readonly: true
  hostnetwork: true
  priorityclassname: system-cluster-critical
  volumes:
  - hostpath:
      path: /etc/ssl/certs
      type: directoryorcreate
    name: ca-certs
  - hostpath:
      path: /etc/ca-certificates
      type: directoryorcreate
    name: etc-ca-certificates
  - hostpath:
      path: /etc/kubernetes/pki
      type: directoryorcreate
    name: k8s-certs
  - hostpath:
      path: /usr/local/share/ca-certificates
      type: directoryorcreate
    name: usr-local-share-ca-certificates
  - hostpath:
      path: /usr/share/ca-certificates
      type: directoryorcreate
    name: usr-share-ca-certificates
status: {}

",<kubernetes><kubectl><roles><rbac>,70617712,2,"what you have defined is only control the service account. here's a tested spec; create a yaml file with:
apiversion: v1
kind: namespace
metadata:
  name: test
---
apiversion: v1
kind: serviceaccount
metadata:
  name: test-sa
  namespace: test
---
kind: clusterrolebinding  # &lt;-- reminder: cluster wide and not namespace specific. use rolebinding for namespace specific.
apiversion: rbac.authorization.k8s.io/v1
metadata:
  name: test-role-binding
subjects:
- kind: serviceaccount
  name: test-sa
  namespace: test
- kind: user
  name: someone
  apigroup: rbac.authorization.k8s.io
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: test-cluster-role
---
kind: clusterrole
apiversion: rbac.authorization.k8s.io/v1
metadata:
  name: test-cluster-role
rules:
- verbs:
  - get
  - list
  - watch
  apigroups:
  - '*'
  resources:
  - configmaps
  - endpoints
  - persistentvolumeclaims
  - pods
  - pods/log
  - pods/portforward
  - podtemplates
  - replicationcontrollers
  - resourcequotas
  - secrets
  - services
  - events
  - daemonsets
  - deployments
  - replicasets
  - ingresses
  - networkpolicies
  - poddisruptionbudgets

apply the above spec: kubectl apply -f &lt;filename&gt;.yaml
work as expected:

delete the test resources: kubectl delete -f &lt;filename&gt;.yaml
"
65277688,kubernetes ingress path: allow all exept for /blog/,"i have a service running on kubernetes available on a domain like example.com. i'm trying to add a path which should redirect to a wordpress blog. so i need to add a rule: everithing that goes in /blog/ should redirect to wordpress, otherwise use the main app.
i tried to include a regexp for the main application path to include all except /blog/
  - host: example.com
    http:
      paths:
      - path: /^(?!blog).*$

but i keep getting must be a valid regex or if i remove the slash, it says must be an absolute path. i can't seem to find a way how to do that, it just keeps redirecting to my root app
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: app
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        backend:
          servicename: mainappservice
          serviceport: 3010
      - path: /blog/*
        backend:
          servicename: blogservice
          serviceport: 3020

",<kubernetes><microservices><kubernetes-ingress><nginx-ingress>,65283452,2,"try this -
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    kubernetes.io/ingress.class: nginx
  name: app
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: /(.*)
        backend:
          servicename: mainappservice
          serviceport: 3010
      - path: /blog/(.*)
        backend: 
          servicename: blogservice
          serviceport: 3020


i guess, this should work
"
58407345,kubernetes - pass public ip of load balance as environment variable into pod,"gist

i have a configmap which provides necessary environment variables to my pods:

apiversion: v1
kind: configmap
metadata:
  name: global-config
data:
  node_env: prod
  level: info

  # i need to set api_url to the public ip address of the load balancer
  api_url: http://&lt;some ip&gt;:3000

  database_url: mongodb://database:27017
  some_service_host: some-service:3000


i am running my kubernetes cluster on google cloud, so it will automatically create a public endpoint for my service:

apiversion: v1
kind: service
metadata:
  name: gateway
spec:
  selector:
    app: gateway
  ports:
    - name: http
      port: 3000
      targetport: 3000
      nodeport: 30000
  type: loadbalancer


issue

i have an web application that needs to make http requests from the client's browser to the gateway service. but in order to make a request to the external service, the web app needs to know it's ip address.

so i've set up the pod, which serves the web application in a way, that it picks up an environment variable ""api_url"" and as a result makes all http requests to this url.

so i just need a way to set the api_url environment variable to the public ip address of the gateway service to pass it into a pod when it starts. 
",<kubernetes><google-cloud-platform><environment-variables><google-kubernetes-engine>,58505622,2,"you are trying to access gateway service from client's browser.

i would like to suggest you another solution that is slightly different from what you are currently trying to achieve
but it can solve your problem.

from your question i was able to deduce that your web app and gateway app are on the same cluster.

in my solution you dont need a service of type loadbalancer and basic ingress is enough to make it work.

you only need to create a service object (notice that option type: loadbalancer is now gone)

apiversion: v1
kind: service
metadata:
name: gateway
spec:
selector:
  app: gateway
ports:
  - name: http
    port: 3000
    targetport: 3000
    nodeport: 30000


and you alse need an ingress object (remember that na ingress controller needs to be deployed to cluster in order to make it work) like one below:
more on how to deploy nginx ingress controller you can finde here
and if you are already using one (maybe different one) then you can skip this step.

apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
name: gateway-ingress
annotations:
  nginx.ingress.kubernetes.io/rewrite-target: /
spec:
rules:
  - host: gateway.foo.bar.com
    http:
      paths:
      - path: /
          backend:
            servicename: gateway
            serviceport: 3000


notice the host field.

the same you need to repeat for your web application. remember to use appropriate host name (dns name)
e.g. for web app: foo.bar.com and for gateway: gateway.foo.bar.com
and then just use the gateway.foo.bar.com dns name to connect to the gateway app from clients web browser.

you also need to create a dns entry that points *.foo.bar.com to ingress's public ip address
as ingress controller will create its own load balancer.

the flow of traffic would be like below:

+-------------+   +---------+   +-----------------+   +---------------------+
| web browser |--&gt;| ingress |--&gt;| gateway service |--&gt;| gateway application |
+-------------+   +---------+   +-----------------+   +---------------------+


this approach is better becaues it won't cause issues with cross-origin resource sharing (cors) in clients browser.

examples of ingress and service manifests i took from official kubernetes documentation and modified slightly.

more on ingress you can find here
and on services here
"
58205446,"certificate issued by cert manager reads as ""issued by: cert-manager.local"" instead of let's encrypt and does not work","when i browse my website from chrome, it says that the certificate is invalid, and if i check the details, this is what i see:

issued to:
common name (cn)    test.x.example.com
organization (o)    cert-manager
organizational unit (ou)    &lt;not part of certificate&gt;

issued by:
common name (cn)    cert-manager.local
organization (o)    cert-manager
organizational unit (ou)    &lt;not part of certificate&gt;


i don't understand what is going wrong. from cert-manager's output it would seem everything is going well:

i1002 15:56:52.761583       1 start.go:76] cert-manager ""level""=0 ""msg""=""starting controller""  ""git-commit""=""95e8b7de"" ""version""=""v0.9.1""
i1002 15:56:52.765337       1 controller.go:169] cert-manager/controller/build-context ""level""=0 ""msg""=""configured acme dns01 nameservers"" ""nameservers""=[""10.44.0.10:53""]
i1002 15:56:52.765777       1 controller.go:134] cert-manager/controller ""level""=0 ""msg""=""starting leader election""
i1002 15:56:52.767133       1 leaderelection.go:235] attempting to acquire leader lease  cert-manager/cert-manager-controller...
i1002 15:56:52.767946       1 metrics.go:203] cert-manager/metrics ""level""=0 ""msg""=""listening for connections on"" ""address""=""0.0.0.0:9402""
i1002 15:58:18.940473       1 leaderelection.go:245] successfully acquired lease cert-manager/cert-manager-controller
i1002 15:58:19.043002       1 controller.go:109] cert-manager/controller ""level""=0 ""msg""=""starting controller"" ""controller""=""challenges""
i1002 15:58:19.043050       1 base_controller.go:132] cert-manager/controller/challenges ""level""=0 ""msg""=""starting control loop""
i1002 15:58:19.043104       1 controller.go:91] cert-manager/controller ""level""=0 ""msg""=""not starting controller as it's disabled"" ""controller""=""certificates-experimental""
i1002 15:58:19.043174       1 controller.go:109] cert-manager/controller ""level""=0 ""msg""=""starting controller"" ""controller""=""orders""
i1002 15:58:19.043200       1 base_controller.go:132] cert-manager/controller/orders ""level""=0 ""msg""=""starting control loop""
i1002 15:58:19.043376       1 controller.go:109] cert-manager/controller ""level""=0 ""msg""=""starting controller"" ""controller""=""certificates""
i1002 15:58:19.043410       1 base_controller.go:132] cert-manager/controller/certificates ""level""=0 ""msg""=""starting control loop""
i1002 15:58:19.043646       1 controller.go:91] cert-manager/controller ""level""=0 ""msg""=""not starting controller as it's disabled"" ""controller""=""certificaterequests-issuer-ca""
i1002 15:58:19.044292       1 controller.go:109] cert-manager/controller ""level""=0 ""msg""=""starting controller"" ""controller""=""clusterissuers""
i1002 15:58:19.044459       1 base_controller.go:132] cert-manager/controller/clusterissuers ""level""=0 ""msg""=""starting control loop""
i1002 15:58:19.044617       1 controller.go:109] cert-manager/controller ""level""=0 ""msg""=""starting controller"" ""controller""=""ingress-shim""
i1002 15:58:19.044742       1 base_controller.go:132] cert-manager/controller/ingress-shim ""level""=0 ""msg""=""starting control loop""
i1002 15:58:19.044959       1 controller.go:109] cert-manager/controller ""level""=0 ""msg""=""starting controller"" ""controller""=""issuers""
i1002 15:58:19.045110       1 base_controller.go:132] cert-manager/controller/issuers ""level""=0 ""msg""=""starting control loop""
e1002 15:58:19.082958       1 base_controller.go:91] cert-manager/controller/certificates/handleownedresource ""msg""=""error getting order referenced by resource"" ""error""=""certificate.certmanager.k8s.io \""api-certificate\"" not found"" ""related_resource_kind""=""certificate"" ""related_resource_name""=""api-certificate"" ""related_resource_namespace""=""staging"" ""resource_kind""=""order"" ""resource_name""=""api-certificate-3031097725"" ""resource_namespace""=""staging""
i1002 15:58:19.143501       1 base_controller.go:187] cert-manager/controller/orders ""level""=0 ""msg""=""syncing item"" ""key""=""staging/api-certificate-3031097725""
i1002 15:58:19.143602       1 base_controller.go:187] cert-manager/controller/certificates ""level""=0 ""msg""=""syncing item"" ""key""=""cert-manager/cert-manager-webhook-ca""
i1002 15:58:19.143677       1 base_controller.go:187] cert-manager/controller/certificates ""level""=0 ""msg""=""syncing item"" ""key""=""cert-manager/cert-manager-webhook-webhook-tls""
i1002 15:58:19.144011       1 sync.go:304] cert-manager/controller/orders ""level""=0 ""msg""=""need to create challenges"" ""resource_kind""=""order"" ""resource_name""=""api-certificate-3031097725"" ""resource_namespace""=""staging"" ""number""=0
i1002 15:58:19.144043       1 logger.go:43] calling getorder
i1002 15:58:19.144033       1 conditions.go:154] setting lasttransitiontime for certificate ""cert-manager-webhook-webhook-tls"" condition ""ready"" to 2019-10-02 15:58:19.144027373 +0000 utc m=+86.444394730
i1002 15:58:19.145112       1 conditions.go:154] setting lasttransitiontime for certificate ""cert-manager-webhook-ca"" condition ""ready"" to 2019-10-02 15:58:19.145103359 +0000 utc m=+86.445470721
i1002 15:58:19.145593       1 base_controller.go:187] cert-manager/controller/certificates ""level""=0 ""msg""=""syncing item"" ""key""=""staging/api-certificate""
i1002 15:58:19.147411       1 issue.go:169] cert-manager/controller/certificates/certificates ""level""=0 ""msg""=""order is not in 'valid' state. waiting for order to transition before attempting to issue certificate."" ""related_resource_kind""=""order"" ""related_resource_name""=""api-certificate-3031097725"" ""related_resource_namespace""=""staging""
i1002 15:58:19.148059       1 base_controller.go:187] cert-manager/controller/issuers ""level""=0 ""msg""=""syncing item"" ""key""=""cert-manager/cert-manager-webhook-ca""
i1002 15:58:19.148099       1 base_controller.go:187] cert-manager/controller/ingress-shim ""level""=0 ""msg""=""syncing item"" ""key""=""staging/example-ingress""
i1002 15:58:19.148906       1 sync.go:71] cert-manager/controller/ingress-shim ""level""=0 ""msg""=""not syncing ingress resource as it does not contain a \""certmanager.k8s.io/issuer\"" or \""certmanager.k8s.io/cluster-issuer\"" annotation"" ""resource_kind""=""ingress"" ""resource_name""=""example-ingress"" ""resource_namespace""=""staging""
i1002 15:58:19.148925       1 base_controller.go:193] cert-manager/controller/ingress-shim ""level""=0 ""msg""=""finished processing work item"" ""key""=""staging/example-ingress""
i1002 15:58:19.148133       1 base_controller.go:187] cert-manager/controller/issuers ""level""=0 ""msg""=""syncing item"" ""key""=""cert-manager/cert-manager-webhook-selfsign""
i1002 15:58:19.148963       1 conditions.go:91] setting lasttransitiontime for issuer ""cert-manager-webhook-selfsign"" condition ""ready"" to 2019-10-02 15:58:19.148956891 +0000 utc m=+86.449324275
i1002 15:58:19.149567       1 setup.go:73] cert-manager/controller/issuers/setup ""level""=0 ""msg""=""signing ca verified"" ""related_resource_kind""=""secret"" ""related_resource_name""=""cert-manager-webhook-ca"" ""related_resource_namespace""=""cert-manager"" ""resource_kind""=""issuer"" ""resource_name""=""cert-manager-webhook-ca"" ""resource_namespace""=""cert-manager""
i1002 15:58:19.149759       1 conditions.go:91] setting lasttransitiontime for issuer ""cert-manager-webhook-ca"" condition ""ready"" to 2019-10-02 15:58:19.149752693 +0000 utc m=+86.450120071
i1002 15:58:19.148155       1 base_controller.go:187] cert-manager/controller/issuers ""level""=0 ""msg""=""syncing item"" ""key""=""default/letsencrypt-staging""
i1002 15:58:19.150457       1 setup.go:160] cert-manager/controller/issuers ""level""=0 ""msg""=""skipping re-verifying acme account as cached registration details look sufficient"" ""related_resource_kind""=""secret"" ""related_resource_name""=""letsencrypt-staging"" ""related_resource_namespace""=""default"" ""resource_kind""=""issuer"" ""resource_name""=""letsencrypt-staging"" ""resource_namespace""=""default""
i1002 15:58:19.148177       1 base_controller.go:187] cert-manager/controller/issuers ""level""=0 ""msg""=""syncing item"" ""key""=""staging/letsencrypt-staging-issuer""
i1002 15:58:19.148630       1 base_controller.go:193] cert-manager/controller/certificates ""level""=0 ""msg""=""finished processing work item"" ""key""=""staging/api-certificate""
i1002 15:58:19.150669       1 base_controller.go:193] cert-manager/controller/issuers ""level""=0 ""msg""=""finished processing work item"" ""key""=""default/letsencrypt-staging""
i1002 15:58:19.151696       1 setup.go:160] cert-manager/controller/issuers ""level""=0 ""msg""=""skipping re-verifying acme account as cached registration details look sufficient"" ""related_resource_kind""=""secret"" ""related_resource_name""=""letsencrypt-staging-secret-key"" ""related_resource_namespace""=""staging"" ""resource_kind""=""issuer"" ""resource_name""=""letsencrypt-staging-issuer"" ""resource_namespace""=""staging""
i1002 15:58:19.151975       1 base_controller.go:193] cert-manager/controller/issuers ""level""=0 ""msg""=""finished processing work item"" ""key""=""staging/letsencrypt-staging-issuer""
i1002 15:58:19.153763       1 base_controller.go:193] cert-manager/controller/certificates ""level""=0 ""msg""=""finished processing work item"" ""key""=""cert-manager/cert-manager-webhook-webhook-tls""
i1002 15:58:19.156512       1 base_controller.go:193] cert-manager/controller/certificates ""level""=0 ""msg""=""finished processing work item"" ""key""=""cert-manager/cert-manager-webhook-ca""
i1002 15:58:19.157047       1 base_controller.go:187] cert-manager/controller/certificates ""level""=0 ""msg""=""syncing item"" ""key""=""cert-manager/cert-manager-webhook-webhook-tls""
i1002 15:58:19.157659       1 base_controller.go:187] cert-manager/controller/certificates ""level""=0 ""msg""=""syncing item"" ""key""=""cert-manager/cert-manager-webhook-ca""
i1002 15:58:19.158671       1 base_controller.go:193] cert-manager/controller/certificates ""level""=0 ""msg""=""finished processing work item"" ""key""=""cert-manager/cert-manager-webhook-ca""
i1002 15:58:19.158827       1 base_controller.go:193] cert-manager/controller/certificates ""level""=0 ""msg""=""finished processing work item"" ""key""=""cert-manager/cert-manager-webhook-webhook-tls""
i1002 15:58:19.171562       1 base_controller.go:193] cert-manager/controller/issuers ""level""=0 ""msg""=""finished processing work item"" ""key""=""cert-manager/cert-manager-webhook-ca""
i1002 15:58:19.172759       1 base_controller.go:187] cert-manager/controller/issuers ""level""=0 ""msg""=""syncing item"" ""key""=""cert-manager/cert-manager-webhook-ca""
i1002 15:58:19.173387       1 setup.go:73] cert-manager/controller/issuers/setup ""level""=0 ""msg""=""signing ca verified"" ""related_resource_kind""=""secret"" ""related_resource_name""=""cert-manager-webhook-ca"" ""related_resource_namespace""=""cert-manager"" ""resource_kind""=""issuer"" ""resource_name""=""cert-manager-webhook-ca"" ""resource_namespace""=""cert-manager""
i1002 15:58:19.173465       1 base_controller.go:193] cert-manager/controller/issuers ""level""=0 ""msg""=""finished processing work item"" ""key""=""cert-manager/cert-manager-webhook-ca""
i1002 15:58:19.173562       1 base_controller.go:187] cert-manager/controller/certificates ""level""=0 ""msg""=""syncing item"" ""key""=""cert-manager/cert-manager-webhook-webhook-tls""
i1002 15:58:19.174168       1 sync.go:329] cert-manager/controller/certificates/certificates ""level""=0 ""msg""=""certificate scheduled for renewal"" ""duration_until_renewal""=""6905h41m20.825882558s"" ""related_resource_kind""=""secret"" ""related_resource_name""=""cert-manager-webhook-webhook-tls"" ""related_resource_namespace""=""cert-manager""
i1002 15:58:19.174487       1 base_controller.go:193] cert-manager/controller/certificates ""level""=0 ""msg""=""finished processing work item"" ""key""=""cert-manager/cert-manager-webhook-webhook-tls""
i1002 15:58:19.175092       1 base_controller.go:193] cert-manager/controller/issuers ""level""=0 ""msg""=""finished processing work item"" ""key""=""cert-manager/cert-manager-webhook-selfsign""
i1002 15:58:19.175489       1 base_controller.go:187] cert-manager/controller/issuers ""level""=0 ""msg""=""syncing item"" ""key""=""cert-manager/cert-manager-webhook-selfsign""
i1002 15:58:19.175743       1 base_controller.go:193] cert-manager/controller/issuers ""level""=0 ""msg""=""finished processing work item"" ""key""=""cert-manager/cert-manager-webhook-selfsign""
i1002 15:58:19.175978       1 base_controller.go:187] cert-manager/controller/certificates ""level""=0 ""msg""=""syncing item"" ""key""=""cert-manager/cert-manager-webhook-ca""
i1002 15:58:19.176791       1 sync.go:329] cert-manager/controller/certificates/certificates ""level""=0 ""msg""=""certificate scheduled for renewal"" ""duration_until_renewal""=""41945h41m15.823245228s"" ""related_resource_kind""=""secret"" ""related_resource_name""=""cert-manager-webhook-ca"" ""related_resource_namespace""=""cert-manager""
i1002 15:58:19.177118       1 base_controller.go:193] cert-manager/controller/certificates ""level""=0 ""msg""=""finished processing work item"" ""key""=""cert-manager/cert-manager-webhook-ca""
i1002 15:58:19.807942       1 base_controller.go:193] cert-manager/controller/orders ""level""=0 ""msg""=""finished processing work item"" ""key""=""staging/api-certificate-3031097725""


here is my configuration.

ingress

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: example-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  tls:
  - hosts:
    - test.x.example.com
    secretname: letsencrypt-staging-certificate-secret
  rules:
  - host: test.x.example.com
    http:
      paths:
      - path: /
        backend:
          servicename: example-frontend
          serviceport: 80


issuer

apiversion: certmanager.k8s.io/v1alpha1
kind: issuer
metadata:
  name: letsencrypt-staging-issuer
spec:
  acme:
    # the acme server url
    server: https://acme-v02.api.letsencrypt.org/directory
    # email address used for acme registration
    email: my-email@example.com
    # name of a secret used to store the acme account private key
    privatekeysecretref:
      name: letsencrypt-staging-secret-key
    # enable the http-01 challenge provider
    solvers:
    - http01: {}


certificate

apiversion: certmanager.k8s.io/v1alpha1
kind: certificate
metadata:
  name: test-x-example-com
spec:
  secretname: letsencrypt-staging-certificate-secret
  issuerref:
    name: letsencrypt-staging-issuer
    kind: issuer
  dnsnames:
    - test.x.example.com
  acme:
    config:
    - http01:
        ingressclass: nginx
      domains:
        - test.x.example.com


additional details: the secrets are in the staging namespace, like everything else except cert manager which is in the cert-manager namespace. the cluster is deployed on gke.

edit: i'm wondering if it's possible that i hit the limits of the production environment in let's encrypt and got blocked. is it possible to verify that somewhere?
",<kubernetes><google-cloud-platform><google-kubernetes-engine><lets-encrypt><cert-manager>,58216695,2,"i finally solved the issue mostly by editing the certificate configuration. i also switched from an issuer to a clusterissuer but that should not have any impact on this issue. i think the problem was acme verification.

here is my new clusterissuer:

apiversion: certmanager.k8s.io/v1alpha1
kind: clusterissuer
metadata:
  name: letsencrypt-staging-issuer
spec:
  acme:
    # the acme server url
    server: https://acme-v02.api.letsencrypt.org/directory
    # email address used for acme registration
    email: my-email@example.com
    # name of a secret used to store the acme account private key
    privatekeysecretref:
      name: letsencrypt-staging-secret-key
    # enable the http-01 challenge provider
    http01: {}


and, more importantly, the new certificate:

apiversion: certmanager.k8s.io/v1alpha1
kind: certificate
metadata:
  name: test-x-example-com
spec:
  secretname: letsencrypt-staging-certificate-secret
  issuerref:
    name: letsencrypt-staging-issuer
    kind: clusterissuer
  dnsnames:
    - test.x.example.com
  acme:
    config:
    - http01:
        ingressclass: nginx
      domains:
        - test.x.example.com

"
64720983,error converting yaml to json: did not find expected key - error in pipeline,"i am getting the below error in my deployment pipeline
error: yaml parse error on cnhsst/templates/deployment.yaml: error converting yaml to json: yaml: line 38: did not find expected key

the yml file corresponding to this error is below:
apiversion: apps/v1
kind: deployment
metadata:
  name: {{ template &quot;fullname&quot; . }}
  namespace: {{ .values.namespace }}
  labels:
    app: {{ template &quot;fullname&quot; . }}
    chart: &quot;{{ .chart.name }}-{{ .chart.version }}&quot;
    release: &quot;{{ .release.name }}&quot;
    heritage: &quot;{{ .release.service }}&quot;
spec:
  replicas: {{ .values.replicas }}
  selector:
    matchlabels:
      app: {{ template &quot;fullname&quot; . }}
      release: &quot;{{ .release.name }}&quot;
  # we dont need a large deployment history limit as helm keeps it's own
  # history
  revisionhistorylimit: 2
  template:
    metadata:
      namespace: {{ .values.namespace }}
      labels:
        app: {{ template &quot;fullname&quot; . }}
        release: &quot;{{ .release.name }}&quot;
      annotations:
        recreatepods: {{ randalphanum 8 | quote }}
    spec:
      containers:
      - name: {{ template &quot;fullname&quot; . }}
        image: {{ template &quot;docker-image&quot; . }}
        imagepullpolicy: always
        ports:
        # the port that our container listens for http requests on
        - containerport: {{ default 8000 .values.portoverride }}
          name: http
        
      {{- if .values.resources }}
        resources:
{{ toyaml .values.resources | indent 10 }}
      {{- end }}
      {{- if and (.values.livenessprobe) (.values.apipod)}}
        livenessprobe:
{{ toyaml .values.livenessprobe | indent 10 }}
      {{- end }}
      {{- if and (.values.readinessprobe) (.values.apipod)}}
        readinessprobe:
{{ toyaml .values.readinessprobe | indent 10 }}
      {{- end }}
      imagepullsecrets:
      - name: regcred
       securitycontext:
        runasnonroot: true
        runasuser: 5000
        runasgroup: 5000 
      affinity:
        podantiaffinity:
          preferredduringschedulingignoredduringexecution:
          - weight: 100
            podaffinityterm:
              labelselector:
                matchexpressions:
                - key: app
                  operator: in
                  values:
                  - {{ template &quot;fullname&quot; . }}
              topologykey: failure-domain.beta.kubernetes.io/zone

i am stuck with this issue for few hours. i have gone through numerous posts, tried online tools trying to figure out syntax errors, but unfortunately no luck. if anyone is able to point out the issue, that would be really great.
",<kubernetes><yaml><kubernetes-helm>,64722469,2,"you can see the mismatched indentation under regcred:
      imagepullsecrets:
      - name: regcred
      # &lt;-- indented &quot;-&quot;
      #vvv not indented
       securitycontext:
        runasnonroot: true

which, as luck would have it, is the 38th line in the output yaml
$ helm template --debug my-chart . 2&gt;&amp;1| sed -e '1,/^apiversion:/d' | sed -ne 38p
       securitycontext:

"
64720327,gke rpc health check with multiple ports and different protocols under the same backend service,"i am trying to spin up a third-party service that accepts connections in 4 different ports:
x-deployment.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: x-deployment
  labels:
    app: x
...
ports:
  - containerport: 8000 # httpgraphqlserver
  - containerport: 8001 # websocketserver
  - containerport: 8020 # jsonrpcserver
  - containerport: 8030 # httpindexingserver
livenessprobe:
  tcpsocket:
    port: 8020

x-service.yaml
apiversion: cloud.google.com/v1
kind: backendconfig
metadata:
  name: x-rpc-config
spec:
  healthcheck:
    checkintervalsec: 7
    timeoutsec: 3
    healthythreshold: 2
    unhealthythreshold: 2
    type: http2
    port: 8020
---
apiversion: v1
kind: service
metadata:
  name: x-service
  annotations:
    beta.cloud.google.com/backend-config: '{&quot;default&quot;: &quot;x-rpc-config&quot;}'
spec:
  selector:
    app: x
  ports:
    - name: graphql
      port: 8000
      targetport: 8000
    - name: subscription
      port: 8001
      targetport: 8001
    - name: indexing
      port: 8030
      targetport: 8030
    - name: jrpc
      port: 8020
      targetport: 8020
  type: nodeport

ingress.yaml
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: backend-ingress
  annotations:
    kubernetes.io/ingress.global-static-ip-name: backend-dev-ip-address
    networking.gke.io/managed-certificates: backend-certificate
spec:
  rules:
    - host: x.dev.domain.io
      http:
        paths:
          - path: /rpc
            backend:
              servicename: x-service
              serviceport: 8020
          - path: /idx
            backend:
              servicename: x-service
              serviceport: 8030
          - path: /ws
            backend:
              servicename: x-service
              serviceport: 8001
          - path: /*
            backend:
              servicename: x-service
              serviceport: 8000

by default the gke loadbalancer runs the health check on http:80, if i spin up the backend-service (x-service.yaml) without the backendconfig (x-rpc-config), it is able to detect only 2 healthy backend-services, both with http ports: 8000 and 8030). however the backend-services listen to ports: 8020 (rpc) and 8030 (ws) are not considered healthy. i believe it happens because of the protocol type, so i've created the backendconfig (x-rpc-config) to run a tpc health check instead, using http2 protocol for port 8020 - which is where the livenessprobe is pointing to.
the pods and services are created properly, but the load balancer still fails to detect them as healthy services. the console simply shows the following warning:

some backend services are in unhealthy state

the goal is to open up the port 8020 (rpc) but also keep the 8000 (http) working. is it possible? do i need another type of load balancer or it is just a config issue?
i could not find any example of healthcheck config for multiple ports with different protocols under the same service. it is probably an anti-pattern?
thanks in advance.
",<kubernetes><load-balancing><google-kubernetes-engine><rpc><kubernetes-health-check>,65005581,2,"solution
instead of using an ingress, which will launch a http/https load balancer on gcp by default, i've changed the service to work as a loadbalancer with a custom http2 health check config. by default this configuration will spin up a tcp load balancer on gcp. for instance:
apiversion: cloud.google.com/v1
kind: backendconfig
metadata:
  name: rpc-config
spec:
  healthcheck:
    checkintervalsec: 10
    timeoutsec: 3
    healthythreshold: 2
    unhealthythreshold: 2
    type: http2
    port: 8020
---
apiversion: v1
kind: service
metadata:
  name: x-service
  annotations:
    cloud.google.com/app-protocols: '{&quot;rpc-a&quot;:&quot;http2&quot;, &quot;rpc-b&quot;:&quot;http2&quot;, &quot;rpc-c&quot;:&quot;http2&quot;}'
    beta.cloud.google.com/backend-config: '{&quot;default&quot;: &quot;rpc-config&quot;}'
spec:
  selector:
    app: x-node
  ports:
    - name: rpc-a
      port: 5001
      protocol: tcp
      targetport: 5001
    - name: rpc-b
      port: 8020
      protocol: tcp
      targetport: 8020
    - name: rpc-c
      port: 8000
      protocol: tcp
      targetport: 8000
  type: loadbalancer

the next step is to enable the ssl for the tcp lb. i saw gcp has the ssl proxy lb, that might solve it. just need to figure out the proper configuration for that, i could not find it in their docs.
"
58106818,what's the easiest way to add tls to a kubernetes service?,"i have a simple web server exposed publicly on kubernetes on gke and a domain registered. i'm looking to add tls to this so it's accessible via https. i've heard a lot about using let's encrypt and ended up attempting this: https://github.com/jetstack/cert-manager/blob/master/docs/tutorials/acme/quick-start/index.rst but found it totally over-whelming. is there a simpler approach to using let's encrypt given that my deployment is just a single service and pod?

the config i'm using is:

apiversion: apps/v1
kind: deployment
metadata:
  name: web
  labels:
    app: web
spec:
  replicas: 1
  selector:
    matchlabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: web
        image: gcr.io/my-repo
        ports:
        - containerport: 8080
        livenessprobe:
          httpget:
            path: /healthz
            port: 8080
        readinessprobe:
          initialdelayseconds: 10
          httpget:
            path: /healthz
            port: 8080
---
apiversion: v1
kind: service
metadata:
  name: web-balancer-service
spec:
  ports:
  - port: 8080
    protocol: tcp
    targetport: 8080
  selector:
    run: web
  type: nodeport
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-ingress-app

spec:
  rules:
  - host: my.domain.com
    http:
      paths:
      - path: /*
        backend:
          servicename: web-balancer-service
          serviceport: 8080


========================================

edit: following @utku zdemir's suggestion i tried to codify those changes into yaml. i created the ip address with 

gcloud compute addresses create example-ip-address --global


and the certificate and provisioning with: https://gist.github.com/nickponline/ab74d3d179e21474551b7596c6478eea 

everything provisions correctly but when i inspect the managedcertificates with kubectl describe managedcertificates example-certificate is says 

spec:
  domains:
    app.domain.xyz
status:
  certificate name:    xxxxxxxxxxxxxxxxxx
  certificate status:  provisioning
  domain status:
    domain:  app.domain
    status:  failednotvisible
events:      &lt;none&gt;


i've waited 24 hours so assume that this isn't going to change. 
",<kubernetes><lets-encrypt><kubernetes-ingress>,58107275,2,"since you use the ingress controller of the gke itself, when you create an ingress resource, it triggers the creation of a load balancer resource in the google cloud platform. normally, ssl termination is a responsibility of the ingress controller, therefore that gcp load balancer is responsible of doing the ssl termination.

this means, cert-manager will not work for your case, since the certificates will live outside of your cluster, and the traffic will be already ssl terminated before coming in your cluster.

luckily, gcp has self-provisioned ssl (let's encrypt) support. to make use of that, yo need to follow the steps below:


go to the load balancing screen on gcp, switch to advanced view, and jump to the certificates tab (or simply click here).
create a new ssl certificate, with ""create google-managed certificate"" chosen. to the domain field, write down the exact domain you want the ssl certificate for. it should look like this:





go to the external ip addresses screen, and reserve a new static ip address. choose the type to be global (at the time of writing, gcp ingress controller only supports global ip addresses). should look like this:





take the static ip that you reserved (in this example it is 34.95.84.106)




go to your domain registrar, and add an a type record for your domain (the one in the ssl certificate) to point to the static ip you allocated. in this example, it would be my-app.example.com -&gt; 34.95.84.106.


finally, you will need to edit your ingress to put 2 annotations, so it will hint the google cloud's ingress controller to use the static ip you reserved, and the certificate you created. see the ingress example below:


apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-ingress-app
  annotations:
    ingress.gcp.kubernetes.io/pre-shared-cert: my-ssl-certificate # the name of the ssl certificate resource you created
    kubernetes.io/ingress.global-static-ip-name: my-static-ip # the name of the static ip resource you created
    kubernetes.io/ingress.allow-http: ""false"" # if you want to block plain http
spec:
  rules:
    - host: my-app.example.com
      http:
        paths:
          - path: /*
            backend:
              servicename: web-balancer-service
              serviceport: 8080


apply it, and verify that the changes are reflected by going to the load balancers screen on gcp.

important notes:


if there already is a gcp load balancer that is created by an ingress, the changes you do (annotations) on the ingress will not reflect to the existing load balancer. therefore, delete your existing ingress, make sure the existing load-balancer disappears, and create the ingress with correct annotations, so the load balancer will be configured correctly.
for the let's encrypt provisioning to work, your dns record should be in place. it checks the owner of the domain using dns before issuing the certificate. also, the initial provisioning can take quite some time (up to half an hour).

"
64770613,how to install helm 3 chart on air gapped system,"i am trying to install a helm chart on an air gapped system. the chart should be pulling an image from a private docker registry and deploy to the kubernetes cluster (both also on the air gapped system). my chart passed linting but when i try to install it, i keep seeing the error:
$ helm install my-mongodb . --debug
install.go:172: [debug] original chart version: &quot;&quot;
install.go:189: [debug] chart path: /opt/helm-charts/my-mongodb

error: unable to build kubernetes objects from release manifest: the server could not find the requested resource
helm.go:94: [debug] the server could not find the requested resource
unable to build kubernetes objects from release manifest
helm.sh/helm/v3/pkg/action.(*install).run
    /home/circleci/helm.sh/helm/pkg/action/install.go:257
main.runinstall
    /home/circleci/helm.sh/helm/cmd/helm/install.go:242
main.newinstallcmd.func2
    /home/circleci/helm.sh/helm/cmd/helm/install.go:120
github.com/spf13/cobra.(*command).execute
    /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:842
github.com/spf13/cobra.(*command).executec
    /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:950
github.com/spf13/cobra.(*command).execute
    /go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:887
main.main
    /home/circleci/helm.sh/helm/cmd/helm/helm.go:93
runtime.main
    /usr/local/go/src/runtime/proc.go:203
runtime.goexit
    /usr/local/go/src/runtime/asm_amd64.s:1373

helm is installed:
$ helm version
version.buildinfo{version:&quot;v3.3.4&quot;, gitcommit:&quot;a61ce5633af99708171414353ed49547cf05013d&quot;, gittreestate:&quot;clean&quot;, goversion:&quot;go1.14.9&quot;}

so is kubernetes:
$ kubectl version
client version: version.info{major:&quot;1&quot;, minor:&quot;5&quot;, gitversion:&quot;v1.5.2&quot;, gitcommit:&quot;269f928217957e7126dc87e6adfa82242bfe5b1e&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2017-06-06t13:19:52z&quot;, goversion:&quot;go1.7.6&quot;, compiler:&quot;gc&quot;, platform:&quot;linux/amd64&quot;}
server version: version.info{major:&quot;1&quot;, minor:&quot;5&quot;, gitversion:&quot;v1.5.2&quot;, gitcommit:&quot;269f928217957e7126dc87e6adfa82242bfe5b1e&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2017-06-06t13:19:52z&quot;, goversion:&quot;go1.7.6&quot;, compiler:&quot;gc&quot;, platform:&quot;linux/amd64&quot;}

$ kubectl config get-clusters
name
dev-cluster

$ kubectl get node -n kube-system
name          status    age
kube-node01   ready     14d
kube-node02   ready     14d
kube01        ready     14d

here is my helm chart.yaml:
apiversion: v2
name: my-mongodb
description: a helm chart for my mongodb kubernetes
type: application
version: 0.1.0
appversion: 5.0.0-1
annotations:
  category: database

and my values.yaml
replicacount: 1

image:
  repository: docker01.dev.local:5000/my-mongodb
  pullpolicy: always
  pullsecrets: 
     - name: regcred  
     
serviceaccount:
  create: true
  annotations: {}
  name: &quot;&quot;

podannotations: {}

podsecuritycontext: {}

securitycontext: {}

service:
  type: clusterip
  port: 80

ingress:
  enabled: false
  annotations: {}
  hosts:
    - host: chart-example.local
      paths: []
  tls: []

resources: {}

autoscaling:
  enabled: false
  minreplicas: 1
  maxreplicas: 100
  targetcpuutilizationpercentage: 80

nodeselector: {}

tolerations: []

affinity: {}

my docker image successfully runs in a docker container, so i'm fairly confident the image isn't the issue. what am i doing wrong?
edit: i am running the helm install command at: /opt/helm-charts/my-mongodb
this is the location of the charts.yaml file. the file structure is:
my-mongodb
 - chart.yaml
 - values.schema.json
 - values.yaml
 - charts
 - templates
   - deployment.yaml
   - _helpers.tpl
   - hpa.yaml
   - ingress.yaml
   - notes.txt
   - serviceaccount.yaml
   - service.yaml
   - tests
     - test-connection.yaml

this chart was created with the command: helm create my-mongodb and then i edited the charts.yaml, values.yaml and deployment.yaml files
here is the deployment.yaml file as well:
apiversion: apps/v1
kind: deployment
metadata:
  name: {{ include &quot;my-mongodb.fullname&quot; . }}
  labels:
    {{- include &quot;my-mongodb.labels&quot; . | nindent 4 }}
spec:
{{- if not .values.autoscaling.enabled }}
  replicas: {{ .values.replicacount }}
{{- end }}
  selector:
    matchlabels:
      {{- include &quot;my-mongodb.selectorlabels&quot; . | nindent 6 }}
  template:
    metadata:
    {{- with .values.podannotations }}
      annotations:
        {{- toyaml . | nindent 8 }}
    {{- end }}
      labels:
        {{- include &quot;my-mongodb.selectorlabels&quot; . | nindent 8 }}
    spec:
      {{- with .values.imagepullsecrets }}
      imagepullsecrets:
        {{- toyaml . | nindent 8 }}
      {{- end }}
      serviceaccountname: {{ include &quot;my-mongodb.serviceaccountname&quot; . }}
      securitycontext:
        {{- toyaml .values.podsecuritycontext | nindent 8 }}
      containers:
        - name: {{ .chart.name }}
          securitycontext:
            {{- toyaml .values.securitycontext | nindent 12 }}
          image: &quot;{{ .values.image.repository }}:{{ .values.image.tag | default .chart.appversion }}&quot;
          imagepullpolicy: {{ .values.image.pullpolicy }}
          ports:
            - name: http
              containerport: 8080
              protocol: tcp
            - name: mongodb
              containerport: 27017
              protocol: tcp
            - name: mongodbshardsvr
              containerport: 27018
              protocol: tcp
            - name: mongodbconfigsvr
              containerport: 27019
              protocol: tcp                                         
          livenessprobe:
            httpget:
              path: /
              port: http
          readinessprobe:
            httpget:
              path: /
              port: http
          resources:
            {{- toyaml .values.resources | nindent 12 }}
      {{- with .values.nodeselector }}
      nodeselector:
        {{- toyaml . | nindent 8 }}
      {{- end }}
      {{- with .values.affinity }}
      affinity:
        {{- toyaml . | nindent 8 }}
      {{- end }}
      {{- with .values.tolerations }}
      tolerations:
        {{- toyaml . | nindent 8 }}
      {{- end }}

",<kubernetes><kubernetes-helm>,64778178,2,"it is probably a version compatibility issue. helm v3.3.x is not compatible with kubernetes v1.5.2.
as per supported version skew:

when a new version of helm is released, it is compiled against a
particular minor version of kubernetes. for example, helm 3.0.0
interacts with kubernetes using the kubernetes 1.16.2 client, so it is
compatible with kubernetes 1.16.
|---------------------|--------------------------------------|
|      helm version   |     supported kubernetes versions    |
|---------------------|--------------------------------------|
|         3.4.x       |           1.19.x - 1.16.x            |
|         3.3.x       |           1.18.x - 1.15.x            |
|         3.2.x       |           1.18.x - 1.15.x            |
|---------------------|--------------------------------------|


more than that, you are using apiversion: apps/v1 for your deployment version. this api version was introduced on kubernetes v1.9.0.
"
57813603,nginx-ingress not forwarding to dashboard,"hello and thank you for taking the time to read my question.

first, i have an eks cluster setup to use public and private subnets.

i generated the cluster using cloudformation as described at https://docs.aws.amazon.com/eks/latest/userguide/getting-started-console.html#vpc-create

i then initialized helm by creating a service account for tiller via kubectl apply -f (below file):

---
apiversion: v1
kind: serviceaccount
metadata:
  name: tiller
  namespace: kube-system
---
apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrolebinding
metadata:
  name: tiller
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: cluster-admin
subjects:
  - kind: serviceaccount
    name: tiller
    namespace: kube-system


and then helm init --service-account=tiller
followed by helm repo update

i then used helm to install the nginx-ingress controller via:

helm install --name nginx-ingress \
        --namespace nginx-project \
        stable/nginx-ingress \
        -f nginx-ingress-values.yaml


where my nginx-ingress-values.yaml is:

controller:
  service:
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval: ""60""
      service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: ""true""
      service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: ""abc-us-west-2-elb-access-logs""
      service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix: ""vault-cluster/nginx""
      service.beta.kubernetes.io/aws-load-balancer-ssl-cert: ""arn:aws:acm:us-west-2:123456789:certificate/bb35b4c4-...""
      service.beta.kubernetes.io/aws-load-balancer-backend-protocol: ""http""
      service.beta.kubernetes.io/aws-load-balancer-ssl-ports: ""https""
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: ""60""


and so far everything looks great, i see the elb get created and hooked up to use acm for https

i then install kubernetes-dashboard via:

kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml


and i can access it via kubectl proxy

but when i add an ingress rule for dashboard via:
kubectl apply -f dashboard-ingress.yaml
where dashboard-ingress.yaml is:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: dashboard
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: ""https""
    nginx.ingress.kubernetes.io/force-ssl-redirect: ""false""
  namespace: kube-system
spec:
#  tls:
#  - hosts:
#    - abc.def.com
  rules:
  - host: abc.def.com
    http:
      paths:
      - path: /dashboard
        backend:
          servicename: kubernetes-dashboard
          serviceport: 8443


then when i try to go http://abc.def.com/ i get stuck in an infinite redirect loop.

same for https://abc.def.com/
and
http://abc.def.com/dashboard

i am new to kubernetes and very stuck on this one. any help would be greatly appreciated

update - 9/5/2019:
when i take out the tls block from the ingress.yaml i then get

to the nginx backend but http://abc.def.com forwards me to https://abc.def.com and i get a 502 bad gateway from openresty/1.15.8.1

when i then try to go to https://abc.def.com/dashboard

i get ""404 page not found"" which is a response from the nginx-ingress controller as i understand it.

update - 9/6/2019:
thanks so much to mk_sta for the answer below which helped me understand what i was missing.

for anyone reading this in the future, my nginx-ingress install via helm works as expected but my kubernetes-dashboard install was missing some key annotations. in the end i was able to configure helm to install the kubernetes-dashboard via:

helm install --name kubernetes-dashboard \
    --namespace kube-system \
    stable/kubernetes-dashboard \
    -f kubernetes-dashboard-values.yaml


where kubernetes-dashboard-values.yaml is:

ingress:
  enabled: true
  hosts: [abc.def.com]
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: ""https""
    nginx.ingress.kubernetes.io/force-ssl-redirect: ""false""
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  paths: [/dashboard(/|$)(.*)]


i can then access dashboard at http://abc.def.com/dashboard/ and https://abc.def.com/dashboard/

for some reason if i leave off the trailing slash it does not work however.

this is good enough for me at the moment.
",<kubernetes><kubernetes-ingress><nginx-ingress><kubernetes-dashboard>,57820317,2,"it seems to me that you've used wrong location path /dashboard within yours origin ingress configuration, even more the relevant k8s dashboard ui endpoint is exposed on 443 port by default across the corresponded k8s service resource, whenever you've not customized this setting. 

ports:
- port: 443
  protocol: tcp
  targetport: 8443


in order to get a proper path based routing, override existing parameters with the following arguments:

paths:
- path: /
  backend:
    servicename: kubernetes-dashboard
    serviceport: 443


once you've decided accessing k8s dashboard ui through indirect path holder url (https://abc.def.com/dashboard), you can manage applying rewrite rules in order to transparently change a part of the authentic url and transmit requests to the faithful target path. actually, nginx ingress controller adds this functionality via specific nginx.ingress.kubernetes.io/rewrite-target annotation:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: dashboard
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: ""https""
    nginx.ingress.kubernetes.io/force-ssl-redirect: ""false""
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  namespace: kube-system
spec:
#  tls:
#  - hosts:
#    - abc.def.com
  rules:
  - host: abc.def.com
    http:
      paths:
      - path: /dashboard(/|$)(.*)
        backend:
          servicename: kubernetes-dashboard
          serviceport: 443

"
57842857,problem setting nodeaffinity on headless service,"following this tutorial ( https://portworx.com/run-ha-kafka-azure-kubernetes-service/) to setup a kafka cluster on azure kubernetes service, i am running in an issue deploying the headless zookeeper service.  

running this yml with kubectl to deploy the zookeeper service, i get the error below.

apiversion: v1
kind: service
metadata:
  name: zk-headless
nodeaffinity:  labels:
    app: zk-headless
spec:
  ports:
  - port: 2888
    name: server
  - port: 3888
    name: leader-election
  clusterip: none
  selector:
    app: zk


error converting yaml to json: yaml: line 5: mapping values are not allowed in this context


my question is, how do i assign the nodeaffinity value?  should i use this:  

requiredduringschedulingignoredduringexecution:
        nodeselectorterms:

",<kubernetes><kubectl><azure-aks>,57843236,2,"kubernetes service objects do not have a nodeaffinity field, since service resources do not run on nodes. in fact, they don't ""run"" at all - they are rather rules for the networking.

the example provided on the website has a copy-paste error there. it should have been:

apiversion: v1
kind: service
metadata:
  name: zk-headless
  labels:
    app: zk-headless
spec:
  ports:
    - port: 2888
      name: server
    - port: 3888
      name: leader-election
  clusterip: none
  selector:
    app: zk

"
57849403,kubernetes deployment fails,"i have pod and service ymal files in my system. i want to run these two using kubectl create -f &lt;file&gt; and connect from outside browser to test connectivity.here what i have followed.

my pod :

apiversion: v1
kind: pod
metadata:
  name: client-nginx
  labels:
    component: web
spec:
  containers:
    - name: client
      image: nginx 
      ports:
        - containerport: 3000


my services file :

apiversion: v1
kind: service
metadata:
    name: client-nginx-port
spec:
  type: nodeport
  ports:
    - port: 3050
      targetport: 3000
      nodeport: 31616
  selector:
    component: web


i used kubectl create -f my_pod.yaml and then kubectl get pods shows my pod client-nginx

and then kubectl create -f my_service.yaml, no errors here and then shows all the services.

when i try to curl to service, it gives 


  curl: (7) failed to connect to 192.168.0.10 port 31616: connection refused.


kubectl get deployments doesnt show my pod. do i have to deploy it? i am a bit confused. if i use instructions given here, i can deploynginxsuccessfully and access from outside browsers.

i used instructions given here to test this.
",<nginx><kubernetes><kubectl><kube-controller-manager>,57849667,2,"try with this service:

apiversion: v1
kind: service
metadata:
    name: client-nginx-port
spec:
  type: nodeport
  ports:
    - port: 3050
      targetport: 80
      nodeport: 31616
  selector:
    component: web

"
64866349,trying to create a namespace in an aws eks cluster with kubectl - getting: error from server (forbidden): namespaces is forbidden,"i am trying to create a namespace in an aws eks cluster and keep getting an error.
i can do everything i want using the default namespace yet when i try to create a new namespace name i am forbidden.
it must be something that i have done incorrectly with the user &quot;thera-eks&quot;.
perhaps the role binding?
it looks like i gave the role access to everything since in the rules i gave it the * wildcard.
the command i use is -
kubectl create namespace ernie

the error i get is -
error from server (forbidden): namespaces is forbidden: user &quot;thera-eks&quot; cannot create resource &quot;namespaces&quot; in api group &quot;&quot; at the cluster scope

my role.yaml is:
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  name: full_access
rules:
- apigroups: [&quot;*&quot;]
  resources: [&quot;*&quot;]
  verbs: [&quot;*&quot;]

my rolebinding.yaml is:
apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: full_access_role_binding
subjects:
- kind: user
  name: thera-eks
  apigroup: rbac.authorization.k8s.io
roleref:
  kind: role
  name: full_access
  apigroup: rbac.authorization.k8s.io

the aws-auth config map is:
data:
  maproles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: arn:aws:iam::9967xxxxxxxx:role/eksctl-ops-nodegroup-linux-ng-sys-nodeinstancerole-346vjptoxi7l
      username: system:node:{{ec2privatednsname}}
    - groups:
      - eks-role
      - system:master
      rolearn: arn:aws:iam::9967xxxxxxxx:role/thera-eks
      username: thera-eks
  mapusers: |
    - userarn: arn:aws:iam::9967xxxxxxxx:user/test-ecr
    username: test-ecr
    groups:
    - eks-role

the aws iam permissions json for the role &quot;thera-eks&quot; is -
 {
    &quot;version&quot;: &quot;2012-10-17&quot;,
    &quot;statement&quot;: [
        {
            &quot;effect&quot;: &quot;allow&quot;,
            &quot;action&quot;: [
                &quot;ecr:batchgetimage&quot;,
                &quot;ecr:batchchecklayeravailability&quot;,
                &quot;ecr:completelayerupload&quot;,
                &quot;ecr:describeimages&quot;,
                &quot;ecr:describerepositories&quot;,
                &quot;ecr:getdownloadurlforlayer&quot;,
                &quot;ecr:initiatelayerupload&quot;,
                &quot;ecr:listimages&quot;,
                &quot;ecr:putimage&quot;,
                &quot;ecr:uploadlayerpart&quot;,
                &quot;ecr:getauthorizationtoken&quot;
            ],
            &quot;resource&quot;: &quot;*&quot;
        },
        {
            &quot;effect&quot;: &quot;allow&quot;,
            &quot;action&quot;: [
                &quot;eks:*&quot;,
                &quot;iam:listroles&quot;,
                &quot;sts:assumerole&quot;
            ],
            &quot;resource&quot;: &quot;*&quot;
        }
    ]
}

",<kubernetes><kubectl><amazon-eks>,64873405,2,"@mdaniel and @pekambaram are right but i would like to expand and back it up with the official docs for better understanding:

an rbac role or clusterrole contains rules that represent a set
of permissions. permissions are purely additive (there are no &quot;deny&quot;
rules).
a role always sets permissions within a particular namespace; when
you create a role, you have to specify the namespace it belongs in.
clusterrole, by contrast, is a non-namespaced resource. the
resources have different names (role and clusterrole) because a
kubernetes object always has to be either namespaced or not
namespaced; it can't be both.
clusterroles have several uses. you can use a clusterrole to:

define permissions on namespaced resources and be granted within individual namespace(s)

define permissions on namespaced resources and be granted across all namespaces

define permissions on cluster-scoped resources


if you want to define a role within a namespace, use a role; if you want to define a role cluster-wide, use a clusterrole.

you will also find an example of a clusterrole:
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  # &quot;namespace&quot; omitted since clusterroles are not namespaced
  name: secret-reader
rules:
- apigroups: [&quot;&quot;]
  #
  # at the http level, the name of the resource for accessing secret
  # objects is &quot;secrets&quot;
  resources: [&quot;secrets&quot;]
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]

and for a clusterrolebinding:
apiversion: rbac.authorization.k8s.io/v1
# this cluster role binding allows anyone in the &quot;manager&quot; group to read secrets in any namespace.
kind: clusterrolebinding
metadata:
  name: read-secrets-global
subjects:
- kind: group
  name: manager # name is case sensitive
  apigroup: rbac.authorization.k8s.io
roleref:
  kind: clusterrole
  name: secret-reader
  apigroup: rbac.authorization.k8s.io

the linked docs will show you all the necessary details with examples that would help understand and setup your rbac.
"
70306209,secret creation with secretproviderclass not working as aspected,"edit:
it was a config error, i was setting wrong kv name :/
as said in title i'm facing an issue with secret creation using secretproviderclass.
i've created my aks and my kv (and filled it) on azure. then i'll proceed to follow those steps using a user-assigned managed identity
but no secret resource get created and pods got stuck on creation with mount failure.
those are the steps i followed
az extension add --name aks-preview
az extension update --name aks-preview
az aks enable-addons --addons azure-keyvault-secrets-provider -g $resource_group -n $aks_cluster 
az aks update -g $resource_group -n $aks_cluster --enable-managed-identity --disable-secret-rotation
$aks_id = (az aks show -g $resource_group -n $aks_cluster --query identityprofile.kubeletidentity.clientid -o tsv)
az keyvault set-policy -n $azurekeyvault --secret-permissions get --spn $aks_id

the secretproviderclass manifest i'm using
apiversion: secrets-store.csi.x-k8s.io/v1
kind: secretproviderclass
metadata:
  name: azure-kvname
spec:
  provider: azure
  secretobjects:
  - secretname: akvsecrets
    type: opaque
    data:
    - objectname: azuresignalrconnectionstring 
      key: azuresignalrconnectionstring
    - objectname: blobstorageconnectionstring 
      key: blobstorageconnectionstring
    - objectname: sqlregistryconnectionstring 
      key: sqlregistryconnectionstring
    - objectname: tokensymmetrickey 
      key: tokensymmetrickey
  parameters:
    usevmmanagedidentity: &quot;true&quot;
    userassignedidentityid: xxx # vmss userassignedidentity
    keyvaultname: &quot;sampleaks001&quot; # the name of the keyvault
    objects:  |
      array:
        - |
          objectname: azuresignalrconnectionstring
          objecttype: secret
        - |
          objectname: blobstorageconnectionstring
          objecttype: secret
        - |
          objectname: sqlregistryconnectionstring
          objecttype: secret
        - |
          objectname: tokensymmetrickey
          objecttype: secret
    resourcegroup: sample # [required for version &lt; 0.0.4] the resource group of the keyvault
    subscriptionid: xxxx # [required for version &lt; 0.0.4] the subscription id of the keyvault
    tenantid: xxx # the tenant id of the keyvault  

and the deploy manifest
apiversion: apps/v1
kind: deployment
metadata:
  name: trm-api-test
  namespace: default
spec:
  replicas: 1
  selector:
    matchlabels:
      app: trm-api-test
  template:
    metadata:
      labels:
        app: trm-api-test
    spec:
      nodeselector:
        &quot;kubernetes.io/os&quot;: linux
      containers:
      - name: trm-api-test
        image: nginx
        resources:
          requests:
            cpu: 100m
            memory: 128mi
          limits:
            cpu: 250m
            memory: 256mi
        ports:
        - containerport: 80
        env:
        - name: azuresignalrconnectionstring
          valuefrom:
            secretkeyref:
              name: akvsecrets
              key: azuresignalrconnectionstring
        - name: tokensymmetrickey
          valuefrom:
            secretkeyref:
              name: akvsecrets
              key: tokensymmetrickey
        - name: blobstorageconnectionstring
          valuefrom:
            secretkeyref:
              name: akvsecrets
              key: blobstorageconnectionstring
        - name: sqlregistryconnectionstring
          valuefrom:
            secretkeyref:
              name: akvsecrets
              key: sqlregistryconnectionstring
        volumemounts:
        - name: secrets-store-inline
          mountpath: &quot;/mnt/secrets-store&quot;
          readonly: true
      volumes:
        - name: secrets-store-inline
          csi:
            driver: secrets-store.csi.k8s.io
            readonly: true
            volumeattributes:
              secretproviderclass: &quot;azure-kvname&quot;
---
apiversion: v1
kind: service
metadata:
  name: trm-api-service-test
  namespace: default
spec:
  type: clusterip
  selector:
    app: trm-api-test
  ports:
  - port: 80
    targetport: 80
    protocol: tcp

i'm sure i'm missing something, but can't understand what.
thanks in advance!
",<kubernetes><azure-aks><kubernetes-secrets>,70307033,2,"you are using the clientid, but it should be the objectid form the kubelet identity:
export kube_id=$(az aks show -g &lt;resource group&gt; -n &lt;aks cluster name&gt; --query identityprofile.kubeletidentity.objectid -o tsv)
export akv_id=$(az keyvault show -g &lt;resource group&gt; -n &lt;akv name&gt; --query id -o tsv)
az role assignment create --assignee $kube_id --role &quot;key vault secrets officer&quot; --scope $akv_id

this is a working secretproviderclass i am using (adjusted to your config):
apiversion: secrets-store.csi.x-k8s.io/v1
kind: secretproviderclass
metadata:
  name: azure-kvname
spec:
  provider: azure
  secretobjects:
  - data:
    - objectname: azuresignalrconnectionstring 
      key: azuresignalrconnectionstring
    - objectname: blobstorageconnectionstring 
      key: blobstorageconnectionstring
    - objectname: sqlregistryconnectionstring 
      key: sqlregistryconnectionstring
    - objectname: tokensymmetrickey 
      key: tokensymmetrickey
    secretname: akvsecrets
    type: opaque
  parameters:
    usepodidentity: &quot;false&quot;
    usevmmanagedidentity: &quot;true&quot;
    userassignedidentityid: xxx # kubelet client id ( nodepool managed idendity )
    keyvaultname: &quot;sampleaks001&quot; # the name of the keyvault
    tenantid: xxx # the tenant id of the keyvault  
    objects:  |
      array:
        - |
          objectname: azuresignalrconnectionstring
          objectalias: azuresignalrconnectionstring
          objecttype: secret
        - |
          objectname: blobstorageconnectionstring
          objectalias: blobstorageconnectionstring
          objecttype: secret
        - |
          objectname: sqlregistryconnectionstring
          objectalias: sqlregistryconnectionstring
          objecttype: secret
        - |
          objectname: tokensymmetrickey
          objectalias: tokensymmetrickey
          objecttype: secret

you can also check documentation here as you will find better examples as on the azure docs.
"
68272422,using nginx ingress with https not on 443,"how do you use an nginx ingress controller to route to an app using ssl that is not running on 443?    i found this post which seems to say it's not possible.
deployment:
apiversion: apps/v1
kind: deployment
metadata:
  name: test-deployment
spec:
  selector:
    matchlabels:
      app: foo
  replicas: 2 
  template:
    metadata:
      labels:
        app: foo
    spec:
      containers:
      - name: foo
        image: bar
        ports:
        - containerport: 3000

trying:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: foo-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: foo.com
  - http:
      paths:
      - path: /
        backend:
          servicename: foo
          serviceport: 3000

",<kubernetes><amazon-eks>,68273876,2,"nginx ingress controller is a layer 7 technology, it does host based (layer 7) routing and not on ports (layer 4). so, your clients are expected to connect using standard port 80/443.
so, your clients will simply connect to the https://example.com (port 443) and kubernetes ingress controller will redirect it to your https service on port 3000.
however, since your service is ssl enabled, you will have to use the proper annotations
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: foo-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;
spec:
rules:
- host: example.com
  http:
    paths:
    - backend:
        servicename: foo
        serviceport: 3000

"
68238075,"deployment in version ""v1"" cannot be handled as a deployment: no kind ""deployment"" is registered for version","apiversion: apps/v1
kind: deployment
metadata:
  name: mongodb-deployment
  labels:
    app: mongodb
spec:
  replicas: 1
  selector:
    matchlebels:
       app: mongodb
  template:
    metadata:
      lebels:
        app: mongodb
    spec:
     containers:
     - name: mongodb
       image: mongo
       ports:
       - containerport: 27017
       env:
       - name: mongo_initdb_root_usernae
         valuefrom:
           secretkeyref:
             name: mongodb-secret
             key: mongo-root-username
       - name: mongo_initdb_root_password
         valuefrom:
          secretkeyref:
            name: mongodb-secret
            key: mongo-root-password

enter image description here
",<kubernetes><cloud><kubectl><minikube>,68259374,2,"
there are multiple typos in the yaml you have provided in the question.
i have corrected them as following , use following yaml and check

apiversion: apps/v1
kind: deployment    #corrected typo deployment to deployment
metadata:
  name: mongodb-deployment
  labels:
    app: mongodb
spec:
  replicas: 1
  selector:
    matchlabels:            #corrected typo matchlebels to matchlabels
       app: mongodb
  template:
    metadata:
      labels:               #corrected typo lebels to labels
        app: mongodb
    spec:
     containers:
     - name: mongodb
       image: mongo
       ports:
       - containerport: 27017
       env:
       - name: mongo_initdb_root_usernae
         valuefrom:
           secretkeyref:                  #corrected typo secretkeyref to secretkeyref
             name: mongodb-secret
             key: mongo-root-username
       - name: mongo_initdb_root_password
         valuefrom:
          secretkeyref:                  #corrected typo secretkeyref to secretkeyref
            name: mongodb-secret
            key: mongo-root-password

"
56577998,env variables from configmap not available inside pod,"i am trying to pass env variable to my pod from configmap. i have the following setup.

i have a file test-config.txt with 2 env variables 

a_sample_env=b
c_sample_env=d


i create a configmap as follows:


  kubectl create configmap test-config --from-file test-config.txt


my pod definition is as follows:

apiversion: v1
kind: pod
metadata:
  name: test-pod
spec:
  containers:
  - name: mycontainer
    image: redis
    envfrom:
      - configmapref:
          name: test-config


but my application doesn't receive the 2 env variables in the test-config.txt file. i logged into the pod using kubectl exec and get empty values for the env variables.

root@test-pod:/data# echo $c_sample_env

root@test-pod:/data# echo $a_sample_env



can anybody point out why the environment variables are not available in the pod?
",<kubernetes><configuration><kubectl>,56578331,2,"you should create configmap as below

apiversion: v1
kind: configmap
metadata:
  name: special-config
  namespace: default
data:
  a_sample_env: b
  c_sample_env: d


if you create configmap using below command

kubectl create configmap test-config --from-file test-config.txt


then you can mount test-config as volume inside container. you will have to create a wrapper/launch script to export all k:v pair from that file as env variable during startup
"
68264425,how do i run create multiple container and run different command inside using k8s,"i have a kubernetes job, job.yaml :
---
apiversion: v1
kind: namespace
metadata:
  name: my-namespace
---
apiversion: batch/v1
kind: job
metadata:
  name: my-job
  namespace: my-namespace
spec:
  template:
    spec:
      containers:
      - name: my-container
        image: gcr.io/project-id/my-image:latest
        command: [&quot;sh&quot;, &quot;run-vpn-script.sh&quot;, &quot;/to/download/this&quot;] # need to run this multiple times
        securitycontext:
          privileged: true
          allowprivilegeescalation: true
      restartpolicy: never

i need to run command for different parameters. i have like 30 parameters to run. i'm not sure what is the best solution here. i'm thinking to create container in a loop to run all parameters. how can i do this? i want to run the commands or containers all simultaneously.
",<docker><kubernetes><google-kubernetes-engine>,68270168,2,"some of the ways that you could do it outside of the solutions proposed in other answers are following:

with a templating tool like helm where you would template the exact specification of your workload and then iterate over it with different values (see the example)
use the kubernetes official documentation on work queue topics:

indexed job for parallel processing with static work assignment - alpha
parallel processing using expansions




helm example:
helm in short is a templating tool that will allow you to template your manifests (yaml files). by that you could have multiple instances of jobs with different name and a different command.
assuming that you've installed helm by following guide:

helm.sh: docs: intro: install

you can create an example chart that you will modify to run your jobs:

helm create chart-name

you will need to delete everything that is in the chart-name/templates/ and clear the chart-name/values.yaml file.
after that you can create your values.yaml file which you will iterate upon:
jobs:
  - name: job1
    command: ['&quot;perl&quot;,  &quot;-mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(3)&quot;']
    image: perl
  - name: job2
    command: ['&quot;perl&quot;,  &quot;-mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(20)&quot;']
    image: perl


templates/job.yaml

{{- range $jobs := .values.jobs }}
apiversion: batch/v1
kind: job
metadata:
  name: {{ $jobs.name }}
  namespace: default # &lt;-- for example purposes only!
spec:
  template:
    spec:
      containers:
      - name: my-container
        image: {{ $jobs.image }}
        command: {{ $jobs.command }}
        securitycontext:
          privileged: true
          allowprivilegeescalation: true
      restartpolicy: never
---
{{- end }}

if you have above files created you can run following command on what will be applied to the cluster beforehand:

$ helm template . (inside the chart-name folder)

---
# source: chart-name/templates/job.yaml
apiversion: batch/v1
kind: job
metadata:
  name: job1
  namespace: default
spec:
  template:
    spec:
      containers:
      - name: my-container
        image: perl
        command: [&quot;perl&quot;,  &quot;-mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(3)&quot;]
        securitycontext:
          privileged: true
          allowprivilegeescalation: true
      restartpolicy: never
---
# source: chart-name/templates/job.yaml
apiversion: batch/v1
kind: job
metadata:
  name: job2
  namespace: default
spec:
  template:
    spec:
      containers:
      - name: my-container
        image: perl
        command: [&quot;perl&quot;,  &quot;-mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(20)&quot;]
        securitycontext:
          privileged: true
          allowprivilegeescalation: true
      restartpolicy: never


a side note #1!
this example will create x amount of jobs where each one will be separate from the other. please refer to the documentation on data persistency if the files that are downloaded are needed to be stored persistently (example: gke).


a side note #2!
you can also add your namespace definition in the templates (templates/namespace.yaml) so it will be created before running your jobs.

you can also run above chart by:

$ helm install chart-name . (inside the chart-name folder)

after that you should be seeing 2 jobs that are completed:

$ kubectl get pods

name         ready   status      restarts   age
job1-2dcw5   0/1     completed   0          82s
job2-9cv9k   0/1     completed   0          82s

and the output that they've created:

$ echo &quot;one:&quot;; kubectl logs job1-2dcw5; echo &quot;two:&quot;; kubectl logs job2-9cv9k

one:
3.14
two:
3.1415926535897932385


additional resources:

stackoverflow.com: questions: kubernetes creation of multiple deployment with one deployment file

"
56785662,best practice deploying kubernetes deployment to different environments and handling buildnumbers in the config,"i have a deployment config lying with each microservice. this looks something like this:

apiversion: extensions/v1beta1
kind: deployment
metadata:
  creationtimestamp: null
  labels:
    io.kompose.service: service_x
  name: service_x
spec:
  replicas: 2
  strategy: {}
  template:
    metadata:
      creationtimestamp: null
      labels:
        io.kompose.service: service_x
    spec:
      containers:
      - env:
        - name: flask_env
          value: ""production""
        image: somewhere/service_x:master_179
        name: service_x
        ports:
        - containerport: 80
        resources: {}
        volumemounts:
          - mountpath: /app/service_x/config/deployed
            name: volume-service_xproduction
      restartpolicy: always
      volumes:
        - name: volume-service_xproduction
          configmap:
            name: service_xproduction
            items:
              - key: production.py
                path: production.py


we have the following environments dev, stage, production. as you can see the image parameter contains the service, branch and build number. i have several ideas to make this dynamic and being able to deploy for example service_x:development_190 in the dev environment and a different build on stage. but before i'm starting to - maybe - inventing the wheel new i wonder how other people solving this challenge... 
btw. we use circleci to build the docker images.

my question here is now; whats the best practice to deploy builds in different environments?


building the deployment.yml for each build?
using variables/templates?
any other solutions i'm not aware of?
maybe it's not the best idea to have the kubernetes files lying with the microservice?

",<amazon-web-services><kubernetes><continuous-deployment><circleci><amazon-eks>,56787007,2,"there are a lot of ways to do what you want to do like helm charts, updating templates etc.

what i do is, structure the code like this:

 .git
 .gitignore
 .gitlab-ci.yml
 license
 makefile
 readme.md
 src
    dockerfile
    index.html
 templates
     autoscaler.yml
     deployment.yml
     ingress.yml
     sa.yml
     sm.yml
     svc.yml


the kubernetes template files will have something like:

apiversion: apps/v1
kind: deployment
metadata:
  name: app-deployment
  namespace: __namespace__
  labels:
    app: app
    environment: __ci_commit_ref_name__
    commit: __ci_commit_short_sha__
spec:
  replicas: 1
  selector:
    matchlabels:
      app: app
  template:
    metadata:
      labels:
        app: app
        environment: __ci_commit_ref_name__
        commit: __ci_commit_short_sha__
      annotations:
        ""cluster-autoscaler.kubernetes.io/safe-to-evict"": ""true""
    spec:
      containers:
        - name: app
          image: &lt;registry&gt;/app:__ci_commit_short_sha__
          ports:
            - containerport: 80


so this template won't change as long as you change the src.

then in the circleci configuration, you can have steps to update the template before applying:

- sed -i ""s/__namespace__/${ci_commit_ref_name}/"" deployment.yml service.yml
- sed -i ""s/__ci_commit_short_sha__/${ci_commit_short_sha}/"" deployment.yml service.yml
- sed -i ""s/__ci_commit_ref_name__/${ci_commit_ref_name}/"" deployment.yml service.yml
- kubectl apply -f deployment.yml
- kubectl apply -f service.yml


the variables will be available to you or set in circleci.
"
56721497,kubernetes-ingress: how do i properly route to two services with https?,"i'm trying to deploy a reactjs app and an express-graphql server through kubernetes. but i'm having trouble setting up an ingress to route traffic to both services. specifically i can no longer reach my back-end.

when i made the react front-end and express back-end as separate services and exposed them, it ran fine. but now i'm trying to enable https and dns. and route to both of them through ingress.

here are my service yaml files

apiversion: v1
kind: service
metadata:
  name: bpmclient
  namespace: default
spec:
  ports:
  - port: 80
    protocol: tcp
    targetport: 5000
  selector:
    run: bpmclient
  type: nodeport


apiversion: v1
kind: service
metadata:
  name: bpmserver
  namespace: default
spec:
  ports:
  - port: 3090
    protocol: tcp
    targetport: 3090
  selector:
    run: bpmserver
  type: nodeport


and my ingress...

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: bpm-nginx
  annotations:
    kubernetes.io/ingress.global-static-ip-name: bpm-ip
    networking.gke.io/managed-certificates: bpmclient-cert
    ingress.kubernetes.io/enable-cors: ""true""
    ingress.kubernetes.io/cors-allow-origin: ""https://example.com""
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: /v2/*
        backend:
          servicename: bpmserver
          serviceport: 3090
      - path: /*
        backend:
          servicename: bpmclient
          serviceport: 80



through this setup i've been able to visit the client successfully using https. but i can't reach my back-end anymore through the client or just browsing to it. i'm getting a 502 server error. but i check the logs for the back-end pod and don't see anything besides 404 logs.

my front-end is reaching the back-end through example.com/v2/graphql. when i run it locally on my machine i go to localhost:3090/graphql. so i don't see why i'm getting a 404 if the routing is done correctly.
",<kubernetes><kubernetes-ingress>,56752351,2,"i see few things that might be wrong here:


ingress objects should be created in the same namespace as the services it routes. i see that you have specified namespace: default in your services' yamls but not in ingress.
i don't know which version of ingress you are using but accorind to the documentation after 0.22.0



  ingress definitions using the annotation
  nginx.ingress.kubernetes.io/rewrite-target are not backwards
  compatible with previous versions. in version 0.22.0 and beyond, any
  substrings within the request uri that need to be passed to the
  rewritten path must explicitly be defined in a capture group.



path: should be nested after backend: and capture group should be added to the nginx.ingress.kubernetes.io/rewrite-target: / in numered placeholder like $1


so you should try something like this:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: bpm-nginx
  namespace: default
  annotations:
    kubernetes.io/ingress.global-static-ip-name: bpm-ip
    networking.gke.io/managed-certificates: bpmclient-cert
    ingress.kubernetes.io/enable-cors: ""true""
    ingress.kubernetes.io/cors-allow-origin: ""https://example.com""
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          servicename: bpmserver
          serviceport: 3090
        path: /v2/?(.*)
      - backend:
          servicename: bpmclient
          serviceport: 80
        path: /?(.*)


please let me know if that helped.
"
56700699,how to write a custom ingressgateway in istio?,"i'm new to istio, i have a simple test yaml file which is a little long. what i want to do is to write a custom ingressgateway service for my gateway. and after testing, the incorrect part is the definition of ingressgateway which is at the top. the entire yaml is below:

apiversion: v1
kind: service
metadata:
  name: batman-ingressgateway
  labels:
    app: batman-ingressgateway
spec:
  type: loadbalancer
  selector:
    app: batman-ingressgateway
  ports:
  - port: 80
    targetport: 80
    nodeport: 31389
    name: http
---
apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: batman-gateway
spec:
  selector:
    app: batman-ingressgateway
      #istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: http
    hosts:
    - ""*""
---
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: batman
spec:
  hosts:
  - ""*""
  gateways:
  - batman-gateway
  http:
    - match:
      route:
      - destination:
          host: batman
          port:
            number: 8000
          subset: v1
        weight: 80
      - destination:
          host: batman
          port:
            number: 8000
          subset: v2
        weight: 20
---
apiversion: networking.istio.io/v1alpha3
kind: destinationrule
metadata:
  name: batman-destination
spec:
  host: batman
  subsets:
  - name: v1
    labels:
      version: v1
      run: batman
  - name: v2
    labels:
      version: v2
      run: batman


i want to access my app from browser with the address like: http://my_host_ip:31389/article. the problem now is the ingressgateway doesn't route traffic to my gateway. is there any one can help me? 
thanks.
",<kubernetes><kubernetes-ingress><istio>,56821177,2,"documentation on istio gateway routing is here  https://istio.io/docs/tasks/traffic-management/ingress/ingress-control/.
if you look at gateway spec they have
selector:
    istio: ingressgateway # use istio default gateway implementation

while you have
selector:
    app: batman-ingressgateway
      #istio: ingressgateway

for virtualservice definition you can look here https://istio.io/docs/reference/config/networking/v1alpha3/virtual-service/
you can try with routing requests to /article to your service
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: article-route
spec:
  hosts:
  - *
  http:
  - match:
    - uri:
        prefix: &quot;/article&quot;
    route:
    - destination:
        host: &lt;name of your service&gt;

"
56374417,why do my pods not fulfill the resource quota limits if the number match?,"after applying the following resourcequota compute-resources to my gke cluster

apiversion: v1
kind: resourcequota
metadata:
  name: compute-resources
spec:
  hard:
    limits.cpu: ""1""
    limits.memory: 1gi


and updating a deployment to

apiversion: apps/v1
kind: deployment
metadata:
  name: my-service
spec:
  selector:
    matchlabels:
      app: my-service
      tier: backend
      track: stable
  replicas: 2
  strategy:
    type: rollingupdate
    rollingupdate:
      maxsurge: 1
      maxunavailable: 50%
  template:
    metadata:
      labels:
        app: my-service
        tier: backend
        track: stable
    spec:
      containers:
        - name: my-service
          image: registry/namespace/my-service:latest
          ports:
            - name: http
              containerport: 8080
          resources:
            requests:
              memory: ""128mi""
              cpu: ""125m""
            limits:
              memory: ""256mi""
              cpu: ""125m""


the scheduling fails 100% of tries due to pods ""my-service-5bc4c68df6-4z8wp"" is forbidden: failed quota: compute-resources: must specify limits.cpu,limits.memory. since limits and requests are specified and they fulfill the limit, i don't see a reason why the pods should be forbidden.

how pod limits resource on kubernetes enforced when the pod exceed limits after pods is created ? is a different question.

i upgraded my cluster to  1.13.6-gke.0.
",<memory><kubernetes><cpu><google-kubernetes-engine><quota>,56378680,2,"i was about to suggest to test within separate namespace, but see that you already tried.

as another workaround try to setup default limits by enabling limitranger admission controller and setting it up e.g.

apiversion: v1
kind: limitrange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      memory: 256mi
      cpu: 125m
    defaultrequest:
      cpu: 125m
      memory: 128mi
    type: container


now if a container is created in the default namespace, and the container does not specify its own values for cpu request and cpu limit, the container is given a default cpu limits of 125m and a default memory limit of 256mi

also, after setting up limitrange, make sure you removed your deployment and there are no pods stuck in failed state. 
"
77656452,is there a way to reference a kubernetes secret in a kong plugin yaml file?,"i have a kong introspection plugin and would like the introspection url to pull the data from a kubernetes secret. is this possible?


apiversion: configuration.konghq.com/v1
kind: kongplugin
metadata:
  name: oauth2-introspection
  namespace: app
  annotations:
    kubernetes.io/ingress.class: kong
consumerref:
plugin: oauth2-introspection
config:
  introspection_url: **&lt;k8-secret&gt;**



",<kubernetes><kong><kubernetes-secrets><kong-plugin>,77685285,2,"kong ingress controller allows you to configure plugins using the contents of a kubernetes secret. the configfrom field in the kongplugin resource allows you to set a secretkeyref pointing to a kubernetes secret.
this only works for a complete configuration. you can not configure individual fields.
this kongplugin definition points to a secret named rate-limit-redis that contains a complete configuration for the plugin:
echo &quot;
apiversion: configuration.konghq.com/v1
kind: kongplugin
metadata:
 name: rate-limiting-example
plugin: rate-limiting
configfrom:
  secretkeyref:
    name: rate-limit-redis
    key: config
&quot; | kubectl apply -f -

the rate-limit-redis secret contains a complete configuration as a string:
echo &quot;
apiversion: v1
kind: secret
metadata:
  name: rate-limit-redis
stringdata:
  config: |
    minute: 10
    policy: redis
    redis_host: redis-master
    redis_password: password
type: opaque
&quot; | kubectl apply -f -

kic will resolve the secrets, build a complete configuration object and send it to kong gateway
"
67621165,how to override configure the environment in kubernetes cluster?,"i have a spring boot application, and i created a configmap of my application.proporties. i have some passwords and other sensible date in this file and i would to override some infos there. how can i do for example to override the elasticsearch.url? i would like to override via command line, in the same way to override the values.yaml, is it possible to do this?
kind: configmap
apiversion: v1
metadata:
  name: application
data:
  application.properties: |-
    server.port = 8080
    elasticsearch.baseuri = url

",<spring><spring-boot><kubernetes><azure-devops><kubernetes-helm>,67622582,2,"if this configmap is part of your chart your can just put your secret stuff inside {{ .values.secretstuff }} and then override it with your helm install command.
example configmap:
kind: configmap
apiversion: v1
metadata:
  name: application
data:
  application.properties: |-
    server.port = 8080
    elasticsearch.baseuri = {{ .values.elasticsearch.baseuri}}

and your helm install command will be
helm install chart ./mychart --set elasticsearch.baseuri=some-super-secret-stuff
"
76996807,create ingress deployment with minikube,"i'm going through this tutorial to create a deployment and an ingress controller on minikube. when i run, as instructed in the tutorial, curl --resolve &quot;hello-world.info:80:$( minikube ip )&quot; -i http://hello-world.info i simply get http/1.1 503 service temporarily unavailable.
here are my deployment, service, ingress and kustomization files:
apiversion: apps/v1
kind: deployment
metadata:
  name: example-deployment
  labels:
    app: example
spec:
  replicas: 2
  selector:
    matchlabels:
      app: example
  template:
    metadata:
      labels:
        app: example
    spec:
      containers:
      - name: nginx
        image: localhost:5000/example-app:latest
        ports:
        - name: web
          containerport: 80

---

apiversion: v1
kind: service
metadata:
  name: example
spec:
  ports:
    - name: web
      port: 80
      targetport: web
  selector:
    app.kubernetes.io/name: example

---

apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
  - host: hello-world.info
    http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: example
            port:
              number: 80

---

apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
metadata:
  name: arbitrary

# example configuration for the webserver
# at https://github.com/monopole/hello
commonlabels:
  app: example

resources:
- deployment.yaml
- service.yaml
- ingress.yaml



here is the output from kubectl get ingress:
name              class   hosts              address        ports   age
example-ingress   nginx   hello-world.info   192.168.49.2   80      3m29s

here is the output from kubectl get services:
kubectl get services
name         type        cluster-ip     external-ip   port(s)   age
example      clusterip   10.109.63.32   &lt;none&gt;        80/tcp    10m

so as you can see everything is up and running. here is my nginx:
[jenia@archlinux ibn-battuta]$ kubectl get pods -n ingress-nginx
name                                        ready   status      restarts   age
ingress-nginx-admission-create-fvj4v        0/1     completed   0          3h37m
ingress-nginx-admission-patch-d4xfn         0/1     completed   1          3h37m
ingress-nginx-controller-7799c6795f-7qgsr   1/1     running     0          3h37m

here are the logs of nginx 503:
192.168.49.1 - - [29/aug/2023:01:11:43 +0000] &quot;get / http/1.1&quot; 503 190 &quot;-&quot; &quot;curl/8.2.1&quot; 79 0.000 [default-example-80] [] - - - - e03c2a5e71db02f27f1afca0905c6c36

if anyone can help me, it'll be greatly appreciated.
",<kubernetes><kubernetes-ingress><minikube><nginx-ingress><kustomize>,76999323,2,"@jenia ivanov
targetport is incorrect in your example service
selector in example service doesnot match the label of your pod in your deployment yaml
could you update  targetport and selector in your service  as shown below and verify?
apiversion: v1
kind: service
metadata:
  name: example
spec:
  ports:
    - name: web
      port: 80
      targetport: 80
  selector:
    app: example

"
67599935,i cannot acces from my master kubernetes cluster to a pod,"if i have a set of deployments that are connected using a networkpolicy ingress. it's work! however, if i have to connect from outside (ip got from kubectl get ep), i have to set another ingress to the endpoint? or egress policy?
apiversion: apps/v1
kind: deployment
metadata:
  namespace: nginx
  annotations:
    kompose.cmd: ./kompose convert
    kompose.version: 1.22.0 (955b78124)
  creationtimestamp: null
  labels:
    io.kompose.service: nginx
  name: nginx
spec:
  replicas: 1
  selector:
    matchlabels:
      io.kompose.service: nginx
  strategy: {}
  template:
    metadata:
      annotations:
        kompose.cmd: ./kompose convert
        kompose.version: 1.22.0 (955b78124)
      creationtimestamp: null
      labels:
        io.kompose.network/nginx: &quot;true&quot;
        io.kompose.service: nginx
    spec:
      containers:
        - image: nginx
          name: nginx
          ports:
            - containerport: 8000
          resources: {}
      restartpolicy: always
status: {}
---
apiversion: apps/v1
kind: deployment
metadata:
  namespace: mariadb
  annotations:
    kompose.cmd: ./kompose convert
    kompose.version: 1.22.0 (955b78124)
  creationtimestamp: null
  labels:
    io.kompose.service: mariadb
  name: mariadb
spec:
  replicas: 1
  selector:
    matchlabels:
      io.kompose.service: mariadb
  strategy: {}
  template:
    metadata:
      annotations:
        kompose.cmd: ./kompose convert
        kompose.version: 1.22.0 (955b78124)
      creationtimestamp: null
      labels:
        io.kompose.network/nginx: &quot;true&quot;
        io.kompose.service: mariadb
    spec:
      containers:
        - image: mariadb
          name: mariadb
          ports:
            - containerport: 5432
          resources: {}
      restartpolicy: always
status: {}
...

you can see more code here http://pastie.org/p/2qpnhjfdak9xj7syuzvgpf
endpoints:
kubectl get ep -n nginx
name       endpoints             age
mariadb  192.168.112.203:5432  2d2h
nginx     192.168.112.204:8000  42h

services:
name       type       cluster-ip     external-ip  port(s)         age
mariadb clusterip  10.99.76.78    &lt;none&gt;       5432/tcp        2d2h
nginx     nodeport   10.111.176.21  &lt;none&gt;       8000:31604/tcp  42h

tests from server:
if i do curl 10.111.176.21:31604 -- no answer
if i do curl 192.168.112.204:8000 -- no answer
if i do curl 192.168.112.204:31604 -- no answer
if i do curl 10.0.0.2:8000 or 31604 -- no answer
10.0.0.2 is a worker node ip.

updated if i do kubectl port-forward nginx-podxxx 8000:8000
i can access it from http://localhost:8000
so what's i am wrong in on?
",<kubernetes><nginx><kubectl>,67600021,2,"it looks like you're using the network policy as an ingress for incoming traffic, but what you probably want to be using is an ingress controller to manage ingress traffic.
egress is for traffic flowing outbound from your services within your cluster to external sources. ingress is for external traffic to be directed to specific services within your cluster.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: my-app-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: my-example.site.tld
      http:
        paths:
          - path: /
            backend:
              servicename: nginx
              serviceport: 5432

"
67573070,how to allow a non-root user to write to a mounted efs in eks,"i am having trouble configuring a statically provisioned efs such that multiple pods, which run as a non-root user, can read and write the file system.
i am using the aws efs csi driver. my version info is as follows:
client version: version.info{major:&quot;1&quot;, minor:&quot;18&quot;, gitversion:&quot;v1.18.18&quot;, gitcommit:&quot;6f6ce59dc8fefde25a3ba0ef0047f4ec6662ef24&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2021-04-15t03:31:30z&quot;, goversion:&quot;go1.13.15&quot;, compiler:&quot;gc&quot;, platform:&quot;linux/amd64&quot;}
server version: version.info{major:&quot;1&quot;, minor:&quot;18+&quot;, gitversion:&quot;v1.18.9-eks-d1db3c&quot;, gitcommit:&quot;d1db3c46e55f95d6a7d3e5578689371318f95ff9&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2020-10-20t22:53:22z&quot;, goversion:&quot;go1.13.15&quot;, compiler:&quot;gc&quot;, platform:&quot;linux/amd64&quot;}

i followed the example from the github repo (https://github.com/kubernetes-sigs/aws-efs-csi-driver/tree/master/examples/kubernetes/multiple_pods) updating the volumehandle appropriately. the busybox containers defined in the specs for the example are able to read and write the file system, but when i add the same pvc to a pod which does not run as the root user the pod is unable to write to the mounted efs.
i have tried a couple other things to get this working as i expected it to:

just applying the annotation described here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#access-control to the persistent volume definition.
applying the aforementioned annotation and including securitycontext.runasgroup to the pod definition (with the appropriate value)
applying the annotation, the runasgroup, as well as fsgroup for the pod

none of these configurations allowed a non-root user to write to the mounted efs.
what am i missing in terms of configuring a statically provisioned efs so that multiple pods, all of which run as a non-root user, can read and write in the mounted efs?
for reference here are the pod definitions:
apiversion: v1
kind: pod
metadata:
  name: app1
spec:
  containers:
  - name: app1
    image: busybox
    command: [&quot;/bin/sh&quot;]
    args: [&quot;-c&quot;, &quot;while true; do echo $(date -u) &gt;&gt; /data/out1.txt; sleep 5; done&quot;]
    volumemounts:
    - name: persistent-storage
      mountpath: /data
  volumes:
  - name: persistent-storage
    persistentvolumeclaim:
      claimname: efs-claim
---
apiversion: v1
kind: pod
metadata:
  name: app2
spec:
  containers:
  - name: app2
    image: busybox
    command: [&quot;/bin/sh&quot;]
    args: [&quot;-c&quot;, &quot;while true; do echo $(date -u) &gt;&gt; /data/out2.txt; sleep 5; done&quot;]
    volumemounts:
    - name: persistent-storage
      mountpath: /data
  volumes:
  - name: persistent-storage
    persistentvolumeclaim:
      claimname: efs-claim
---
apiversion: v1
kind: pod
metadata:
  name: app3
spec:
  containers:
  - name: app3
    image: busybox
    command: [&quot;/bin/sh&quot;]
    args: [&quot;-c&quot;, &quot;while true; do echo $(date -u) &gt;&gt; /data/out3.txt; sleep 5; done&quot;]
    volumemounts:
    - name: persistent-storage
      mountpath: /data
  securitycontext:
    runasuser: 1000
    runasgroup: 1337
    fsgroup: 1337
  volumes:
  - name: persistent-storage
    persistentvolumeclaim:
      claimname: efs-claim

and the sc/pvc/pv:
kind: storageclass
apiversion: storage.k8s.io/v1
metadata:
  name: efs-sc
provisioner: efs.csi.aws.com
---
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: efs-claim
spec:
  accessmodes:
    - readwritemany
  storageclassname: efs-sc
  resources:
    requests:
      storage: 5gi  
---
apiversion: v1
kind: persistentvolume
metadata:
  name: efs-pv
  annotations:
    pv.beta.kubernetes.io/gid: {{ .values.groupid | quote }}
spec:
  capacity:
    storage: 5gi
  volumemode: filesystem
  accessmodes:
    - readwritemany
  persistentvolumereclaimpolicy: retain
  storageclassname: efs-sc
  csi:
    driver: efs.csi.aws.com
    volumehandle: fs-asdf123

",<kubernetes><kubernetes-helm><amazon-eks><amazon-efs>,68597523,2,"i worked out two ways of resolving this and thought i should update this in case someone else runs into the same problem.
the first, probably better way is to just use a dynamically provisioned efs persistentvolume. this way creates an access point in efs that is shared by all containers that utilize the persistentvolumeclaim.
here is an example of the storageclass, persistentvolumeclaim, and a pod that utilizes the pvc.
kind: storageclass
apiversion: storage.k8s.io/v1
metadata:
  name: efs-sc
provisioner: efs.csi.aws.com
parameters:
  provisioningmode: efs-ap
  filesystemid:  {{ .values.efsvolumehandle }}
  directoryperms: &quot;775&quot;
reclaimpolicy: retain
---
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: efs-claim
spec:
  accessmodes:
    - readwritemany
  storageclassname: efs-sc
  resources:
    requests:
      storage: 5gi  # not actually used - see https://aws.amazon.com/blogs/containers/introducing-efs-csi-dynamic-provisioning/
---
apiversion: v1
kind: pod
metadata:
  name: app3
spec:
  containers:
  - name: app3
    image: busybox
    command: [&quot;/bin/sh&quot;]
    args: [&quot;-c&quot;, &quot;while true; do echo $(date -u) &gt;&gt; /data/out3.txt; sleep 5; done&quot;]
    volumemounts:
    - name: persistent-storage
      mountpath: /data
  securitycontext:
    runasuser: 1000
    runasgroup: 1337
    fsgroup: 1337
  volumes:
  - name: persistent-storage
    persistentvolumeclaim:
      claimname: efs-claim

note the directoryperms (775) specified in the storageclass, as well as the runasgroup and fsgroup specified in the pod. when utilizing this pvc in a pod that runs as a non-root user shared a user group number is the key.
runasuser is only specified to ensure the busybox stuff does not run as root

the second method is what i worked out initially and probably is the &quot;nuclear&quot; option, but does work for statically provisioned efs.
i have omitted the rest of the pod definition for the sake of brevity. you can use an initcontainer to ensure certain permissions are set on the mounted efs volume.
      initcontainers:
      - name: fs-permission-update
        image: busybox
        command:
        - chown
        - &quot;root:{{ .values.groupid }}&quot;
        - &quot;/efs-fs&quot;
        volumemounts:
        - mountpath: /efs-fs
          name: efs-storage

again make sure any pod which mounts the volume and runs as a non-root user uses the fsgroup and runasgroup to make sure the user is part of the allowed user group.

in summary, probably don't use statically provisioned efs, instead use dynamically provisioned efs. note that this is specific to the efs csi driver for kubernetes. check out the eks csi driver github for more examples and some additional details.
"
67637854,"ambassador service stays ""pending""","currently running a fresh &quot;all in one vm&quot; (stacked master/worker approach) kubernetes v1.21.1-00 on ubuntu server 20 lts, using

cri-o as container runtime interface
calico for networking/security

also installed the kubernetes-dashboard (but i guess that's not important for my issue ). taking this guide for installing ambassador: https://www.getambassador.io/docs/edge-stack/latest/topics/install/yaml-install/ i come along the issue that the service is stuck in status &quot;pending&quot;.
 kubectl get svc -n ambassador prints out the following stuff
name               type           cluster-ip       external-ip   port(s)                      age
ambassador         loadbalancer   10.97.117.249    &lt;pending&gt;     80:30925/tcp,443:32259/tcp   5h
ambassador-admin   clusterip      10.101.161.169   &lt;none&gt;        8877/tcp,8005/tcp            5h
ambassador-redis   clusterip      10.110.32.231    &lt;none&gt;        6379/tcp                     5h
quote              clusterip      10.104.150.137   &lt;none&gt;        80/tcp                       5h

while changing the type from loadbalancer to nodeport in the service sets it up correctly, i'm not sure of the implications coming along. again, i want to use ambassador as an ingress component here - with my setup (only one machine), &quot;real&quot; loadbalancing might not be necessary.
for covering all the subdomain stuff, i setup a wildcard recording for pointing to my machine, means i got a cname for *.k8s.my-domain.com which points to this host. don't know, if this approach was that smart for setting up an ingress.
edit: list of events, as requested below:
events:
  type    reason     age   from               message
  ----    ------     ----  ----               -------
  normal  scheduled  116s  default-scheduler  successfully assigned ambassador/ambassador-redis-584cd89b45-js5nw to dev-bvpl-099
  normal  pulled     116s  kubelet            container image &quot;redis:5.0.1&quot; already present on machine
  normal  created    116s  kubelet            created container redis
  normal  started    116s  kubelet            started container redis

additionally, here's the service pending in yaml presenation (exported via kubectl get svc -n ambassador -o yaml ambassador)
apiversion: v1
kind: service
metadata:
  annotations:
    a8r.io/bugs: https://github.com/datawire/ambassador/issues
    a8r.io/chat: http://a8r.io/slack
    a8r.io/dependencies: ambassador-redis.ambassador
    a8r.io/description: the ambassador edge stack goes beyond traditional api gateways
      and ingress controllers with the advanced edge features needed to support developer
      self-service and full-cycle development.
    a8r.io/documentation: https://www.getambassador.io/docs/edge-stack/latest/
    a8r.io/owner: ambassador labs
    a8r.io/repository: github.com/datawire/ambassador
    a8r.io/support: https://www.getambassador.io/about-us/support/
    kubectl.kubernetes.io/last-applied-configuration: |
      {&quot;apiversion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;service&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;a8r.io/bugs&quot;:&quot;https://github.com/datawire/ambassador/issues&quot;,&quot;a8r.io/chat&quot;:&quot;http://a8r.io/slack&quot;,&quot;a8r.io/dependencies&quot;:&quot;ambassador-redis.ambassador&quot;,&quot;a8r.io/description&quot;:&quot;the ambassador edge stack goes beyond traditional api gateways and ingress controllers with the advanced edge features needed to support developer self-service and full-cycle development.&quot;,&quot;a8r.io/documentation&quot;:&quot;https://www.getambassador.io/docs/edge-stack/latest/&quot;,&quot;a8r.io/owner&quot;:&quot;ambassador labs&quot;,&quot;a8r.io/repository&quot;:&quot;github.com/datawire/ambassador&quot;,&quot;a8r.io/support&quot;:&quot;https://www.getambassador.io/about-us/support/&quot;},&quot;labels&quot;:{&quot;app.kubernetes.io/component&quot;:&quot;ambassador-service&quot;,&quot;product&quot;:&quot;aes&quot;},&quot;name&quot;:&quot;ambassador&quot;,&quot;namespace&quot;:&quot;ambassador&quot;},&quot;spec&quot;:{&quot;ports&quot;:[{&quot;name&quot;:&quot;http&quot;,&quot;port&quot;:80,&quot;targetport&quot;:8080},{&quot;name&quot;:&quot;https&quot;,&quot;port&quot;:443,&quot;targetport&quot;:8443}],&quot;selector&quot;:{&quot;service&quot;:&quot;ambassador&quot;},&quot;type&quot;:&quot;loadbalancer&quot;}}
  creationtimestamp: &quot;2021-05-22t07:18:23z&quot;
  labels:
    app.kubernetes.io/component: ambassador-service
    product: aes
  name: ambassador
  namespace: ambassador
  resourceversion: &quot;4986406&quot;
  uid: 68e4582c-be6d-460c-909e-dfc0ad84ae7a
spec:
  clusterip: 10.107.194.191
  clusterips:
  - 10.107.194.191
  externaltrafficpolicy: cluster
  ipfamilies:
  - ipv4
  ipfamilypolicy: singlestack
  ports:
  - name: http
    nodeport: 32542
    port: 80
    protocol: tcp
    targetport: 8080
  - name: https
    nodeport: 32420
    port: 443
    protocol: tcp
    targetport: 8443
  selector:
    service: ambassador
  sessionaffinity: none
  type: loadbalancer
status:
  loadbalancer: {}

edit#2: i wonder, if https://stackoverflow.com/a/44112285/667183 applies for my process as well?
",<kubernetes><kubernetes-ingress><ambassador>,67712183,2,"answer is pretty much here: https://serverfault.com/questions/1064313/ambassador-service-stays-pending . after installing a load balancer the whole setup worked. i decided to go with metallb (https://metallb.universe.tf/installation/#installation-by-manifest for installation). i decided to go with the following configuration for a single-node kubernetes cluster:
apiversion: v1
kind: configmap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 10.16.0.99-10.16.0.99

after a few seconds the load balancer is detected and everything goes fine.
"
67668387,did not find expected key error when trying to limit memory,"i am trying to run a test container through the kubernetes but i got error. the below is my deploy.yaml:
apiversion: apps/v1
kind: deployment
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.22.0 (955b78124)
  creationtimestamp: null
  labels:
    io.kompose.service: nginx
  name: nginx
spec:
  containers:
      requests:
          storage: 2gi
          cpu: 0.5
          memory: &quot;128m&quot;
      limits:
          cpu: 0.5
          memory: &quot;128m&quot; # this is the line throws error
          - type: persistentvolumeclaim
          max:
            storage: 2gi
          min:
            storage: 1gi
.
.
.

when i run kubectl apply -k ., i get this error:
error: accumulating resources: accumulation err='accumulating resources from 'deploy.yaml': yaml: line 19: did not find expected key': got file 'deploy.yaml', but '/home/kus/deploy.yaml' must be a directory to be a root

i tried to read the kubernetes but i cannot understand why i'm getting error.
edit 1
i changed my deploy.yaml file as @whites11 said, now it's like:
.
.
.
spec:
  limits:
    memory: &quot;128mi&quot;
    cpu: 0.5
.
.
.

now i'm getting this error:
resourcequota/storagequota unchanged
service/nginx configured
error: error validating &quot;.&quot;: error validating data: validationerror(deployment.spec): unknown field &quot;limits&quot; in io.k8s.api.apps.v1.deploymentspec; if you choose to ignore these errors, turn validation off with --validate=false

",<kubernetes><kubectl>,67668529,2,"the file you shared is not valid yaml.
limits:
     cpu: 0.5
     memory: &quot;128m&quot; # this is the line throws error
     - type: persistentvolumeclaim
     max:
       storage: 2gi
     min:
       storage: 1gi

the limits field has mixed type (hash and collection) and is obviously wrong.
seems like you took the syntax for specifying pods resource limits and mixed it with the one used to limit storage consuption.
in your deployment spec you might want to set limits simply like this:
apiversion: apps/v1
kind: deployment
...
spec:
  ...
  template:
  ...
    spec:
      containers:
      - ...
        resources:
          limits:
            cpu: 0.5
            memory: &quot;128m&quot;

and deal with storage limits at the namespace level as described in the documentation.
"
67710199,argo workflows pods missing cpu/memory resources,"i'm running into a missing resources issue when submitting a workflow. the kubernetes namespace my-namespace has a quota enabled, and for whatever reason the pods being created after submitting the workflow are failing with:
pods &quot;hello&quot; is forbidden: failed quota: team: must specify limits.cpu,limits.memory,requests.cpu,requests.memory

i'm submitting the following workflow,
apiversion: &quot;argoproj.io/v1alpha1&quot;
kind: &quot;workflow&quot;
metadata:
  name: &quot;hello&quot;
  namespace: &quot;my-namespace&quot;
spec:
  entrypoint: &quot;main&quot;
  templates:
  - name: &quot;main&quot;
    container:
      image: &quot;docker/whalesay&quot;
      resources:
        requests:
          memory: 0
          cpu: 0
        limits:
          memory: &quot;128mi&quot;
          cpu: &quot;250m&quot;

argo is running on kubernetes 1.19.6 and was deployed with the official helm chart version 0.16.10. here are my helm values:
controller:
  workflownamespaces:
  - &quot;my-namespace&quot;
  resources:
    requests:
      memory: 0
      cpu: 0
    limits:
      memory: 500mi
      cpu: 0.5
  pdb:
    enabled: true
  # see https://argoproj.github.io/argo-workflows/workflow-executors/
  # docker container runtime is not present in the tkgi clusters
  containerruntimeexecutor: &quot;k8sapi&quot;
workflow:
  namespace: &quot;my-namespace&quot;
  serviceaccount:
    create: true
  rbac:
    create: true
server:
  replicas: 2
  secure: false
  resources:
    requests:
      memory: 0
      cpu: 0
    limits:
      memory: 500mi
      cpu: 0.5
  pdb:
    enabled: true
executer:
  resources:
    requests:
      memory: 0
      cpu: 0
    limits:
      memory: 500mi
      cpu: 0.5

any ideas on what i may be missing? thanks, weldon
update 1: i tried another namespace without quotas enabled and got past the missing resources issue. however i now see: failed to establish pod watch: timed out waiting for the condition. here's what the spec looks like for this pod. you can see the wait container is missing resources. this is the container causing the issue reported by this question.
spec:
  containers:
  - command:
    - argoexec
    - wait
    env:
    - name: argo_pod_name
      valuefrom:
        fieldref:
          apiversion: v1
          fieldpath: metadata.name
    - name: argo_container_runtime_executor
      value: k8sapi
    image: argoproj/argoexec:v2.12.5
    imagepullpolicy: ifnotpresent
    name: wait
    resources: {}
    terminationmessagepath: /dev/termination-log
    terminationmessagepolicy: file
    volumemounts:
    - mountpath: /argo/podmetadata
      name: podmetadata
    - mountpath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-v4jlb
      readonly: true
  - image: docker/whalesay
    imagepullpolicy: always
    name: main
    resources:
      limits:
        cpu: 250m
        memory: 128mi
      requests:
        cpu: &quot;0&quot;
        memory: &quot;0&quot;


",<kubernetes><kubernetes-helm><argo-workflows><argoproj>,67710525,2,"try deploying the workflow on another namespace if you can, and verify if it's working or not.
if you can try with removing the quota for respective namespace.
instead of quota you can also use the
apiversion: v1
kind: limitrange
metadata:
  name: default-limit-range
spec:
  limits:
  - default:
      memory: 512mi
      cpu: 250m
    defaultrequest:
      cpu: 50m
      memory: 64mi
    type: container

so any container have not resource request, limit mentioned that will get this default config of 50m cpu &amp; 64 mi memory.
https://kubernetes.io/docs/concepts/policy/limit-range/
"
68117948,"unable to create deployment in a namespace with service account, clusterrole and clusterrolebinding created","i was getting my hands dirty practicing the security k8s. this was a practice question i came across to solve.
question:
create serviceaccount 'john' with permissions to create delete get deployments, statefulsets, daemonsets in a given namespace 'hr' create clusterrole and clusterrolebindings required.
approach:
have tried creating sa and clusterrole and clusterrolebinding (binded the clusterrole with the sa created)
but when i checked it is giving a 'no'
kubectl auth can-i create deploy --as john -n hr

no

to create sa:
kubectl create sa john

to create clusterrole:
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  # &quot;namespace&quot; omitted since clusterroles are not namespaced
  name: hrcrole
rules:
- apigroups: [&quot;apps&quot;]
  #
  # at the http level, the name of the resource for accessing secret
  # objects is &quot;secrets&quot;
  resources: [&quot;deployments&quot;, &quot;statefulsets&quot;, &quot;daemonsets&quot;]
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;, &quot;delete&quot;]

to create clusterrolebinding:
apiversion: rbac.authorization.k8s.io/v1
# this cluster role binding allows anyone in the &quot;manager&quot; group to read secrets in any namespace.
kind: clusterrolebinding
metadata:
  name: hrcrolebind
subjects:
- kind: user
  name: hruser # name is case sensitive
  apigroup: rbac.authorization.k8s.io
roleref:
  kind: clusterrole
  name: hrcrole
  apigroup: rbac.authorization.k8s.io

i have also tried creating serviceaccount in the namespace, creating clusterrolebinding in namespace but still i get no. unfortunately i don't have a solution for this problem. appreciate any help here.
",<kubernetes><kubectl><kubernetes-security>,68118032,2,"you are trying to create a deployment:
kubectl auth can-i create deploy --as john -n hr

but you don't have the create verb allowed in the cluster role:
verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;, &quot;delete&quot;]

try recreating the cluster role like this:
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  # &quot;namespace&quot; omitted since clusterroles are not namespaced
  name: hrcrole
rules:
- apigroups: [&quot;apps&quot;]
  #
  # at the http level, the name of the resource for accessing secret
  # objects is &quot;secrets&quot;
  resources: [&quot;deployments&quot;, &quot;statefulsets&quot;, &quot;daemonsets&quot;]
  verbs: [&quot;create&quot;, &quot;get&quot;, &quot;watch&quot;, &quot;list&quot;, &quot;delete&quot;]

"
68105809,error converting yaml to json: yaml: line 13: did not find expected key,"i am trying to deploy this yaml to create a service for 6 different grok-exporters deployment and i have this error below whenever i try to deploy this manifest:
error converting yaml to json: yaml: line 13: did not find expected key and i can't figure out the issue 

the yaml manifest:
apiversion: v1
kind: service
metadata:
  name: grok-exporter-centralized-service
spec:
    type: clusterip
    ports:
    - name: cluster-autoscaler-grok-exporter-metrics
      protocol: tcp
      targetport: 9144
      port: 9148
    selector:
      app: cluster-autoscaler-grok-exporter
    - name: evicted-pods-grok-exporter
      protocol: tcp
      targetport: 9144
      port: 9149
    selector:
      app: evicted-pods-grok-exporter
    - name: flux-hr-grok-exporter
      protocol: tcp
      targetport: 9144
      port: 9144
    selector:
      app: flux-hr-grok-exporter
    - name: flux-system-grok-exporter
      protocol: tcp
      targetport: 9144
      port: 9145
    selector:
      app: flux-system-grok-exporter
    - name: ha-proxy-grok-exporter
      protocol: tcp
      targetport: 9144
      port: 9146
    selector:
      app: ha-proxy-grok-exporter
    - name: spot-requests-grok-exporter
      protocol: tcp
      targetport: 9144
      port: 9147
    selector:
      app: spot-requests-grok-exporter

i checked the similar issues here but unfortunately, i couldn't get this script working, so i really appreciate the assistance.
thanks all!
",<kubernetes><yaml><kubernetes-service>,68106230,2,"your issue is that you have some spec.selector.app after each spec.ports. there should be only one selector, the ports array should list all ports with no interruption
apiversion: v1
kind: service
metadata:
  name: grok-exporter-centralized-service
spec:
    type: clusterip
    ports:
    - name: cluster-autoscaler-grok-exporter-metrics
      protocol: tcp
      targetport: 9144
      port: 9148
    - name: evicted-pods-grok-exporter
      protocol: tcp
      targetport: 9144
      port: 9149
    - name: flux-hr-grok-exporter
      protocol: tcp
      targetport: 9144
      port: 9144
    - name: flux-system-grok-exporter
      protocol: tcp
      targetport: 9144
      port: 9145
    - name: ha-proxy-grok-exporter
      protocol: tcp
      targetport: 9144
      port: 9146
    - name: spot-requests-grok-exporter
      protocol: tcp
      targetport: 9144
      port: 9147
    selector:
      app: spot-requests-grok-exporter


second take: assuming that we want a single service pointing to several exporters, driven by distinct deployments. and that all those exporters would listen on the same port, 9144.
in addition to the current labels configured on your deployments, we would add a new one, shared between all those exporters / deployments. say foo=bar.
now, i can create a single &quot;exporter&quot; service, whose endpoints would target all my exporter ports:
apiversion: v1
kind: service
metadata:
  name: generic-exporter
spec:
    type: clusterip
    ports:
    - name: generic-exporter
      protocol: tcp
      targetport: 9144
      port: 9144
    selector:
      foo: bar

"
68197155,"adding an ssh github repository to argocd using declarative dsl gives ""authentication required""","i have an argocd installation and want to add a github repository using ssh access with an ssh key pair to it using the declarative dsl.
what i have is:
apiversion: v1
data:
  sshprivatekey: &lt;my private ssh key base64 encoded&gt;
  url: &lt;url base64 encoded&gt;
kind: secret
metadata:
  annotations:
    meta.helm.sh/release-name: argocd-config
    meta.helm.sh/release-namespace: argocd
  creationtimestamp: &quot;2021-06-30t12:39:35z&quot;
  labels:
    app.kubernetes.io/managed-by: helm
    argocd.argoproj.io/secret-type: repo-creds
  name: repo-creds
  namespace: argocd
  resourceversion: &quot;364936&quot;
  selflink: /api/v1/namespaces/argocd/secrets/repo-creds
  uid: 8ca64883-302b-4a41-aaf6-5277c34dfbfc
type: opaque
---
apiversion: v1
data:
  url: &lt;url base64 encoded&gt;
kind: secret
metadata:
  annotations:
    meta.helm.sh/release-name: argocd-config
    meta.helm.sh/release-namespace: argocd
  creationtimestamp: &quot;2021-06-30t12:39:35z&quot;
  labels:
    app.kubernetes.io/managed-by: helm
    argocd.argoproj.io/secret-type: repository
  name: argocd-repo
  namespace: argocd
  resourceversion: &quot;364935&quot;
  selflink: /api/v1/namespaces/argocd/secrets/argocd-repo
  uid: 09de56e0-3b0a-4032-8fb5-81b3a6e1899e
type: opaque


i can manually connect to  that github private repo using that ssh key pair, but using the dsl, the repo doesn't appear in the argocd gui.
in the log of the argocd-repo-server i am getting the error:
time=&quot;2021-06-30t14:48:25z&quot; level=error msg=&quot;finished unary call with code unknown&quot; error=&quot;authentication required&quot; grpc.code=unknown grpc.method=generatemanifest grpc.request.deadline=&quot;2021-06-30t14:49:25z&quot; grpc.service=repository.reposerverservice grpc.start_time=&quot;2021-06-30t14:48:25z&quot; grpc.time_ms=206.505 span.kind=server system=grpc


i deploy the secrets with helm.
so can anyone help me point in the right direction? what am i doing wrong?
i basically followed the declarative documentation under: https://argoproj.github.io/argo-cd/operator-manual/declarative-setup/
thanks in advance.
best regards,
rforberger
",<kubernetes><repository><kubernetes-helm><argocd>,68510138,2,"i am not sure about helm, since i am working with the yaml files for now, before moving into helm. you could take a look at this github issue here to configure ssh key for helm
i had this issue, when i was working with manifests. the repo config should be in argocd-cm configmap. the fix was this:
---
apiversion: v1
kind: configmap
metadata:
  name: argocd-cm
  namespace: argocd
  labels:
    app.kubernetes.io/name: argocd-cm
    app.kubernetes.io/part-of: argocd
data:
  repositories: |
    - name: my-test-repo
      url: ssh://git@repo-url/path/to/repo.git
      type: git
      insecure: true.                  // to skip verification
      insecureignorehostkey: true      // to ignore host key for ssh
      sshprivatekeysecret:
        name: private-repo-creds
        key: sshprivatekey
---
apiversion: v1
kind: secret
metadata:
  name: private-repo-creds
  namespace: argocd
  labels:
    argocd.argoproj.io/secret-type: repo-creds
data:
  sshprivatekey: &lt;my private ssh key base64 encoded&gt;

and i am not sure if the documentation is correct or not, because i can see the document in stable is a bit different, although both your link and this stable doc link are from the same version
"
55356694,connection issue between services in kubernetes,"i have three different images related to my application which works fine in docker-compose and has issues running on kubernetes cluster in gcp. 

below is the deployment file. 

apiversion: v1
kind: service
metadata:
  name: mysql
  labels:
    app: mysql-database
spec:
  type: nodeport
  ports:
    - port: 3306
      targetport: 3306      
  selector:
    app: mysql-database
    tier: database
---
apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: mysql
  labels:
    app: mysql-database
spec:
  strategy:
    type: recreate
  template:
    metadata:
      labels:
        app: mysql-database
        tier: database
    spec:
      hostname: mysql
      containers:
        - image: mysql/mysql-server:5.7
          name: mysql
          env:
            - name: ""mysql_user""
              value: ""root""
            - name: ""mysql_host""
              value: ""mysql""
            - name: ""mysql_database""
              value: ""xxxx""
            - name: ""mysql_port""
              value: ""3306""
            - name: ""mysql_password""
              value: ""password""
            - name: ""mysql_root_password""
              value: ""password""
            - name: ""rails_env""
              value: ""production""
          ports:
            - containerport: 5432
              name: db
---
apiversion: v1
kind: service
metadata:
  name: dgservice
  labels:
    app: dgservice
spec:
  type: nodeport
  ports:
    - port: 8080
      targetport: 8080
  selector:
    name: dgservice
    tier: dgservice
---
apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: dgservice
  labels:
    app: dgservice
spec:
  strategy:
    type: recreate
  template:
    metadata:
      labels:
        app: dgservice
        tier: dgservice
    spec:
      hostname: dgservice
      containers:
        - image: gcr.io/sample/sample-image:check_1
          name: dgservice
          ports:
            - containerport: 8080
              name: dgservice
---
apiversion: v1
kind: service
metadata:
  name: dg-ui
  labels:
    name: dg-ui
spec:
  type: nodeport
  ports:
    - nodeport: 30156 
      port: 8000
      protocol: tcp
      targetport: 8000
  selector:
    app: dg-ui
    tier: dg
---
apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: dg-ui
  labels:
    app: dg-ui
spec:
  strategy:
    type: recreate
  template:
    metadata:
      labels:
        app: dg-ui
        tier: dg
    spec:
      hostname: dg-ui
      containers:
        - image: gcr.io/sample/sample:latest
          name: dg-ui
          env:
            - name: ""mysql_user""
              value: ""root""
            - name: ""mysql_host""
              value: ""mysql""
            - name: ""mysql_database""
              value: ""xxxx""
            - name: ""mysql_port""
              value: ""3306""
            - name: ""mysql_password""
              value: ""password""
            - name: ""mysql_root_password""
              value: ""password""
            - name: ""rails_env""
              value: ""production""
            - name: ""dg_service_host""
              value: ""dgservice""
          ports:
            - containerport: 8000
              name: dg-ui


the image is being pulled successfully from gcr as well. 

the connection between mysql and ui service also works fine and my data's are getting migrated without any issues. but the connection is not established between the service and the ui. 

why ui is not able to access service in my application?
",<docker><kubernetes><google-kubernetes-engine>,55376789,2,"as your deployment has the following lables so service need to have same labels in order to create endpoint object 

endpoints are the api object behind a service. the endpoints are where a service will route connections to when a connection is made to the clusterip of a service

following are the labels of deployments

      labels:
        app: dgservice
        tier: dgservice


new service definition with correct labels

apiversion: v1
kind: service
metadata:
  name: dgservice
  labels:
    app: dgservice
spec:
  type: nodeport
  ports:
    - port: 8080
      targetport: 8080
  selector:
    app: dgservice
    tier: dgservice

"
55490685,setting up release pipeline of kubernetes+istio with terraform+helm gives forbidden error,"i have tried now so many times setting up this pipeline in azure devops where i want to deploy a aks cluster and place istio on top.

deploying the aks using terraform works great.

after this i try to install istio using helm but the command i use gives forbidden error.

helm.exe install --namespace istio-system --name istio-init --wait c:\istio\install\kubernetes\helm\istio


i used the local path since this was the only good way i could find for helm to find the istio chart i have on the build agent.

the error message

error: release istio-init failed: clusterroles.rbac.authorization.k8s.io ""istio-galley-istio-system"" is forbidden: attempt to grant extra privileges: [{[*] [admissionregistration.k8s.io] [validatingwebhookconfigurations] [] []} {[get] [config.istio.io] [*] [] []} {[list] [config.istio.io] [*] [] []} {[watch] [config.istio.io] [*] [] []} {[get] [*] [deployments] [istio-galley] []} {[get] [*] [endpoints] [istio-galley] []}] user=&amp;{system:serviceaccount:kube-system:tillerserviceaccount 56632fa4-55e7-11e9-a4a1-9af49f3bf03a [system:serviceaccounts system:serviceaccounts:kube-system system:authenticated] map[]} ownerrules=[] ruleresolutionerrors=[[clusterroles.rbac.authorization.k8s.io ""cluster-admin"" not found, clusterroles.rbac.authorization.k8s.io ""system:discovery"" not found, clusterroles.rbac.authorization.k8s.io ""cluster-admin"" not found, clusterroles.rbac.authorization.k8s.io ""system:discovery"" not found, clusterroles.rbac.authorization.k8s.io ""system:discovery"" not found, clusterroles.rbac.authorization.k8s.io ""cluster-admin"" not found]]


the serviceaccount i use (system:serviceaccount:kube-system:tillerserviceaccount as you can see in error message) are configured using this rbac config:

apiversion: v1
kind: serviceaccount
metadata:
  name: tillerserviceaccount
  namespace: kube-system
---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: tillerbinding
roleref:
  apigroup: """"
  kind: clusterrole
  name: cluster-admin
subjects:
  - kind: serviceaccount
    name: tillerserviceaccount
    namespace: kube-system


still the error message says in the ruleresolutionerrors that it looks for cluster-admin but it is not found.

i even tried the extreme and set all service accounts as cluster admins to test:

kubectl create clusterrolebinding serviceaccounts-admins --clusterrole=cluster-admin --group=system:serviceaccounts


but even after that i get the same error with the same ruleresolutionerrors.

i am stuck and appriciate any help in what i can do differently.
",<kubernetes><kubernetes-helm><istio><azure-aks>,55491426,2,"this is the role binding we are using in dev clusters:

---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: tillerbinding
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: cluster-admin
subjects:
  - kind: serviceaccount
    name: tillerserviceaccount
    namespace: kube-system


edit: in this case the error was due to aks created without rbac.
"
72323136,helm iterate over nested list and added output in yaml with decoded value,"i have a bunch of secretkey in values.yaml like below. i need to add each value of secretkey as a key and decoded value in template.data like as a value like below.
how can i achieve this ?
{{- range $externalsecretname, $externalsecret := .values.externalsecrets }}
apiversion: external-secrets.io/v1beta1
kind: externalsecret
metadata:
  name: {{ $externalsecretname }}
spec:
  refreshinterval: 1m
  secretstoreref:
    name: secret
    kind: secretstore
  target:
    name: {{ $externalsecretname }}
    creationpolicy: owner
    template:
      data:
        ## needs to insert/add each secretkey value here like below
        {
          keyname1: &quot;{{ .keyname1 | b64dec }}&quot;.  
          keyname2: &quot;{{ .keyname2 | b64dec }}&quot;.  
        }

  data:
  {{- toyaml $externalsecret.data | nindent 4 }}
---
{{- end }}


values.yaml:
===========

extraenvsecret:
  fromsecret:
    name: master-tf-address-handling
    data:
      prefix_keyname1: keyname1
      prefix_keyname2: keyname2

externalsecrets:
  demo-app:
    data:
      - secretkey: keyname1
        remoteref:
          key: value1
      - secretkey: keyname2
        remoteref:
          key: value1

",<kubernetes><kubernetes-helm><go-templates>,72327681,2,"{{- range $externalsecretname, $externalsecret := .values.externalsecrets }}
apiversion: external-secrets.io/v1beta1
kind: externalsecret
metadata:
  name: {{ $externalsecretname }}
spec:
  refreshinterval: 1m
  secretstoreref:
    name: secret
    kind: secretstore
  target:
    name: {{ $externalsecretname }}
    creationpolicy: owner
    template:
      data:
        {
          {{- range $externalsecret.data }}
             {{ .secretkey }}: &quot;{{ .remoteref.key | b64enc }}&quot;,
          {{- end }}
        }
  data:
    {{- toyaml $externalsecret.data | nindent 4 }}
{{- end }}

"
55559504,kubernetes hpa fails to detect a successfully published custom metric from stackdriver,"i'm trying to scale a kubernetes deployment using a horizontalpodautoscaler, which listens to a custom metrics through stackdriver.

i'm having a gke cluster, with a stackdriver adapter enabled.
i'm able to publish the custom metric type to stackdriver, and following is the way it's being displayed in stackdriver's metric explorer.





this is how i have defined my hpa:

apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  name: example-hpa
spec:
  minreplicas: 1
  maxreplicas: 10
  metrics:
  - type: external
    external:
      metricname: custom.googleapis.com|worker_pod_metrics|baz
      targetvalue: 400
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: test-app-group-1-1


after successfully creating example-hpa, executing kubectl get hpa example-hpa, always shows targets as &lt;unknown&gt;, and never detects the value from custom metrics.

name          reference                       targets         minpods   maxpods   replicas   age
example-hpa   deployment/test-app-group-1-1   &lt;unknown&gt;/400   1         10        1          18m


i'm using a java client which runs locally to publish my custom metrics.
i have given the appropriate resource labels as mentioned here (hard coded - so that it can run without a problem in local environment). i have followed this document to create the java client.

private static monitoredresource preparemonitoredresourcedescriptor() {
        map&lt;string, string&gt; resourcelabels = new hashmap&lt;&gt;();
        resourcelabels.put(""project_id"", ""&lt;&lt;&lt;my-project-id&gt;&gt;&gt;);
        resourcelabels.put(""pod_id"", ""&lt;my pod uid&gt;"");
        resourcelabels.put(""container_name"", """");
        resourcelabels.put(""zone"", ""asia-southeast1-b"");
        resourcelabels.put(""cluster_name"", ""my-cluster"");
        resourcelabels.put(""namespace_id"", ""mynamespace"");
        resourcelabels.put(""instance_id"", """");

        return monitoredresource.newbuilder()
                .settype(""gke_container"")
                .putalllabels(resourcelabels)
                .build();
    }


what am i doing wrong in the above-mentioned steps please? thank you in advance for any answers provided!



edit [resolved]:
i think i have had some misconfigurations, since kubectl describe hpa [name] --v=9 showed me some 403 status code, as well as i was using type: external instead of type: pods (thanks mwz for your answer, pointing out this mistake).

i managed to fix it by creating a new project, a new service account, and a new gke cluster (basically everything from the beginning again). then i changed my yaml file as follows, exactly as this document explains.

apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  name: test-app-group-1-1
  namespace: default
spec:
  scaletargetref:
    apiversion: apps/v1beta1
    kind: deployment
    name: test-app-group-1-1
  minreplicas: 1
  maxreplicas: 5
  metrics:
  - type: pods                 # earlier this was type: external
    pods:                      # earlier this was external:
      metricname: baz                               # metricname: custom.googleapis.com|worker_pod_metrics|baz
      targetaveragevalue: 20


i'm now exporting as custom.googleapis.com/baz, and not as custom.googleapis.com/worker_pod_metrics/baz. also, now i'm explicitly specifying the namespace for my hpa in the yaml.
",<kubernetes><google-kubernetes-engine><stackdriver><google-cloud-stackdriver>,55575451,2,"since you can see your custom metric in stackdriver gui i'm guessing metrics are correctly exported. based on autoscaling deployments with custom metrics i believe you wrongly defined metric to be used by hpa to scale the deployment.

please try using this yaml:

apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  name: example-hpa
spec:
  minreplicas: 1
  maxreplicas: 10
  metrics:
  - type: pods
    pods:
      metricname: baz
      targetaveragevalue: 400
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: test-app-group-1-1


please have in mind that:


  the hpa uses the metrics to compute an average and compare it to the
  target average value. in the application-to-stackdriver export
  example, a deployment contains pods that export metric. the following
  manifest file describes a horizontalpodautoscaler object that scales a
  deployment based on the target average value for the metric.


troubleshooting steps described on the page above can also be useful.

side-note
since above hpa is using beta api autoscaling/v2beta1 i got error when running kubectl describe hpa [deployment_name]. i ran kubectl describe hpa [deployment_name] --v=9 and got response in json.
"
55497678,authentication process is not triggered when using ibm app id in ibm cloud kubernetes service,"i'm trying to use this feature: https://cloud.ibm.com/docs/services/appid?topic=appid-kube-auth#kube-auth

i've followed the steps in the documentation, but the authentication process is not triggered. unfortunately i don't see any errors and don't know what else to do.

here is my sample service (nginx.yaml):

---
apiversion: apps/v1 
kind: deployment
metadata:
  name: nginx
spec:
  strategy:
    type: recreate
  selector:
    matchlabels:
      app: nginx
  replicas: 3 
  template: 
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerport: 80
---
apiversion: v1
kind: service
metadata:
  name: nginx
  namespace: default
  labels:
    app: nginx
spec:
  ports:
  - name: http
    port: 80
    protocol: tcp
  selector:
    app: nginx
  type: nodeport


here is my sample service (ingress.yaml). replace 'niklas-heidloff-4' with your cluster name and 'niklas-heidloff-appid' with the name of your app id service instance.

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-with-app-id
  annotations:   
    ingress.bluemix.net/appid-auth: ""bindsecret=binding-niklas-heidloff-appid namespace=default requesttype=web""
spec:
  tls:
  - hosts:
    - niklas.niklas-heidloff-4.us-south.containers.appdomain.cloud
    secretname: niklas-heidloff-4
  rules:
  - host: niklas.niklas-heidloff-4.us-south.containers.appdomain.cloud
    http:
      paths:
      - path: /
        backend:
          servicename: nginx
          serviceport: 80


here are the steps to reproduce the sample:

first create a new cluster with at least two worker nodes in dallas as described in the documentation. note that it can take some extra time to get a public ip for your cluster.

then create a app id service instance.

then invoke the following commands (replace 'niklas-heidloff-4' with your cluster name):

$ ibmcloud login -a https://api.ng.bluemix.net
$ ibmcloud ks region-set us-south
$ ibmcloud ks cluster-config niklas-heidloff-4 (and execute export....)
$ ibmcloud ks cluster-service-bind --cluster niklas-heidloff-4 --namespace default --service niklas-heidloff-appid
$ kubectl apply -f nginx.yaml
$ kubectl apply -f ingress.yaml


after this i could open 'https://niklas.niklas-heidloff-4.us-south.containers.appdomain.cloud/' but the authentication process is not triggered and the page opens without authentication. 
",<kubernetes><ibm-cloud><kubernetes-ingress><ibm-appid>,55509291,2,"i tried the steps mentioned in the link and this is how it worked for me. 

ingress.yaml

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: myingress
  annotations:
    ingress.bluemix.net/appid-auth: ""bindsecret=binding-appid-ks namespace=default requesttype=web servicename=nginx idtoken=false""
spec:
  tls:
    - hosts:
        - test.vidya-think-cluster.us-south.containers.appdomain.cloud
      secretname: vidya-think-cluster
  rules:
    - host: test.vidya-think-cluster.us-south.containers.appdomain.cloud
      http:
        paths:
          - path: /
            backend:
              servicename: nginx
              serviceport: 80


i added the following web redirect url in the authentication settings of app id service - http://test.vidya-think-cluster.us-south.containers.appdomain.cloud/appid_callback.

now, when you try accessing the app at http://test.vidya-think-cluster.us-south.containers.appdomain.cloud/ you should see the redirection to app id

looks like idtoken=false is a mandatory parameter as there is an error when you run kubectl describe myingress 

error:  failed to apply ingress.bluemix.net/appid-auth annotation. error annotation format error : one of the mandatory fields not valid/missing for annotation ingress.bluemix.net/appid-auth
"
72388148,access helm context of dependencies,"we are using this chart:
apiversion: v2
appversion: 1.0
version: 0.0.1
description: helm chart for setting up kafka platform (kafka, zookeeper, kafka rest, kafka schema registry, kafka connect, ksql server)
name: kafka-platform

dependencies:
  - name: cp-helm-charts
    version: 0.6.0
    repository: &quot;https://confluentinc.github.io/cp-helm-charts&quot;

we need to create new local template that does not exist in the public chart, and this new template should access built-in variables as defined in the public chart. for example, the .chart.name
apiversion: v1
appversion: &quot;1.0&quot;
description: a helm chart for confluent kafka on kubernetes
name: cp-kafka
version: 0.1.0

how could we access &quot;cp-kafka&quot; from a local .tpl named template or template? all i manage to access right now is either an error of type &quot; at &lt;.chart.name&gt;: can't evaluate field chart in type int&quot; or the value of the local chart &quot;kafka-platform&quot;.
i've searched for this question/answer but although i see things that look alike, i did not find one that fully fits or answers this exact question.
",<kubernetes><kubernetes-helm>,72388370,2,"you can use the subcharts map.
{{ $sub := index .subcharts &quot;cp-kafka&quot; }}

this will give you all built in variables and even the .values from the values file.
for example, i create a chart and a subchart:
helm create foo
helm create foo/charts/bar
rm -rf foo/templates/*
rm -rf foo/charts/bar/templates/*

in bar values i put this:
some:
  values: from
  this: subchart

in foo templates i put this configmap:
kind: configmap
apiversion: v1
metadata:
  name: example
data:
  bar-chart: |
    {{- .subcharts.bar | toyaml | nindent 4 }}

now when i render the chart with helm template foo i get the below output:
---
# source: foo/templates/cm.yaml
kind: configmap
apiversion: v1
metadata:
  name: example
data:
  bar-chart: |
    capabilities:
      apiversions:
      - v1
      - admissionregistration.k8s.io/v1
      - admissionregistration.k8s.io/v1beta1
      - internal.apiserver.k8s.io/v1alpha1
      - apps/v1
      - apps/v1beta1
      - apps/v1beta2
      - authentication.k8s.io/v1
      - authentication.k8s.io/v1beta1
      - authorization.k8s.io/v1
      - authorization.k8s.io/v1beta1
      - autoscaling/v1
      - autoscaling/v2
      - autoscaling/v2beta1
      - autoscaling/v2beta2
      - batch/v1
      - batch/v1beta1
      - certificates.k8s.io/v1
      - certificates.k8s.io/v1beta1
      - coordination.k8s.io/v1beta1
      - coordination.k8s.io/v1
      - discovery.k8s.io/v1
      - discovery.k8s.io/v1beta1
      - events.k8s.io/v1
      - events.k8s.io/v1beta1
      - extensions/v1beta1
      - flowcontrol.apiserver.k8s.io/v1alpha1
      - flowcontrol.apiserver.k8s.io/v1beta1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - networking.k8s.io/v1
      - networking.k8s.io/v1beta1
      - node.k8s.io/v1
      - node.k8s.io/v1alpha1
      - node.k8s.io/v1beta1
      - policy/v1
      - policy/v1beta1
      - rbac.authorization.k8s.io/v1
      - rbac.authorization.k8s.io/v1beta1
      - rbac.authorization.k8s.io/v1alpha1
      - scheduling.k8s.io/v1alpha1
      - scheduling.k8s.io/v1beta1
      - scheduling.k8s.io/v1
      - storage.k8s.io/v1beta1
      - storage.k8s.io/v1
      - storage.k8s.io/v1alpha1
      - apiextensions.k8s.io/v1beta1
      - apiextensions.k8s.io/v1
      helmversion:
        git_commit: 6e3701edea09e5d55a8ca2aae03a68917630e91b
        git_tree_state: clean
        go_version: go1.17.5
        version: v3.8.2
      kubeversion:
        major: &quot;1&quot;
        minor: &quot;23&quot;
        version: v1.23.0
    chart:
      isroot: false
      apiversion: v2
      appversion: 1.16.0
      description: a helm chart for kubernetes
      name: bar
      type: application
      version: 0.1.0
    files:
      .helmignore: iybqyxr0zxjucyb0bybpz25vcmugd2hlbibidwlszgluzybwywnrywdlcy4kiybuaglzihn1chbvcnrzihnozwxsigdsb2igbwf0y2hpbmcsihjlbgf0axzlihbhdgggbwf0y2hpbmcsigfuzaojig5lz2f0aw9uichwcmvmaxhlzcb3axroiceplibpbmx5ig9uzsbwyxr0zxjuihblcibsaw5llgourfnfu3rvcmukiybdb21tb24gvkntigrpcnmklmdpdc8klmdpdglnbm9yzqouynpylwouynpyawdub3jlci5ozy8klmhnawdub3jlci5zdm4vcimgq29tbw9uigjhy2t1ccbmawxlcwoqlnn3caoqlmjhawoqlnrtcaoqlm9yawckkn4kiybwyxjpb3vzielerxmklnbyb2ply3qklmlkzwevcioudg1wcm9qci52c2nvzguvcg==
    release:
      isinstall: true
      isupgrade: false
      name: release-name
      namespace: default
      revision: 1
      service: helm
    subcharts: {}
    values:
      global: {}
      some:
        this: subchart
        values: from

"
68000968,gke - expose service with ingress and internal load balancing,"i have rest api web service on internal gke cluster which i would like to expose with internal http load balancing.
let's call this service &quot;blue&quot; service:
i would like to expose it in following mapping:
http://api.xxx.yyy.internal/blue/isalive -&gt; http://blue-service/isalive
http://api.xxx.yyy.internal/blue/v1/get -&gt; http://blue-service/v1/get
http://api.xxx.yyy.internal/blue/v1/create -&gt; http://blue-service/v1/create
http://api.xxx.yyy.internal/ -&gt; http://blue-service/ (expose swagger)

i'm omitting deployment yaml, since it's less relevant to discussion.
but my service yaml looks like this:
apiversion: v1
kind: service
metadata:
  name: blue-service
spec:
  type: nodeport
  ports:
  - port: 80
    targetport: 8080
  selector:
    app: blue-service

my ingress configuration is the following:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: blue-ingress
  annotations:
    kubernetes.io/ingress.class: &quot;gce-internal&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
    ingress.kubernetes.io/rewrite-target: /$2
spec:
  rules:
  - host: api.xxx.yyy.internal
    http:
      paths:
      - path: /blue/*
        backend:
          servicename: blue-service
          serviceport: 80

however, i'm receiving 404 for all requests. /blue/v1/get, /blue/v1/create and /blue/isalive returns 404.
in my &quot;blue&quot; application i log all my notfound requests and i can clearly see that my uris are not being rewritten, the requests hitting the application are /blue/v1/get, /blue/v1/create and /blue/isalive.
what am i missing in ingress configuration? how can i fix those rewrites?
",<nginx><kubernetes><url-rewriting><google-kubernetes-engine><kubernetes-ingress>,68023401,2,"i solved the problem and writing it here to memo it and hopefully someone will find it as useful.

first problem is that i have mixed annotations types. one of gke ingress controller and second for nginx server controller. currently gke ingress controller doesn't support url rewrite feature, so i need to use nginx ingress controller.

so i need to install nginx based ingress controller. it cloud be done easily using helm chart or or deployment yaml. however, by default this controller will expose ingress using external load balancer and this not what i want. so we need to modify deployment charts or yaml file of this controller.
i'm not using helm, so i downoaded yaml itself using wget command.



    wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.47.0/deploy/static/provider/cloud/deploy.yaml

open it in editor and find the definition of service names ingress-nginx-controller in namespace ingress-nginx. add the following annotation.
cloud.google.com/load-balancer-type: &quot;internal&quot;  

after it i can run kubectl apply -f deploy.yaml command which will create ingress controller for me. it will take a few minutes to provision it.

in addition i need to open firewall rule which will allow master nodes access worker nodes on port 8443/tcp.

and the last item is an ingress yaml itself which should look like this:
 
 apiversion: networking.k8s.io/v1beta1
 kind: ingress
 metadata:
   annotations:
     nginx.ingress.kubernetes.io/rewrite-target: /$2
     kubernetes.io/ingress.class: ""nginx""
   name: blue-ingress
   namespace: default
 spec:
   rules:
   - host: api.xxx.yyy.internal
     http:
       paths:
       - backend:
           servicename: blue-service
           serviceport: 80
         path: /blue(/|$)(.*)


"
68035633,how can i create different auth-type for different target in ingress controller?,"i am deploying a eks cluster to aws and using alb ingress controller points to my k8s service. the ingress spec is shown as below.
there are two targets path: /* and path: /es/*. and i also configured alb.ingress.kubernetes.io/auth-type to use cognito as authentication method.
my question is how can i configure different auth-type for different target? i'd like to use cognito for /* and none for /es/*. how can i achieve that?
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: sidecar
  namespace: default
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/group.name: sidecar
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/group.order: '1'
    alb.ingress.kubernetes.io/healthcheck-path: /health
    alb.ingress.kubernetes.io/listen-ports: '[{&quot;http&quot;: 80}, {&quot;https&quot;: 443}]'
    # auth
    alb.ingress.kubernetes.io/auth-type: cognito
    alb.ingress.kubernetes.io/auth-idp-cognito: '{&quot;userpoolarn&quot;:&quot;xxxx&quot;,&quot;userpoolclientid&quot;:&quot;xxxx&quot;,&quot;userpooldomain&quot;:&quot;xxxx&quot;}'
    alb.ingress.kubernetes.io/auth-scope: 'email openid aws.cognito.signin.user.admin'
    alb.ingress.kubernetes.io/certificate-arn: xxxx

spec:
  rules:
    - http:
        paths:
          - path: /es/*
            backend:
              servicename: sidecar-entrypoint
              serviceport: 8080
          - path: /*
            backend:
              servicename: server-entrypoint
              serviceport: 8081



",<amazon-web-services><kubernetes><amazon-eks>,68037942,2,"this question comes up a lot, so i guess it needs to be pr-ed into their documentation.
ingress resources are cumulative, so you can separate your paths into two separate ingress resources in order to annotate each one differently. they will be combined with all other ingress resources across the entire cluster to form the final config
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: sidecar-star
  namespace: default
  annotations:
    kubernetes.io/ingress.class: alb
    # ... and the rest ...
spec:
  rules:
    - http:
        paths:
          - path: /*
            backend:
              servicename: server-entrypoint
              serviceport: 8081
---
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: sidecar-es
  namespace: default
  annotations:
    kubernetes.io/ingress.class: alb
    # ... and the rest ...
spec:
  rules:
    - http:
        paths:
          - path: /es/*
            backend:
              servicename: sidecar-entrypoint
              serviceport: 8080

"
57494451,{{ if }} clause inside of range scope doesn't see values,"the task is to range over workers collection and if the current worker has autoscaling.enabled=true create an hpa for it.

i've tried to compare .autoscaling.enabled to ""true"" but it returned ""error calling eq: incompatible types for comparison"". here people say that it actually means that .autoscaling.enabled is nil. so {{ if .autoscaling.enabled }} somehow doesn't see the variable and assumes it doesn't exist.

values:

...
workers:
  - name: worker1
    command: somecommand1
    memoryrequest: 500mi
    memorylimit: 1400mi
    cpurequest: 50m
    cpulimit: 150m
    autoscaling:
      enabled: false
  - name: worker2
    command: somecommand2
    memoryrequest: 512mi
    memorylimit: 1300mi
    cpurequest: 50m
    cpulimit: 150m
    autoscaling:
      enabled: false
  - name: workerwithautoscaling
    command: somecommand3
    memoryrequest: 600mi
    memorylimit: 2048mi
    cpurequest: 150m
    cpulimit: 400m
    autoscaling:
      enabled: true
      minreplicas: 1
      maxreplicas: 5
      targetcpuutilization: 50
      targetmemoryutilization: 50
...


template:

...
{{- range .values.workers }}
{{- if .autoscaling.enabled }}
apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  labels:
    ...
  name: ""hpa-{{ .name }}-{{ $.realeas.name }}""
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: {{ .name }}
  minreplicas: {{ .minreplicas }}
  maxreplicas: {{ .maxreplicas }}
  metrics:
{{- with .targetcpuutilization}}
    - type: resource
      resource:
        name: cpu
        targetaverageutilization: {{ . }}
{{- end }}
{{- with .targetmemoryutilization}}
    - type: resource
      resource:
        name: memory
        targetaverageutilization: {{ . }}
{{- end }}
---
{{- end }}
{{- end }}


i expect the manifest for one hpa that targets workerwithautoscaling, but the actual output is totally empty.
",<kubernetes><kubernetes-helm>,57494685,2,"your use of {{- range .values.workers }} and {{- if .autoscaling.enabled }} is fine. you are not getting any values because .minreplicas, .maxreplicas, etc, are inside .autoscaling scope. 

see modifying scope using with

adding {{- with .autoscaling}} will solve the issue.

{{- range .values.workers }}
{{- if .autoscaling.enabled }}
apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  labels:
  name: ""hpa-{{ .name }}-{{ $.release.name }}""
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: {{ .name }}
{{- with .autoscaling}}
  minreplicas: {{ .minreplicas }}
  maxreplicas: {{ .maxreplicas }}
  metrics:
    - type: resource
      resource:
        name: cpu
        targetaverageutilization: {{ .targetcpuutilization}}
    - type: resource
      resource:
        name: memory
        targetaverageutilization: {{ .targetmemoryutilization}}
{{- end }}
{{- end }}
{{- end }}


helm template .

---
# source: templates/hpa.yaml

apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  labels:
  name: ""hpa-workerwithautoscaling-release-name""
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: workerwithautoscaling
  minreplicas: 1
  maxreplicas: 5
  metrics:
    - type: resource
      resource:
        name: cpu
        targetaverageutilization: 50
    - type: resource
      resource:
        name: memory
        targetaverageutilization: 50

"
68034771,create multiple configmaps on kubernetes with using yaml files,"i have a few yaml files that contains some values. i want to read that files while helm deploying and create configmaps for each of them.
i've added config file under the helm charts. ( same level with templates folder )
chart structure
and then i've tried to create 'configmap-creator.yaml' which is located under the 'templates' folder.
i simply run 'helm upgrade --install ealpkar --namespace ealpkar --create-namespace .'
it was complete successfully but there is only one configmap which is called 'config2-configmap'. i missed the first one ( config1-configmap )
here is the 'configmap-creator.yaml'
{{- $files := .files }}
{{- range $key, $value := .files }}
{{- if hasprefix &quot;config/&quot; $key }}
apiversion: v1
kind: configmap
metadata:
name: {{ $key | trimprefix &quot;config/&quot; | replace &quot;.yaml&quot; &quot;&quot; | replace &quot;_&quot; &quot;-&quot; }}-configmap
data:
{{ $key | trimprefix &quot;config/&quot; }}: {{ $files.get $key | quote }}
{{- end }}
{{- end }}

example of yaml file  which is under 'config' folder;

config1.yaml
dummy_product:
  ip: 10.10.10.10
  port: 22


config2.yaml
dummy_product_2:
  ip: 10.10.10.20
  port: 22



",<kubernetes><kubernetes-helm><configmap>,68036246,2,"fix your template, adding a separator between objects.
{{- $files := .files }}
{{- range $key, $value := .files }}
{{- if hasprefix &quot;config/&quot; $key }}
---
apiversion: v1
kind: configmap
metadata:
name: {{ $key | trimprefix &quot;config/&quot; | replace &quot;.yaml&quot; &quot;&quot; | replace &quot;_&quot; &quot;-&quot; }}-configmap
data:
  {{ $key | trimprefix &quot;config/&quot; }}: {{ $files.get $key | quote }}
{{- end }}
{{- end }}

"
72208634,is there a simple way to grab the first line from a file with helm?,"i am attempting to create a kubernetes configmap with helm, which simply consists of the first line within a config file. i put my file in helm/config/file.txt, which has several lines of content, but i only want to extract the first. my first attempt at this was to loop over the lines of the file (naturally), but quit out after the first loop:
apiversion: v1
kind: configmap
metadata:
  name: one-line-cm
data:
  first-line:
    {{- range .files.lines &quot;config/file.txt&quot; }}
    {{ . }}
    {{ break }} # not a real thing
    {{- end }}

unfortunately, break doesn't seem to be a concept/function in helm, even though it is within golang. i discovered this the hard way, as well as reading about a similar question in this other post: helm: break loop (range) in template
i'm not stuck on using a loop, i'm just wondering if there's another solution to perform the simple task of extracting the first line from a file with helm syntax.
",<go><kubernetes><kubernetes-helm><configmap>,72208711,2,"edit:
i've determined the following is the cleanest solution:
.files.lines &quot;config/file.txt&quot; | first

(as a side note, i had to pipe to squote in my acutal solution due to my file contents containing special characters)

after poking around in the helm docs for alternative functions, i came up with a solution that works, it's just not that pretty:
apiversion: v1
kind: configmap
metadata:
  name: one-line-cm
data:
  first-line: |
    {{ index (regexsplit &quot;\\n&quot; (.files.get &quot;config/file.txt&quot;) -1) 0 }}

this is what's happening above (working inside outward):

.files.get &quot;config/file.txt&quot; is returning a string representation of the file contents.
regexsplit &quot;\\n&quot; &lt;step-1&gt; -1 is splitting the file contents from step-1 by newline (-1 means return the max number of substring matches possible)
index &lt;step-2&gt; 0 is grabbing the first item (index 0) from the list returned by step-2.

hope this is able to help others in similar situations, and i am still open to alternative solution suggestions.
"
57317501,kubernetes: modeling jobs/cron tasks for postgres + tomcat application,"i work on an open source system that is comprised of a postgres database and a tomcat server.  i have docker images for each component.  we currently use docker-compose to test the application.

i am attempting to model this application with kubernetes.

here is my first attempt.

apiversion: v1
kind: pod
metadata:
  name: dspace-pod
spec:
  volumes:
  - name: ""pgdata-vol""
    emptydir: {}
  - name: ""assetstore""
    emptydir: {}
  - name: my-local-config-map
    configmap:
      name: local-config-map
  containers:
  - image: dspace/dspace:dspace-6_x
    name: dspace
    ports:
    - containerport: 8080
      name: http
      protocol: tcp
    volumemounts:
    - mountpath: ""/dspace/assetstore""
      name: ""assetstore""
    - mountpath: ""/dspace/config/local.cfg""
      name: ""my-local-config-map""
      subpath: local.cfg
  #
  - image: dspace/dspace-postgres-pgcrypto
    name: dspacedb
    ports:
    - containerport: 5432
      name: http
      protocol: tcp
    volumemounts:
    - mountpath: ""/pgdata""
      name: ""pgdata-vol""
    env:
    - name: pgdata
      value: /pgdata


i have a configmap that is setting the hostname to the name of the pod.

apiversion: v1
kind: configmap
metadata:
  creationtimestamp: 2016-02-18t19:14:38z
  name: local-config-map
  namespace: default
data:
  local.cfg: |-
    dspace.dir = /dspace
    db.url = jdbc:postgresql://dspace-pod:5432/dspace
    dspace.hostname = dspace-pod
    dspace.baseurl = http://dspace-pod:8080
    solr.server=http://dspace-pod:8080/solr


this application has a number of tasks that are run from the command line.  

i have created a 3rd docker image that contains the jars that are needed on the command line.

i am interested in modeling these command line tasks as jobs in kubernetes.  assuming that is a appropriate way to handle these tasks, how do i specify that a job should run within a pod that is already running?

here is my first attempt at defining a job.  

apiversion: batch/v1
kind: job
#https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/
metadata:
  name: dspace-create-admin
spec:
  template:
    spec:
      volumes:
      - name: ""assetstore""
        emptydir: {}
      - name: my-local-config-map
        configmap:
          name: local-config-map
      containers:
      - name: dspace-cli
        image: dspace/dspace-cli:dspace-6_x
        command: [
          ""/dspace/bin/dspace"",
          ""create-administrator"",
          ""-e"", ""test@test.edu"",
          ""-f"", ""test"",
          ""-l"", ""admin"",
          ""-p"", ""admin"",
          ""-c"", ""en""
        ]
        volumemounts:
        - mountpath: ""/dspace/assetstore""
          name: ""assetstore""
        - mountpath: ""/dspace/config/local.cfg""
          name: ""my-local-config-map""
          subpath: local.cfg
      restartpolicy: never

",<docker><kubernetes><kubernetes-pod>,57333976,2,"the following configuration has allowed me to start my services (tomcat and postgres) as i hoped.

apiversion: v1
kind: configmap
metadata:
  creationtimestamp: 2016-02-18t19:14:38z
  name: local-config-map
  namespace: default
data:
  # example of a simple property defined using --from-literal
  #example.property.1: hello
  #example.property.2: world
  # example of a complex property defined using --from-file
  local.cfg: |-
    dspace.dir = /dspace
    db.url = jdbc:postgresql://dspacedb-service:5432/dspace
    dspace.hostname = dspace-service
    dspace.baseurl = http://dspace-service:8080
    solr.server=http://dspace-service:8080/solr
---
apiversion: v1
kind: service
metadata:
  name: dspacedb-service
  labels:
    app: dspacedb-app
spec:
  type: nodeport
  selector:
    app: dspacedb-app
  ports:
  - protocol: tcp
    port: 5432
  #  targetport: 5432
---
apiversion: apps/v1
kind: deployment
metadata:
  name: dspacedb-deploy
  labels:
    app: dspacedb-app
spec:
  selector:
    matchlabels:
      app: dspacedb-app
  template:
    metadata:
      labels:
        app: dspacedb-app
    spec:
      volumes:
      - name: ""pgdata-vol""
        emptydir: {}
      containers:
      - image: dspace/dspace-postgres-pgcrypto
        name: dspacedb
        ports:
        - containerport: 5432
          name: http
          protocol: tcp
        volumemounts:
        - mountpath: ""/pgdata""
          name: ""pgdata-vol""
        env:
        - name: pgdata
          value: /pgdata
---
apiversion: v1
kind: service
metadata:
  name: dspace-service
  labels:
    app: dspace-app
spec:
  type: nodeport
  selector:
    app: dspace-app
  ports:
  - protocol: tcp
    port: 8080
    targetport: 8080
    name: http
---
apiversion: apps/v1
kind: deployment
metadata:
  name: dspace-deploy
  labels:
    app: dspace-app
spec:
  selector:
    matchlabels:
      app: dspace-app
  template:
    metadata:
      labels:
        app: dspace-app
    spec:
      volumes:
      - name: ""assetstore""
        emptydir: {}
      - name: my-local-config-map
        configmap:
          name: local-config-map
      containers:
      - image: dspace/dspace:dspace-6_x-jdk8-test
        name: dspace
        ports:
        - containerport: 8080
          name: http
          protocol: tcp
        volumemounts:
        - mountpath: ""/dspace/assetstore""
          name: ""assetstore""
        - mountpath: ""/dspace/config/local.cfg""
          name: ""my-local-config-map""
          subpath: local.cfg


after applying the configuration above, i have the following results.

$ kubectl get services -o wide
name               type        cluster-ip       external-ip   port(s)          age       selector
dspace-service     nodeport    10.104.224.245   &lt;none&gt;        8080:32459/tcp   3s        app=dspace-app
dspacedb-service   nodeport    10.96.212.9      &lt;none&gt;        5432:30947/tcp   3s        app=dspacedb-app
kubernetes         clusterip   10.96.0.1        &lt;none&gt;        443/tcp          22h       &lt;none&gt;

$ kubectl get pods
name                               ready     status      restarts   age
dspace-deploy-c59b77bb8-mr47k      1/1       running     0          10m
dspacedb-deploy-58dd85f5b9-6v2lf   1/1       running     0          10


i was pleased to see that the service name can be used for port forwarding.

$ kubectl port-forward service/dspace-service 8080:8080
forwarding from 127.0.0.1:8080 -&gt; 8080
forwarding from [::1]:8080 -&gt; 8080


i am also able to run the following job using the defined service names in the configmap.

apiversion: batch/v1
kind: job
metadata:
  name: dspace-create-admin
spec:
  template:
    spec:
      volumes:
      - name: ""assetstore""
        emptydir: {}
      - name: my-local-config-map
        configmap:
          name: local-config-map
      containers:
      - name: dspace-cli
        image: dspace/dspace-cli:dspace-6_x
        command: [
          ""/dspace/bin/dspace"",
          ""create-administrator"",
          ""-e"", ""test@test.edu"",
          ""-f"", ""test"",
          ""-l"", ""admin"",
          ""-p"", ""admin"",
          ""-c"", ""en""
        ]
        volumemounts:
        - mountpath: ""/dspace/assetstore""
          name: ""assetstore""
        - mountpath: ""/dspace/config/local.cfg""
          name: ""my-local-config-map""
          subpath: local.cfg
      restartpolicy: never


results

$ kubectl get pods
name                               ready     status      restarts   age
dspace-create-admin-kl6wd          0/1       completed   0          5m
dspace-deploy-c59b77bb8-mr47k      1/1       running     0          10m
dspacedb-deploy-58dd85f5b9-6v2lf   1/1       running     0          10m


i still have some work to do persisting the volumes.
"
57289381,activemq on kuberenetes with shared storage,"i have existing applications built with apache camel and activemq. as part of migration to kubernetes, what we are doing is moving the same services developed with apache camel to kubernetes. i need to deploy activemq such that i do not lose the data in case one of the pod dies. 

what i am doing now is running a deployment with relicaset value to 2. this will start 2 pods and with a service in front, i can serve any request while atleast 1 pod is up. however, if one pod dies, i do not want to lose the data. i want to implement something like a shared file system between the pods. my environment is in aws so i can use ebs. can you suggest, how to achieve that.

below is my deployment and service yaml.

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: smp-activemq
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: smp-activemq
    spec:
      containers:
        - name: smp-activemq
          image: dasdebde/activemq:5.15.9
          imagepullpolicy: ifnotpresent
          ports:
            - containerport: 61616
          resources:
            limits:
              memory: 512mi

---
apiversion: v1
kind: service
metadata:
  name: smp-activemq
spec:
  type: nodeport
  selector:
    app: smp-activemq
  ports:
    - nodeport: 32191
      port: 61616
      targetport: 61616

",<kubernetes><activemq-classic><kubernetes-pvc>,57322415,2,"statefulsets are valuable for applications that require stable, persistent storage. deleting and/or scaling a statefulset down will not delete the volumes associated with the statefulset. this is done to ensure data safety. the ""volumeclaimtemplates"" part in yaml will provide stable storage using persistentvolumes provisioned by a persistentvolume provisioner.

in your case, statefulset file definition will look similar to this:

apiversion: v1
kind: service
metadata:
  name: smp-activemq
  labels:
    app: smp-activemq
spec:
  type: nodeport
  selector:
    app: smp-activemq
  ports:
  - nodeport: 32191
    port: 61616
    name: smp-activemq
    targetport: 61616

---
apiversion: apps/v1
kind: statefulset
metadata:
  name: smp-activemq
spec:
  selector:
    matchlabels:
      app: smp-activemq
  servicename: smp-activemq
  replicas: 1
  template:
    metadata:
      labels:
        app: smp-activemq
    spec:
      containers:
      - name: smp-activemq
        image: dasdebde/activemq:5.15.9
        imagepullpolicy: ifnotpresent
        ports:
        - containerport: 61616
          name: smp-activemq
        volumemounts:
        - name: www
          mountpath: &lt;mount-path&gt;
  volumeclaimtemplates:
  - metadata:
      name: www
    spec:
      accessmodes: [ ""readwriteonce"" ]
      storageclassname: ""&lt;storageclass-name&gt;""
      resources:
        requests:
          storage: 1gi


that what you need to define is your storageclass name and mountpath. i hope it will helps you.
"
71787572,k8s jobs and pods differences as uses of host+subdomain,"i have k8s used by helm 3.

i need to access a k8s job while running in yaml file (created by helm).

the kubectl version:

client version: version.info{major:&quot;1&quot;, minor:&quot;21&quot;,
gitversion:&quot;v1.21.6&quot;,
gitcommit:&quot;d921bc6d1810da51177fbd0ed61dc811c5228097&quot;,
gittreestate:&quot;clean&quot;, builddate:&quot;2021-10-27t17:50:34z&quot;,
goversion:&quot;go1.16.9&quot;, compiler:&quot;gc&quot;, platform:&quot;linux/amd64&quot;} server
version: version.info{major:&quot;1&quot;, minor:&quot;21&quot;, gitversion:&quot;v1.21.6&quot;,
gitcommit:&quot;d921bc6d1810da51177fbd0ed61dc811c5228097&quot;,
gittreestate:&quot;clean&quot;, builddate:&quot;2021-10-27t17:44:26z&quot;,
goversion:&quot;go1.16.9&quot;, compiler:&quot;gc&quot;, platform:&quot;linux/amd64&quot;}

helm version:

version.buildinfo{version:&quot;v3.3.4&quot;,
gitcommit:&quot;a61ce5633af99708171414353ed49547cf05013d&quot;,
gittreestate:&quot;clean&quot;, goversion:&quot;go1.14.9&quot;}

as the following link:
dns concept
it works fine for pod, but not for job.
as explained, for putting hostname and subdomain in pod's yaml file, and add service that holds the domain...

need to check the state if running.

for pod, it is ready state.
kubectl wait pod/pod-name --for=condition=ready ...

for job there is no ready state (while pod behind is running).
how can i check the state of pod behind the job (job is running) and how can i use host + subdomain for jobs?
my code ...
(i removed some security tags, but the same. important - it may be complicated.
i create a listener - running when listen, with job that need to do some curl command, and this can be achieved whether it has access to that pod behind the job):
listener (the pod is the last job):
what i added is hostname and subdomain (which work for pod, and not for job). if it ever was on pod - no problem.
i also realized that the name of the pod (created by the job) has a hash automatic extension.
apiversion: batch/v1
kind: job
metadata:
  name: {{ include &quot;my-project.fullname&quot; . }}-listener
  namespace: {{ .release.namespace }}
  labels:
    name: {{ include &quot;my-project.fullname&quot; . }}-listener
    app: {{ include &quot;my-project.fullname&quot; . }}-listener
    component: {{ .chart.name }}
    subcomponent: {{ .chart.name }}-listener
  annotations:
    &quot;prometheus.io/scrape&quot;: {{ .values.prometheus.scrape | quote }}
    &quot;prometheus.io/path&quot;: {{ .values.prometheus.path }}
    &quot;prometheus.io/port&quot;: {{ .values.ports.api.container | quote }}
spec:
  template: #podtemplatespec (core/v1)
    spec: #podspec (core/v1)
      hostname: {{ include &quot;my-project.fullname&quot; . }}-listener
      subdomain: {{ include &quot;my-project.fullname&quot; . }}-listener-dmn
      initcontainers:
        # twice - can add in helers.tpl
        - name: wait-mysql-exist-pod
          image: {{ .values.global.registry }}/{{ .values.global.k8s.image }}:{{ .values.global.k8s.tag | default &quot;latest&quot; }}
          imagepullpolicy: ifnotpresent
          env:
            - name: mysql_pod_name
              value: {{ .release.name }}-mysql
            - name: component_name
              value: {{ .values.global.mysql.database.name }}
          command:
            - /bin/sh
          args:
            - -c
            - |-
              while [ &quot;$(kubectl get pod $mysql_pod_name 2&gt;/dev/null | grep $mysql_pod_name | awk '{print $1;}')&quot; \!= &quot;$mysql_pod_name&quot; ];do
                echo 'waiting for mysql pod to be existed...';
                sleep 5;
              done
        - name: wait-mysql-ready
          image: {{ .values.global.registry }}/{{ .values.global.k8s.image }}:{{ .values.global.k8s.tag | default &quot;latest&quot; }}
          imagepullpolicy: ifnotpresent
          env:
            - name: mysql_pod_name
              value: {{ .release.name }}-mysql
          command:
            - kubectl
          args:
            - wait
            - pod/$(mysql_pod_name)
            - --for=condition=ready
            - --timeout=120s
        - name: wait-mysql-has-db
          image: {{ .values.global.registry }}/{{ .values.global.k8s.image }}:{{ .values.global.k8s.tag | default &quot;latest&quot; }}
          imagepullpolicy: ifnotpresent
          env:
            {{- include &quot;k8s.db.env&quot; . | nindent 12 }}
            - name: mysql_pod_name
              value: {{ .release.name }}-mysql
          command:
            - /bin/sh
          args:
            - -c
            - |-
             while [ &quot;$(kubectl exec $mysql_pod_name -- mysql -uroot -p$mysql_root_password -e 'show databases' 2&gt;/dev/null | grep $mysql_database | awk '{print $1;}')&quot; \!= &quot;$mysql_database&quot; ]; do
                echo 'waiting for mysql database up...';
                sleep 5;
             done
      containers:
        - name: {{ include &quot;my-project.fullname&quot; . }}-listener
          image:  {{ .values.global.registry }}/{{ .values.image.repository }}:{{ .values.image.tag | default &quot;latest&quot; }}
          imagepullpolicy: {{ .values.image.pullpolicy }}
          env:
          {{- include &quot;k8s.db.env&quot; . | nindent 12 }}
            - name: scheduler_db
              value: $(connection_string)
          command: {{- toyaml .values.image.entrypoint | nindent 12 }}
          args: # some args ...
          ports:
            - name: api
              containerport: 8081
          resources:
            limits:
              cpu: 1
              memory: 1024mi
            requests:
              cpu: 100m
              memory: 50mi
          readinessprobe:
            httpget:
              path: /api/scheduler/healthcheck
              port: api
              scheme: http
            initialdelayseconds: 10
            periodseconds: 5
            timeoutseconds: 1
          livenessprobe:
            tcpsocket:
              port: api
            initialdelayseconds: 120
            periodseconds: 10
            timeoutseconds: 5
          volumemounts:
            - name: {{ include &quot;my-project.fullname&quot; . }}-volume
              mountpath: /etc/test/scheduler.yaml
              subpath: scheduler.yaml
              readonly: true
      volumes:
      - name: {{ include &quot;my-project.fullname&quot; . }}-volume
        configmap:
          name: {{ include &quot;my-project.fullname&quot; . }}-config
      restartpolicy: never

the service (for the subdomain):
apiversion: v1
kind: service
metadata:
  name: {{ include &quot;my-project.fullname&quot; . }}-listener-dmn
spec:
  selector:
    name: {{ include &quot;my-project.fullname&quot; . }}-listener
  ports:
    - name: api
      port: 8081
      targetport: 8081
  type: clusterip

roles + rolebinding (to enable access for curl command):
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  name: {{ include &quot;my-project.fullname&quot; . }}-role
rules:
- apigroups: [&quot;&quot;] # &quot;&quot; indicates the core api group
  resources: [&quot;pods&quot;]
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;, &quot;update&quot;]
- apigroups: [&quot;&quot;] # &quot;&quot; indicates the core api group
  resources: [&quot;pods/exec&quot;]
  verbs: [&quot;create&quot;, &quot;delete&quot;, &quot;deletecollection&quot;, &quot;get&quot;, &quot;list&quot;, &quot;patch&quot;, &quot;update&quot;, &quot;watch&quot;]
- apigroups: [&quot;&quot;, &quot;app&quot;, &quot;batch&quot;] # &quot;&quot; indicates the core api group
  resources: [&quot;jobs&quot;]
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]

role-binding:
apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: {{ include &quot;go-scheduler.fullname&quot; . }}-rolebinding
subjects:
- kind: serviceaccount
  name: default
roleref:
  kind: role
  name: {{ include &quot;go-scheduler.fullname&quot; . }}-role
  apigroup: rbac.authorization.k8s.io

and finally a tester that doing a curl command:
(for check i put tail -f), and enter the pod.
apiversion: batch/v1
kind: job
metadata:
  name: {{ include &quot;my-project.fullname&quot; . }}-test
  namespace: {{ .release.namespace }}
  labels:
    name: {{ include &quot;my-project.fullname&quot; . }}-test
    app: {{ include &quot;my-project.fullname&quot; . }}-test 
  annotations:
    &quot;prometheus.io/scrape&quot;: {{ .values.prometheus.scrape | quote }}
    &quot;prometheus.io/path&quot;: {{ .values.prometheus.path }}
    &quot;prometheus.io/port&quot;: {{ .values.ports.api.container | quote }}
spec:
  template: #podtemplatespec (core/v1)
    spec: #podspec (core/v1)
      initcontainers:
        # twice - can add in helers.tpl
        #
        - name: wait-sched-listener-exists
          image: {{ .values.global.registry }}/{{ .values.global.k8s.image }}:{{ .values.global.k8s.tag | default &quot;latest&quot; }}
          imagepullpolicy: ifnotpresent
          env:
            - name: pod_name
              value: {{ include &quot;my-project.fullname&quot; . }}-listener
          command:
            - /bin/sh
          args:
            - -c
            - |-
              while [ &quot;$(kubectl get job $pod_name 2&gt;/dev/null | grep $pod_name | awk '{print $1;}')&quot; \!= &quot;$pod_name&quot; ];do
                echo 'waiting for scheduler pod to exist ...';
                sleep 5;
              done
        - name: wait-listener-running
          image: {{ .values.global.registry }}/{{ .values.global.k8s.image }}:{{ .values.global.k8s.tag | default &quot;latest&quot; }}
          imagepullpolicy: ifnotpresent
          env:
            - name: pod_name
              value: {{ include &quot;my-project.fullname&quot; . }}-listener
          command:
            - /bin/sh
          args:
            - -c
            - |-
              while [ &quot;$(kubectl get pods 2&gt;/dev/null | grep $pod_name | awk '{print $3;}')&quot; \!= &quot;running&quot; ];do
                echo 'waiting for scheduler pod to run ...';
                sleep 5;
              done
      containers:
        - name: {{ include &quot;my-project.fullname&quot; . }}-test
          image:  {{ .values.global.registry }}/{{ .values.global.k8s.image }}:{{ .values.global.k8s.tag | default &quot;latest&quot; }}
          imagepullpolicy: {{ .values.image.pullpolicy }}
          command:
            - /bin/sh
          args:
            - -c
            - &quot;tail -f&quot;
     # instead of above can be curl: &quot;curl -h 'accept: application/json' -x get my-project-listener.my-project-listener-dmn:8081/api/scheduler/jobs&quot;

      restartpolicy: never

i enter the test pod
kubectl exec -it my-tester-&lt;hash&gt; -- /bin/sh

... and run the command:
ping my-project-listener.my-project-listener-dmn

got:
ping: bad address 'my-project-listener.my-project-listener-dmn'

when doing that for pod:

ping pod-hostname.pod-subdomain (): ... data bytes

",<kubernetes><kubernetes-helm><wait>,71795535,2,"there's a lot here, but i think you should be able to resolve all of this with a couple of small changes.
in summary, i'd suggest changing:
apiversion: apps/v1
kind: deployment     # &lt;-- not a job
metadata: &amp;original-job-metadata-from-the-question
spec:
  template:
    metadata:
      labels:   # vvv matching the service selector
        name: {{ include &quot;my-project.fullname&quot; . }}-listener
    spec:
      # delete all of the initcontainers:
      containers: &amp;original-container-list-from-the-question
      volumes: &amp;original-volume-list-from-the-question
      # delete restartpolicy: (default value always)

delete the role and rolebinding objects; connect to the service http://my-project-listener-dmn:8081 and not an individual pod; and you can kubectl wait --for=condition=available on the deployment.
connect to services, not individual pods (or jobs or deployments).  the service is named {{ include &quot;my-project.fullname&quot; . }}-listener-dmn and that is the host name you should connect to.  the service acts as a very lightweight in-cluster load balancer, and will forward requests on to one of the pods identified by its selector.
so in this example you'd connect to the service's name and port, http://my-project-listener-dmn:8081.  your application doesn't answer the very-low-level icmp protocol and i'd avoid ping(1) in favor of a more useful diagnostic.  also consider setting the service's port to the default http port 80; it doesn't necessarily need to match the pod's port.
the service selector needs to match the pod labels (and not the job's or deployment's labels).  a service attaches to pods; a job or a deployment has a template to create pods; and it's those labels that need to match up.  you need to add labels to the pod template:
spec:
  template:
    metadata:
      labels:
        name: {{ include &quot;my-project.fullname&quot; . }}-listener

or, in a helm chart where you have a helper to generate these labels,
      labels: {{- include &quot;my-project.labels&quot; | nindent 8 }}

the thing to check here is kubectl describe service my-project-listener-dmn.  there should be a line at the bottom that says endpoints: with some ip addresses (technically some individual pod ip addresses, but you don't usually need to know that).  if it says endpoints: &lt;none&gt; that's usually a sign that the labels don't match up.
you probably want some level of automatic restarts.  a pod can fail for lots of reasons, including code bugs and network hiccups.  if you set restartpolicy: never then you'll have a failed pod, and requests to the service will fail until you take manual intervention of some sort.  i'd suggest setting this to at least restartpolicy: onfailure, or (for a deployment) leaving it at its default value of always.  (there is more discussion on job restart policies in the kubernetes documentation.)
you probably want a deployment here.  a job is meant for a case where you do some set of batch processing and then the job completes; that's part of why kubectl wait doesn't have the lifecycle option you're looking for.
i'm guessing you want a deployment instead.  with what you've shown here i don't think you need to make any changes at all besides
apiversion: apps/v1
kind: deployment

everything so far about services and dns and labels still applies.
you can kubectl wait for a deployment to be available.  since a job is expected to run to completion and exit, that's the state kubectl wait allows.  a deployment is &quot;available&quot; if there is at least a minimum number of managed pods running that pass their health checks, which i think is the state you're after.
kubectl wait --for=condition=available deployment/my-project-listener

there are simpler ways to check for database liveness.  a huge fraction of what you show here is an involved sequence with special permissions to see if the database is running before the pod starts up.
what happens if the database fails while the pod is running?  one common thing that will happen is you'll get a cascading sequence of exceptions and your pod will crash.  then with restartpolicy: always kubernetes will try to restart it; but if the database still isn't available, it will crash again; and you'll get to a crashloopbackoff state.  if the database does become available again then eventually kubernetes will try to restart the pod and it will succeed.
this same logic can apply at startup time.  if the pod tries to start up, and the database isn't ready yet, and it crashes, kubernetes will by default restart it, adding some delays after the first couple of attempts.  if the database starts up within 30 seconds or so then the application will be up within a minute or so.  the restart count will be greater than 0, but kubectl logs --previous will hopefully have a clear exception.
this will let you delete about half of what you show here.  delete all of the initcontainers: block; then, since you're not doing any kubernetes api operations, delete the role and rolebinding objects too.
if you really do want to force the pod to wait for the database and treat startup as a special case, i'd suggest a simpler shell script using the mysql client tool, or even the wait-for script that makes basic tcp calls (the mechanism described in docker compose wait for container x before starting y).  this still lets you avoid all of the kubernetes rbac setup.
"
60964196,how does spring boot connect localhost mysql in k8s?,"i'm new to k8s, using docker-mac-desktop k8s test spring boot app.

i'm can connect success when spring boot in local connect mysql.

transform to docker k8s can not connected.

error is:

com.mysql.cj.jdbc.exceptions.communicationsexception: communications link failure
at com.mysql.cj.jdbc.exceptions.sqlerror.createcommunicationsexception(sqlerror.java:174) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
at com.mysql.cj.jdbc.exceptions.sqlexceptionsmapping.translateexception(sqlexceptionsmapping.java:64) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
at com.mysql.cj.jdbc.connectionimpl.createnewio(connectionimpl.java:836) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
at com.mysql.cj.jdbc.connectionimpl.&lt;init&gt;(connectionimpl.java:456) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
at com.mysql.cj.jdbc.connectionimpl.getinstance(connectionimpl.java:246) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
at com.mysql.cj.jdbc.nonregisteringdriver.connect(nonregisteringdriver.java:197) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]


is is local listening port:

redis-ser  2008    f    6u  ipv4 0x135c39363e412e7f      0t0  tcp 127.0.0.1:6379 (listen)
kubectl    2909    f    7u  ipv4 0x135c39365cf8a6ff      0t0  tcp 127.0.0.1:8500 (listen)
kubectl    2909    f    8u  ipv6 0x135c39365e7fb2cf      0t0  tcp [::1]:8500 (listen)
mysqld     4091    f   28u  ipv4 0x135c39364db92abf      0t0  tcp 127.0.0.1:3306 (listen)
mysqld     4091    f   32u  ipv4 0x135c39364dc7e6ff      0t0  tcp 127.0.0.1:33060 (listen)
com.docke 41823    f   20u  ipv4 0x135c39365b9a4d1f      0t0  tcp 127.0.0.1:6443 (listen)
com.docke 41823    f   25u  ipv6 0x135c39365e7f9a4f      0t0  tcp *:32003 (listen)
com.docke 41926    f    8u  ipv4 0x135c393660b3785f      0t0  tcp 127.0.0.1:60701 (listen)


spring boot database config:

spring:
datasource:
url: jdbc:mysql://mysqldb:3306/foo
# mysqldb.default.svc.cluster.local tried
#
driver-class-name: com.mysql.cj.jdbc.driver
username: f


spring boot pod.yml:

apiversion: v1
kind: pod
metadata:
  name: user-service
  labels:
    app: user-service
spec:
  containers:
    - name: user-service
      image: user_service:v1


map database ref: https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-mapping-external-services

mysql_svc.yml:

apiversion: v1
kind: service
metadata:
name: mysqldb
spec:
ports:
    - port: 3306
    targetport: 3306

---
kind: endpoints
apiversion: v1
metadata:
name: mysqldb
subsets:
- addresses:
    - ip: 192.168.0.101
    ports:
    - port: 3306


while the ip is come from ifconfig

en0: flags=8863&lt;up,broadcast,smart,running,simplex,multicast&gt; mtu 1500
    options=400&lt;channel_io&gt;
    inet6 fe80::c2c:cc27:8121:fbac%en0 prefixlen 64 secured scopeid 0x5 
    inet 192.168.0.101 netmask 0xffffff00 broadcast 192.168.0.255
    nd6 options=201&lt;performnud,dad&gt;
    media: autoselect
    status: active


how can connect from k8s pod connect localhost database? :)
",<java><mysql><spring-boot><kubernetes><google-kubernetes-engine>,60968347,2,"i'm solved this issue.

changed mysql_svc.yml

apiversion: v1
kind: service
metadata:
name: mysqldb
spec:
type: externalname
externalname: host.docker.internal
ports:
    - port: 3306
    targetport: 3306


map local port

change datasource config:

spring.datasource.url=jdbc:mysql://mysqldb/foo


then connect success.

ref answer
"
60865160,"""line 2: mapping values are not allowed in this context"" when creating an ingress resource","i'm trying to create an ingress resource on eks (aws kubernetes).

when doing kubect get all i have:

name                                                       ready   status    restarts   age
pod/auth-demo-6dfb9b5d78-n8znm                             1/1     running   0          36m
pod/mysql-79945f6847-8jsss                                 1/1     running   0          4d
pod/ngnix-nginx-ingress-controller-54988f47d7-k4j5h        1/1     running   0          64m
pod/ngnix-nginx-ingress-default-backend-77457bd5ff-dppwr   1/1     running   0          64m

name                                          type           cluster-ip       external-ip                                                                      port(s)                      age
service/auth-demo                             nodeport       10.100.133.171   &lt;none&gt;                                                                           8080:32330/tcp               43h
service/kubernetes                            clusterip      10.100.0.1       &lt;none&gt;                                                                       443/tcp                      6d
service/mysql                                 clusterip      10.100.132.124   &lt;none&gt;                                                                       3306/tcp                     4d
service/ngnix-nginx-ingress-controller        loadbalancer   10.100.187.138   ad7e991b46f3e11ea82750210ef3e95f-&lt;ommited&gt;.&lt;ommited&gt;.elb.amazonaws.com   80:32282/tcp,443:30047/tcp   64m
service/ngnix-nginx-ingress-default-backend   clusterip      10.100.89.32     &lt;none&gt;                                                                       80/tcp                       64m

name                                                  ready   up-to-date   available   age
deployment.apps/auth-demo                             1/1     1            1           43h
deployment.apps/mysql                                 1/1     1            1           4d
deployment.apps/ngnix-nginx-ingress-controller        1/1     1            1           64m
deployment.apps/ngnix-nginx-ingress-default-backend   1/1     1            1           64m

name                                                             desired   current   ready   age
replicaset.apps/auth-demo-6dfb9b5d78                             1         1         1       43h
replicaset.apps/mysql-79945f6847                                 1         1         1       4d
replicaset.apps/ngnix-nginx-ingress-controller-54988f47d7        1         1         1       64m
replicaset.apps/ngnix-nginx-ingress-default-backend-77457bd5ff   1         1         1       


64m

this is my super simple yaml:

apiversion: networking.k8s.io/v1beta1
  kind: ingress
  metadata:
    annotations:
      kubernetes.io/ingress.class: nginx
    name: &lt;ommited&gt;
    namespace: default
  spec:
    rules:
        http:
          paths:
            - backend:
                servicename: auth-demo
                serviceport: 8080
              path: /


i get: error: error parsing deployment.yaml: error converting yaml to json: yaml: line 2: mapping values are not allowed in this context on kubectl apply -f deployment.yaml.

does any know of a way to diagnose? thanks!
",<amazon-web-services><kubernetes><kubernetes-ingress><amazon-eks>,60865281,2,"there is indentation issue in the yaml and below should work.

apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: test-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
  namespace: default
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          servicename: auth-demo
          serviceport: 8080

"
69189695,k8s groundnuty/k8s-wait-for image failing to start as init container (with helm),"i'm facing a problem with the image groundnuty/k8s-wait-for. project at github and repo at dockerhub.
i'm pretty sure that error is in command arguments, as the init container fails with init:crashloopbackoff.
about image:
this image is used for init containers, which need to postpone a pod deployment. the script that is in the image waits for the pod or job to complete, after it completes it lets the main container and all replicas start deploying.
in my example, it should wait for a job named {{ .release.name }}-os-server-migration-{{ .release.revision }} to finish, and after it detects it is finished it should let main containers start. helm templates are used.
by my understanding, the job name is {{ .release.name }}-os-server-migration-{{ .release.revision }} and the second command argument at the init container in deployment.yml needs to be the same so the init container can depend on the named job. any other opinions or experiences with this approach?
there are templates attached.
deployment.yml:
apiversion: apps/v1
kind: deployment
metadata:
  name: {{ .release.name }}-os-{{ .release.revision }}
  namespace: {{ .values.namespace }}
  labels:
    app: {{ .values.fullname }}
spec:
  replicas: {{ .values.replicacount }}
  selector:
    matchlabels:
      app: {{ .values.fullname }}
  template:
    metadata:
      labels:
        app: {{ .values.fullname }}
    spec:
      {{- with .values.imagepullsecrets }}
      imagepullsecrets:
        {{- toyaml . | nindent 8 }}
      {{- end }}
      containers:
        - name: {{ .chart.name }}
          image: &quot;{{ .values.image.repository }}:{{ .values.image.tag }}&quot;
          imagepullpolicy: {{ .values.image.pullpolicy }}
          ports:
            - name: http
              containerport: 8080
          resources:
            {{- toyaml .values.resources | nindent 12 }}
      initcontainers:
        - name: &quot;{{ .chart.name }}-init&quot;
          image: &quot;groundnuty/k8s-wait-for:v1.3&quot;
          imagepullpolicy: &quot;{{ .values.init.pullpolicy }}&quot;
          args:
            - &quot;job&quot;
            - &quot;{{ .release.name }}-os-server-migration-{{ .release.revision }}&quot;

job.yml:
apiversion: batch/v1
kind: job
metadata:
  name: {{ .release.name }}-os-server-migration-{{ .release.revision }}
  namespace: {{ .values.migration.namespace }}
spec:
  backofflimit: {{ .values.migration.backofflimit }}
  template:
    spec:
      {{- with .values.migration.imagepullsecrets }}
      imagepullsecrets:
        {{- toyaml . | nindent 8 }}
      {{- end }}
      containers:
        - name: {{ .values.migration.fullname }}
          image: &quot;{{ .values.migration.image.repository }}:{{ .values.migration.image.tag }}&quot;
          imagepullpolicy: {{ .values.migration.image.pullpolicy }}
          command:
            - sh
            - /app/migration-entrypoint.sh
      restartpolicy: {{ .values.migration.restartpolicy }}

logs:
  normal   scheduled  46s                default-scheduler  successfully assigned development/octopus-dev-release-os-1-68cb9549c8-7jggh to minikube
  normal   pulled     41s                kubelet            successfully pulled image &quot;groundnuty/k8s-wait-for:v1.3&quot; in 4.277517553s
  normal   pulled     36s                kubelet            successfully pulled image &quot;groundnuty/k8s-wait-for:v1.3&quot; in 3.083126925s
  normal   pulling    20s (x3 over 45s)  kubelet            pulling image &quot;groundnuty/k8s-wait-for:v1.3&quot;
  normal   created    18s (x3 over 41s)  kubelet            created container os-init
  normal   started    18s (x3 over 40s)  kubelet            started container os-init
  normal   pulled     18s                kubelet            successfully pulled image &quot;groundnuty/k8s-wait-for:v1.3&quot; in 1.827195139s
  warning  backoff    4s (x4 over 33s)   kubelet            back-off restarting failed container

kubectl get all -n development
name                                                        ready   status                  restarts   age
pod/octopus-dev-release-os-1-68cb9549c8-7jggh   0/1     init:crashloopbackoff   2          44s
pod/octopus-dev-release-os-1-68cb9549c8-9qbdv   0/1     init:crashloopbackoff   2          44s
pod/octopus-dev-release-os-1-68cb9549c8-c8h5k   0/1     init:error              2          44s
pod/octopus-dev-release-os-migration-1-9wq76    0/1     completed               0          44s
......
......
name                                                       completions   duration   age
job.batch/octopus-dev-release-os-migration-1   1/1           26s        44s


",<kubernetes><kubernetes-helm>,69343497,2,"for anyone facing the same issue, i will explain my fix.
problem was that the containers inside deployment.yaml had no permissions to use kube api. so, groundnuty/k8s-wait-for:v1.3 container could not check has the job {{ .release.name }}-os-server-migration-{{ .release.revision }} completed or not. that's why init containers instantly failed with crashlooperror.
after adding service account, role, and role binding everything worked great, and groundnuty/k8s-wait-for:v1.3 successfully waited for the job(migration) to finish, in order to let the main container run.
here are the examples of the code for the service account, role, and role binding that solved the issue.
sa.yaml
apiversion: v1
kind: serviceaccount
metadata:
  name: sa-migration
  namespace: development

role.yaml
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  name: migration-reader
rules:
  - apigroups: [&quot;batch&quot;,&quot;extensions&quot;]
    resources: [&quot;jobs&quot;]
    verbs: [&quot;get&quot;,&quot;watch&quot;,&quot;list&quot;]

role-binding.yaml
apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: migration-reader
subjects:
- kind: serviceaccount
  name: sa-migration
roleref:
  kind: role
  name: migration-reader
  apigroup: rbac.authorization.k8s.io

"
69218033,gke ingress; apply rewrite to specific service,"given this yaml:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: test-ingress-2
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
  - host: test.com
    http:
      paths:
      - backend:
          servicename: admin
          serviceport: 8080
        path: /admin/*
        pathtype: implementationspecific
      - backend:
          servicename: keycloak
          serviceport: 8080
        path: /auth/*
        pathtype: implementationspecific

i would like the rewrite-target to only apply to the service admin. requests to keycloak should not be affected. how can i achieve that?
",<kubernetes><kubernetes-ingress>,69218094,2,"you can separate out ingress file or config, just make sure you keep the different name
so you can you create two ingress
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: test-ingress-1
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
  - host: test.com
    http:
      paths:
      - backend:
          servicename: admin
          serviceport: 8080
        path: /admin/*
        pathtype: implementationspecific

ingress: 2 with different config, you can edit anything as per need now remove annotation.
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: test-ingress-2
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
  - host: test.com
    http:
      paths:
      - backend:
          servicename: keycloak
          serviceport: 8080
        path: /auth/*
        pathtype: implementationspecific

if you want to store everything in a single yaml file you can also do it by merging them with ---.
"
69260816,db migration with helm,"i am trying to migrate our cassandra tables to use liquibase. basically the idea is trivial, have a pre-install and pre-upgrade job that will run some liquibase scripts and will manage our database upgrade.
for that purpose i have created a custom docker image that will have the actual liquibase cli and then i can invoke it from the job. for example:
apiversion: batch/v1
kind: job
metadata:
  name: &quot;{{ .release.name }}-update-job&quot;
  namespace: spring-k8s
  labels:
    app.kubernetes.io/managed-by: {{ .release.service | quote }}
    app.kubernetes.io/instance: {{ .release.name | quote }}
    app.kubernetes.io/version: {{ .chart.appversion }}
    helm.sh/chart: &quot;{{ .chart.name }}-{{ .chart.version }}&quot;
  annotations:
    # this is what defines this resource as a hook. without this line, the
    # job is considered part of the release.
    &quot;helm.sh/hook&quot;: pre-install, pre-upgrade
    &quot;helm.sh/hook-weight&quot;: &quot;5&quot;
    &quot;helm.sh/hook-delete-policy&quot;: before-hook-creation
spec:
  template:
    metadata:
      name: &quot;{{ .release.name }}-cassandra-update-job&quot;
      namespace: spring-k8s
      labels:
        app.kubernetes.io/managed-by: {{ .release.service | quote }}
        app.kubernetes.io/instance: {{ .release.name | quote }}
        helm.sh/chart: &quot;{{ .chart.name }}-{{ .chart.version }}&quot;
    spec:
      restartpolicy: never
      containers:
        - name: pre-install-upgrade-job
          image: &quot;lq/liquibase-cassandra:1.0.0&quot;
          command: [&quot;/bin/bash&quot;]
          args:
            - &quot;-c&quot;
            - &quot;./liquibase-cassandra.sh --username {{ .values.liquibase.username }} --password {{ .values.liquibase.username }} --url {{ .values.liquibase.url | squote }} --file {{ .values.liquibase.file }}&quot;

where .values.liquibase.file == databasechangelog.json.
so this image lq/liquibase-cassandra:1.0.0 basically has a script liquibase-cassandra.sh that when passed some arguments can do its magic and update the db schema (not going to go into the details).
the problem is that last argument: --file {{ .values.liquibase.file }}. this file resides not in the image, obviously, but in each micro-services repository.
i need a way to &quot;copy&quot; that file to the image, so that i could invoke it. one way would be to build this lq/liquibase-cassandra all the time (with the same lifecycle of the project itself) and copy the file into it, but that will take time and seems like at least cumbersome. what am i missing?
",<kubernetes><hook><kubernetes-helm>,69260817,2,"it turns out that helm hooks can be used for other things, not only jobs. as such, i can mount this file into a configmap before the job even starts (the file i care about resides in resources/databasechangelog.json):
apiversion: v1
kind: configmap
metadata:
  name: &quot;liquibase-changelog-config-map&quot;
  namespace: spring-k8s
  annotations:
    helm.sh/hook: pre-install, pre-upgrade
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: &quot;1&quot;
data:
{{ (.files.glob &quot;resources/*&quot;).asconfig | indent 2 }}

and then just reference it inside the job:
  .....
  spec:
  restartpolicy: never
  volumes:
    - name: liquibase-changelog-config-map
      configmap:
        name: liquibase-changelog-config-map
        defaultmode: 0755
  containers:
    - name: pre-install-upgrade-job
      volumemounts:
        - name: liquibase-changelog-config-map
          mountpath: /liquibase-changelog-file
      image: &quot;lq/liquibase-cassandra:1.0.0&quot;
      command: [&quot;/bin/bash&quot;]
      args:
        - &quot;-c&quot;
        - &quot;./liquibase-cassandra.sh --username {{ .values.liquibase.username }} --password {{ .values.liquibase.username }} --url {{ .values.liquibase.url | squote }} --file {{ printf &quot;/liquibase-changelog-file/%s&quot; .values.liquibase.file }}&quot;

"
47076858,kubernetes helm chart - debugging,"i'm unable to find good information describing these errors:

[sarah@localhost helm] helm install statefulset --name statefulset --debug
[debug] created tunnel using local port: '33172'

[debug] server: ""localhost:33172""

[debug] original chart version: """"
[debug] chart path: /home/helm/statefulset/

error: error validating """": error validating data: [field spec.template for v1beta1.statefulsetspec is required, field spec.servicename for v1beta1.statefulsetspec is required, found invalid field containers for v1beta1.statefulsetspec]


i'm still new to helm; i've built two working charts that were similar to this template and didn't have these errors, even though the code isn't much different. i'm thinking there might be some kind of formatting error that i'm not noticing. either that, or it's due to the different type (the others were pods, this is statefulset). 

the yaml file it's referencing is here:

apiversion: apps/v1beta1
kind: statefulset
metadata:
  name: ""{{.values.primaryname}}""
  labels:
    name: ""{{.values.primaryname}}""
    app: ""{{.values.primaryname}}""
    chart: ""{{.chart.name}}-{{.chart.version}}""
  annotations:
    ""helm.sh/created"": {{.release.time.seconds | quote }}
spec:
  #serviceaccount: ""{{.values.primaryname}}-sa""
  containers:
  - name: {{.values.containername}}
    image: ""{{.values.postgresimage}}""
    ports:
    - containerport: 5432
      protocol: tcp
      name: postgres
    resources:
      requests:
        cpu: {{default ""100m"" .values.cpu}}
        memory: {{default ""100m"" .values.memory}}
    env:
    - name: pghost
      value: /tmp
    - name: pg_primary_user
      value: primaryuser
    - name: pg_mode
      value: set
    - name: pg_primary_port
      value: ""5432""
    - name: pg_primary_password
      value: ""{{.values.primarypassword}}""
    - name: pg_user
      value: testuser
    - name: pg_password
      value: ""{{.values.userpassword}}""
    - name: pg_database
      value: userdb
    - name: pg_root_password
      value: ""{{.values.rootpassword}}""
    volumemounts:
    - name: pgdata
      mountpath: ""/pgdata""
      readonly: false
    volumes:
    - name: pgdata
      persistentvolumeclaim:
       claimname: {{.values.pvcname}}


would someone be able to a) point me in the right direction to find out how to implement the spec.template and spec.servicename required fields, b) understand why the field 'containers' is invalid, and/or c) give mention of any tool that can help debug helm charts? i've attempted 'helm lint' and the '--debug' flag but 'helm lint' shows no errors, and the flag output is shown with the errors above.

is it possible the errors are coming from a different file, also? 
",<postgresql><kubernetes><containers><kubernetes-helm><statefulset>,47077814,2,"statefulsets objects has different structure than pods are. you need to modify your yaml file a little:

apiversion: apps/v1beta1
kind: statefulset
metadata:
  name: ""{{.values.primaryname}}""
  labels:
    name: ""{{.values.primaryname}}""
    app: ""{{.values.primaryname}}""
    chart: ""{{.chart.name}}-{{.chart.version}}""
  annotations:
    ""helm.sh/created"": {{.release.time.seconds | quote }}
spec:
  selector:
    matchlabels:
      app: """" # has to match .spec.template.metadata.labels
  servicename: """" # put your servicename here
  replicas: 1 # by default is 1
  template:
    metadata:
      labels:
        app: """" # has to match .spec.selector.matchlabels
    spec:
      terminationgraceperiodseconds: 10
      containers:
      - name: {{.values.containername}}
        image: ""{{.values.postgresimage}}""
        ports: 
        - containerport: 5432
          protocol: tcp
          name: postgres
        resources:
          requests:  
            cpu: {{default ""100m"" .values.cpu}}
            memory: {{default ""100m"" .values.memory}}
        env:
        - name: pghost
          value: /tmp
        - name: pg_primary_user
          value: primaryuser
        - name: pg_mode
          value: set
        - name: pg_primary_port
          value: ""5432""
        - name: pg_primary_password
          value: ""{{.values.primarypassword}}""
        - name: pg_user
          value: testuser
        - name: pg_password
          value: ""{{.values.userpassword}}
        - name: pg_database
          value: userdb
        - name: pg_root_password
          value: ""{{.values.rootpassword}}""
        volumemounts:
        - name: pgdata
          mountpath: ""/pgdata""
          readonly: false
      volumes:
      - name: pgdata
        persistentvolumeclaim:
          claimname: {{.values.pvcname}}

"
46765099,pending status of pods when running gitlab ci runner on kubernetes,"i am currently trying to use a kubernetes cluster for the gitlab ci.
while following the not so good docs (https://docs.gitlab.com/runner/install/kubernetes.html), what i did was manually register a runner with the token from gitlab ci section so i could get another token and use it in the configmap i use for the deployment.

-configmap

apiversion: v1
kind: configmap
metadata:
  name: gitlab-runner
  namespace: gitlab
data:
  config.toml: |
    concurrent = 4
    [[runners]]
        name = ""kubernetes runner""
        url = ""https://url/ci""
        token = ""token""
        executor = ""kubernetes""
        [runners.kubernetes]
            namespace = ""gitlab""


-deployment

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: gitlab-runner
  namespace: gitlab
spec:
  replicas: 4
  selector:
    matchlabels:
      name: gitlab-runner
  template:
    metadata:
      labels:
        name: gitlab-runner
    spec:
      containers:
      - args:
        - run
        image: gitlab/gitlab-runner:latest
        imagepullpolicy: always
        name: gitlab-runner
        volumemounts:
        - mountpath: /etc/gitlab-runner
          name: config
      restartpolicy: always
      volumes:
      - configmap:
          name: gitlab-runner
        name: config


with these two i get to see the runner in the gitlab runner section but whenever i start a job, the new created pods stay in pending status.

i would like to fix it but all i know is that the nodes and pods get these events:

-pods:

events:
 firstseen lastseen count from subobjectpath type reason message
 --------- -------- ----- ---- ------------- -------- ------ -------
 35s 4s 7 {default-scheduler } warning failedscheduling no nodes are available that match all of the following predicates:: matchnodeselector (2).


-nodes:

events:
 firstseen lastseen count from subobjectpath type reason message
 --------- -------- ----- ---- ------------- -------- ------ -------
 4d 31s 6887 {kubelet gitlab-ci-hc6k3ffax54o-master-0} warning failednodeallocatableenforcement failed to update node allocatable limits """": failed to set supported cgroup subsystems for cgroup : failed to set config for supported subsystems : failed to write 3783761920 to memory.limit_in_bytes: write /rootfs/sys/fs/cgroup/memory/memory.limit_in_bytes: invalid argument


any idea of why this is happening?

edit: kubectl describe added:

name:           runner-45384765-project-1570-concurrent-00mb7r
namespace:      gitlab
node:           /
labels:         &lt;none&gt;
status:         pending
ip:
controllers:    &lt;none&gt;
containers:
  build:
    image:      blablabla:latest
    port:
    command:
      sh
      -c
      if [ -x /usr/local/bin/bash ]; then
        exec /usr/local/bin/bash
elif [ -x /usr/bin/bash ]; then
        exec /usr/bin/bash
elif [ -x /bin/bash ]; then
        exec /bin/bash
elif [ -x /usr/local/bin/sh ]; then
        exec /usr/local/bin/sh
elif [ -x /usr/bin/sh ]; then
        exec /usr/bin/sh
elif [ -x /bin/sh ]; then
        exec /bin/sh
else
        echo shell not found
        exit 1
fi


    volume mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-1qm5n (ro)
      /vcs from repo (rw)
    environment variables:
      ci_project_dir:           blablabla
      ci_server:                yes
      ci_server_tls_ca_file:    -----begin certificate-----
blablabla
-----end certificate-----
-----begin certificate-----
blablabla
-----end certificate-----
-----begin certificate-----
blablabla 
-----end certificate-----
-----begin certificate-----
blablabla
-----end certificate-----
-----begin certificate-----
blablabla
-----end certificate-----

      ci:                       true
      gitlab_ci:                true
      ci_server_name:           gitlab
      ci_server_version:        9.5.5-ee
      ci_server_revision:       cfe2d5c
      ci_job_id:                5625
      ci_job_name:              pylint
      ci_job_stage:             build
      ci_commit_sha:            ece31293f8eeb3a36a8585b79d4d21e0ebe8008f
      ci_commit_ref_name:       master
      ci_commit_ref_slug:       master
      ci_registry_user:         gitlab-ci-token
      ci_build_id:              5625
      ci_build_ref:             ece31293f8eeb3a36a8585b79d4d21e0ebe8008f
      ci_build_before_sha:      ece31293f8eeb3a36a8585b79d4d21e0ebe8008f
      ci_build_ref_name:        master
      ci_build_ref_slug:        master
      ci_build_name:            pylint
      ci_build_stage:           build
      ci_project_id:            1570
      ci_project_name:          blablabla
      ci_project_path:          blablabla
      ci_project_path_slug:     blablabla
      ci_project_namespace:     vcs
      ci_project_url:           https://blablabla
      ci_pipeline_id:           2574
      ci_config_path:           .gitlab-ci.yml
      ci_pipeline_source:       push
      ci_runner_id:             111
      ci_runner_description:    testing on kubernetes
      ci_runner_tags:           docker-image-build
      ci_registry:              blablabla
      ci_registry_image:        blablabla
      pylinthome:               ./pylint-home
      gitlab_user_id:           2277
      gitlab_user_email:        blablabla
  helper:
    image:      gitlab/gitlab-runner-helper:x86_64-a9a76a50
    port:
    command:
      sh
      -c
      if [ -x /usr/local/bin/bash ]; then
        exec /usr/local/bin/bash
elif [ -x /usr/bin/bash ]; then
        exec /usr/bin/bash
elif [ -x /bin/bash ]; then
        exec /bin/bash
elif [ -x /usr/local/bin/sh ]; then
        exec /usr/local/bin/sh
elif [ -x /usr/bin/sh ]; then
        exec /usr/bin/sh
elif [ -x /bin/sh ]; then
        exec /bin/sh
else
        echo shell not found
        exit 1
fi


    volume mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-1qm5n (ro)
      /vcs from repo (rw)
    environment variables:
      ci_project_dir:           blablabla
      ci_server:                yes
      ci_server_tls_ca_file:    -----begin certificate-----
blablabla
-----end certificate-----
-----begin certificate-----
blablabla
-----end certificate-----
-----begin certificate-----
blablabla
-----end certificate-----
-----begin certificate-----
blablabla
-----end certificate-----
-----begin certificate-----
blablabla
-----end certificate-----

      ci:                       true
      gitlab_ci:                true
      ci_server_name:           gitlab
      ci_server_version:        9.5.5-ee
      ci_server_revision:       cfe2d5c
      ci_job_id:                5625
      ci_job_name:              pylint
      ci_job_stage:             build
      ci_commit_sha:            ece31293f8eeb3a36a8585b79d4d21e0ebe8008f
      ci_commit_ref_name:       master
      ci_commit_ref_slug:       master
      ci_registry_user:         gitlab-ci-token
      ci_build_id:              5625
      ci_build_ref:             ece31293f8eeb3a36a8585b79d4d21e0ebe8008f
      ci_build_before_sha:      ece31293f8eeb3a36a8585b79d4d21e0ebe8008f
      ci_build_ref_name:        master
      ci_build_ref_slug:        master
      ci_build_name:            pylint
      ci_build_stage:           build
      ci_project_id:            1570
      ci_project_name:          blablabla
      ci_project_path:          blablabla
      ci_project_path_slug:     blablabla
      ci_project_namespace:     vcs
      ci_project_url:           blablabla
      ci_pipeline_id:           2574
      ci_config_path:           .gitlab-ci.yml
      ci_pipeline_source:       push
      ci_runner_id:             111
      ci_runner_description:    testing on kubernetes
      ci_runner_tags:           docker-image-build
      ci_registry:              blablabla
      ci_registry_image:        blablabla
      pylinthome:               ./pylint-home
      gitlab_user_id:           2277
      gitlab_user_email:        blablabla
conditions:
  type          status
  podscheduled  false
volumes:
  repo:
    type:       emptydir (a temporary directory that shares a pod's lifetime)
    medium:
  default-token-1qm5n:
    type:       secret (a volume populated by a secret)
    secretname: default-token-1qm5n
qos class:      besteffort
tolerations:    &lt;none&gt;
events:
  firstseen     lastseen        count   from                    subobjectpath   type            reason                  message
  ---------     --------        -----   ----                    -------------   --------        ------                  -------
  39s           8s              7       {default-scheduler }                    warning         failedscheduling        no nodes are available that match all of the following predicates:: matchnodeselector (2).

",<kubernetes><gitlab><gitlab-ci><gitlab-ci-runner><kubectl>,50576716,2,"@djuarez as long the deployment selector matches the pods label in the template section, and in this case from what i can see that is the case:

selector:
  matchlabels:
    name: gitlab-runner
template:
  metadata:
    labels:
      name: gitlab-runner


it should not be a problem; provided the correct api is used which in this case apiversion: extensions/v1beta1 is also correct. the describe output shows matchnodeselector which has nothing to do with the deployment selector. my guess is the full deployment config is not being shown here and something else is wrong like trying to deploy the pods to specific nodes via nodeseletor that do not have the requested label in the nodeselector condition.
"
46180273,kubernetes deployment file error: found invalid field selector for v1.podspec,"i'm getting an invalid field selector error when i try and create my deployment using a yaml file. the error is error validating data: found invalid field selector for v1.podspec and my file can be seen below.

apiversion: apps/v1beta1
kind: deployment
metadata:
 name: zalenium-deployment
spec:
 replicas: 1
 template:
  metadata:
   labels:
    app: zalenium
  spec:
   serviceaccountname: zalenium
   serviceaccount: zalenium
   selector:
    app: zalenium
    role: grid
   containers:
    - name: zalenium-pod
      image: dosel/zalenium
      ports:
      - containerport: 4444
        protocol: tcp
      volumemounts:
      - name: zalenium-shared
        mountpath: /tmp/mounted
      - name: zalenium-videos
        mountpath: /home/seluser/videos
      resources:
       requests:
        memory: ""250m""
        cpu: ""500m""
       limits:
        memory: ""1gi""
   volumes:
   - name: zalenium-shared
     persistentvolumeclaim:
      claimname: zalenium-shared-claim
   - name: zalenium-videos
     persistentvolumeclaim:
      claimname: zalenium-videos-claim


i have tried using online yaml file validator and they don't seem to show anything wrong with the format. when i try and create the deployment above with the validate=false flag, the deployment runs, but then the pods continuously crash and restart (crashloopbackoff). what should i be looking into? i'm still getting familiar with k8s but from the error i would assume it had something to do with the container specs in my deployment. any tips on approaching this? thanks!
",<docker><kubernetes><yaml><selenium-grid><kubectl>,46180724,2,"as the error message states selector is an invalid field for v1.podspec - so this field is not valid at .spec.template.spec.selector. i think what you are looking for is a .spec.selector. 

that being said, the doc states:


  if specified, .spec.selector must match
  .spec.template.metadata.labels, or it will be rejected by the api.


so you must add role: grid also to your metadata labels (at .spec.template.metadata.labels). your .yaml file would look sth like that then:

apiversion: apps/v1beta1
kind: deployment
metadata:
 name: zalenium-deployment
spec:
 selector:
  matchlabels:
   app: zalenium
   role: grid
 replicas: 1
 template:
  metadata:
   labels:
    app: zalenium
    role: grid
  spec:
   serviceaccountname: zalenium
   serviceaccount: zalenium
   containers:
    - name: zalenium-pod
      image: dosel/zalenium
      ports:
      - containerport: 4444
        protocol: tcp
      volumemounts:
      - name: zalenium-shared
        mountpath: /tmp/mounted
      - name: zalenium-videos
        mountpath: /home/seluser/videos
      resources:
       requests:
        memory: ""250m""
        cpu: ""500m""
       limits:
        memory: ""1gi""
   volumes:
   - name: zalenium-shared
     persistentvolumeclaim:
      claimname: zalenium-shared-claim
   - name: zalenium-videos
     persistentvolumeclaim:
      claimname: zalenium-videos-claim

"
61429781,kubernetes expose does not work from file,"if i run command

kubectl expose deployments/some-app --type=nodeport


it works.

if i run command 

kubectl apply -f expose.yml


where the content of expose.yml is

apiversion: v1
kind: service
metadata:
  name: some-app
  labels:
    app: some-app
spec:
  type: nodeport
  ports:
    - port: 8080
  selector:
    app: some-app


i cannot reach the service.

what is the difference? why the 2nd approach does not work?

edit: use nodeport in the yml as well

edit:
result of command kubectl expose deployments/some-app --type=nodeport --dry-run -o yaml:

apiversion: v1
kind: service
metadata:
  creationtimestamp: null
  labels:
    app: some-app
    type: spring-app
  name: some-app
spec:
  ports:
  - port: 8080
    protocol: tcp
    targetport: 8080
  selector:
    name: some-app
  type: nodeport
status:
  loadbalancer: {}

",<kubernetes><kubectl>,61429873,2,"in your expose command you use --type=nodeport, but in svc type=clusterip. if you want to see what expose command created then add --dry-run --o yaml at the end of the command. you should see like following.

apiversion: v1
kind: service
metadata:
  labels:
    run: some-app
  name: some-app
spec:
  ports:
  - port: 8080
    protocol: tcp
    targetport: 8080
  selector:
    app: some-app
  type: nodeport


nb: after discussion in comment, you need to ensure app: some-app exists on pod leve.
"
61528135,"when using a nginx kubernetes routing loadbalancer with path redirects, why can i not access my service correctly?","i am using aks with helm v2.2 to try deploying a chart that utilizes an nginx loadbalancer pod to control all ingress into my services via a single ip address. this is very much in the experimental phase but i have proven that when i use the following helm ingress configuration for my .net core webapi service:

ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: nginx
  hosts:
    - host:
      paths:
        - /


that i can indeed then visit my exposed api and see the swagger ui at

http://[my external ip]/index.html 


what i then want to do is place several services behind the same loadbalancer (as you are intended to) so my expectations were that i could then change the above service configuration to something like this:

ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: nginx
  hosts:
    - host:
      paths:
        - /servicea


which should then mean i can access the same service via the now updated url:

http://[my external ip]/servicea/index.html


is this what i should be expecting to work? do i need to use any sort of re-write system as so far i get errors back from this url saying that it cannot find certain missing resources. any attempts at using the re-write annotation have not resulted in helping me here either. could someone help me out and point out what i may be doing wrong? with the new url path i end up with the following types of errors on what appears to be the files that the index.html page is trying to load suggesting it is half working but needs some re-writing or something?

failed to load resource: the server responded with a status of 404 ()


as a result of the helm chart template engine the following ingress yaml file is created:

apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: myrelease-release-manager
  labels:
    app.kubernetes.io/name: release-manager
    helm.sh/chart: release-manager-0.1.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: ""1.0""
    app.kubernetes.io/managed-by: tiller
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/force-ssl-redirect: ""false""
    nginx.ingress.kubernetes.io/rewrite-target: /servicea
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""

spec:
  rules:
    - host:
      http:
        paths:
          - path: /servicea
            backend:
              servicename: myrelease-release-manager
              serviceport: 80


as a result of this ingress file i want to visit this service when i go to my external ip address with the path /servicea/index.html.
",<kubernetes><kubernetes-helm><nginx-ingress><azure-aks>,61603608,2,"close, you need to update the rewrite target to /$2

nginx.ingress.kubernetes.io/rewrite-target: /$2


rewrites

/serviceb/foo -> /foo 

/servicea/foo -> /foo 

but each one will be directed to the services for that path 

apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: myrelease-release-manager
  labels:
    app.kubernetes.io/name: release-manager
    helm.sh/chart: release-manager-0.1.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: ""1.0""
    app.kubernetes.io/managed-by: tiller
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/force-ssl-redirect: ""false""
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
    nginx.ingress.kubernetes.io/rewrite-target: /$2
 spec:
   rules:
   - http:
       paths:
       - backend:
           servicename: serviceb
           serviceport: 80
         path: /serviceb(/|$)(.*)
       - backend:
           servicename: servicea
           serviceport: 80
         path: /servicea(/|$)(.*)

"
69099980,certificate always in 'false' state using letsencrypt with cluster issuer in k8s,"i am unable to issue a working certificate for my ingress host in k8s. i use a clusterissuer to issue certificates and the same clusterissuer has issued certificates in the past for my ingress hosts under my domain name *xyz.com. but all of a sudden neither i can issue new certificate  with state 'true' for my host names nor a proper certificate secret (kubernetes.io/tls) gets created (but instead an opaque secret gets created).

**strong text**

**kubectl describe certificate ingress-cert -n abc**

name:         ingress-cert
namespace:    abc
labels:       &lt;none&gt;
annotations:  &lt;none&gt;
api version:  cert-manager.io/v1beta1
kind:         certificate
metadata:
  creation timestamp:  2021-09-08t07:48:32z
  generation:          1
  owner references:
    api version:           extensions/v1beta1
    block owner deletion:  true
    controller:            true
    kind:                  ingress
    name:                  test-ingress
    uid:                   c03ffec0-df4f-4dbb-8efe-4f3550b9dcc1
  resource version:        146643826
  self link:               /apis/cert-manager.io/v1beta1/namespaces/abc/certificates/ingress-cert
  uid:                     90905ab7-22d2-458c-b956-7100c4c77a8d
spec:
  dns names:
    abc.xyz.com
  issuer ref:
    group:      cert-manager.io
    kind:       clusterissuer
    name:       letsencrypt
  secret name:  ingress-cert
status:
  conditions:
    last transition time:        2021-09-08t07:48:33z
    message:                     issuing certificate as secret does not exist
    reason:                      doesnotexist
    status:                      false
    type:                        ready
    last transition time:        2021-09-08t07:48:33z
    message:                     issuing certificate as secret does not exist
    reason:                      doesnotexist
    status:                      true
    type:                        issuing
  next private key secret name:  ingress-cert-gdq7g
events:
  type    reason     age   from          message
  ----    ------     ----  ----          -------
  normal  issuing    11m   cert-manager  issuing certificate as secret does not exist
  normal  generated  11m   cert-manager  stored new private key in temporary secret resource &quot;ingress-cert-gdq7g&quot;
  normal  requested  11m   cert-manager  created new certificaterequest resource &quot;ingress-cert-dp6sp&quot;

i checked the certificate request and it contains no events. also i can see no challenges. i have added the logs below. any help would be appreciated

kubectl describe certificaterequest ingress-cert-dp6sp -n abc

namespace:    abc
labels:       &lt;none&gt;
annotations:  cert-manager.io/certificate-name: ingress-cert
              cert-manager.io/certificate-revision: 1
              cert-manager.io/private-key-secret-name: ingress-cert-gdq7g
api version:  cert-manager.io/v1beta1
kind:         certificaterequest
metadata:
  creation timestamp:  2021-09-08t07:48:33z
  generate name:       ingress-cert-
  generation:          1
  owner references:
    api version:           cert-manager.io/v1alpha2
    block owner deletion:  true
    controller:            true
    kind:                  certificate
    name:                  ingress-cert
    uid:                   90905ab7-22d2-458c-b956-7100c4c77a8d
  resource version:        146643832
  self link:               /apis/cert-manager.io/v1beta1/namespaces/abc/certificaterequests/ingress-cert-dp6sp
  uid:                     fef72617-fc1d-4384-9f4b-a7e4502582d8
spec:
  issuer ref:
    group:  cert-manager.io
    kind:   clusterissuer
    name:   letsencrypt
  request:  ls0tls1crudjtibdrvjusuzjq0fursbsrvfvrvnuls0tls0ktuljq2z6q0nbv2ndqvfbd0feq0nbu0l3rffzsktvwklodmnoqvffqkjrqurnz0vqqurdq0frb0nnz0vcquxmngphtghznjhunnhmmuprylf5ek9ov1j4dgtloxjrbjh5wutmd2l4zefmvul0terra0t6uksyb3lozzrmmthsqmqvcknjagj5rxbynnlrditkclrtoc84t1a0mwdwtuxblzrodvhxwwtyewhtzfdnaflqa21oofpituk1slzzcvv2cvkkrwq1b2cydmvmsju1qljprexsd0o3yjbza3hxckuwmgjxq1exwer6zzfhm08yq2jwd1nqt29wv2x6uy9cdzryvgpmevdms3e4qu52b2dzmuxxru8xcg9yelrobm9lk2u2yvzuedjvq1zldgxpag1iyxrhyxnsatjkl1fkk0dowhovcnfznxvbslhzyveruzlxohivbmvmoxnpynn2owd1qmxck09yqvg2ehhknhzuduiwvenfu00zwis2c2mwmfnyrxaknk01rly3dkffedqytwpuejvoa0nbd0vbqwfbnk1ez0ddu3fhu0lim0rrrupeakvytunrd0p3wurwujbsqknbdwpib0ljy25kemmyzhdmbu5zyjnwa1oyrjbauzv0yvdoewiywnbiatvrwlrbtkjna3foa2lhoxcwqkfrc0zbqu9dckfrruftq0cwtxvhmjzrbvfltlbfdmphnhzquuzovfvinwvumkxdcxloy2zuwmxocwpmbnjqzurul2jtv1hwdviktnhxtnkxs0ewszhtmg0reknpbwluzljrs1k2ehkvzu1wykw4dtgrtgxscdevrhl3ugxvree2tkpvotfpadm3tgpdq0e4nwphly9fyvvvk0p5ahbzatzus1d4uxrpyxdmyxhunun4senpwgf5qzg0q0izdgz2wwp6yuf3ykx4akxycmxvd09luhnxse51zktfm0ntcjzmwggramd5vwhxamywoujhegxcwefssvnbnkn5dzz2umpwamfbow82tmhatxukbmdhewzon00zuzbbynazvffczw8xyzc3qlfgagzlsue5sk51swtfd3evnxppyvy1rdernuxssnr5zkvpdnjltwpmvjq5wkpcl1bgotdiejhjnnyvvw9cskc2zz09ci0tls0tru5eienfulrjrkldqvrfifjfuvvfu1qtls0tlqo=
status:
  conditions:
    last transition time:  2021-09-08t07:48:33z
    message:               waiting on certificate issuance from order abc/ingress-cert-dp6sp-3843501305: &quot;&quot;
    reason:                pending
    status:                false
    type:                  ready
events:                    &lt;none&gt;


here is the ingress.yaml
kind: ingress
apiversion: extensions/v1beta1
metadata:
  name: test-ingress
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: 20m
    kubernetes.io/ingress.class: &quot;nginx&quot;
    cert-manager.io/cluster-issuer: &quot;letsencrypt&quot;
spec:
  rules:
    - host: abc.xyz.com
      http:
        paths:
          - path: /static
            backend:
              servicename: app-service
              serviceport: 80
          - path: /
            backend:
              servicename: app-service
              serviceport: 8000
  tls:
  - hosts:
    - abc.xyz.com
    secretname: ingress-cert

here is the clusterissuer:
apiversion: cert-manager.io/v1alpha2
kind: clusterissuer
metadata:
  name: letsencrypt
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: example@user.de
    privatekeysecretref:
      name: letsencrypt-key
    solvers:
    - http01:
        ingress:
          class: nginx

",<kubernetes><kubernetes-ingress><lets-encrypt><acme>,72498076,2,"works only with nginx ingress controller
i was using clusterissuer but i changed it to issuer and it works.
-- install cert-manager (installed version 1.6.1) and be sure that the three pods are running
-- create an issuer by appling this yml be sure that the issuer is running.
apiversion: cert-manager.io/v1
kind: issuer
metadata:
  name: letsencrypt-nginx
  namespace: default
spec:
 acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: example@example.com
    privatekeysecretref:
      name: letsencrypt-nginx-private-key
    solvers:
    - http01:
       ingress:
         class: nginx

-- add this to your ingress annotations
cert-manager.io/issuer: letsencrypt-nginx

-- add the secretname to your ingress spec.tls.hosts
spec:
  tls:
  - hosts:
    - yourdomain.com
    secretname: letsencrypt-nginx

notice that the nginx ingress controller is able to generate the certificate crd automatically via a special annotation: cert-manager.io/issuer. this saves work and time, because you don't have to create and maintain a separate manifest for certificates as well (only the issuer manifest is required). for other ingresses you may need to provide the certificate crd as well.
"
49036199,how to enable https tls on kubernetes gce,"i successfully deployed my web app on kubernetes in google cloud. it is serving via http. i followed all guides on how to add ssl certificate and it was added according to google cloud console however, it only work as http , when you try to access the web app as https. the browser says ""this site cant be reached""

my ingress yaml looks like this

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: no-rules-map
spec:
  tls:
  - secretname: testsecret
  backend:
    servicename: s1
    serviceport: 80


for secret

apiversion: v1
data:
  tls.crt: [crt]
  tls.key: [key]
kind: secret
metadata:
  name: testsecret
  namespace: default
type: opaque

",<kubernetes><cloud><google-compute-engine><google-kubernetes-engine><kubernetes-security>,49050273,2,"i used this command to upload my ssl certificate 

kubectl create secret tls tls-secret --key=/tmp/tls.key --cert=/tmp/tls.crt


instead of yaml file secret below and it works better. at least for google cloud 

apiversion: v1
data:
  tls.crt: [crt]
  tls.key: [key]
kind: secret
metadata:
  name: testsecret
  namespace: default
type: opaque


make sure when you go to kubernates engine -&gt; configuration in google cloud console that your secret type is  secret: kubernetes.io/tls and not only secret. when you create your secret using yaml it is created as secret only and not secret: kubernetes.io/tls. 

for more information you can take a look at these following links:
https://github.com/kubernetes/ingress-gce#backend-https

enter link description here

https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer#remarks
"
69401495,folder deleted/not created inside the common dir mounted with emptydir{} type on eks fargate pod,"we are facing strange issue with eks fargate pods. we want to push logs to cloudwatch with sidecar fluent-bit container and for that we are mounting the separately created /logs/boot and /logs/access folders on both the containers with emptydir: {} type. but somehow the access folder is getting deleted. when we tested this setup in local docker it produced desired results and things were working fine but not when deployed in the eks fargate. below is our manifest files
dockerfile
from anapsix/alpine-java:8u201b09_server-jre_nashorn

arg log_dir=/logs

# install base packages
run apk update
run apk upgrade
# run apk add ca-certificates &amp;&amp; update-ca-certificates

# dynamically set the java_home path
run export java_home=&quot;$(dirname $(dirname $(readlink -f $(which java))))&quot; &amp;&amp; echo $java_home

# add curl
run apk --no-cache add curl

run mkdir -p $log_dir/boot $log_dir/access
run chmod -r 0777 $log_dir/*

# add metadata to the image to describe which port the container is listening on at runtime.

# change timezone
run apk add --update tzdata
env tz=&quot;asia/kolkata&quot;

# clean apk cache
run rm -rf /var/cache/apk/*

# setting java home
env java_home=/opt/jdk

# copy all files and folders
copy . .
run rm -rf /opt/jdk/jre/lib/security/cacerts
copy cacerts /opt/jdk/jre/lib/security/cacerts
copy standalone.xml /jboss-eap-6.4-integration/standalone/configuration/

# set the working directory.
workdir /jboss-eap-6.4-integration/bin

expose 8177

cmd [&quot;./erctl&quot;]

deployment
apiversion: apps/v1
kind: deployment
metadata:
  name: vinintegrator
  namespace: eretail
  labels:
    app: vinintegrator
    pod: fargate
spec:
  selector:
    matchlabels:
      app: vinintegrator
      pod: fargate
  replicas: 2
  template:
    metadata:
      labels:
        app: vinintegrator
        pod: fargate
    spec:
      securitycontext:
        fsgroup: 0
      serviceaccount: eretail
      containers:
      - name: vinintegrator
        imagepullpolicy: ifnotpresent
        image: 653580443710.dkr.ecr.ap-southeast-1.amazonaws.com/vinintegrator-service:latest
        resources:
          limits:
            memory: &quot;7629mi&quot;
            cpu: &quot;1.5&quot;
          requests:
            memory: &quot;5435mi&quot;
            cpu: &quot;750m&quot;
        ports:
        - containerport: 8177
          protocol: tcp
        # securitycontext:
          # runasuser: 506
          # runasgroup: 506
        volumemounts:
          - mountpath: /jboss-eap-6.4-integration/bin
            name: bin
          - mountpath: /logs
            name: logs
      - name: fluent-bit
        image: 657281243710.dkr.ecr.ap-southeast-1.amazonaws.com/fluent-bit:latest
        imagepullpolicy: ifnotpresent
        env:
          - name: host_name
            valuefrom:
              fieldref:
                fieldpath: spec.nodename
          - name: pod_name
            valuefrom:
              fieldref:
                fieldpath: metadata.name
          - name: pod_namespace
            valuefrom:
              fieldref:
                fieldpath: metadata.namespace
        resources:
          limits:
            memory: 200mi
          requests:
            cpu: 200m
            memory: 100mi
        volumemounts:
        - name: fluent-bit-config
          mountpath: /fluent-bit/etc/
        - name: logs
          mountpath: /logs
          readonly: true
      volumes:
        - name: fluent-bit-config
          configmap:
            name: fluent-bit-config
        - name: logs
          emptydir: {}
        - name: bin
          persistentvolumeclaim:
            claimname: vinintegrator-pvc

below is the /logs folder ownership and permission. please notice the 's' in drwxrwsrwx
drwxrwsrwx    3 root     root          4096 oct  1 11:50 logs

below is the content inside logs folder. please notice the access folder is not created or deleted.
/logs # ls -lrt
total 4
drwxr-sr-x    2 root     root          4096 oct  1 11:50 boot
/logs #

below is the configmap of fluent-bit
apiversion: v1
kind: configmap
metadata:
  name: fluent-bit-config
  namespace: eretail
  labels:
    k8s-app: fluent-bit
data:
  fluent-bit.conf: |
    [service]
        flush                     5
        log_level                 info
        daemon                    off
        parsers_file              parsers.conf
        http_server               on
        http_listen               0.0.0.0
        http_port                 2020
        
    @include application-log.conf
  
  application-log.conf: |
    [input]
        name                tail
        path                /logs/boot/*.log
        tag                 boot
        
    [input]
        name                tail
        path                /logs/access/*.log
        tag                 access
        
    [output]
        name                cloudwatch_logs
        match               *boot*
        region              ap-southeast-1
        log_group_name      eks-fluent-bit
        log_stream_prefix   boot-log-
        auto_create_group   on
        
    [output]
        name                cloudwatch_logs
        match               *access*
        region              ap-southeast-1
        log_group_name      eks-fluent-bit
        log_stream_prefix   access-log-
        auto_create_group   on
        
  parsers.conf: |
    [parser]
        name                docker
        format              json
        time_key            time
        time_format         %y-%m-%dt%h:%m:%s.%lz

below is error log of fluent-bit container
aws for fluent bit container image version 2.14.0
fluent bit v1.7.4
* copyright (c) 2019-2021 the fluent bit authors
* copyright (c) 2015-2018 treasure data
* fluent bit is a cncf sub-project under the umbrella of fluentd
* https://fluentbit.io

[2021/10/01 06:20:33] [ info] [engine] started (pid=1)
[2021/10/01 06:20:33] [ info] [storage] version=1.1.1, initializing...
[2021/10/01 06:20:33] [ info] [storage] in-memory
[2021/10/01 06:20:33] [ info] [storage] normal synchronization mode, checksum disabled, max_chunks_up=128
[2021/10/01 06:20:33] [error] [input:tail:tail.1] read error, check permissions: /logs/access/*.log
[2021/10/01 06:20:33] [ warn] [input:tail:tail.1] error scanning path: /logs/access/*.log
[2021/10/01 06:20:38] [error] [net] connection #33 timeout after 5 seconds to: 169.254.169.254:80
[2021/10/01 06:20:38] [error] [net] socket #33 could not connect to 169.254.169.254:80

",<docker><kubernetes><amazon-eks><aws-fargate><fluent-bit>,69402891,2,"suggest remove the following from your dockerfile:
run mkdir -p $log_dir/boot $log_dir/access
run chmod -r 0777 $log_dir/*

use the following method to setup the log directories and permissions:
apiversion: v1  
kind: pod    # deployment
metadata:
  name: busy
  labels:
    app: busy
spec:
  volumes:
  - name: logs  # shared folder with ephemeral storage
    emptydir: {}

  initcontainers:    # setup your log directory here
  - name: setup
    image: busybox
    command: [&quot;bin/ash&quot;, &quot;-c&quot;]
    args:
    - &gt;
      mkdir -p /logs/boot /logs/access;
      chmod -r 777 /logs
    volumemounts:
    - name: logs
      mountpath: /logs

  containers:
  - name: app    # run your application and logs to the directories
    image: busybox
    command: [&quot;bin/ash&quot;,&quot;-c&quot;]
    args:
    - &gt;
      while :; do echo &quot;$(date): $(uname -r)&quot; | tee -a /logs/boot/boot.log /logs/access/access.log; sleep 1; done
    volumemounts:
    - name: logs
      mountpath: /logs

  - name: logger    # any logger that you like
    image: busybox
    command: [&quot;bin/ash&quot;,&quot;-c&quot;]
    args:           # tail the app logs, forward to cw etc...
    - &gt;
      sleep 5;
      tail -f /logs/boot/boot.log /logs/access/access.log
    volumemounts:
    - name: logs
      mountpath: /logs

the snippet runs on fargate as well, run kubectl logs -f busy -c logger to see the tailing. in real world, the &quot;app&quot; is your java app, &quot;logger&quot; is any log agent you desired. note fargate has native logging capability using aws fluent-bit, you do not need to run aws fluent-bit as sidecar.
"
69497900,installation failed with chart dependency to redis,"in my umbrella helm chart, i defined a dependency to redis:
apiversion: v2
appversion: &quot;1.0&quot;
description: a helm chart for kubernetes
name: my-project
version: 0.1.0

dependencies:
  - name: redis
    version: ~6.2.x
    repository: https://charts.bitnami.com/bitnami

at time of writing, the latest version is 6.2.6 (see https://bitnami.com/stack/redis/helm).
but when i execute helm dependency update my-project, helm downloads version 6.2.0 instead of 6.2.6.
when i try to install my chart, it fails:
error: installation failed: unable to build kubernetes objects from release manifest: [unable to recognize &quot;&quot;: no matches for kind &quot;deployment&quot; in version &quot;extensions/v1beta1&quot;, unable to recognize &quot;&quot;: no matches for kind &quot;statefulset&quot; in version &quot;apps/v1beta2&quot;]
without the dependency to redis, my chart installs fine.
i also tried to point at that specific redis version in chart.yaml, but then helm dependency list returns:
name    version repository                              status
redis   6.2.6   https://charts.bitnami.com/bitnami      wrong version

i'm running kubernetes in docker desktop on my laptop.
the versions i'm using:

helm version: 3.7.0-rc.2
k8s server: 1.21.2
k8s client: 1.21.4

when i install redis independently using helm install my-release bitnami/redis, the installation succeeds.
how do i use redis 6.2.6 as a dependency in my chart?
",<kubernetes><redis><kubernetes-helm>,69500629,2,"k8s version 1.21 have deployment latest api
simple ref : https://stackoverflow.com/a/66164857/5525824
while the chart you are using the older api you might need to some changes or use the latest chart for installation.
the latest deployment api version is : apps/v1
you can check your k8s cluster supported api using
for kind in `kubectl api-resources | tail +2 | awk '{ print $1 }'`; do kubectl explain $kind; done | grep -e &quot;kind:&quot; -e &quot;version:&quot;

output
kind:     deployment
version:  v1
kind:     statefulset
version:  v1

or use simple command : kubectl api-versions
you should checkout this bitnami redis document : https://artifacthub.io/packages/helm/bitnami/redis
it updated a few days back and could work with minor changes of api only in your case.
if you check the stable redis version helm chart : https://github.com/helm/charts/blob/master/stable/redis/templates/redis-master-statefulset.yaml
stateful api version : apiversion: apps/v1
you change your bitnami helm chart api using the : https://github.com/bitnami/charts/tree/master/bitnami/redis#common-parameters
you can change the api version at : https://github.com/bitnami/charts/blob/9f9d8aa887608e39aaab4ca1a80677605825b888/bitnami/redis/templates/master/statefulset.yaml#l2

previous versions of this helm chart use apiversion: v1 (installable
by both helm 2 and 3), this helm chart was updated to apiversion: v2
(installable by helm 3 only). here you can find more information about
the apiversion field. the different fields present in the chart.yaml
file has been ordered alphabetically in a homogeneous way for all the
bitnami helm charts

read more at : https://helm.sh/docs/topics/charts/#the-apiversion-field
or : https://github.com/bitnami/charts/tree/master/bitnami/redis#to-1200
you have two option

either you change the helm chart edit it with latest stable api
downgrade the k8s cluster to 1.16 or 1.18 and use your old redis chart which is giving error with 1.21

"
69447844,kubernetes qbittorrentwebui showing only on path '/',"i am trying to host a qbittorrent server with kubernetes. i have composed a yaml for the https://hub.docker.com/r/linuxserver/qbittorrent docker container.
the problem is that it is accessible only from path /. as soon as i move it to /torrent it does not find it anymore: 404 not found.
steps to replicate:

apply following yamls
helm install nginx ingress-nginx/ingress-nginx
go to service_ip:8080, settings, webui, uncheck &quot;enable host header validation&quot;
go to localhost:nginx_port/torrent

result:

page not loading

expected result:

qbittorrent webui appears and works

what i tried:

adding nginx.ingress.kubernetes.io/rewrite-target: / to annotations

server.yaml:
apiversion: apps/v1
kind: deployment
metadata:
  name: torrent-deployment
  labels:
    app: torrent
spec:
  replicas: 1
  selector:
    matchlabels:
      pod-label: torrent-pod
  template:
    metadata:
      labels:
        pod-label: torrent-pod
    spec:
      containers:
      - name: linuxserver
        image: linuxserver/qbittorrent:amd64-latest
---
apiversion: v1
kind: service
metadata:
  name: torrent-service
  labels:
    app: torrent
spec:
  selector:
    pod-label: torrent-pod
  ports:
  - port: 8080
    name: torrent-deployment

ingress.yaml:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: torrent-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
  labels:
    app: torrent
spec:
  rules:
  - http:
      paths:
      - path: /torrent
        pathtype: prefix
        backend:
          service:
            name: torrent-service
            port:
              number: 8080

",<nginx><kubernetes><kubernetes-ingress>,69492637,2,"thanks to @matt_j i have found a workaround. i wrote and yaml for nginx myself and added the configurations from the post mentioned by matt ( https://github.com/qbittorrent/qbittorrent/wiki/nginx-reverse-proxy-for-web-ui ) and it worked.
these are the yamls i came up with:
server.yaml:
apiversion: apps/v1
kind: deployment
metadata:
  name: nginx
  labels:
    app: nginx
  namespace: nginx
spec:
  selector:
    matchlabels:
      pod-label: nginx
  template:
    metadata:
      labels:
        pod-label: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        volumemounts:
          - name: nginx-conf
            mountpath: /etc/nginx/
      volumes:
        - name: nginx-conf
          configmap:
            name: nginx-conf
            items:
            - key: nginx.conf
              path: nginx.conf
  replicas: 1
# status:
---
apiversion: v1
kind: service
metadata:
  namespace: nginx
  name: nginx
  labels:
    app: nginx
spec:
  selector:
    pod-label: nginx
  ports:
  - port: 80
    name: nginx

config.yaml:
apiversion: v1
kind: configmap
metadata:
  name: nginx-conf
  namespace: nginx
data:
  nginx.conf: |
    user  nginx;
    worker_processes  auto;

    error_log  /var/log/nginx/error.log notice;
    pid        /var/run/nginx.pid;

    http {
            server {
                    server_name 10.152.183.95;
                    listen 80;
                    location /torrent/ {
                            proxy_pass              http://torrent-service.qbittorrent:8080/;
                            #proxy_http_version      1.1;
                            proxy_set_header        x-forwarded-host        $http_host;
                            proxy_set_header        x-forwarded-for         $remote_addr;
                            #proxy_cookie_path      /                       &quot;/; secure&quot;;
                            }
            }
    }

    events {
        worker_connections  1024;
    }

"
59606889,how to upgrade istio service mesh from http to http2?,"we are on kubernetes and use istio service mesh. currently, there is ssl termination for https in gateway. i see in the istio-proxy logs that the http protocol is http 1.1.

i want to upgrade http 1.1 to http2 due to its various advantages. clients should call our services http2 over ssl/tls.

i am using this blog for an internal demo on this topic. 

these are the bottlenecks:

1) i want to propose a plan which will causes least amount of changes. i understand i need to update the gateway from 

apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: bookinfo-gateway
spec:
  selector:
    istio: ingressgateway # use istio default controller
  servers:
  - port:
      number: 443
      name: https
      protocol: https
    hosts:
    - ""*""
    tls:
      mode: simple
      servercertificate: /etc/certs/server.pem
      privatekey: /etc/certs/privatekey.pem


to 

apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: bookinfo-gateway
spec:
  selector:
    istio: ingressgateway # use istio default controller
  servers:
  - port:
      number: 80
      name: http2
      protocol: http2
    hosts:
    - ""*""
    tls:
      mode: simple
      servercertificate: /etc/certs/server.pem
      privatekey: /etc/certs/privatekey.pem


based on the examples i see in the istio's gateway documentation.

i want to know: will this allow http2 over tls connections from browsers (which support only this mode)? can i provide tls details for http2, like i did with https?

2) what are some of the other istio configurations to update?

3) will this change be break microservices which are using http protocol currently? how can i mitigate this?

4) i was reading about destinationrule and upgrade policy. is this a good fit? 
",<http><kubernetes><http2><kubernetes-ingress><istio>,59610581,2,"based on my knowledge, istio documentation and istio feature stages(http2 in stable phase)


  1) will this allow http2 over tls connections from browsers (which support only this mode)? can i provide tls details for http2, like i did with https?


yes, it should allow http2.




  2) what are some of the other istio configurations to update?


places when you have options to apply http2 :




gateway




apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: my-ingress
spec:
  selector:
    app: my-ingress-gateway
  servers:
  - port:
      number: 80
      name: **http2**
      protocol: **http2**
    hosts:
    - ""*""





service protocol selection 




manual protocol selection


  protocols can be specified manually by naming the service port name: [-]. the following protocols are supported:



grpc 
grpc
web 
http 
http2 
https 
mongo 
mysql* 
redis* 
tcp 
tls 
udp



  *these protocols are disabled by default to avoid accidentally enabling experimental features. to enable them, configure the corresponding pilot environment variables.




kind: service
metadata:
  name: myservice
spec:
  ports:
  - number: 80
    name: http2





  3) will this change be break microservices which are using http protocol currently? how can i mitigate this?
  
  4) i was reading about destinationrule and upgrade policy. is this a good fit?


i think it should be a good fit,you would have to upgrade h2upgradepolicy and change services to http2.



i hope it will help you.
"
69420357,aks hpa setting configurable properties,"we are using aks 1.19.11 version and would like to know whether we could enable the configurable scaling behavior in aks also as horizontal pod autoscaler.
if yes, the current hpa setting used is with apiversion: autoscaling/v1. is it possible to configure these hpa behavior properties with these api version?
",<kubernetes><azure-aks><kubernetes-pod><hpa>,69440727,2,"if you ask specifically about behavior field, the answer is: no, it's not available in apiversion: autoscaling/v1 and if you want to leverage it, you need to use autoscaling/v2beta2. it's clearly stated here:

starting from v1.18 the v2beta2 api allows scaling behavior to be
configured through the hpa behavior field.

if you have doubts, you can easily check it on your own by trying to apply a new horizontalpodautoscaler object definition, containing this field, but instead of required autoscaling/v2beta2, use autoscaling/v1. you should see the error message similar to the one below:
error: error validating &quot;nginx-multiple.yaml&quot;: error validating data: [validationerror(horizontalpodautoscaler.spec): unknown field &quot;behavior&quot; in io.k8s.api.autoscaling.v1.horizontalpodautoscalerspec, validationerror(horizontalpodautoscaler.spec): unknown field &quot;metrics&quot; in io.k8s.api.autoscaling.v1.horizontalpodautoscalerspec]; if you choose to ignore these errors, turn validation off with --validate=false

as you can see both metrics and behavior fields in spec are not valid in autoscaling/v1 however they are perfectly valid in autoscaling/v2beta2 api.
to check whether your aks cluster supports this api version, run:
$ kubectl api-versions | grep autoscaling
autoscaling/v1
autoscaling/v2beta1
autoscaling/v2beta2

if your result is similar to mine (i.e. you can see autoscaling/v2beta2), it means your aks cluster supports this api version.
"
51297136,kubectl error: the object has been modified; please apply your changes to the latest version and try again,"i am getting below error while trying to apply patch :

core@dgoutam22-1-coreos-5760 ~ $ kubectl apply -f ads-central-configuration.yaml
warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
error from server (conflict): error when applying patch:
{""data"":{""default"":""{\""dedicated_redis_cluster\"": {\""nodes\"": [{\""host\"": \""192.168.1.94\"", \""port\"": 6379}]}}""},""metadata"":{""annotations"":{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiversion\"":\""v1\"",\""data\"":{\""default\"":\""{\\\""dedicated_redis_cluster\\\"": {\\\""nodes\\\"": [{\\\""host\\\"": \\\""192.168.1.94\\\"", \\\""port\\\"": 6379}]}}\""},\""kind\"":\""configmap\"",\""metadata\"":{\""annotations\"":{},\""creationtimestamp\"":\""2018-06-27t07:19:13z\"",\""labels\"":{\""acp-app\"":\""acp-discovery-service\"",\""version\"":\""1\""},\""name\"":\""ads-central-configuration\"",\""namespace\"":\""acp-system\"",\""resourceversion\"":\""1109832\"",\""selflink\"":\""/api/v1/namespaces/acp-system/configmaps/ads-central-configuration\"",\""uid\"":\""64901676-79da-11e8-bd65-fa163eaa7a28\""}}\n""},""creationtimestamp"":""2018-06-27t07:19:13z"",""resourceversion"":""1109832"",""uid"":""64901676-79da-11e8-bd65-fa163eaa7a28""}}
to:
&amp;{0xc4200bb380 0xc420356230 acp-system ads-central-configuration ads-central-configuration.yaml 0xc42000c970 4434 false}
**for: ""ads-central-configuration.yaml"": operation cannot be fulfilled on configmaps ""ads-central-configuration"": the object has been modified; please apply your changes to the latest version and try again**
core@dgoutam22-1-coreos-5760 ~ $ 

",<kubernetes><kubectl>,57982624,2,"i am able to reproduce the issue in my test environment. steps to reproduce:


create a deployment from kubernetes engine > workloads > deploy
input your application name, namespace, labels
select cluster or create new cluster


you are able to view the yaml file here and here is the sample:

---
apiversion: ""apps/v1""
kind: ""deployment""
metadata:
  name: ""nginx-1""
  namespace: ""default""
  labels:
    app: ""nginx-1""
spec:
  replicas: 3
  selector:
    matchlabels:
      app: ""nginx-1""
  template:
    metadata:
      labels:
        app: ""nginx-1""
    spec:
      containers:
      - name: ""nginx""
        image: ""nginx:latest""
---
apiversion: ""autoscaling/v2beta1""
kind: ""horizontalpodautoscaler""
metadata:
  name: ""nginx-1-hpa""
  namespace: ""default""
  labels:
    app: ""nginx-1""
spec:
  scaletargetref:
    kind: ""deployment""
    name: ""nginx-1""
    apiversion: ""apps/v1""
  minreplicas: 1
  maxreplicas: 5
  metrics:
  - type: ""resource""
    resource:
      name: ""cpu""
      targetaverageutilization: 80


after deployment if you go to kubernetes engine > workloads > nginx-1 (click on it)  

a.) you will get deployment details (overview, details, revision history, events, yaml)
  b.) click on yaml and copy the content from yaml tab
  c.) create new yaml file and paste the content and save the file
  d.) now if you run the command $kubectl apply -f newyamlfile.yaml, it will shows you the below error:  

warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
error from server (conflict): error when applying patch:
{""metadata"":{""annotations"":{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiversion\"":\""apps/v1\"",\""kind\"":\""deployment\"",\""metadata\"":{\""annotations\"":{\""deployment.kubernetes.io/revision\"":\""1\""},\""creationtimestamp\"":\""2019-09-17t21:34:39z\"",\""generation\"":1,\""labels\"":{\""app\"":\""nginx-1\""},\""name\"":\""nginx-1\"",\""namespace\"":\""default\"",\""resourceversion\"":\""218884\"",\""selflink\"":\""/apis/apps/v1/namespaces/default/deployments/nginx-1\"",\""uid\"":\""f41c5b6f-d992-11e9-9adc-42010a80023b\""},\""spec\"":{\""progressdeadlineseconds\"":600,\""replicas\"":3,\""revisionhistorylimit\"":10,\""selector\"":{\""matchlabels\"":{\""app\"":\""nginx-1\""}},\""strategy\"":{\""rollingupdate\"":{\""maxsurge\"":\""25%\"",\""maxunavailable\"":\""25%\""},\""type\"":\""rollingupdate\""},\""template\"":{\""metadata\"":{\""creationtimestamp\"":null,\""labels\"":{\""app\"":\""nginx-1\""}},\""spec\"":{\""containers\"":[{\""image\"":\""nginx:latest\"",\""imagepullpolicy\"":\""always\"",\""name\"":\""nginx\"",\""resources\"":{},\""terminationmessagepath\"":\""/dev/termination-log\"",\""terminationmessagepolicy\"":\""file\""}],\""dnspolicy\"":\""clusterfirst\"",\""restartpolicy\"":\""always\"",\""schedulername\"":\""default-scheduler\"",\""securitycontext\"":{},\""terminationgraceperiodseconds\"":30}}},\""status\"":{\""availablereplicas\"":3,\""conditions\"":[{\""lasttransitiontime\"":\""2019-09-17t21:34:47z\"",\""lastupdatetime\"":\""2019-09-17t21:34:47z\"",\""message\"":\""deployment has minimum availability.\"",\""reason\"":\""minimumreplicasavailable\"",\""status\"":\""true\"",\""type\"":\""available\""},{\""lasttransitiontime\"":\""2019-09-17t21:34:39z\"",\""lastupdatetime\"":\""2019-09-17t21:34:47z\"",\""message\"":\""replicaset \\\""nginx-1-7b4bb7fbf8\\\"" has successfully progressed.\"",\""reason\"":\""newreplicasetavailable\"",\""status\"":\""true\"",\""type\"":\""progressing\""}],\""observedgeneration\"":1,\""readyreplicas\"":3,\""replicas\"":3,\""updatedreplicas\"":3}}\n""},""generation"":1,""resourceversion"":""218884""},""spec"":{""replicas"":3},""status"":{""availablereplicas"":3,""observedgeneration"":1,""readyreplicas"":3,""replicas"":3,""updatedreplicas"":3}}
to:
resource: ""apps/v1, resource=deployments"", groupversionkind: ""apps/v1, kind=deployment""
name: ""nginx-1"", namespace: ""default""
object: &amp;{map[""apiversion"":""apps/v1"" ""metadata"":map[""name"":""nginx-1"" ""namespace"":""default"" ""selflink"":""/apis/apps/v1/namespaces/default/deployments/nginx-1"" ""uid"":""f41c5b6f-d992-11e9-9adc-42010a80023b"" ""generation"":'\x02' ""labels"":map[""app"":""nginx-1""] ""annotations"":map[""deployment.kubernetes.io/revision"":""1""] ""resourceversion"":""219951"" ""creationtimestamp"":""2019-09-17t21:34:39z""] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchlabels"":map[""app"":""nginx-1""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""nginx-1""] ""creationtimestamp"":&lt;nil&gt;] ""spec"":map[""containers"":[map[""imagepullpolicy"":""always"" ""name"":""nginx"" ""image"":""nginx:latest"" ""resources"":map[] ""terminationmessagepath"":""/dev/termination-log"" ""terminationmessagepolicy"":""file""]] ""restartpolicy"":""always"" ""terminationgraceperiodseconds"":'\x1e' ""dnspolicy"":""clusterfirst"" ""securitycontext"":map[] ""schedulername"":""default-scheduler""]] ""strategy"":map[""type"":""rollingupdate"" ""rollingupdate"":map[""maxunavailable"":""25%"" ""maxsurge"":""25%""]] ""revisionhistorylimit"":'\n' ""progressdeadlineseconds"":'\u0258'] ""status"":map[""observedgeneration"":'\x02' ""replicas"":'\x01' ""updatedreplicas"":'\x01' ""readyreplicas"":'\x01' ""availablereplicas"":'\x01' ""conditions"":[map[""message"":""deployment has minimum availability."" ""type"":""available"" ""status"":""true"" ""lastupdatetime"":""2019-09-17t21:34:47z"" ""lasttransitiontime"":""2019-09-17t21:34:47z"" ""reason"":""minimumreplicasavailable""] map[""lasttransitiontime"":""2019-09-17t21:34:39z"" ""reason"":""newreplicasetavailable"" ""message"":""replicaset \""nginx-1-7b4bb7fbf8\"" has successfully progressed."" ""type"":""progressing"" ""status"":""true"" ""lastupdatetime"":""2019-09-17t21:34:47z""]]] ""kind"":""deployment""]}
for: ""test.yaml"": operation cannot be fulfilled on deployments.apps ""nginx-1"": the object has been modified; please apply your changes to the latest version and try again


to solve the problem, you need to find the exact yaml file and then edit it as per your requirement, after that you can run $kubectl apply -f nginx-1.yaml  

hope this information finds you well.
"
51444980,unable to kubectl connect my kubernetes cluster,"with a kubernetes cluster up and running and the ability to go to the master over ssh with ssh-keys and run kubectl commands there; i want to run kubectl commands on my local machine. so i try to setup the configuration, following the kubectl config:

kubectl config set-cluster mykube --server=https://&lt;master-ip&gt;:6443
kubectl config set-context mykube --cluster=mykube --user=mykube-adm
kubectl config set-credentials mykube-adm --client-key=path/to/private/keyfile 


activate the context:

kubectl config use-context mykube


when i run a kubectl command:

kubectl get nodes


it returns:


  the connection to the server localhost:8080 was refused - did you specify the right host or port?


the output of kubectl config view

apiversion: v1
clusters:
- cluster:
    server: https://&lt;master-ip&gt;:6443
  name: mykubecontexts:
- context:
    cluster: mykube
    user: mykube-adm
  name: mykube
current-context: mykube
kind: config
preferences: {}
users:
- name: mykube-adm
  user:
    client-key: path/to/private/keyfile

",<kubernetes><kubectl>,51462626,2,"unfortunately, above kubectl config file is incorrect. it seems an error appeared due to manual formatting or something else. 

new line is missing in this part (name: mykubecontexts:):

clusters:
- cluster:
    server: https://&lt;master-ip&gt;:6443
  name: mykubecontexts:
- context:
    cluster: mykube
    user: mykube-adm
  name: mykube


correct one is:

clusters:
- cluster:
    server: https://&lt;master-ip&gt;:6443
  name: mykube
contexts:
- context:
    cluster: mykube
    user: mykube-adm
  name: mykube


that's why cluster's name is mykubecontexts::

clusters:
- cluster:
    server: https://&lt;master-ip&gt;:6443
  name: mykubecontexts:


and that's why there is no context in it, because contexts: is not defined.

kubectl cannot find context mykube and switches to default one where server=localhost:8080 is by default.

kubectl config is located in ${home}/.kube/config file by default if --kubeconfig flag or $kubeconfig environment variable are not set.

please correct it to the following one:

apiversion: v1
clusters:
- cluster:
    server: https://&lt;master-ip&gt;:6443
  name: mykube
contexts:
- context:
    cluster: mykube
    user: mykube-adm
  name: mykube
current-context: mykube
kind: config
preferences: {}
users:
- name: mykube-adm
  user:
    client-key: path/to/private/keyfile

"
71613787,creating kubernetes pod per kubernetes job and cleanup,"i'm trying to create kubernetes job with the following requirements:

only one pod can be created for each job at most
if the pod failed - the job will fail
max run time of the pod will be 1 hour
if the job finished successfully - delete the job

i tried the following configurations:
apiversion: batch/v1
kind: job
metadata:
  name: {{ .release.name }}
  annotations:
    &quot;helm.sh/hook&quot;: post-install
    &quot;helm.sh/hook-delete-policy&quot;: hook-succeeded
spec:
  template:
    spec:
      containers:
        - name: {{ .release.name }}
          image: {{ .values.image }}
          env:
            - name: arg1
              value: {{ required &quot;arg1 is mandatory&quot; .values.env.arg1 }}
            - name: gitlab_user_email
              value: {{ .values.env.gitlab_user_email }}
          envfrom:
            - secretref:
                name: {{ .release.name }}
      restartpolicy: never
  backofflimit: 1
  activedeadlineseconds: 3600

but it's not working as expected, any ideas?
thanks !
",<kubernetes><kubernetes-helm>,71615200,2,"
only one pod can be created for each job at most

the requested parallelism (.spec.parallelism) can be set to any non-negative value. if it is unspecified, it defaults to 1. if it is specified as 0, then the job is effectively paused until it is increased.

for cronjobs could be helpful successfuljobshistorylimit: 0, failedjobshistorylimit: 0  this will
remove the pods if it's get failed or success so no history or
pod will stays. so only one pod will get created or run.


if the pod failed - the job will fail

that will be the default behavior, also restartpolicy: never so it won't get restarted.

max run time of the pod will be 1 hour
activedeadlineseconds: 3600 you have already added

if the job finished successfully - delete the job


ttlsecondsafterfinished: 100 will solve your issue.
apiversion: batch/v1
kind: job
metadata:
  name: {{ .release.name }}
  annotations:
    &quot;helm.sh/hook&quot;: post-install
    &quot;helm.sh/hook-delete-policy&quot;: hook-succeeded
spec:
  template:
    spec:
      containers:
        - name: {{ .release.name }}
          image: {{ .values.image }}
          env:
            - name: arg1
              value: {{ required &quot;arg1 is mandatory&quot; .values.env.arg1 }}
            - name: gitlab_user_email
              value: {{ .values.env.gitlab_user_email }}
          envfrom:
            - secretref:
                name: {{ .release.name }}
      restartpolicy: never
  backofflimit: 1
  ttlsecondsafterfinished: 100
  activedeadlineseconds: 3600

"
71652878,dynamic routing by hostname only - kubernetes ingress rules,"i have a k8s cluster with multiple different services deployed and would like to use a single ingress to route each incoming request to the appropriate service via a unique hostname dns.
currently, i've only been able to resolve a request when using the root path i.e. service-123.app.com.
as soon as i try to make a request with a path it doesn't resolve. the paths are valid paths to each service. for example, service-123.app.com/page/12345 would be expected by the application.
i might not fully understand how k8s ingress rules are expected to work, but i hoped that it would match based on hostname only and simply forward on the path to the appropriate service.
am i missing something very simple here? any help is much appreciated. thanks!
here are my config files.
ingress.yaml
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    ......
  name: app-name
  namespace: default
spec:
  rules:
  - host: service-123.app.com
    http:
      - path: &quot;/*&quot;
        backend:
          servicename: service-123
          serviceport: 80
  - host: service-456.app.com
    http:
      paths:
      - path: &quot;/*&quot;
        backend:
          servicename: service-456
          serviceport: 80

service.yaml
---
apiversion: v1
kind: service
metadata:
  annotations: {}
  labels:
    app: service-123
  name: service-123
  namespace: default
spec:
  ports:
  - name: port8080
    port: 80
    targetport: 8080
  selector:
    app: service-123
  type: nodeport

",<kubernetes><dynamic><routes><dns><kubernetes-ingress>,71653763,2,"not sure which k8s and ingress controller you are using, but in the later k8s you can specify the pathtype which takes care of path wildcards more nicely.
you would have something like this:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    ......
  name: app-name
  namespace: default
spec:
  rules:
  - host: service-123.app.com
    http:
      - path: /
        pathtype: prefix
        backend:
          servicename: service-123
          serviceport: 80
  - host: service-456.app.com
    http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          servicename: service-456
          serviceport: 80

if you are using an nginx ingress controller a good way to see the right nginx configuration is by looking at the actual nginx.conf generated by the ingress controller.
$ kubectl cp &lt;nginx-ingress-controller-pod&gt;:nginx.conf nginx.conf
$ cat nginx.conf

"
71704023,"how to use kustomize to configure traefik 2.x ingressroute (metadata.name, spec.routes[0].services[0].name & spec.routes[0].match = host() )","we have a eks cluster running with traefik deployed in crd style (full setup on github) and wan't to deploy our app https://gitlab.com/jonashackt/microservice-api-spring-boot with the kubernetes objects deployment, service and ingressroute (see configuration repository here). the manifests look like this:
deployment.yml:
apiversion: apps/v1
kind: deployment
metadata:
  name: microservice-api-spring-boot
spec:
  replicas: 3
  revisionhistorylimit: 3
  selector:
    matchlabels:
      app: microservice-api-spring-boot
      branch: main
  template:
    metadata:
      labels:
        app: microservice-api-spring-boot
        branch: main
    spec:
      containers:
        - image: registry.gitlab.com/jonashackt/microservice-api-spring-boot:c25a74c8f919a72e3f00928917dc4ab2944ab061
          name: microservice-api-spring-boot
          ports:
            - containerport: 8098
      imagepullsecrets:
        - name: gitlab-container-registry

service.yml:
apiversion: v1
kind: service
metadata:
  name: microservice-api-spring-boot
spec:
  ports:
    - port: 80
      targetport: 8098
  selector:
    app: microservice-api-spring-boot
    branch: main

traefik-ingress-route.yml:
apiversion: traefik.containo.us/v1alpha1
kind: ingressroute
metadata:
  name: microservice-api-spring-boot-ingressroute
  namespace: default
spec:
  entrypoints:
    - web
  routes:
    - match: host(`microservice-api-spring-boot-branchname.tekton-argocd.de`)
      kind: rule
      services:
        - name: microservice-api-spring-boot
          port: 80

we already use kustomize and especially the kustomize cli (on a mac or in github actions install with brew install kustomize) with the following folder structure:
 deployment.yml
 kustomization.yaml
 service.yml
 traefik-ingress-route.yml

our kustomization.yaml looks like this:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization

resources:
- deployment.yml
- service.yml
- traefik-ingress-route.yml

images:
- name: registry.gitlab.com/jonashackt/microservice-api-spring-boot
  newtag: foobar

commonlabels:
  branch: foobar

namesuffix: foobar

now changing the metadata.name dynamically to add a suffix to the deployment's, service's and ingressroute's .metadata.name from within our github actions workflow is easy with kustomize cli (because we want the suffix to use a prefixed -, we need to use the -- -barfoo syntax here):
kustomize edit set namesuffix -- -barfoo

check the result with
kustomize build .

also changing the .spec.selector.matchlabels.branch, .spec.template.metadata.labels.branch and .spec.selector.branch in the deployment and service is no problem:
kustomize edit set label branch:barfoo

changing the .spec.template.spec.containers[0].image of our deployment works with:
kustomize edit set image registry.gitlab.com/jonashackt/microservice-api-spring-boot:barfoo

but looking into our ingressroute it seems that .spec.routes[0].services[0].name and .spec.routes[0].match = host() can't be changed with kustomize out of the box?! so how can we change both fields without the need for a replacement tooling like yq or even sed/ envsubst?
",<kubernetes><kubernetes-ingress><traefik><traefik-ingress><kustomize>,71704024,2,"1. change the ingressroutes .spec.routes[0].services[0].name with kustomize
changing the ingressroutes .spec.routes[0].services[0].name is possible with kustomize using a namereference transformer (see docs here) - luckily i found inspiration in this issue. therefore we need to include the configurations keyword in our kustomize.yaml:
namesuffix: foobar
configurations:
  # tie target service metadata.name to ingressroute's spec.routes.services.name
  # once service name is changed, the ingressroute referrerd service name will be changed as well.
  - namereference.yml

we also need to add file called namereference.yml:
namereference:
  - kind: service
    fieldspecs:
      - kind: ingressroute
        path: spec/routes/services/name

as you can see we tie the service's name to the ingressroutes spec/routes/services/name. now running
kustomize edit set namesuffix barfoo

will not only change the metadata.name tags of the deployment, service and ingressroute - but also the .spec.routes[0].services[0].name of the ingressroute, since it is now linked to the metadata.name of the service. note that this only, if both the referrer and the target's have a name tag.
2. change a part of the ingressroutes .spec.routes[0].match = host()
the second part of the question ask how to change a part of the ingressroutes .spec.routes[0].match = host(). there's an open issue in the kustomize github project. right now kustomize doesn't support this use case - only writing a custom generator plugin for kustomize. as this might not be a preferred option, there's another way inspired by this blog post. as we can create yaml files inline in our console using the syntax cat &gt; ./myyamlfile.yml &lt;&lt;eof ... eof we could also use the inline variable substitution.
so first define the branch name as variable:
rule_host_branchname=foobar

and then use the described syntax to create a ingressroute-patch.yml file inline:
cat &gt; ./ingressroute-patch.yml &lt;&lt;eof
apiversion: traefik.containo.us/v1alpha1
kind: ingressroute
metadata:
  name: microservice-api-spring-boot-ingressroute
  namespace: default
spec:
  entrypoints:
    - web
  routes:
    - match: host(\`microservice-api-spring-boot-$rule_host_branchname.tekton-argocd.de\`)
      kind: rule
      services:
        - name: microservice-api-spring-boot
          port: 80

eof

the last step is to use the ingressroute-patch.yml file as patchesstrategicmerge inside our kustomization.yaml like this:
patchesstrategicmerge:
  - ingressroute-patch.yml

now running kustomize build . should output the correct deployment, service and ingressroute for our setup:
apiversion: v1
kind: service
metadata:
  labels:
    branch: barfoo
  name: microservice-api-spring-boot-barfoo
spec:
  ports:
  - port: 80
    targetport: 8098
  selector:
    app: microservice-api-spring-boot
    branch: barfoo
---
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    branch: barfoo
  name: microservice-api-spring-boot-barfoo
spec:
  replicas: 3
  revisionhistorylimit: 3
  selector:
    matchlabels:
      app: microservice-api-spring-boot
      branch: barfoo
  template:
    metadata:
      labels:
        app: microservice-api-spring-boot
        branch: barfoo
    spec:
      containers:
      - image: registry.gitlab.com/jonashackt/microservice-api-spring-boot:barfoo
        name: microservice-api-spring-boot
        ports:
        - containerport: 8098
      imagepullsecrets:
      - name: gitlab-container-registry
---
apiversion: traefik.containo.us/v1alpha1
kind: ingressroute
metadata:
  labels:
    branch: barfoo
  name: microservice-api-spring-boot-ingressroute-barfoo
  namespace: default
spec:
  entrypoints:
  - web
  routes:
  - kind: rule
    match: host(`microservice-api-spring-boot-barfoo.tekton-argocd.de`)
    services:
    - name: microservice-api-spring-boot-barfoo
      port: 80

"
51822525,configure kubernetes traefik ingress with different path rewrites for each service,"i'm in the process of migration of our application from single instance docker-compose configuration to kubernetes. i currently have the following example nginx configuration, running as a reverse proxy of my application:

server {
  server_name             example.com;
  ssl_certificate         /etc/nginx/certs/${cert_name};
  ssl_certificate_key     /etc/nginx/certs/${key_name};

  listen                  443 ssl;
  keepalive_timeout       70;

  access_log              /var/log/nginx/access.log mtail;

  ssl_protocols           xxxxxx
  ssl_ciphers             xxxxxx
  ssl_session_cache       shared:ssl:10m;
  ssl_session_timeout     10m;

  rewrite_log             on;
  resolver                127.0.0.11 ipv6=off;

  location /push/ {
        auth_basic                    ""restricted"";
        auth_basic_user_file          /etc/nginx/htpasswd;
        rewrite /push/(.*)        /index.php/$1 break;
        proxy_pass                    pushinterface:3080;
  }

  location /flights/ {
        rewrite /flights/(.*)         /$1 break;
        proxy_pass                    flightstats:3090;
  }

  location /api/ {
        proxy_pass                    $api;
  }

  location /grafana/ {
        access_log                    off;
        log_not_found                 off;
        proxy_pass                    http://grafana:3000;
        rewrite ^/grafana/(.*)        /$1 break;
  }


}

my initial plans for the reverse proxy part was implementing an ingress with nginx ingress controller, but i saw that my configuration can be created as ingress only with nginx plus. that's why i decided to try with traefik, but i'm not sure if it's still possible to have different rewrites of the path for each service. 

i tried the following ingress configuration, but it seems it's not working:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: example-traefik 
  annotations:
    kubernetes.io/ingress.class: traefik
    traefik.frontend.rule.type: replacepathregex
spec:
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          servicename: pushinterface
          serviceport: 80
        path: /push/(.*) /index/$1
      - backend:
          servicename: flights
          serviceport: 80
        path: /flights/(.*) /$1
       - backend:
          servicename: api
          serviceport: 80
        path: /api
      - backend:
          servicename: grafana
          serviceport: 80
        path: /grafana/(.*) /$1


i will appreciate any help for solving this task
",<nginx><kubernetes><traefik><kubernetes-ingress><traefik-ingress>,52038910,2,"after several hours of unsuccessful attempts to solve my issue, i did it with nginx ingress controller and it works great! here's the ingress configuration:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      rewrite /push/(.*) /index/$1 break;
      rewrite /flights/(.*) /$1 break;
      rewrite /grafana/(.*) /$1 break;

spec:
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          servicename: pushinterface
          serviceport: 80
        path: /push
      - backend:
          servicename: flights
          serviceport: 80
        path: /flights
       - backend:
          servicename: api
          serviceport: 80
        path: /api
      - backend:
          servicename: grafana
          serviceport: 80
        path: /grafana


thanks to everyone for the answers! :)
"
59470624,does kubernetes kubectl run with image creates deployment yaml file,"i am trying to use minikube and docker to understand the concepts of kubernetes architecture.

i created a spring boot application with dockerfile, created tag and pushed to dockerhub.

in order to deploy the image in k8s cluster, i issued the below command,

# deployed the image
$ kubectl run &lt;deployment-name&gt; --image=&lt;username/imagename&gt;:&lt;version&gt; --port=&lt;port the app runs&gt;

# exposed the port as nodeport
$ kubectl expose deployment &lt;deployment-name&gt; --type=nodeport


everything worked and i am able to see the 1 pods running kubectl get pods

the docker image i pushed to dockerhub didn't had any deployment yaml file.

below command produced an yaml output

does kubectl command creates deployment yaml file out of the box?

 $ kubectl get deployments --output yaml 


apiversion: v1
items:
- apiversion: apps/v1
  kind: deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: ""1""
    creationtimestamp: ""2019-12-24t14:59:14z""
    generation: 1
    labels:
      run: hello-service
    name: hello-service
    namespace: default
    resourceversion: ""76195""
    selflink: /apis/apps/v1/namespaces/default/deployments/hello-service
    uid: 90950172-1c0b-4b9f-a339-b47569366f4e
  spec:
    progressdeadlineseconds: 600
    replicas: 1
    revisionhistorylimit: 10
    selector:
      matchlabels:
        run: hello-service
    strategy:
      rollingupdate:
        maxsurge: 25%
        maxunavailable: 25%
      type: rollingupdate
    template:
      metadata:
        creationtimestamp: null
        labels:
          run: hello-service
      spec:
        containers:
        - image: thirumurthi/hello-service:0.0.1
          imagepullpolicy: ifnotpresent
          name: hello-service
          ports:
          - containerport: 8800
            protocol: tcp
          resources: {}
          terminationmessagepath: /dev/termination-log
          terminationmessagepolicy: file
        dnspolicy: clusterfirst
        restartpolicy: always
        schedulername: default-scheduler
        securitycontext: {}
        terminationgraceperiodseconds: 30
  status:
    availablereplicas: 1
    conditions:
    - lasttransitiontime: ""2019-12-24t14:59:19z""
      lastupdatetime: ""2019-12-24t14:59:19z""
      message: deployment has minimum availability.
      reason: minimumreplicasavailable
      status: ""true""
      type: available
    - lasttransitiontime: ""2019-12-24t14:59:14z""
      lastupdatetime: ""2019-12-24t14:59:19z""
      message: replicaset ""hello-service-75d67cc857"" has successfully progressed.
      reason: newreplicasetavailable
      status: ""true""
      type: progressing
    observedgeneration: 1
    readyreplicas: 1
    replicas: 1
    updatedreplicas: 1
kind: list
metadata:
  resourceversion: """"
  selflink: """"

",<docker><kubernetes><kubectl>,59471803,2,"i think the easiest way to understand whats going on under the hood when you create kubernetes resources using imperative commands (versus declarative approach by writing and applying yaml definition files) is to run a simple example with 2 additional flags:

--dry-run


and

--output yaml


names of these flags are rather self-explanatory so i think there is no further need for comment explaining what they do. you can simply try out the below examples and you'll see the effect:

kubectl run nginx-example --image=nginx:latest --port=80 --dry-run --output yaml


as you can see it produces the appropriate yaml manifest without applying it and creating actual deployment:

apiversion: apps/v1beta1
kind: deployment
metadata:
  creationtimestamp: null
  labels:
    run: nginx-example
  name: nginx-example
spec:
  replicas: 1
  selector:
    matchlabels:
      run: nginx-example
  strategy: {}
  template:
    metadata:
      creationtimestamp: null
      labels:
        run: nginx-example
    spec:
      containers:
      - image: nginx:latest
        name: nginx-example
        ports:
        - containerport: 80
        resources: {}
status: {}


same with expose command:

kubectl expose deployment nginx-example --type=nodeport --dry-run --output yaml


produces the following output:

apiversion: v1
kind: service
metadata:
  creationtimestamp: null
  labels:
    run: nginx-example
  name: nginx-example
spec:
  ports:
  - port: 80
    protocol: tcp
    targetport: 80
  selector:
    run: nginx-example
  type: nodeport
status:
  loadbalancer: {}


and now the coolest part. you can use simple output redirection:

kubectl run nginx-example --image=nginx:latest --port=80 --dry-run --output yaml &gt; nginx-example-deployment.yaml

kubectl expose deployment nginx-example --type=nodeport --dry-run --output yaml &gt; nginx-example-nodeport-service.yaml


to save generated deployment and nodeport service definitions so you can further modify them if needed and apply using either kubectl apply -f filename.yaml or kubectl create -f filename.yaml.

btw. kubectl run and kubectl expose are generator-based commands and as you may have noticed when creating your deployment (as you probably got the message: kubectl run --generator=deployment/apps.v1beta1 is deprecated and will be removed in a future version. use kubectl create instead.) they use --generator flag. if you don't specify it explicitly it gets the default value which for kubectl run is --generator=deployment/apps.v1beta1 so by default it creates a deployment. but you can modify it by providing --generator=run-pod/v1 nginx-example and instead of deployment it will create a single pod. when we go back to our previous example it may look like this:

kubectl run --generator=run-pod/v1 nginx-example --image=nginx:latest --port=80 --dry-run --output yaml


i hope this answered your question and clarified a bit the mechanism of creating kubernetes resources using imperative commands.
"
71484256,how to copy kubernetes secret content to single file,"i have kubernetes secrets as following and want to copy all content as is to single file.
api: 
  url: https://app.a.com/test
application: 
  metadata: name=&quot;myname&quot;


in yaml i have following
apiversion: apps/v1
kind: deployment
metadata:
  name: active-mq
  labels:
    app: active-mq
spec:
  replicas: 1
  selector:
    matchlabels:
      app: active-mq
  template:
    metadata:
      labels:
        app: active-mq
    spec:
      containers:
        - image: [name-of-my-image-from-docker-hub]
          name: active-mq
          imagepullpolicy: always
          resources:
            requests:
              memory: 500mi
              cpu: 200m
            limits:
              memory: 1000mi
              cpu: 400m
          volumemounts:
          - name: active-creds
            mountpath: /home/my.properties
            subpath: my.properties
      volumes:
      - name: active-creds
        secret:
          secretname: creds
      restartpolicy: always


when i bash to container i see it create the directory name as my.properties under /home. is that something i am missing here?
i am expecting my.properties should contain following
api: 
  url: https://app.a.com/test
application: 
  metadata: name=&quot;myname&quot;


",<kubernetes><kubectl><kubernetes-secrets>,71485738,2,"it's unclear from your question but i suspect not creating the secret to reflect the keys that you need. in this case, the key becomes the filename (i.e. my.properties). you don't want the keys to be api and application.
# create the file locally
echo '
api: 
  url: https://app.a.com/test
application: 
  metadata: name=&quot;myname&quot;' &gt; my.foo

# create the kubernetes secret from it
# nb this renames to &quot;my.properties&quot; from &quot;my.foo&quot;
kubectl create secret generic test \
--from-file=my.properties=${pwd}/my.foo

get secret test \
--output=yaml

yields:
apiversion: v1
data:
  my.properties: yxbpoiak...
kind: secret
metadata:
  name: test
type: opaque


note data contains a key my.properties

then:
# using your deployment
kubectl apply \
--filename=71484256.yaml

# i replaced your image with busybox
kubectl exec \
--stdin --tty \
deployment/test \
-- ash

then from within the container's shell:
# ls /home
my.properties

# more /home/my.properties
api: 
  url: https://app.a.com/test
application: 
  metadata: name=myname

"
59472367,"kubernetes ingress controller redirects to basic login path, does not hold the ingress port or the service path for the specified service","i am attempting to use an ingress controller within k8s to access my web servers running within the cluster. i am testing using path based routing ingress resources, and testing by accessing a worker ip address (as the ingress is using a node port service accessible through any of the worker nodes). 

when i access other test  web applications (without a login page), i am able to access my service as the following:
http://{worker-ip}:{ingress-node-port}/{svc-name}

when i am trying to access an application that has a login page, i am redirected to the following after entering the above:
http://{worker-ip}/vui/login

this obviously does not redirect me to the correct login page, and i am shown an error on the screen. is there any way to hold the path and port name throughout this process so that the /vui/login path is sent with the correct service name and port number? if i input the entire path directly, i am still not able to access the service. i am thinking that since the path is changed to the correct login page, but cannot actually access the service, the redirection is working up to a certain point, and then failing out once the url is changed to not use the path and service name.

any advice on ingress, path based routing, and using them both with a login page redirection would be much appreciated.

below is the ingress definition that i am using in my testing:

apiversion: extensions/v1beta1
kind: ingress 
metadata: 
  name: test-ingress 
  annotations: ingress.kubernetes.io/rewrite-target: / 
spec: 
  rules: 
  - http: 
      paths: 
      - path: /test1 
      backend: 
        servicename: test1-service 
        serviceport: 5678 
      - path: /test2 
      backend: 
        servicename: test2-service 
        serviceport: 5678 
      - path: /test3 
      backend: 
        servicename: test3-service 
        serviceport: 8080

",<kubernetes><kubernetes-ingress><nginx-ingress>,59541154,2,"you can use the next url parameter for your case. for example, you are entering 

https://example.com/test3


this redirects you to the login page if i've understood your problem correctly. the url then becomes the login page url - 

https://example.com/vui/login


in this case, the url that it originally came from isn't preserved.

so, i think you can use the next parameter in your url and redirect the route to that url once the user is authenticated. the url will be - 

https://example.com/vui/login?next=/test3/


then you can deploy your k8s ingress resource with the query-routing annotation -

ingress.kubernetes.io/query-routing: default/query-routing


and add a configmap from where the url params will be exerted -

kind:configmap
apiversion: v1
metadata:
  name: query-routing
data:
  mapping: |-
   [{
    ""field"": ""login"",
    ""value"": ""1"",
    ""path"": ""/test3"",
    ""service"": ""test3-service "",
    ""port"": ""8080""
   }]

"
71527500,running command line for kubectl image in yaml for k8s,"i want to declare in yaml file command line that uses the kubectl image, for any command of `kubectl', i.e. waiting to another pod to be for ready state.
if i run in command:
kubectl wait pod/mypod --for=condition=ready --timeout=120s

i get a true message of:

pod/mypod condition met

first - how to run the command prompt, for simple use?
i.e use of kubectl version, so the output, is the version of the kube, for using the image: kubectl:
kubectl run test -it --rm --image=bitnami/kubectl get pods --restart=never --command -- 
/bin/kubectl version

(i want to run once, and delete the pod automatically when it ends. same thing for the command: kubectl wait pod/mypod --for=condition=ready --timeout=120s or any command uses kubectl image).
the above doesn't work.
also - how should i convert the above to kubernetes yaml file (one time run - when it completed, the pod will be deleted automatically) ?
the following doesn't work, when i am waiting, i.e. for mypod to complete.
apiversion: apps/v1
kind: deployment
metadata:
  name: myapp
spec:
  selector:
    matchlabels:
      app: myapp
  replicas: 3
  template:
    metadata:
      labels:
        app: myapp
    spec:
      initcontainers:
        - name: wait-for-pod
          image: bitnami/kubectl
          args:
            - wait
            - pod/mypod
            - --for=condition=ready
            - --timeout=120s
      containers:
        - name: myapp
          image: myapp

has the status: init:containercannotrun.
and when i run: kubectl describe pod &lt;mypod&gt;, i get the message:
oci runtime create failed: container_linux.go:380: starting container process caused: exec: &quot;wait&quot;: executable file not found in $path: unknown
thanks.
",<kubernetes><kubectl>,71554399,2,"your kubectl run command is wrong. the --image=bitnami/kubectl get pods part is incorrect. you need to specify just the image, not the command.
proper, working command would be
kubectl run test -it --rm --image=bitnami/kubectl --restart=never -- version

when it comes to the deployment manifest, you were almost there. just add command list to the manifest, and it should work.
apiversion: apps/v1
kind: deployment
metadata:
  name: myapp
spec:
  selector:
    matchlabels:
      app: myapp
  replicas: 3
  template:
    metadata:
      labels:
        app: myapp
    spec:
      initcontainers:
        - name: wait-for-pod
          image: bitnami/kubectl
          command:
            - kubectl
          args:
            - wait
            - --for=condition=ready
            - pod/mypod
            - --timeout=120s
      containers:
        - name: myapp
          image: myapp


now, you need to remember, system:serviceaccount:default:default service account, which is attached to every pod, does not have sufficient priviledges to list pods in cluster. all of the above wil not work unless you give default service account proper priviledges
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  namespace: default
  name: service-reader
rules:
- apigroups: [&quot;&quot;]
  resources: [&quot;services&quot;]
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]
---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: service-reader-pod
subjects:
  - kind: serviceaccount
    name: default
    namespace: default
roleref:
  kind: clusterrole
  name: service-reader
  apigroup: rbac.authorization.k8s.io

"
59572721,basic networking through kubernetes services not working in minikube,"i am running a cluster of


3 services that hold a deployment for: mongodb, postgres, and a rest-server
the mongo and postgres service as clusterip but the rest-server uses nodeport
when i kubectl exec and shell into the pods themselves, i can access mongo/postgres but using the docker network ip address
when i try to use the kubernetes service ip address (as given by the clusterip on minikube) i can't get through


here is some sample commands that show the problem

shell in:

host$ kubectl exec -it my-system-mongo-54b8c75798-lptzq /bin/bash


once in, i connect to mongo using the docker network ip:

mongo-pod# mongo mongodb://172.17.0.6
welcome to the mongodb shell.
&gt; exit
bye


now i try to use the k8 service ip (the dns works, as it gets translated to 10.96.154.36 as seen below)

mongo-pod# mongo mongodb://my-system-mongo
mongodb shell version v3.6.3
connecting to: mongodb://my-system-mongo
2020-01-03t02:39:55.883+0000 w network  [thread1] failed to connect to 10.96.154.36:27017 after 5000ms milliseconds, giving up.
2020-01-03t02:39:55.903+0000 e query    [thread1] error: couldn't connect to server my-system-mongo:27017, connection attempt failed :
connect@src/mongo/shell/mongo.js:251:13
@(connect):1:6
exception: connect failed


ping also doesn't work

mongo-pod# ping my-system-mongo
ping my-system-mongo.default.svc.cluster.local (10.96.154.36) 56(84) bytes of data.
--- my-system-mongo.default.svc.cluster.local ping statistics ---
112 packets transmitted, 0 received, 100% packet loss, time 125365ms


my set up is running minikube 1.6.2 with kubernetes 1.17 and helm 3.0.2. here is my full (helm created) dry run yaml file:

name: mysystem-1578018793
last deployed: thu jan  2 18:33:13 2020
namespace: default
status: pending-install
revision: 1
hooks:
---
# source: mysystem/templates/tests/test-connection.yaml
apiversion: v1
kind: pod
metadata:
  name: ""my-system-test-connection""
  labels:
    helm.sh/chart: mysystem-0.1.0
    app.kubernetes.io/name: mysystem
    app.kubernetes.io/instance: mysystem-1578018793
    app.kubernetes.io/version: ""1.16.0""
    app.kubernetes.io/managed-by: helm
  annotations:
    ""helm.sh/hook"": test-success
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args:  ['my-system:']
  restartpolicy: never
manifest:
---
# source: mysystem/templates/configmap.yaml
apiversion: v1
kind: configmap
metadata:
  name: my-system-configmap
  labels:
    helm.sh/chart: mysystem-0.1.0
    app.kubernetes.io/name: mysystem
    app.kubernetes.io/instance: mysystem-1578018793
    app.kubernetes.io/version: ""1.16.0""
    app.kubernetes.io/managed-by: helm
data:
  _lots_of_key_value_pairs: here-i-shortened-it
---
# source: mysystem/templates/my-system-mongo-service.yaml
apiversion: v1
kind: service
metadata:
  name: my-system-mongo
  labels:
    helm.sh/chart: mysystem-0.1.0
    app.kubernetes.io/name: mysystem
    app.kubernetes.io/instance: mysystem-1578018793
    app.kubernetes.io/version: ""1.16.0""
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/component: mongo
spec:
  type: clusterip
  ports:
  - port: 27017
    targetport: 27017
    protocol: tcp
    name: mongo
  selector:
    app.kubernetes.io/name: mysystem
    app.kubernetes.io/instance: mysystem-1578018793
    app.kubernetes.io/component: mongo
---
# source: mysystem/templates/my-system-pg-service.yaml
apiversion: v1
kind: service
metadata:
  name: my-system-postgres
  labels:
    helm.sh/chart: mysystem-0.1.0
    app.kubernetes.io/name: mysystem
    app.kubernetes.io/instance: mysystem-1578018793
    app.kubernetes.io/version: ""1.16.0""
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/component: postgres
spec:
  type: clusterip
  ports:
  - port: 5432
    targetport: 5432
    protocol: tcp
    name: postgres
  selector:
    app.kubernetes.io/name: mysystem
    app.kubernetes.io/instance: mysystem-1578018793
    app.kubernetes.io/component: postgres
---
# source: mysystem/templates/my-system-restsrv-service.yaml
apiversion: v1
kind: service
metadata:
  name: my-system-rest-server
  labels:
    helm.sh/chart: mysystem-0.1.0
    app.kubernetes.io/name: mysystem
    app.kubernetes.io/instance: mysystem-1578018793
    app.kubernetes.io/version: ""1.16.0""
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/component: rest-server
spec:
  type: nodeport
  ports:
  #- port: 8009
  #  targetport: 8009
  #  protocol: tcp
  #  name: jpda
  - port: 8080
    targetport: 8080
    protocol: tcp
    name: http
  selector:
    app.kubernetes.io/name: mysystem
    app.kubernetes.io/instance: mysystem-1578018793
    app.kubernetes.io/component: rest-server
---
# source: mysystem/templates/my-system-mongo-deployment.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: my-system-mongo
  labels:
    helm.sh/chart: mysystem-0.1.0
    app.kubernetes.io/name: mysystem
    app.kubernetes.io/instance: mysystem-1578018793
    app.kubernetes.io/version: ""1.16.0""
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/component: mongo
spec:
  replicas: 1
  selector:
    matchlabels:
      app.kubernetes.io/name: mysystem
      app.kubernetes.io/instance: mysystem-1578018793
      app.kubernetes.io/component: mongo
  template:
    metadata:
      labels:
        app.kubernetes.io/name: mysystem
        app.kubernetes.io/instance: mysystem-1578018793
        app.kubernetes.io/component: mongo
    spec:
      imagepullsecrets:
        - name: regcred
      serviceaccountname: default
      securitycontext:
        {}
      containers:
      - name: my-system-mongo-pod
        securitycontext:
            {}
        image: private.hub.net/my-system-mongo:latest
        imagepullpolicy: always
        envfrom:
          - configmapref:
              name: my-system-configmap
        ports:
        - name: ""mongo""
          containerport: 27017
          protocol: tcp
        resources:
            {}
---
# source: mysystem/templates/my-system-pg-deployment.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: my-system-postgres
  labels:
    helm.sh/chart: mysystem-0.1.0
    app.kubernetes.io/name: mysystem
    app.kubernetes.io/instance: mysystem-1578018793
    app.kubernetes.io/version: ""1.16.0""
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/component: postgres
spec:
  replicas: 1
  selector:
    matchlabels:
      app.kubernetes.io/name: mysystem
      app.kubernetes.io/instance: mysystem-1578018793
      app.kubernetes.io/component: postgres
  template:
    metadata:
      labels:
        app.kubernetes.io/name: mysystem
        app.kubernetes.io/instance: mysystem-1578018793
        app.kubernetes.io/component: postgres
    spec:
      imagepullsecrets:
        - name: regcred
      serviceaccountname: default
      securitycontext:
        {}
      containers:
      - name: mysystem
        securitycontext:
            {}
        image: private.hub.net/my-system-pg:latest
        imagepullpolicy: always
        envfrom:
          - configmapref:
              name: my-system-configmap
        ports:
        - name: postgres
          containerport: 5432
          protocol: tcp
        resources:
            {}
---
# source: mysystem/templates/my-system-restsrv-deployment.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: my-system-rest-server
  labels:
    helm.sh/chart: mysystem-0.1.0
    app.kubernetes.io/name: mysystem
    app.kubernetes.io/instance: mysystem-1578018793
    app.kubernetes.io/version: ""1.16.0""
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/component: rest-server
spec:
  replicas: 1
  selector:
    matchlabels:
      app.kubernetes.io/name: mysystem
      app.kubernetes.io/instance: mysystem-1578018793
      app.kubernetes.io/component: rest-server
  template:
    metadata:
      labels:
        app.kubernetes.io/name: mysystem
        app.kubernetes.io/instance: mysystem-1578018793
        app.kubernetes.io/component: rest-server
    spec:
      imagepullsecrets:
        - name: regcred
      serviceaccountname: default
      securitycontext:
        {}
      containers:
      - name: mysystem
        securitycontext:
            {}
        image: private.hub.net/my-system-restsrv:latest
        imagepullpolicy: always
        envfrom:
          - configmapref:
              name: my-system-configmap
        ports:
        - name: rest-server
          containerport: 8080
          protocol: tcp
        #- name: ""jpda""
        #  containerport: 8009
        #  protocol: tcp
        resources:
            {}

notes:
1. get the application url by running these commands:
  export pod_name=$(kubectl get pods --namespace default -l ""app.kubernetes.io/name=mysystem,app.kubernetes.io/instance=mysystem-1578018793"" -o jsonpath=""{.items[0].metadata.name}"")
  echo ""visit http://127.0.0.1:8080 to use your application""
  kubectl --namespace default port-forward $pod_name 8080:80


my best theory (in part after working through this) is that the kube-proxy is not working properly in minikube, however i am not sure how to troubleshoot this. when is shell into minikube and grep through journalctl for proxy i get this:

# grep proxy journal.log
jan 03 02:16:02 minikube sudo[2780]:   docker : tty=unknown ; pwd=/home/docker ; user=root ; command=/bin/touch -d 2020-01-02 18:16:03.05808666 -0800 /var/lib/minikube/certs/proxy-client.crt
jan 03 02:16:02 minikube sudo[2784]:   docker : tty=unknown ; pwd=/home/docker ; user=root ; command=/bin/touch -d 2020-01-02 18:16:03.05908666 -0800 /var/lib/minikube/certs/proxy-client.key
jan 03 02:16:15 minikube kubelet[2821]: e0103 02:16:15.423027    2821 reflector.go:156] object-""kube-system""/""kube-proxy"": failed to list *v1.configmap: configmaps ""kube-proxy"" is forbidden: user ""system:node:minikube"" cannot list resource ""configmaps"" in api group """" in the namespace ""kube-system"": no relationship found between node ""minikube"" and this object
jan 03 02:16:15 minikube kubelet[2821]: i0103 02:16:15.503466    2821 reconciler.go:209] operationexecutor.verifycontrollerattachedvolume started for volume ""kube-proxy-token-n78g9"" (uniquename: ""kubernetes.io/secret/50fbf70b-724a-4b76-af7f-5f4b91735c84-kube-proxy-token-n78g9"") pod ""kube-proxy-pbs6s"" (uid: ""50fbf70b-724a-4b76-af7f-5f4b91735c84"")
jan 03 02:16:15 minikube kubelet[2821]: i0103 02:16:15.503965    2821 reconciler.go:209] operationexecutor.verifycontrollerattachedvolume started for volume ""xtables-lock"" (uniquename: ""kubernetes.io/host-path/50fbf70b-724a-4b76-af7f-5f4b91735c84-xtables-lock"") pod ""kube-proxy-pbs6s"" (uid: ""50fbf70b-724a-4b76-af7f-5f4b91735c84"")
jan 03 02:16:15 minikube kubelet[2821]: i0103 02:16:15.530948    2821 reconciler.go:209] operationexecutor.verifycontrollerattachedvolume started for volume ""lib-modules"" (uniquename: ""kubernetes.io/host-path/50fbf70b-724a-4b76-af7f-5f4b91735c84-lib-modules"") pod ""kube-proxy-pbs6s"" (uid: ""50fbf70b-724a-4b76-af7f-5f4b91735c84"")
jan 03 02:16:15 minikube kubelet[2821]: i0103 02:16:15.538938    2821 reconciler.go:209] operationexecutor.verifycontrollerattachedvolume started for volume ""kube-proxy"" (uniquename: ""kubernetes.io/configmap/50fbf70b-724a-4b76-af7f-5f4b91735c84-kube-proxy"") pod ""kube-proxy-pbs6s"" (uid: ""50fbf70b-724a-4b76-af7f-5f4b91735c84"")
jan 03 02:16:15 minikube systemd[1]: started kubernetes transient mount for /var/lib/kubelet/pods/50fbf70b-724a-4b76-af7f-5f4b91735c84/volumes/kubernetes.io~secret/kube-proxy-token-n78g9.
jan 03 02:16:16 minikube kubelet[2821]: e0103 02:16:16.670527    2821 configmap.go:200] couldn't get configmap kube-system/kube-proxy: failed to sync configmap cache: timed out waiting for the condition
jan 03 02:16:16 minikube kubelet[2821]: e0103 02:16:16.670670    2821 nestedpendingoperations.go:270] operation for ""\""kubernetes.io/configmap/50fbf70b-724a-4b76-af7f-5f4b91735c84-kube-proxy\"" (\""50fbf70b-724a-4b76-af7f-5f4b91735c84\"")"" failed. no retries permitted until 2020-01-03 02:16:17.170632812 +0000 utc m=+13.192986021 (durationbeforeretry 500ms). error: ""mountvolume.setup failed for volume \""kube-proxy\"" (uniquename: \""kubernetes.io/configmap/50fbf70b-724a-4b76-af7f-5f4b91735c84-kube-proxy\"") pod \""kube-proxy-pbs6s\"" (uid: \""50fbf70b-724a-4b76-af7f-5f4b91735c84\"") : failed to sync configmap cache: timed out waiting for the condition""


and while that does show some problems, i am not sure how to act on them or correct it.

update: 

i spotted this when grepping through the journal:

# grep conntrack journal.log
jan 03 02:16:04 minikube kubelet[2821]: w0103 02:16:04.286682    2821 hostport_manager.go:69] the binary conntrack is not installed, this can cause failures in network connection cleanup.


looking into conntrack, though the minikube vm doesn't have yum or apt!
",<kubernetes><kubernetes-helm><kubectl><minikube>,59577816,2,"let's look at the relevant service:

apiversion: v1
kind: service
metadata:
  name: my-system-mongo
spec:
  ports:
  - port: 27017       # note typo here, see @aviator's answer
    targetport: 27017
    protocol: tcp
    name: mongo
  selector:
    app.kubernetes.io/name: mysystem
    app.kubernetes.io/instance: mysystem-1578018793


in particular note the selector:; this can route traffic to any pod that has these two labels.  for example, this is a valid target:

apiversion: apps/v1
kind: deployment
metadata:
  name: my-system-postgres
spec:
  selector:
    matchlabels:
      app.kubernetes.io/name: mysystem
      app.kubernetes.io/instance: mysystem-1578018793
  template:
    metadata:
      labels:
        app.kubernetes.io/name: mysystem
        app.kubernetes.io/instance: mysystem-1578018793


since every pod has the same pair of labels, any service can send traffic to any pod; your ""mongodb"" service isn't necessarily targeting the actual mongodb pod.  your deployment specs have the same problem and i wouldn't be surprised if the kubectl get pods output is a little bit confused.

the right answer here is to add another label that distinguishes the different parts of your application from each other.  the helm docs recommend

app.kubernetes.io/component: mongodb


this must appear in the labels of the pod spec embedded in the deployments, the matching deployment selector, and the matching service selector; simply setting it on all related objects including the deployment and service labels makes sense.
"
59580634,kubernetes deployment not publicly accesible,"im trying to access a deloyment on our kubernetes cluster on azure. this is a azure kubernetes service (aks). here are the configuration files for the deployment and the service that should expose the deployment.

configurations

apiversion: apps/v1
kind: deployment
metadata:
  name: mira-api-deployment
spec:
  replicas: 1
  selector:
    matchlabels:
      app: mira-api
  template:
    metadata:
      labels:
        app: mira-api
    spec:
      containers:
        - name: backend
          image: registry.gitlab.com/izit/mira-backend
          ports:
            - containerport: 8080
              name: http
              protocol: tcp
      imagepullsecrets:
        - name: regcred


apiversion: v1
kind: service
metadata:
  name: mira-api-service
spec:
  type: loadbalancer
  ports:
    - port: 80
      targetport: 8080
      protocol: tcp
      name: http
  selector:
    run: mira-api


when i check the cluster after applying these configurations i, i see the pod running correctly. also the service is created and has public ip assigned.



after this deployment i don't see any requests getting handled. i get a error message in my browser saying the site is inaccessible. any ideas what i could have configured wrong? 
",<kubernetes><azure-aks><kubernetes-service>,59581021,2,"your service selector labels and pod labels do not match.

you have app: mira-api label in deployment's pod template but have run: mira-api in service's label selector. 

change your service selector label to match the pod label as follows.

apiversion: v1
kind: service
metadata:
  name: mira-api-service
spec:
  type: loadbalancer
  ports:
    - port: 80
      targetport: 8080
      protocol: tcp
      name: http
  selector:
    app: mira-api


to make sure your service is selecting the backend pods or not, you can run kubectl describe svc &lt;svc name&gt; command and check if it has any endpoints listed.

# kubectl describe svc postgres
name:              postgres
namespace:         default
labels:            app=postgres
annotations:       kubectl.kubernetes.io/last-applied-configuration:
                     {""apiversion"":""v1"",""kind"":""service"",""metadata"":{""annotations"":{},""labels"":{""app"":""postgres""},""name"":""postgres"",""namespace"":""default""},""s...
selector:          app=postgres
type:              clusterip
ip:                10.106.7.183
port:              default  5432/tcp
targetport:        5432/tcp
endpoints:         10.244.2.117:5432    &lt;------- this line
session affinity:  none
events:            &lt;none&gt;

"
59393991,gcp gke load balancer connectio refused,"i'm doing a deployment on the gke service and i find that when i try to access the page the message 

err_connection_refused

i have defined a load balancing service for deployment and the configuration is as follows.

this is the .yaml for the deployment

apiversion: apps/v1
kind: deployment
metadata:
  name: bonsai-onboarding
spec:
  selector:
    matchlabels:
      app: bonsai-onboarding
  replicas: 2
  template:
    metadata:
      labels:
        app: bonsai-onboarding
    spec:
     containers:
     - name: bonsai-onboarding
       image: ""eu.gcr.io/diaphanum/onboarding-iocash-master_web:v1""
       ports:
       - containerport: 3000


this is the service .yaml file.

apiversion: v1
kind: service
metadata:
  name: lb-onboarding
spec:
  type: loadbalancer
  selector:
    app: bonsai-onboarding
  ports:
  - protocol: tcp
    port: 3000
    targetport: 3000


this working fine, and all is green in gke  :)

kubectl get pods,svc
name                                     ready   status    restarts   age
pod/bonsai-onboarding-8586b9b699-flhbn   1/1     running   0          3h23m
pod/bonsai-onboarding-8586b9b699-p9sn9   1/1     running   0          3h23m

name                    type           cluster-ip      external-ip    port(s)          age
service/kubernetes      clusterip      xx.xx.yy.yy      &lt;none&gt;         443/tcp          29d
service/lb-onboarding   loadbalancer   xx.xx.yy.yy   xx.xx.yy.yy   3000:32618/tcp   3h


then when i tried to connect the error is err_connection_refused

i think is about the network because y did the next test from my local machine

ping  [load balancer ip]  ---&gt;  correct
telnet [load balancer ip] 3000  ---&gt;  correct


from cloud shell i forward the port 3000 to 8080 and in other cloudshell make a curl http://localhost:8080, and work fine.

any idea about the problem?

thanks in advance
",<kubernetes><google-cloud-platform><load-balancing><google-kubernetes-engine>,59396405,2,"i've changed a little bit your deployment to check it on my cluster because your image was unreachable:


deployment:

apiversion: apps/v1
kind: deployment
metadata:
  name: bonsai-onboarding
spec:
  selector:
    matchlabels:
      app: bonsai-onboarding
  replicas: 2
  template:
    metadata:
      labels:
        app: bonsai-onboarding
    spec:
     containers:
     - name: bonsai-onboarding
       image: nginx:latest
       ports:
       - containerport: 80

service:

apiversion: v1
    kind: service
    metadata:
      name: lb-onboarding
    spec:
      type: loadbalancer
      selector:
        app: bonsai-onboarding
      ports:
      - protocol: tcp
        port: 3000
        targetport: 80



and it works out of the box:

kubectl get pods,svc
name                                     ready   status    restarts   age
pod/bonsai-onboarding-7bdf584499-j2nv7   1/1     running   0          6m58s
pod/bonsai-onboarding-7bdf584499-vc7kh   1/1     running   0          6m58s

name                    type           cluster-ip      external-ip     port(s)        age
service/kubernetes      clusterip      10.xxx.xxx.1     &lt;none&gt;          443/tcp        8m35s
service/lb-onboarding   loadbalancer   10.xxx.xxx.230   35.xxx.xxx.235   3000:31637/tcp   67s


and i'm able reach 35.xxx.xxx.235:3000 from any ip:

welcome to nginx!
...
thank you for using nginx.


you can check if your app is reachable using this command:

nmap -pn $(kubectl get svc lb-onboarding -o jsonpath='{.status.loadbalancer.ingress[*].ip}')


maybe the cause of your problem with ""err_connection_refused"" in configuration of your image? i found no problem with your deployment and load balancer configuration.
"
59553001,kubernetes service selector used to select another service and not deployment?,"i just want to know, is it possible to refer to a service rather than a deployment (using service labels instead of deployment matchlabels) in a kubernetes service definition?
what i mean to say is suppose i have a service a defined which exposes a deployment a-d and now i want to define another service b but this time instead of its selector referring to a deployment a-d i want it to point to the previous service defined i.e. service a? is this even possible in kubernetes? for eg see the scenario below

**deployment a-d**
apiversion: apps/v1
kind: deployment
metadata:
  name: my-nginx
spec:
  selector:
    matchlabels:
      run: my-nginx
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx
        ports:
        - containerport: 80

**servicea**
apiversion: v1
kind: service
metadata:
  name: my-nginx
  labels:
    run: my-nginx-1
spec:
  ports:
  - port: 80
    protocol: tcp
  selector:
    run: my-nginx

**serviceb**
apiversion: v1
kind: service
metadata:
  name: my-nginx-wrapper-service
  labels:
    run: my-nginx-2
spec:
  ports:
  - port: 80
    protocol: tcp
  selector:
    run: my-nginx-1  //service label instead of deployment


update:

headless service
    apiversion: v1
    kind: service
    metadata:
      name: access-service
      annotations:
        getambassador.io/config: |
          ---
          apiversion: ambassador/v1
          kind: mapping
          name: productreadservice-mapping
          prefix: /apps/productreadservice/.*
          prefix_regex: true
          rewrite: """"
          service: access-service:80
    spec:
      clusterip: none
      ports:
      - name: http
        port: 80
        targetport: 8082

endpoint object
apiversion: v1
kind: endpoints
metadata:
  name: access-service
subsets:
- addresses:
  - ip: ip of the service i wish to access
  ports:
  - port: 8082
    protocol: tcp

",<kubernetes><kubernetes-service><kubernetes-deployment>,59554340,2,"yes, it is possible! not through selectors though.

ones you have the service pointing to the pods a-d, you have an ip address. you can create an endpoints object  with that ip address. then, you can create a headless service without selectors with the same name as the endpoints object.

example:

say your service ip address (the one pointing to the depoyments a-d) is 10.0.0.10. create the endpoints object:

apiversion: v1
kind: endpoints
metadata:
  name: my-headless-service
subsets:
- addresses:
  - ip: 10.0.0.10
  ports:
  - port: 80
    protocol: tcp


now, create the headless service with the same name as the endpointsobject. note that it has no label selectors, so it is not selecting any backend. when this happens, the request is send to the dns, and there it will search for  either en externalname type service with the same name or an endpoints object with the same name.

apiversion: v1
kind: service
metadata:
  name: my-headless-service
spec:
  clusterip: none
  ports:
  - name: http
    port: 80
    targetport: 80


the resolution happens at dns, not at iptables.
"
59984014,pod access pvc subdirectory that already existed,"i have a pod created using a deployment using git-sync image and mount the volume to a pvc

kind: deployment
metadata:
  name: config
  namespace: test
spec:
  replicas: 1
  selector:
    matchlabels:
      demo: config
  template:
    metadata:
      labels:
        demo: config
    spec:
      containers:
        - args:
            - '-ssh'
            - '-repo=git@domain.com:org/repo.git'
            - '-dest=conf'
            - '-branch=master'
            - '-depth=1'
          image: 'k8s.gcr.io/git-sync:v3.1.1'
          name: git-sync
          securitycontext:
            runasuser: 65533
          volumemounts:
            - mountpath: /etc/git-secret
              name: git-secret
              readonly: true
            - mountpath: /config
              name: cus-config
      securitycontext:
        fsgroup: 65533
      volumes:
        - name: git-secret
          secret:
            defaultmode: 256
            secretname: git-creds
        - name: cus-config
          persistentvolumeclaim:
            claimname: cus-config


after the deployment, i checked the pod and got a file path like this.
/tmp/git/conf/subdira/some.files
then i created a second pod from another deployment and want to mount the tmp/git/conf/subdira on the second pod. this is the example of my second deployment script.  

kind: deployment
metadata:
  name: test-mount-config
  namespace: test
spec:
  replicas: 1
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - image: 'nginx:1.7.9'
          name: nginx
          ports:
            - containerport: 80
          volumemounts:
            - mountpath: /root/conf
              name: config
              subpath: tmp/git/conf/subdira
      volumes:
        - name: config
          persistentvolumeclaim:
            claimname: cus-config


this is my pvc  

kind: persistentvolumeclaim
metadata:
  annotations:
    volume.beta.kubernetes.io/storage-class: conf
  name: config
  namespace: test
spec:
  accessmodes:
    - readwritemany
  resources:
    requests:
      storage: 10mi
---
apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  name: conf
  namespace: test
provisioner: spdbyz
reclaimpolicy: retain


i already read about subpath on pvc, but everytime i checked the folder /root/conf on the second pod, there is nothing inside it.
any idea on how to mount specific pvc subdirectory on another pod? 
",<kubernetes><amazon-eks><kubernetes-pvc>,59984293,2,"very basic example on how share file content between pods using pv/pvc

first create a persistent volume refer below yaml example with hostpath configuration

apiversion: v1
kind: persistentvolume
metadata:
  name: my-pv-1
  labels:
    pv: my-pv-1
spec:
  capacity:
    storage: 1gi
  volumemode: filesystem
  accessmodes:
    - readwritemany
  persistentvolumereclaimpolicy: retain
  hostpath:
    path: /var/log/mypath

$ kubectl create -f pv.yaml
persistentvolume/my-pv-1 created


second create a persistent volume claim using below yaml example 

apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: my-pvc-claim-1
spec:
  accessmodes:
    - readwritemany
  resources:
    requests:
      storage: 1gi
  selector:
    matchlabels:
      pv: my-pv-1


$ kubectl create -f pvc.yaml
persistentvolumeclaim/my-pvc-claim-1 created


verify the pv and pvc status is  set to bound 

$ kubectl get persistentvolume
name      capacity   access modes   reclaim policy   status   claim                  storageclass   reason   age
my-pv-1   1gi        rwx            retain           bound    default/my-pvc-claim-1                         62s

$ kubectl get persistentvolumeclaims
name             status   volume    capacity   access modes   storageclass   age
my-pvc-claim-1   bound    my-pv-1   1gi        rwx                           58


third consume the pvc in required pods refer below example yaml where the volume is mounted on two pods nginx-1 and nginx-2.

apiversion: v1
kind: pod
metadata:
  name: nginx-1
spec:
  containers:
  - image: nginx
    name: nginx-1
    volumemounts:
    - mountpath: /var/log/mypath
      name: test-vol
      subpath: testsubpath
  volumes:
  - name: test-vol
    persistentvolumeclaim:
        claimname: my-pvc-claim-1


$ kubectl create -f nginx-1.yaml
pod/nginx-1 created


$ kubectl get pods -o wide
name             ready   status    restarts   age   ip            node         nominated node   readiness gates
nginx-1   1/1     running   0          35s   10.244.3.53   k8s-node-3   &lt;none&gt;           &lt;none&gt;


create second pod and consume same pvc

apiversion: v1
kind: pod
metadata:
  name: nginx-2
spec:
  containers:
  - image: nginx
    name: nginx-2
    volumemounts:
    - mountpath: /var/log/mypath
      name: test-vol
      subpath: testsubpath
  volumes:
  - name: test-vol
    persistentvolumeclaim:
        claimname: my-pvc-claim-1


$ kubectl create -f nginx-2.yaml
pod/nginx-2 created


$ kubectl get pods -o wide
name             ready   status    restarts   age   ip            node         nominated node   readiness gates
nginx-1   1/1     running   0          55s   10.244.3.53   k8s-node-3   &lt;none&gt;           &lt;none&gt;
nginx-2   1/1     running   0          35s   10.244.3.54   k8s-node-3   &lt;none&gt;           &lt;none&gt;


test by connecting to container 1 and write to the file on mount-path.

root@nginx-1:/# df -kh
filesystem      size  used avail use% mounted on
overlay          12g  7.3g  4.4g  63% /
tmpfs            64m     0   64m   0% /dev
tmpfs           3.9g     0  3.9g   0% /sys/fs/cgroup
/dev/vda1        12g  7.3g  4.4g  63% /etc/hosts
shm              64m     0   64m   0% /dev/shm
tmpfs           3.9g   12k  3.9g   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs           3.9g     0  3.9g   0% /proc/acpi
tmpfs           3.9g     0  3.9g   0% /proc/scsi
tmpfs           3.9g     0  3.9g   0% /sys/firmware
root@nginx-1:/# cd /var/log/mypath/
root@nginx-1:/var/log/mypath# date &gt;&gt; date.txt
root@nginx-1:/var/log/mypath# date &gt;&gt; date.txt
root@nginx-1:/var/log/mypath# cat date.txt
thu jan 30 10:44:42 utc 2020
thu jan 30 10:44:43 utc 2020


now connect tow second pod/container and it should see the file from first as below

$ kubectl exec -it nginx-2 -- /bin/bash
root@nginx-2:/# cat /var/log/mypath/date.txt
thu jan 30 10:44:42 utc 2020
thu jan 30 10:44:43 utc 2020

"
50439679,traefik ingress custom error in kubernetes,"i need to set a custom error in traefik ingress on kubernetes so that when there is no endpoint or when the status is ""404"", or ""[500-600]"" it redirects to another error service or another custom error message i used the annotation as it's in the documentation in the ingress file as this (note: this a helm template output of passing the annotation as a yaml in the values.yaml file)

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: frontend
  namespace: ""default""
  annotations:
      external-dns.alpha.kubernetes.io/target: ""domain.com""
      kubernetes.io/ingress.class: ""traefik""
      traefik.ingress.kubernetes.io/error-pages: ""map[/:map[backend:hello-world status:[502 503]]]""
spec:
  rules:
  - host: frontend.domain.com
    http:
      paths:
      - backend:
          servicename: frontend
          serviceport: 3000
        path: /

",<kubernetes><traefik><kubernetes-helm><kubernetes-ingress>,51194357,2,"the answer by ldez is correct, but there are a few caveats:


first off, these annotations only work for traefik >= 1.6.x (earlier versions may support error pages, but not for the kubernetes backend)
second, the traefik backend must be configured through kubernetes. you cannot create a backend in a config file and use it with kubernetes, at least not in traefik 1.6.x 


here's how the complete thing looks like. foo is just a name, as explained in the other answer, and can be anything:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: frontend
  namespace: ""default""
  annotations:
      external-dns.alpha.kubernetes.io/target: ""domain.com""
      kubernetes.io/ingress.class: ""traefik""
      traefik.ingress.kubernetes.io/error-pages: |-
        foo:
          status:
          - ""404""
          - ""500""
          # see below on where ""error-pages"" comes from
          backend: error-pages
          query: ""/{{status}}.html""
spec:
  rules:
   # this creates an ingress on an non-existing host name,
   # which binds to a service. as part of this a traefik
   # backend ""error-pages"" will be created, which is the one
   # we use above
   - host: error-pages
     http:
       paths:
       - backend:
         servicename: error-pages-service
         serviceport: https
- host: frontend.domain.com
    http:
    # the configuration for your ""real"" ingress goes here

# this is the service to back the ingress defined above
# note that you can use anything for this, including an internal app
# also: if you use https, the cert on the other side has to be valid
---
kind: service
apiversion: v1
metadata:
  name: error-pages-service
  namespace: default
spec:
  ports:
  - name: https
    port: 443
  type: externalname
  externalname: my-awesome-errors.mydomain.test


if you use this configuration, and your app sends a 404, then https://my-awesome-errors.mydomain.test/404.html would be shown as the error page.
"
50237572,kubernetes trouble with statefulset and 3 persistentvolumes,"i'm in the process of creating a statefulset based on this yaml, that will have 3 replicas. i want each of the 3 pods to connect to a different persistentvolume.

for the persistent volume i'm using 3 objects that look like this, with only the name changed (pvvolume, pvvolume2, pvvolume3):

kind: persistentvolume
apiversion: v1
metadata:
  name: pvvolume
  labels:
    type: local
spec:
  storageclassname: standard
  capacity:
    storage: 10gi
  accessmodes:
    - readwriteonce
  hostpath:
    path: ""/nfs""
  claimref:
    kind: persistentvolumeclaim
    namespace: default
    name: mongo-persistent-storage-mongo-0


the first of the 3 pods in the statefulset seems to be created without issue.

the second fails with the error pod has unbound persistentvolumeclaims
back-off restarting failed container.



yet if i go to the tab showing persistentvolumeclaims the second one that was created seems to have been successful.



if it was successful why does the pod think it failed?
",<kubernetes><kubernetes-pvc>,50249935,2,"
  i want each of the 3 pods to connect to a different persistentvolume.



for that to work properly you will either need:


provisioner (in link you posted there are example how to set provisioner on aws, azure, googlecloud and minicube) or
volume capable of being mounted multiple times (such as nfs volume). note however that in such a case all your pods read/write to the same folder and this can lead to issues when they are not meant to lock/write to same data concurrently. usual use case for this is upload folder that pods are saving to, that is later used for reading only and such use cases. sql databases (such as mysql) on the other hand, are not meant to write to such shared folder.

instead of either of mentioned requirements in your claim manifest you are using hostpath (pointing to /nfs) and set it to readwriteonce (only one can use it). you are also using 'standard' as storage class and in url you gave there are fast and slow ones, so you probably created your storage class as well.



  the second fails with the error pod has unbound persistentvolumeclaims
  back-off restarting failed container



that is because first pod already took it's claim (read write once, host path) and second pod can't reuse same one if proper provisioner or access is not set up.



  if it was successful why does the pod think it failed?



all pvc were successfully bound to accompanying pv. but you are never bounding second and third pvc to second or third pods. you are retrying with first claim on second pod, and first claim is already bound (to fist pod) in readwriteonce mode and can't be bound to second pod as well and you are getting error...


suggested approach

since you reference /nfs as your host path, it may be safe to assume that you are using some kind of nfs-backed file system so here is one alternative setup that can get you to mount dynamically provisioned persistent volumes over nfs to as many pods in stateful set as you want

notes:


this only answers original question of mounting persistent volumes across stateful set replicated pods with the assumption of nfs sharing.
nfs is not really advisable for dynamic data such as database. usual use case is upload folder or moderate logging/backing up folder. database (sql or no sql) is usually a no-no for nfs.
for mission/time critical applications you might want to time/stresstest carefully prior to taking this approach in production since both k8s and external pv are adding some layers/latency in-between. although for some application this might suffice, be warned about it.
you have limited control of name for pv that are being dynamically created (k8s adds suffix to newly created, and reuses available old ones if told to do so), but k8s will keep them after pod get terminated and assign first available to new pod so you won't loose state/data. this is something you can control with policies though.


steps:


for this to work you will first need to install nfs provisioner from here:


https://github.com/kubernetes-incubator/external-storage/tree/master/nfs. mind you that installation is not complicated but has some steps where you have to take careful approach (permissions, setting up nfs shares etc) so it is not just fire-and-forget deployment. take your time installing nfs provisioner correctly. once this is properly set up you can continue with suggested manifests below:

storage class manifest:

kind: storageclass
apiversion: storage.k8s.io/v1beta1
metadata:
  name: sc-nfs-persistent-volume
# if you changed this during provisioner installation, update also here
provisioner: example.com/nfs 

stateful set (important excerpt only):

apiversion: apps/v1
kind: statefulset
metadata:
  name: ss-my-app
spec:
  replicas: 3
  ...
  selector:
    matchlabels:
      app: my-app
      tier: my-mongo-db
  ...
  template:
    metadata:
      labels:
        app: my-app
        tier: my-mongo-db
    spec:
      ...
      containers:
        - image: ...
          ...
          volumemounts:
            - name: persistent-storage-mount
              mountpath: /wherever/on/container/you/want/it/mounted
      ...
  ...
  volumeclaimtemplates:
  - metadata:
      name: persistent-storage-mount
  spec:
    storageclassname: sc-nfs-persistent-volume
    accessmodes: [ readwriteonce ]
    resources:
      requests:
        storage: 10gi
  ...


"
59785357,problem sub path ingress controller for backend service,"i have problem setting path ingress controller for backend service. for example i want setup :


frontend app with angular (path :/)  
backend service with nodejs (path :/webservice).


nodejs : index.js

const express = require('express')
const app = express()
const port = 4000

app.get('/', (req, res) =&gt; res.send('welcome to myapp!'))

app.use('/data/office', require('./roffice'));
app.listen(port, () =&gt; console.log(`example app listening on port ${port}!`))


another route:roffice.js

var express = require('express')
var router = express.router()

router.get('/getoffice', async function (req, res) {
   res.send('get data office')
}); 

module.exports = router


deployment.yaml

apiversion: apps/v1
kind: deployment
metadata:
  name: ws-stack
spec:
  selector:
   matchlabels:
     run: ws-stack
  replicas: 2
  template:
    metadata:
      labels:
      run: ws-stack
  spec:
     containers:
     - name: ws-stack
       image: wsstack/node/img
       imagepullpolicy: ifnotpresent
       ports:
         - containerport: 4000


service.yaml

apiversion: v1
kind: service
metadata:
  name: service-wsstack
   labels:
    run: service-wsstack
  spec:
   type: nodeport
   ports:
   - port: 80
     protocol: tcp
     nodeport: 30009
     targetport: 4000
   selector:
     run: ws-stack


ingress.yaml

 apiversion: networking.k8s.io/v1beta1
 kind: ingress
 metadata:
   name: stack-ingress
   annotations:
     nginx.ingress.kubernetes.io/rewrite-target: /
     nginx.ingress.kubernetes.io/use-regex: ""true""
  spec:
   rules:
    - host: hello-world.info
    - http:
        paths:
        - path: /
          backend:
            servicename: service-ngstack --&gt; frondend
            serviceport: 80
        - path: /webservice
          backend:
            servicename: service-wsstack --&gt; backend
            serviceport: 80


i setup deployment, service and ingress successfully. but when i called with curl

curl http://&lt;minikubeip&gt;/webservice  --&gt; welcome to myapp! =&gt; correct
curl http://&lt;minikubeip&gt;/webservice/data/office/getoffice --&gt; welcome to myapp! =&gt; not correct


if i called another route, the result is the same 'welcome to myapp'. but if i used nodeport 

curl http://&lt;minikubeip&gt;:30009/data/office/getoffice =&gt; 'get data office', working properly.


what is the problem? any solution? thank you
",<kubernetes><kubernetes-ingress>,59786485,2,"tl;dr


nginx.ingress.kubernetes.io/rewrite-target: /$2
path: /webservice($|/)(.*)


explanation

the problem is from that line in your ingress:

nginx.ingress.kubernetes.io/rewrite-target: /

you're telling nginx to rewrite your url to / whatever it matched.


/webservice => /
/webservice/data/office/getoffice => /


to do what you're trying to do use regex, here is a simple example:

 apiversion: networking.k8s.io/v1beta1
 kind: ingress
 metadata:
   name: stack-ingress
   annotations:
     nginx.ingress.kubernetes.io/rewrite-target: /$2
     nginx.ingress.kubernetes.io/use-regex: ""true""
  spec:
   rules:
    - host: hello-world.info
    - http:
        paths:
        - path: /
          backend:
            servicename: service-ngstack --&gt; frondend
            serviceport: 80
        - path: /webservice($|/)(.*)
          backend:
            servicename: service-wsstack --&gt; backend
            serviceport: 80


this way you're asking nginx to rewrite your url with the second matching group.
finally it gives you:


/webservice => /
/webservice/data/office/getoffice => /data/office/getoffice

"
59790967,"managed certificate in ingress, domain status is failednotvisible","i'm simply following the tutorial here: https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs#creating_an_ingress_with_a_managed_certificate

everything works fine until i deploy my certificate and wait 20 minutes for it to show up as:

status:
  certificate name:    daojnfiwlefielwrfn
  certificate status:  provisioning
  domain status:
    domain:  moviedecisionengine.com
    status:  failednotvisible


that domain clearly works so what am i missing?

edit:

here's the cert:

apiversion: networking.gke.io/v1beta1
kind: managedcertificate
metadata:
    name: moviedecisionengine
spec:
    domains:
        - moviedecisionengine.com


the ingress:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    ingress.gcp.kubernetes.io/pre-shared-cert: mcrt-14cb8169-25ba-4712-bca5-cb612562a00b
    ingress.kubernetes.io/backends: '{""k8s-be-31721--1cd1f38313af9089"":""healthy""}'
    ingress.kubernetes.io/forwarding-rule: k8s-fw-default-showcase-mde-ingress--1cd1f38313af9089
    ingress.kubernetes.io/https-forwarding-rule: k8s-fws-default-showcase-mde-ingress--1cd1f38313af9089
    ingress.kubernetes.io/https-target-proxy: k8s-tps-default-showcase-mde-ingress--1cd1f38313af9089
    ingress.kubernetes.io/ssl-cert: mcrt-14cb8169-25ba-4712-bca5-cb612562a00b
    ingress.kubernetes.io/target-proxy: k8s-tp-default-showcase-mde-ingress--1cd1f38313af9089
    ingress.kubernetes.io/url-map: k8s-um-default-showcase-mde-ingress--1cd1f38313af9089
    kubernetes.io/ingress.global-static-ip-name: 34.107.208.110
    networking.gke.io/managed-certificates: moviedecisionengine
  creationtimestamp: ""2020-01-16t19:44:13z""
  generation: 4
  name: showcase-mde-ingress
  namespace: default
  resourceversion: ""1039270""
  selflink: /apis/extensions/v1beta1/namespaces/default/ingresses/showcase-mde-ingress
  uid: 92a2f91f-3898-11ea-b820-42010a800045
spec:
  backend:
    servicename: showcase-mde
    serviceport: 80
  rules:
  - host: moviedecisionengine.com
    http:
      paths:
      - backend:
          servicename: showcase-mde
          serviceport: 80
  - host: www.moviedecisionengine.com
    http:
      paths:
      - backend:
          servicename: showcase-mde
          serviceport: 80
status:
  loadbalancer:
    ingress:
    - ip: 34.107.208.110


and lastly, the load balancer:

apiversion: v1
kind: service
metadata:
  creationtimestamp: ""2020-01-13t22:41:27z""
  labels:
    app: showcase-mde
  name: showcase-mde
  namespace: default
  resourceversion: ""2298""
  selflink: /api/v1/namespaces/default/services/showcase-mde
  uid: d5a77d7b-3655-11ea-af7f-42010a800157
spec:
  clusterip: 10.31.251.46
  externaltrafficpolicy: cluster
  ports:
  - nodeport: 31721
    port: 80
    protocol: tcp
    targetport: 80
  selector:
    app: showcase-mde
  sessionaffinity: none
  type: loadbalancer
status:
  loadbalancer:
    ingress:
    - ip: 35.232.156.172


for the full output of kubectl describe managedcertificate moviedecisionengine:

name:         moviedecisionengine
namespace:    default
labels:       &lt;none&gt;
annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {""apiversion"":""networking.gke.io/v1beta1"",""kind"":""managedcertificate"",""metadata"":{""annotations"":{},""name"":""moviedecisionengine"",""namespace...
api version:  networking.gke.io/v1beta1
kind:         managedcertificate
metadata:
  creation timestamp:  2020-01-17t16:47:19z
  generation:          3
  resource version:    1042869
  self link:           /apis/networking.gke.io/v1beta1/namespaces/default/managedcertificates/moviedecisionengine
  uid:                 06c97b69-3949-11ea-b820-42010a800045
spec:
  domains:
    moviedecisionengine.com
status:
  certificate name:    mcrt-14cb8169-25ba-4712-bca5-cb612562a00b
  certificate status:  provisioning
  domain status:
    domain:  moviedecisionengine.com
    status:  failednotvisible
events:      &lt;none&gt;

",<kubernetes><google-kubernetes-engine><kubernetes-ingress>,59845408,2,"i was successful in using managedcertificate with gke ingress resource. 

let me elaborate on that:

steps to reproduce:


create ip address with gcloud
update the dns entry
create a deployment 
create a service
create a certificate
create a ingress resource 


create ip address with gcloud

invoke below command to create static ip address:

$ gcloud compute addresses create example-address --global

check newly created ip address with below command: 

$ gcloud compute addresses describe example-address --global

update the dns entry

go to gcp -> network services -> cloud dns.

edit your zone with a record with the same address that was created above.  

wait for it to apply. 

check with $ nslookup domain.name if the entry is pointing to the appropriate address.

create a deployment

below is example deployment which will respond to traffic:

apiversion: apps/v1
kind: deployment
metadata:
  name: hello
spec:
  selector:
    matchlabels:
      app: hello
      version: 1.0.0
  replicas: 3
  template:
    metadata:
      labels:
        app: hello
        version: 1.0.0
    spec:
      containers:
      - name: hello
        image: ""gcr.io/google-samples/hello-app:1.0""
        env:
        - name: ""port""
          value: ""50001""


apply it with command $ kubectl apply -f file_name.yaml

you can change this deployment to suit your application but be aware of the ports that your application will respond to. 

create a service

use the nodeport as it's the same as in the provided link: 

apiversion: v1
kind: service
metadata:
  name: hello-service
spec:
  type: nodeport
  selector:
    app: hello
    version: 1.0.0
  ports:
  - name: hello-port
    protocol: tcp
    port: 50001
    targetport: 50001


apply it with command $ kubectl apply -f file_name.yaml

create a certificate

as shown in guide you can use below example to create managedcertificate:

apiversion: networking.gke.io/v1beta1
kind: managedcertificate
metadata:
  name: example-certificate 
spec:
  domains:
    - domain.name


apply it with command $ kubectl apply -f file_name.yaml


  the status failed_not_visible indicates that certificate provisioning failed for a domain because of a problem with dns or the load balancing configuration. make sure that dns is configured so that the certificate's domain resolves to the ip address of the load balancer.
  --  google cloud documentation 


creation of this certificate should be affected by dns entry that you provided earlier. 

create a ingress resource

below is example for ingress resource which will use managedcertificate:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress
  annotations:
    kubernetes.io/ingress.global-static-ip-name: example-address
    networking.gke.io/managed-certificates: example-certificate
spec:
  rules:
  - host: domain.name
    http:
      paths:
      - path: /
        backend:
          servicename: hello-service
          serviceport: hello-port


apply it with command $ kubectl apply -f file_name.yaml

it took about 20-25 minutes for it to fully work. 
"
50817860,k8s doesn't download docker container,"when i run my command to apply the modification or just to create ( pods, service,  deployments)  

kubectl apply -f hello-kubernetes-oliver.yml


i dont have an error. 

but when i do docker ps to see if the container was downloaded from my private registery. i've nothing :( 

if i run the command docker-all.attanea.net/hello_world:latestit download the container.

i dont understand why it doesn't download my container with the first command ?

you will find below my hello-kubernetes-oliver.yml

apiversion: v1
kind: service
metadata:
  name: hello-kubernetes-oliver
spec:
  type: nodeport
  ports:
  - port: 80
    targetport: 8080
  selector:
    app: hello-kubernetes-oliver
---
apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: hello-kubernetes-oliver
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: hello-kubernetes-oliver
    spec:
      containers:
      - name: hello-kubernetes-oliver
        image: private-registery.net/hello_world:latest
        ports:
        - containerport: 80

",<docker><kubernetes><docker-compose><google-kubernetes-engine>,50818002,2,"in order to download images from the private registry, you need to create a secret which is used in the deployment manifest.  


  kubectl create secret docker-registry regcred --docker-server= --docker-username=""your-name"" --docker-password=""your-pword"" --docker-email=""your-email""
  https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#create-a-secret-in-the-cluster-that-holds-your-authorization-token


regcred is the name of the secret resources. 

then you attach regcred secret in your deployment file

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: hello-kubernetes-oliver
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: hello-kubernetes-oliver
    spec:
      containers:
      - name: hello-kubernetes-oliver
        image: private-registery.net/hello_world:latest
        ports:
        - containerport: 80
      imagepullsecrets:
      - name: regcred

"
60099222,helm3 - reading json file into configmap produces a string?,"problem:

i want to read a json file into a configmap so it looks like:

apiversion: v1
kind: configmap
metadata:
  name: json-test
data:
  test.json: |-
    {
      ""key"": ""val""
    }


instead i get

apiversion: v1
kind: configmap
metadata:
  name: json-test
data:
  test.json: |-
      ""{\r\n    \""key\"": \""val\""\r\n}""


what i've done:

i have the following helm chart:

mode                 lastwritetime         length name
----                 -------------         ------ ----
d-----        2020-02-06  10:51 am                static
d-----        2020-02-06  10:55 am                templates
-a----        2020-02-06  10:51 am             88 chart.yaml


static/ contains a single file: test.json:

{
    ""key"": ""val""
}


templates/ contains a single configmap that reads test.json: test.yml:

apiversion: v1
kind: configmap
metadata:
  name: json-test
data:
  test.json: |-
  {{ tojson ( .files.get ""static/test.json"" ) | indent 4}}


when i run helm install test . --dry-run --debug i get the following output

name: test
last deployed: thu feb  6 10:58:18 2020
namespace: default
status: pending-install
revision: 1
test suite: none
user-supplied values:
{}

computed values:
{}

hooks:
manifest:
---
# source: sandbox/templates/test.yml
apiversion: v1
kind: configmap
metadata:
  name: json-test
data:
  test.json: |-
      ""{\r\n    \""key\"": \""val\""\r\n}""


the problem here is my json is wrapped in double quotes. my process that wants to read the json is expecting actual json, not a string.
",<json><kubernetes><kubernetes-helm><sprig-template-functions>,60119809,2,"i see that this is not specific behavior only for helm 3. it generally works in kubernetes this way.

i've just tested it on kubernetes v1.13.

first i created a configmap based on this file:

apiversion: v1
kind: configmap
metadata:
  name: json-test
data:
  test.json: |-
    {   
      ""key"": ""val""
    }


when i run:

$ kubectl get configmaps json-test -o yaml


i get the expected output:

apiversion: v1
data:
  test.json: |-
    {
      ""key"": ""val""
    }
kind: configmap
metadata:
...


but when i created my configmap based on json file with the following content:

{   
      ""key"": ""val""
} 


by running:

$ kubectl create configmap json-configmap --from-file=test-json.json


then when i run:

kubectl get cm json-configmap --output yaml


i get:

apiversion: v1
data:
  test-json.json: ""    {   \n      \""key\"": \""val\""\n    } \n""
kind: configmap
metadata:
...


so it looks like it's pretty normal for kubernetes to transform the original json format into string when a configmap is created from file.

it doesn't seem to be a bug as kubectl doesn't have any problems with extracting properly formatted json format from such configmap:

kubectl get cm json-configmap -o jsonpath='{.data.test-json\.json}'


gives the correct output:

{   
  ""key"": ""val""
}


i would say that it is application responsibility to be able to extract json from such string and it can be done probably in many different ways e.g. making direct call to kube-api or using serviceaccount configured to use kubectl in pod.
"
71257667,gke ingress with multiple backend services returns 404,"i'm trying to create a gke ingress that points to two different backend services based on path. i've seen a few posts explaining this is only possible with an nginx ingress because gke ingress doesn't support rewrite-target. however, this google documentation, gke ingresss - multiple backend services, seems to imply otherwise. i've followed the steps in the docs but haven't had any success. only the service that is available on the path prefix of / is returned. any other path prefix, like /v2, returns a 404 not found.
details of my setup are below. is there an obvious error here -- is the google documentation incorrect and this is only possible using nginx ingress?
-- ingress
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: app-ingress
  annotations:
    kubernetes.io/ingress.global-static-ip-name: app-static-ip
    networking.gke.io/managed-certificates: app-managed-cert
spec:
  rules:
  - http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: api-service
            port:
              number: 80
      - path: /v2
        pathtype: prefix
        backend:
          service:
            name: api-2-service
            port:
              number: 8080

-- service 1
apiversion: v1
kind: service
metadata:
  name: api-service
  labels:
    app: api
spec:
  type: nodeport
  selector:
    app: api
  ports:
  - port: 80
    targetport: 5000

-- service 2
apiversion: v1
kind: service
metadata:
  name: api-2-service
  labels:
    app: api-2
spec:
  type: nodeport
  selector:
    app: api-2
  ports:
  - port: 8080
    targetport: 5000

",<kubernetes><google-kubernetes-engine><kubernetes-ingress><nginx-ingress>,71264724,2,"gcp ingress supports multiple paths. this is also well described in setting up http(s) load balancing with ingress. for my test i've used both hello-world v1 and v2.
there are 3 possible issues.

issue is with container ports opened. you can check it using netstat:

$ kk exec -ti first-55bb869fb8-76nvq -c container -- bin/sh
/ # netstat -plnt
active internet connections (only servers)
proto recv-q send-q local address           foreign address         state       pid/program name
tcp        0      0 :::8080                 :::*                    listen      1/hello-app


issue might be also caused by the firewall configuration. make sure you have proper settings. (in general, in the new cluster i didn't need to add anything but if you have more stuff and have specific firewall configurations it might block).

misconfiguration between port, containerport and targetport.


below my example:
1st deployment with
apiversion: apps/v1
kind: deployment
metadata:
  name: first
  labels:
    app: api
spec:
  selector:
    matchlabels:
      app: api
  template:
    metadata:
      labels:
        app: api
    spec:
      containers:
        - name: container
          image: gcr.io/google-samples/hello-app:1.0
          ports:
          - containerport: 8080
---
apiversion: v1
kind: service
metadata:
  name: api-service
  labels:
    app: api
spec:
  type: nodeport
  selector:
    app: api
  ports:
  - port: 5000
    targetport: 8080

2nd deployment
apiversion: apps/v1
kind: deployment
metadata:
  name: second
  labels:
    app: api-2
spec:
  selector:
    matchlabels:
      app: api-2
  template:
    metadata:
      labels:
        app: api-2
    spec:
      containers:
        - name: container
          image: gcr.io/google-samples/hello-app:2.0
          ports:
          - containerport: 8080
---
apiversion: v1
kind: service
metadata:
  name: api-2-service
  labels:
    app: api-2
spec:
  type: nodeport
  selector:
    app: api-2
  ports:
  - port: 6000
    targetport: 8080

ingress
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: app-ingress
spec:
  rules:
  - http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: api-service
            port:
              number: 5000
      - path: /v2
        pathtype: prefix
        backend:
          service:
            name: api-2-service
            port:
              number: 6000

outputs:
$ curl 35.190.xx.249
hello, world!
version: 1.0.0
hostname: first-55bb869fb8-76nvq
$ curl 35.190.xx.249/v2
hello, world!
version: 2.0.0
hostname: second-d7d87c6d8-zv9jr

please keep in mind that you can also use nginx ingress on gke by adding specific annotation.
kubernetes.io/ingress.class: &quot;nginx&quot; 

main reason why people use nginx ingress on gke is using rewrite annotation and possibility to use clusterip or nodeport as servicetype, where gcp ingress allows only nodeport servicetype.
additional information you can find in gke ingress for http(s) load balancing
"
53693030,how can i access to services outside the cluster using kubectl proxy?,"when we spin up a cluster with kubeadm in kubernetes, and the service's .yaml file looks like this :

apiversion: v1
kind: service
metadata:
  name: neo4j
  labels:
    app: neo4j
    component: core
spec:
  clusterip: none
  ports:
    - port: 7474
      targetport: 7474
      name: browser
    - port: 6362
      targetport: 6362
      name: backup
  selector:
    app: neo4j
    component: core


after all pods and services run, i do kubectl proxy and it says :

starting to serve on 127.0.0.1:8001


so when i want to access to this service like :

curl localhost:8001/api/


it's just reachable inside the cluster! how can i reach to services outside the cluster?
",<kubernetes><kubectl><kubeadm><kube-apiserver>,53694256,2,"you should expose your service using nodeport:

apiversion: v1
kind: service
metadata:
  name: neo4j
  labels:
    app: neo4j
    component: core
spec:
  externaltrafficpolicy: local
  type: nodeport
  ports:
    - port: 7474
      targetport: 7474
      name: browser
    - port: 6362
      targetport: 6362
      name: backup
  selector:
    app: neo4j
    component: core


now if you describe your service using 

 kubectl describe svc neo4j


you will get a nodeport value which will be in between 30000-32767 and you can access your service from outside the cluster using

curl http://&lt;node_ip&gt;:&lt;node_port&gt;


hope this helps.

edit: yes you can't directly use clusterip: none in case of exposing service through nodeport. now clusterip: none means there is no internal load balancing done by kubernetes and for that we can also use externaltrafficpolicy=local in service definition. 

alternatively, you might be able to use an ingress to route traffic to the correct service.
"
71156026,helm lookup always empty,"while deploying a kubernetes application, i want to check if a resource is already present. if so it shall not be rendered. to archive this behaviour the lookup function of helm is used. as it seems is always empty while deploying (no dry-run). any ideas what i am doing wrong?
    ---
{{- if not (lookup &quot;v1&quot; &quot;serviceaccount&quot; &quot;my-namespace&quot; &quot;my-sa&quot;) }}
apiversion: v1
kind: serviceaccount
metadata:
  name: {{ .chart.name }}-{{ .values.environment }}
  namespace: {{ .values.namespace }}
  labels: 
    app:  {{ $.chart.name }}
    environment: {{ .values.environment }}
  annotations:
    &quot;helm.sh/resource-policy&quot;: keep
    iam.gke.io/gcp-service-account: &quot;{{ .chart.name }}-{{ .values.environment }}@{{ .values.gcpprojectid }}.iam.gserviceaccount.com&quot;
{{- end }}

running the corresponding kubectl command return the expected service account
kubectl get serviceaccount my-sa -n my-namespace lists the expected service account
helm version: 3.5.4
",<kubernetes><google-kubernetes-engine><kubernetes-helm>,71161245,2,"i think you cannot use this if-statement to validate what you want.
the lookup function returns a list of objects that were found by your lookup. so, if you want to validate that there are no serviceaccounts with the properties you specified, you should check if the returned list is empty.
test something like
---
{{ if eq (len (lookup &quot;v1&quot; &quot;serviceaccount&quot; &quot;my-namespace&quot; &quot;my-sa&quot;)) 0 }}
apiversion: v1
kind: serviceaccount
metadata:
  name: {{ .chart.name }}-{{ .values.environment }}
  namespace: {{ .values.namespace }}
  labels: 
    app:  {{ $.chart.name }}
    environment: {{ .values.environment }}
  annotations:
    &quot;helm.sh/resource-policy&quot;: keep
    iam.gke.io/gcp-service-account: &quot;{{ .chart.name }}-{{ .values.environment }}@{{ .values.gcpprojectid }}.iam.gserviceaccount.com&quot;
{{- end }}

see: https://helm.sh/docs/chart_template_guide/functions_and_pipelines/#using-the-lookup-function
"
53755517,"kubernetes pod is changing status from running to completed very soon ,how do i prevent that","created a pod using yaml and once pod is created i am running kubectl exec to run my gatling perf test code

kubectl exec gradlecommandfromcommandline -- ./gradlew gatlingrun- 
simulations.runtimeparameters -dusers=500 -dramp_duration=5 -dduration=30


but this is ending at kubectl console with below message :-


  command terminated with exit code 137


on investigation its found that pod is changing status from running to completed stage.

how do i increase life span of a pod so that it waits for my command to get executed.here is pod yaml

apiversion: v1
kind: pod
metadata:
  name: gradlecommandfromcommandline
labels:
  purpose: gradlecommandfromcommandline
spec:
  containers:
    - name: gradlecommandfromcommandline
      image: tarunkumard/tarungatlingscript:v1.0
      workingdir: /opt/gatling-fundamentals/
      command: [""./gradlew""]
      args: [""gatlingrun-simulations.runtimeparameters"", ""-dusers=500"", ""- 
dramp_duration=5"", ""-dduration=30""]
  restartpolicy: onfailure

",<kubernetes><kubernetes-pod>,53764705,2,"here is yaml file to make pod running always
    apiversion: v1

kind: pod
metadata:
name: gradlecommandfromcommandline
labels:
purpose: gradlecommandfromcommandline
spec:
volumes:
- name: docker-sock
  hostpath:
    path: /home/vagrant/k8s/pods/gatling/user-files/simulations    # a file or 
directory location on the node that you want to mount into the pod
  #  command: [ ""git clone https://github.com/tarunkdas2k18/perfgatl.git"" ]
containers:
- name: gradlecommandfromcommandline
  image: tarunkumard/tarungatlingscript:v1.0
  workingdir: /opt/gatling-fundamentals/
  command: [""./gradlew""]
  args: [""gatlingrun-simulations.runtimeparameters"", ""-dusers=500"", ""- 
 dramp_duration=5"", ""-dduration=30""]

- name: gatlingperftool
  image: tarunkumard/gatling:firstscript     # run the ubuntu 16.04
  command: [ ""/bin/bash"", ""-c"", ""--"" ]       # you need to run some task inside a 
  container to keep it running
  args: [ ""while true; do sleep 10; done;"" ] # our simple program just sleeps inside 
  an infinite loop
  volumemounts:
    - mountpath: /opt/gatling/user-files/simulations     # the mount path within the 
  container
      name: docker-sock                   # name must match the hostpath volume name
  ports:
    - containerport: 80

"
63689505,expose grafana publicly using istio,"we are using prometheus operator and we need to expose grafana publicly (outside) using istio,
https://github.com/helm/charts/tree/master/stable/prometheus-operator
normally when i have application which i need to expose publicly with istio, i adding something like following to my micro service and it works and exposed outside.
service.yaml
apiversion: v1
kind: service
metadata:
  name: po-svc
  namespace: po
spec:
  ports:
    - name: http
      port: 3000
      targetport: 3000
  selector:
    app: myapp  //i take the name from deployment.yaml --in the chart not sure which value i should take from the chart---

and add a virtual service
virtualservice.yaml
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: po-virtualservice
  namespace: po
spec:
  gateways:
    - gw-system.svc.cluster.local
  hosts:
    - po.eu.trial.appos.cloud.mvn
  http:
    - route:
        - destination:
            host: po-svc
            port:
              number: 3000

then i was able to access to my application publicly.
now i want to the same for grafana from the prometheus operator chart
in the values.yaml there is service entry
https://github.com/helm/charts/blob/master/stable/prometheus-operator/values.yaml#l576
however not sure if it should replace the service.yaml and if yes how to fill the data like  app: myapp (which in regualr application i take from the deployment.yaml the `name' field)  to be the grafana that the service have the reference to grafana application
in addition, in the virutalservice.yaml there is a reference to the service (host: po-svc)

my question is: how should i fill those two values and be able to
expose grafana using istio ?

btw, if i change the values from the chart to loadbalancer like below, im getting a public url to access outside, however i want to expose it via istio.
  service:
    portname: service
    type: loadbalancer

update
i've created the following virtual service
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: po-virtualservice
  namespace: po
spec:
  gateways:
    - gw-system.svc.cluster.local
  hosts:
    - po.eu.trial.appos.cloud.mvn
  http:
    - route:
        - destination:
            host: po-grafana. // this is the name of the service that promethues operator created when applying the chart .
            port:
              number: 3000

and update the values.yaml like following
  service:
    portname: service
    port: 3000
    targetport: 3000

now when i hit the browser for the application url (po.eu.trial.appos.cloud.mvn) i got error
upstream connect error or disconnect/reset before headers. reset reason: connection termination  any idea what could be the problem? how should i trace this issue ?
i would think(not sure 100%) i may be missing something on the service config in the chart but not sure what...
i've found this post which have similar error: (but not sure we have the same issue)
https://github.com/istio/istio/issues/19966
however not sure how should i add the nameport to the chart yaml service definition
",<kubernetes><grafana><kubernetes-helm><istio><prometheus-operator>,63707555,2,"there is a working example for istio with version 1.7.0
istioctl version
client version: 1.7.0
control plane version: 1.7.0
data plane version: 1.7.0 (1 proxies)

1.i have used helm fetch to get prometheus operator.
helm fetch stable/prometheus-operator --untar

2.i changed these in values.yaml.
grafana service.
service:
  portname: http-service
  port: 3000
  targetport: 3000

grafana host.
hosts:
  - grafana.domain.com

3.i have created po namespace and installed prometheus operator
kubectl create namespace po
helm install prometheus-operator ./prometheus-operator -n po

4.i have checked the grafana service name with
kubectl get svc -n po
prometheus-operator-grafana                    clusterip

5.i have used below yamls for istio, used grafana service name which is prometheus-operator-grafana as my virtual service and destination rule host.
apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: grafana-gateway
  namespace: po
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http-grafana
      protocol: http
    hosts:
    - &quot;grafana.domain.com&quot;
---
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: grafana-vs
  namespace: po
spec:
  hosts:
  - &quot;grafana.domain.com&quot;
  gateways:
  - grafana-gateway
  http:
  - route:
    - destination:
        host: prometheus-operator-grafana.po.svc.cluster.local
        port:
          number: 3000
---
apiversion: networking.istio.io/v1alpha3
kind: destinationrule
metadata:
  name: grafana
  namespace: po
spec:
  host: prometheus-operator-grafana.po.svc.cluster.local
  trafficpolicy:
    tls:
      mode: disable

5.test with curl, it's 302 instead of 200 as we have to login.
curl -v -h &quot;host: grafana.domain.com&quot; xx.xx.xxx.xxx/

get / http/1.1
&gt; host: grafana.domain.com
&gt; user-agent: curl/7.64.0
&gt; accept: */*
&gt;
&lt; http/1.1 302 found


let me know if it worked or if you have any other questions. maybe there is a problem with the 1.4.3 version you use.
"
70068941,kubernetes ingress - cannot access,"i'm new to ingress in kubernetes. i installed ingress nginx controller in my aks cluster. my application runs in a pod (port 80) in testns (namespace). i created a service for it in the same namespace. now i want to access my application from outside. so i created an ingress yaml for it.
service.yaml
 kubectl get svc -a | grep my-svc
    testns    my-svc                         clusterip      10.0.116.192   &lt;none&gt;          80/tcp                             93m

ingress
name:             example-ingress
namespace:        testns
address:
default backend:  default-http-backend:80 (&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;)
rules:
  host        path  backends
  ----        ----  --------
  *
              /apple   my-svc:80 (10.244.1.33:80)
annotations:  ingress.kubernetes.io/rewrite-target: /
events:       &lt;none&gt;

ingress.yaml
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: example-ingress
  annotations:
    ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
        - path: /apple
          backend:
            servicename: my-svc
            serviceport: 80

when i access using the &lt;ip_address_of_the_ingress&gt;/apple, it gives an 404 error. i want to know why the application can't be accessed.
",<nginx><kubernetes><kubernetes-ingress><azure-aks>,70069235,2,"try adding the ingress class into yaml config kubernetes.io/ingress.class: &quot;nginx&quot;
as annotation
kubernetes.io/ingress.class: &quot;nginx&quot;
nginx.ingress.kubernetes.io/rewrite-target: /

example
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: example-ingress
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
        - path: /apple
          backend:
            servicename: my-svc
            serviceport: 80

"
63523073,kubernetes network policies. unable to 'wget' on to pod running on different namespace?,"i have created two name-spaces named 'a' and 'b'
i have file structure like below..
on folder a
nginx-deployment.yml
apiversion: apps/v1
kind: deployment
metadata:
  name: nginx-deployment-a
  labels:
    app-tier: ui
  namespace: a
spec:
  
  selector:
    matchlabels:
      app-tier: ui
  template:
    metadata:
      labels:
        app-tier: ui
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerport: 80

network-policy.yml
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: np-a
  namespace: a
spec:
  podselector: {}
  policytypes:
  - ingress
  - egress
  ingress:
  - from:
    - namespaceselector:
        matchlabels:
          name: b
  
    ports:
    - protocol: tcp
      port: 80
    
  egress:
  - to:
    - namespaceselector:
        matchlabels:
          name: b
    
    ports:
    - protocol: tcp
      port: 53
    - protocol: udp
      port: 53

and applied both yml files using kubectl apply -f
on folder b
nginx-deployment.yml
apiversion: apps/v1
kind: deployment
metadata:
  name: nginx-deployment-b
  labels:
    app-tier: ui
  namespace: b
spec:
  
  selector:
    matchlabels:
      app-tier: ui
  template:
    metadata:
      labels:
        app-tier: ui
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerport: 80

network-policy.yml
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: np-b
  namespace: b
spec:
  podselector: {}
  policytypes:
  - ingress
  - egress
  ingress:
  - from:
    - namespaceselector:
        matchlabels:
          name: a
    
    ports:
    - protocol: tcp
      port: 80
  egress:
  - to:
    - namespaceselector:
        matchlabels:
          name: a
    
    ports:
    - protocol: tcp
      port: 53
    - protocol: udp
      port: 53

and applied both yml files using kubectl apply -f
the problem
so basically i want to allow traffic from namespace a to namespace b and vice-versa.
and i have exposed services using
$$ kubectl expose deployment nginx-deployment-b -n b --port=80

$$ kubectl expose deployment nginx-deployment-a -n a --port=80

and i have created busybox in namespace a using
kubectl run myshell --image=busybox -n a --command -- sh -c &quot;sleep 3600&quot;

and i have exec into busybox using
kubectl exec myshell -n a -it -- sh

now here is the output of wget
/ # wget nginx-deployment-b.b.svc.cluster.local
^z[5]+  stopped                    wget nginx-deployment-b.b.svc.cluster.local
/ # wget nginx-deployment-a.a.svc.cluster.local
^z[6]+  stopped                    wget nginx-deployment-a.a.svc.cluster.local
/ # wget nginx-deployment-a.a.svc
^z[7]+  stopped                    wget nginx-deployment-a.a.svc
/ # wget nginx-deployment-b.b.svc
^z[8]+  stopped                    wget nginx-deployment-b.b.svc
/ # 


you can see that i'm neither able to connect to service running on namespace a nor b
what should i do to allow allow traffic from namespace a to namespace b and vice-versa?
any suggestions or modifications.
thanks
edit-1
descriptions of networks policies ,
np-a
name:         np-a
namespace:    a
created on:   2020-08-21 18:41:12 +0530 ist
labels:       &lt;none&gt;
annotations:  spec:
  podselector:     &lt;none&gt; (allowing the specific traffic to all pods in this namespace)
  allowing ingress traffic:
    to port: 80/tcp
    from:
      namespaceselector: name=b
  allowing egress traffic:
    to port: 53/tcp
    to port: 53/udp
    to:
      namespaceselector: name=b
  policy types: ingress, egress

np-b
name:         np-b
namespace:    b
created on:   2020-08-21 18:21:07 +0530 ist
labels:       &lt;none&gt;
annotations:  spec:
  podselector:     &lt;none&gt; (allowing the specific traffic to all pods in this namespace)
  allowing ingress traffic:
    to port: 80/tcp
    from:
      namespaceselector: name=a
  allowing egress traffic:
    to port: 53/tcp
    to port: 53/udp
    to:
      namespaceselector: name=a
  policy types: ingress, egress

service descriptions
name:              nginx-deployment-a
namespace:         a
labels:            app-tier=ui
annotations:       &lt;none&gt;
selector:          app-tier=ui
type:              clusterip
ip:                10.107.112.202
port:              &lt;unset&gt;  80/tcp
targetport:        80/tcp
endpoints:         10.0.0.147:80
session affinity:  none
events:            &lt;none&gt;

and
name:              nginx-deployment-b
namespace:         b
labels:            app-tier=ui
annotations:       &lt;none&gt;
selector:          app-tier=ui
type:              clusterip
ip:                10.98.228.141
port:              &lt;unset&gt;  80/tcp
targetport:        80/tcp
endpoints:         10.0.0.79:80
session affinity:  none
events:            &lt;none&gt;

output of kubectl get pods -n kube-system
name                               ready   status    restarts   age
cilium-operator-868c78f7b5-44nhn   0/1     pending   0          7h58m
cilium-operator-868c78f7b5-jl5cq   1/1     running   2          7h58m
cilium-qgzxs                       1/1     running   2          7h58m
coredns-66bff467f8-lpck8           1/1     running   2          8h
etcd-minikube                      1/1     running   1          7h8m
kube-apiserver-minikube            1/1     running   1          7h8m
kube-controller-manager-minikube   1/1     running   3          8h
kube-proxy-f9vgr                   1/1     running   2          8h
kube-scheduler-minikube            1/1     running   2          8h
storage-provisioner                1/1     running   5          8h


",<kubernetes><kubernetes-ingress><kubernetes-pod><kubernetes-networkpolicy><kubernetes-networking>,63523372,2,"you need to allow egress on port 53 for dns resolution
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: dns
spec:
  podselector: {}
  egress:
  - to:
    ports:
    - protocol: tcp
      port: 53
    - protocol: udp
      port: 53
  policytypes:
  - egress

you can have separate network policy like above in both the namespaces dedicated for dns.
also when you access a service which is in a different namespace you need to use &lt;servicename&gt;.&lt;namespacename&gt;.svc or &lt;servicename&gt;.&lt;namespacename&gt;.svc.cluster.local.
hence the command to access nginx-deployment-b should be nginx-deployment-b.b.svc or nginx-deployment-b.b.svc.cluster.local
"
70189442,kubernetes ingress not forwarding routes,"i am fairly new to kubernetes and have just deployed my first cluster to ibm cloud. when i created the cluster, i get a dedicated ingress subdomain, which i will be referring to as &lt;long-k8subdomain&gt;.cloud for the scope of this post. now, this subdomain works for my app. for example: &lt;long-k8subdomain&gt;.cloud/ping works from my browser/curl just fine- i get the expected json response back. but, if i add this subdomain to a cname record on my domain provider's dns settings (i have used bluehost and ibm cloud's internet services), i get a 404 response back from all routes. however this response is the default nginx 404 response (it says &quot;nginx&quot; under &quot;404 not found&quot;). i believe this means that this means the ingress load balancer is being reached, but the request does not get routed right. i am using kubernetes version 1.20.12_1561 on vpc gen 2 and this is my ingress-config.yaml file:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: my-ingress-resource
  annotations:
    kubernetes.io/ingress.class: &quot;public-iks-k8s-nginx&quot;
    nginx.ingress.kubernetes.io/configuration-snippet: |
      more_set_headers &quot;host: &lt;long-k8subdomain&gt;.cloud&quot;;
spec:
  rules:
  - host: &lt;long-k8subdomain&gt;.cloud
    http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: my-service-name
            port:
              number: 80

i am pretty sure this problem is due to the annotations. maybe i am using the wrong ones or i do not have enough. ideally, i would like something like this: api..com/ to route correctly. i have also read a little bit about default backends, but i have not dove too much into that just yet. any help would be greatly appreciated, as i have spent multiple hours trying to fix this.
some sources i have used:

https://cloud.ibm.com/docs/containers?topic=containers-cs_network_planning
https://cloud.ibm.com/docs/containers?topic=containers-ingress-types
https://cloud.ibm.com/docs/containers?topic=containers-comm-ingress-annotations#annotations

note: the reason why i have the second annotation is because for some reason, requests without that header were not being routed directly. so that was part of my debugging process and i just ended up leaving it as i am not sure if that annotation solves that, so i left it for now.
",<kubernetes><dns><ibm-cloud><kubernetes-ingress><nginx-ingress>,70262568,2,"for the nginx ingress controller to route requests for your own domain's cname record to the service instead of the ibm cloud one, you need a rule in the ingress where the host identifies your domain.
for instance, if your domain's dns entry is api.example.com, then change the resource yaml to:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: my-ingress-resource
  annotations:
    kubernetes.io/ingress.class: &quot;public-iks-k8s-nginx&quot;
spec:
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: my-service-name
            port:
              number: 80

you should not need the second annotation for this to work.
if you want both of the hosts to work, then you could add a second rule instead of replacing host in the existing one.
"
63581972,inject file into helm template,"i have a list of properties defined in values.yaml as follows:
files:
 - &quot;file1&quot;
 - &quot;file2&quot;

then in my template i want to create config maps out of my values.
i came up with the following template:
{{- range $value := .values.files }}
---
apiversion: v1
kind: configmap
metadata: 
    name: {{ $value }}
data:
    {{ $value }}: {{ .files.get (printf &quot;%s/%s&quot; &quot;files&quot; $value) | indent 4 }}
{{- end }}

as you can see i want to have configmaps with same name as files. i mixed up several parts of documentation, however my template does not work as expected.
how can i achieve configmap creation through templates?
//edit
i expect to have the following configmap:
apiversion: v1
kind: configmap
metadata: 
    name: file1
data:
    file1: &lt;file1 content&gt;
---
apiversion: v1
kind: configmap
metadata: 
    name: file2
data:
    file2: &lt;file2 content&gt;

",<kubernetes><kubernetes-helm>,63593625,2,"try the following:
{{- $files := .files }}
{{- range $value := .values.files }}
---
apiversion: v1
kind: configmap
metadata: 
  name: {{ $value }}
  data:
    {{ $value }}: |
{{ $files.get (printf &quot;%s/%s&quot; &quot;files&quot; $value) | indent 6 }}
{{- end }}

you problem seems to be with incorrect indentation. make sure the line with $files.get starts with no spaces.
and i also added {{- $files := .files }} to access .files, because for some reason range is changeing the . scope.
i have found this example in documentation but it seems to work only if file's content are one-line. so if your files are one line only, then you should check the example.
also notice the | after {{ $value }}:. you need it because file content is a multiline string. check this stackquestion on how to use multiline strings in yaml.
"
52953815,securing specific nginx-ingress location with client cert verification,"i'm setting up an instance of ghost and i'm trying to secure the /ghost path with client cert verification. 

i've got an initial ingress up and running that serves the site quite happily with the path specified as /. 

i'm trying to add a second ingress (that's mostly the same) for the /ghost path. if i do this and add the annotations for basic auth, everything seems to work. i.e. if i browse to /ghost i am prompted for credentials in the basic-auth secret, if i browse to any other url it is served without auth. 

i then switched to client cert verification based on this example: https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/auth/client-certs

when i try this either the whole site or none of the site is secured, rather than the path-based separation, i got with basic-auth. looking at the nginx.conf from the running pod the  proxy_set_header ssl-client-verify, proxy_set_header ssl-client-subject-dn &amp; proxy_set_header ssl-client-issuer-dn elements are added under the root / path and the /ghost path. i've tried removing those (from the root only) and copying the config directly back to the pod but not luck there either.

i'm pulling nginx-ingress (chart version 0.23.0) in as a dependency via helm

ingress definition for / location - this one works

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    certmanager.k8s.io/cluster-issuer: letsencrypt-staging
    kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: ""true""
  labels:
    app: my-app
    chart: my-app-0.1.1
    heritage: tiller
    release: my-app
  name: my-app
  namespace: default
spec:
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          servicename: my-app
          serviceport: http
        path: /
  tls:
  - hosts:
    - example.com
    secretname: mysite-tls


ingress definition for /ghost location - this one doesn't work

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/auth-tls-verify-client: ""on""
    nginx.ingress.kubernetes.io/auth-tls-secret: ""default/auth-tls-chain""
    nginx.ingress.kubernetes.io/auth-tls-verify-depth: ""1""
    nginx.ingress.kubernetes.io/auth-tls-error-page: ""http://www.example.com/error-cert.html""
    nginx.ingress.kubernetes.io/auth-tls-pass-certificate-to-upstream: ""false""
    kubernetes.io/ingress.class: ""nginx""
  labels:
    app: my-app
    chart: my-app-0.1.1
    heritage: tiller
    release: my-app
  name: my-app-secure
  namespace: default
spec:
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          servicename: my-app
          serviceport: http
        path: /ghost
  tls:
  - hosts:
    - example.com
    secretname: mysite-tls

",<ssl><nginx><kubernetes><nginx-ingress><kubernetes-helm>,52980862,2,"you need a '*' on your path on your second ingress if you want to serve all the pages securely under /ghost and if you want just /ghost you need another rule. something like this:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/auth-tls-verify-client: ""on""
    nginx.ingress.kubernetes.io/auth-tls-secret: ""default/auth-tls-chain""
    nginx.ingress.kubernetes.io/auth-tls-verify-depth: ""1""
    nginx.ingress.kubernetes.io/auth-tls-error-page: ""http://www.example.com/error-cert.html""
    nginx.ingress.kubernetes.io/auth-tls-pass-certificate-to-upstream: ""false""
    kubernetes.io/ingress.class: ""nginx""
  labels:
    app: my-app
    chart: my-app-0.1.1
    heritage: tiller
    release: my-app
  name: my-app-secure
  namespace: default
spec:
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          servicename: my-app
          serviceport: http
        path: /ghost
      - backend:
          servicename: my-app
          serviceport: http
        path: /ghost/*
  tls:
  - hosts:
    - example.com
    secretname: mysite-tls


however, if you want something like / unsecured and /ghost secured, i believe you won't be able to do it. for example, if you are using nginx, this is a limitation from nginx itself, when you configure a server {} block with tls in nginx it looks something like this:

server {
    listen              443 ssl;
    server_name         example.com;
    ssl_certificate     example.com.crt;
    ssl_certificate_key example.com.key;
    ssl_protocols       tlsv1 tlsv1.1 tlsv1.2;
    ssl_ciphers         high:!anull:!md5;
    ...
}


the ingress controller creates paths like this:

server {
    listen              443 ssl;
    server_name         example.com;
    ssl_certificate     example.com.crt;
    ssl_certificate_key example.com.key;
    ssl_protocols       tlsv1 tlsv1.1 tlsv1.2;
    ssl_ciphers         high:!anull:!md5;
    ...

    location / {
       ...
    }

    location /ghost {
       ...
    }

}


so when you configure another server {} block with the same hostname and with no ssl it will override the first one.

you could do it with different - host: rules in your ingress for example ghost.example.com with tls and main.example.com without tls. so in your nginx.conf you would have different server {} blocks.

you can always shell into the ingress controller pod to check the configs, for example:

$ kubectl exec -it nginx-ingress-controller-xxxxxxxxx-xxxxx bash
www-data@nginx-ingress-controller-6bd7c597cb-8kzjh:/etc/nginx$ cat nginx.conf

"
63556649,deploy elk stack in kubernetes with helm volumebinding error,"i'm trying to deploy elk stack in kubernetes cluster with helm, using this chart. when i launch
helm install elk-stack stable/elastic-stack
i receive the following message:

name: elk-stack
last deployed: mon aug 24 07:30:31 2020
namespace: default
status: deployed
revision: 1
notes:
the elasticsearch cluster and associated extras have been installed.
kibana can be accessed:

  * within your cluster, at the following dns name at port 9200:

    elk-stack-elastic-stack.default.svc.cluster.local

  * from outside the cluster, run these commands in the same shell:

    export pod_name=$(kubectl get pods --namespace default -l ""app=elastic-stack,release=elk-stack"" -o jsonpath=""{.items[0].metadata.name}"")
    echo ""visit http://127.0.0.1:5601 to use kibana""
    kubectl port-forward --namespace default $pod_name 5601:5601

but when i run

kubectl get pods

the result is:

name                                              ready   status              restarts   age
elk-stack-elasticsearch-client-7fcfc7b858-5f7fw   0/1     running   0          12m
elk-stack-elasticsearch-client-7fcfc7b858-zdkwd   0/1     running   1          12m
elk-stack-elasticsearch-data-0                    0/1     pending   0          12m
elk-stack-elasticsearch-master-0                  0/1     pending   0          12m
elk-stack-kibana-cb7d9ccbf-msw95                  1/1     running   0          12m
elk-stack-logstash-0                              0/1     pending   0          12m

using kubectl describe pods command, i see that for elasticsearch pods the problem is:

 warning  failedscheduling  6m29s      default-scheduler  running ""volumebinding"" filter plugin for pod ""elk-stack-elasticsearch-data-0"": pod has unbound immediate persistentvolumeclaims

and for logstash pods:

warning  failedscheduling  7m53s      default-scheduler  running ""volumebinding"" filter plugin for pod ""elk-stack-logstash-0"": pod has unbound immediate persistentvolumeclaims

output of kubectl get pv,pvc,sc -a:

name                                      capacity   access modes   reclaim policy   status        claim                           storageclass   reason   age
persistentvolume/elasticsearch-data       10gi       rwo            retain           bound         default/elasticsearch-data      manual                  16d

namespace      name                                                                status    volume                   capacity   access modes   storageclass   age
default        persistentvolumeclaim/claim1                                        pending                                                      slow           64m
default        persistentvolumeclaim/data-elk-stack-elasticsearch-data-0           pending                                                                     120m
default        persistentvolumeclaim/data-elk-stack-elasticsearch-master-0         pending                                                                     120m
default        persistentvolumeclaim/data-elk-stack-logstash-0                     pending                                                                     120m
default        persistentvolumeclaim/elasticsearch-data                            bound     elasticsearch-data       10gi       rwo            manual         16d
default        persistentvolumeclaim/elasticsearch-data-elasticsearch-data-0       pending                                                                     17d
default        persistentvolumeclaim/elasticsearch-data-elasticsearch-data-1       pending                                                                     17d
default        persistentvolumeclaim/elasticsearch-data-quickstart-es-default-0    pending                                                                     16d
default        persistentvolumeclaim/elasticsearch-master-elasticsearch-master-0   pending                                                                     17d
default        persistentvolumeclaim/elasticsearch-master-elasticsearch-master-1   pending                                                                     17d
default        persistentvolumeclaim/elasticsearch-master-elasticsearch-master-2   pending                                                                     16d

namespace   name                                         provisioner            reclaimpolicy   volumebindingmode   allowvolumeexpansion   age
            storageclass.storage.k8s.io/slow (default)   kubernetes.io/gce-pd   delete          immediate           false                  66m


storage class slow and persistent volume claim claim1 are my experiments. i create they using kubectl create and a yaml file, the others is automatically created by helm (i think).
output of kubectl get pvc data-elk-stack-elasticsearch-master-0 -o yaml:

apiversion: v1
kind: persistentvolumeclaim
metadata:
  creationtimestamp: ""2020-08-24t07:30:38z""
  finalizers:
  - kubernetes.io/pvc-protection
  labels:
    app: elasticsearch
    release: elk-stack
  managedfields:
  - apiversion: v1
    fieldstype: fieldsv1
    fieldsv1:
      f:metadata:
        f:labels:
          .: {}
          f:app: {}
          f:release: {}
      f:spec:
        f:accessmodes: {}
        f:resources:
          f:requests:
            .: {}
            f:storage: {}
        f:volumemode: {}
      f:status:
        f:phase: {}
    manager: kube-controller-manager
    operation: update
    time: ""2020-08-24t07:30:38z""
  name: data-elk-stack-elasticsearch-master-0
  namespace: default
  resourceversion: ""201123""
  selflink: /api/v1/namespaces/default/persistentvolumeclaims/data-elk-stack-elasticsearch-master-0
  uid: de58f769-f9a7-41ad-a449-ef16d4b72bc6
spec:
  accessmodes:
  - readwriteonce
  resources:
    requests:
      storage: 4gi
  volumemode: filesystem
status:
  phase: pending

can somebody please help me to fix this problem? thanks in advance.
",<elasticsearch><kubernetes><kubernetes-helm>,63562137,2,"the reason why pod is pending is below pvcs are pending because corresponding pvs are not created.
data-elk-stack-elasticsearch-master-0
data-elk-stack-logstash-0
data-elk-stack-elasticsearch-data-0

since you have mentioned this is for local development you can use hostpath volume for the pv. so create pv for each of the pending pvcs using the sample pv below. so you will create 3 pvs in total.
apiversion: v1
kind: persistentvolume
metadata:
  name: elk-master
  labels:
    type: local
spec:
  capacity:
    storage: 4gi
  accessmodes:
    - readwriteonce
  hostpath:
    path: &quot;/mnt/data&quot;
---
apiversion: v1
kind: persistentvolume
metadata:
  name: elk-logstash
  labels:
    type: local
spec:
  capacity:
    storage: 2gi
  accessmodes:
    - readwriteonce
  hostpath:
    path: &quot;/mnt/data&quot;
---
apiversion: v1
kind: persistentvolume
metadata:
  name: elk-data
  labels:
    type: local
spec:
  capacity:
    storage: 30gi
  accessmodes:
    - readwriteonce
  hostpath:
    path: &quot;/mnt/data&quot;

"
52402376,istio-proxy does not intercept outgoing traffic with global.proxy.includeipranges config,"having aws eks cluster in vpc with cidr 172.20.0.0/16 and installed istio 1.0.2 with helm:

helm upgrade -i istio install/kubernetes/helm/istio \
--namespace istio-system \
--set tracing.enabled=true \
--set grafana.enabled=true \
--set telemetry-gateway.grafanaenabled=true \
--set telemetry-gateway.prometheusenabled=true \
--set global.proxy.includeipranges=""172.20.0.0/16"" \
--set servicegraph.enabled=true \
--set galley.enabled=false


then deploy some pods for testing:

apiversion: v1
kind: service
metadata:
  name: service-one
  labels:
    app: service-one
spec:
  ports:
  - port: 80
    targetport: 8080
    name: http
  selector:
    app: service-one
---
apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: service-one
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: service-one
    spec:
      containers:
      - name: app
        image: gcr.io/google_containers/echoserver:1.4
        ports:
        - containerport: 8080
---
apiversion: v1
kind: service
metadata:
  name: service-two
  labels:
    app: service-two
spec:
  ports:
  - port: 80
    targetport: 8080
    name: http-status
  selector:
    app: service-two
---
apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: service-two
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: service-two
    spec:
      containers:
      - name: app
        image: gcr.io/google_containers/echoserver:1.4
        ports:
        - containerport: 8080


and deploy it with:

kubectl apply -f &lt;(istioctl kube-inject -f app.yaml) 


then inside service-one pod, i'm requesting service-two and there are no logs about outgoing request inside service-one's istio-proxy container, but if i reconfigure istio without setting global.proxy.includeipranges it works as expected (but i need this config to allow multiple external connections). how can i debug what is going on?
",<kubernetes><istio><amazon-eks>,52422874,2,"setting global.proxy.includeipranges is deprecated and should not work. there was a discussion on git about this. the new closest thing is includeoutboundipranges in pod's sidecar-injector config-map or traffic.sidecar.istio.io/includeoutboundipranges pod annotation. annotation looks easier. for now, it is not clear in the official documentation.

you could add the annotation to your deployment:

apiversion: extensions/v1beta1
kind: deployment
metadata:
  annotations:
     traffic.sidecar.istio.io/includeoutboundipranges: ""172.20.0.0/16""       
  name: service-one
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: service-one
    spec:
      containers:
      - name: app
        image: gcr.io/google_containers/echoserver:1.4
        ports:
        - containerport: 8080


and the same for second deployment.
"
63273727,"error ""no route matched with those values"" with the kong ingress controller","attempting to connect to a jupyter lab container (ultimately other applications as well) running on a cloud managed kubernetes service using kong as the ingress controller. receiving &quot;no route matched with those values&quot; on the http response to kong's public ip and the ingress-controller logs indicate:
service kong/rjup2 does not have any active endpoints
no configuration change, skipping sync to kong

deployment config:
apiversion: apps/v1
kind: deployment
metadata:
  name: rjup2
  namespace: kong
spec:
  selector:
    matchlabels:
      run: rjup2
  replicas: 1
  template:
    metadata:
      labels:
        run: rjup2
    spec:
      restartpolicy: always
      containers:
        - name: rjup2
          image: jupyter/minimal-notebook
          imagepullpolicy: always
          ports:
            - containerport: 8888
              protocol: tcp

service config:
apiversion: v1
kind: service
metadata:  
  name: rjup2
  namespace: kong
spec:
  selector:    
    app: rjup2
  type: clusterip
  ports:  
  - name: http
    port: 80
    targetport: 8888
    protocol: tcp

ingress resource config:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: rjup2
  namespace: kong
spec:
  tls:
  - hosts:
      - &lt;aks api server address&gt;
  rules:
  - host: &lt;aks api server address&gt;
    http:
      paths:
      - path: /
        backend:
          servicename: rjup2
          serviceport: 80

the api server address is properly populated in the deployed yaml. i have tried different namespaces before consolidating them under kong's default namespace and also tried making the service ports 8888 in addition to the containers target port.
thanks for any assistance in debugging this.
",<kubernetes><kubernetes-ingress><kong><kubernetes-networking><kong-ingress>,63282380,2,"your rjup2 service doesn't have a valid selector. note that the pods you are trying to expose are labelled with run: rjup2 label and your service has app: rjup2 selector.
btw. you get very clear error message that indicates where the problem could be:
service kong/rjup2 does not have any active endpoints

if your rjup2 service in kong namespace doesn't have any active endpoints, it means it doesn't expose your pods properly which may indicate a possible mismatch in your configuration.
you can check it by running:
kubectl get ep -n kong

normally you should see the matching endpoints object. in your case you won't see it as your service cannot expose any pods untill it has a valid selector.
if you fix your service definition, everything should work just fine:
apiversion: v1
kind: service
metadata:  
  name: rjup2
  namespace: kong
spec:
  selector:    
    run: rjup2
  type: clusterip
  ports:  
  - name: http
    port: 80
    targetport: 8888
    protocol: tcp

"
52630591,serviceaccount in gke fails authentication,"i need to create serviceaccounts that can access a gke cluster. internally i do this with the following commands:

kubectl create serviceaccount onboarding --namespace kube-system
kubectl apply -f onboarding.clusterrole.yaml
kubectl create clusterrolebinding onboarding --clusterrole=onboarding --serviceaccount=kube-system:onboarding


where the contents of the file onboarding.clusterrole.yaml are something like this:

apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrole
metadata:
  name: onboarding
rules:
- apigroups:
  - '*'
  resources:
  - 'namespace,role,rolebinding,resourcequota'
  verbs:
  - '*'


the serviceaccount resource is created as expected and the clusterrole and clusterrolebinding also look right, but when i attempt to access the api using this new role, i get an authentication failure.

curl -k -x get -h ""authorization: bearer [redacted]"" https://36.195.83.167/api/v1/namespaces
{
  ""kind"": ""status"",
  ""apiversion"": ""v1"",
  ""metadata"": {

  },
  ""status"": ""failure"",
  ""message"": ""namespaces is forbidden: user \""system:serviceaccount:kube-system:onboarding\"" cannot list namespaces at the cluster scope: unknown user \""system:serviceaccount:kube-system:onboarding\"""",
  ""reason"": ""forbidden"",
  ""details"": {
    ""kind"": ""namespaces""
  },
  ""code"": 403


the response suggests an unknown user, but i confirmed the serviceaccount exists and is in the subjects of the clusterrolebinding. is it possible to define a serviceaccount in this way for gke?

i am using the exact process successfully on kubernetes clusters we run in our datacenters.
",<kubernetes><google-cloud-platform><google-kubernetes-engine>,52633576,2,"gke should have the same process. does your kubectl version match that of the gke cluster? not sure if this is the issue but the clusterrole needs plurals for the resources and the resources are represented as lists:

apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrole
metadata:
  name: onboarding
rules:
- apigroups:
  - '*'
  resources:
  - namespaces
  - roles
  - rolebindings
  - resourcequotas
  verbs:
  - '*'


works for me on k8s 1.11.x:

curl -k -x get -h ""authorization: bearer [redacted]"" https://127.0.0.1:6443/api/v1/namespaces
{
  ""kind"": ""namespacelist"",
  ""apiversion"": ""v1"",
  ""metadata"": {
    ""selflink"": ""/api/v1/namespaces"",
    ""resourceversion"": ""12345678""
  },
  ...

"
63283380,pod's labels weren't created after k8s deployment,"i'm running a k8s cluster on rasberry pi(ubuntu 20.04) . when i try to deploy the following k8s deployment, the labels 'rel' and 'env' weren't created on pods.
k8s versions:
client version: version.info{major:&quot;1&quot;, minor:&quot;14&quot;, gitversion:&quot;v1.14.7&quot;, gitcommit:&quot;8fca2ec50a6133511b771a11559e24191b1aa2b4&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2019-09-18t14:47:22z&quot;, goversion:&quot;go1.12.9&quot;, compiler:&quot;gc&quot;, platform:&quot;windows/a
md64&quot;}  
                                                                                                                                                                                                                                        
server version: version.info{major:&quot;1&quot;, minor:&quot;18&quot;, gitversion:&quot;v1.18.6&quot;, gitcommit:&quot;dff82dc0de47299ab66c83c626e08b245ab19037&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2020-07-15t16:51:04z&quot;, goversion:&quot;go1.13.9&quot;, compiler:&quot;gc&quot;, platform:&quot;linux/arm&quot;}   

                                                                                                                                                                                                                                       

-- deployment yaml (kubectl apply -f .)
apiversion: apps/v1 
kind: deployment
metadata:
  name: product-catalog-deployment
  namespace: default
  labels: 
   app: product-catalog
   rel: beta
   env: prod
spec:
  selector:
    matchlabels:
      app: product-catalog
  replicas: 3
  template:
    metadata:
      labels:
        app: product-catalog
    spec:
      containers:
      - name: product-catalog
        image: marveltracker/netcore_fun:netcore_3_1
        ports:
        - containerport: 80
          name: http
        - containerport: 443
          name: https
      

---get prods (kubectl get po --show-labels)
name                                          ready   status    restarts   age   labels
product-catalog-deployment-65c7bcbf48-8nxbw   1/1     running   0          16s   app=product-catalog,pod-template-hash=65c7bcbf48
product-catalog-deployment-65c7bcbf48-f764h   1/1     running   0          16s   app=product-catalog,pod-template-hash=65c7bcbf48
product-catalog-deployment-65c7bcbf48-pcz4q   1/1     running   0          16s   app=product-catalog,pod-template-hash=65c7bcbf48

what was the issue here ?
",<kubernetes><yaml><kubectl>,63283488,2,"your yaml file should be like this
apiversion: apps/v1 
kind: deployment
metadata:
  name: product-catalog-deployment
  namespace: default
  labels: 
    app: product-catalog
    rel: beta
    env: prod
spec:
  replicas: 3
  selector:
    matchlabels:
      app: product-catalog
      rel: beta               #----these all should same 
      env: prod
  template:
    metadata:
      labels:
        app: product-catalog  
        rel: beta             #----same like above
        env: prod
    spec:
      containers:
      - name: product-catalog
        image: marveltracker/netcore_fun:netcore_3_1
        ports:
        - containerport: 80
          name: http
        - containerport: 443
          name: https
  

this is because deployment manages replicasets in the background and you apply the label on replica sets pods. and replicaset add label to pods and manages those pod (means the number of pod availability)
"
69698545,ingress only routes traffic to one route,"i have two pods, each with a loadbalancer svc. each service's ip address is working.
my first service is:
apiversion: v1
kind: service
metadata:
  name: hello-world-1
spec:
  type: loadbalancer
  selector:
    greeting: hello
    version: one
  ports:
  - protocol: tcp
    port: 60000
    targetport: 50000

my second service is:
apiversion: v1
kind: service
metadata:
  name: hello-world-2
spec:
  type: loadbalancer
  selector:
    greeting: hello
    version: two
  ports:
  - protocol: tcp
    port: 5000
    targetport: 5000

my ingress is:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: my-ingress
  annotations:
    kubernetes.io/ingress.class: gce
spec:
  defaultbackend:
    service:
      name: hello-world-1
      port:
        number: 60000
  rules:
  - http:
      paths:
      - path: /
        pathtype: implementationspecific
        backend:
          service:
            name: hello-world-1
            port:
              number: 60000
      - path: /v2
        pathtype: implementationspecific
        backend:
          service:
            name: hello-world-2
            port:
              number: 5000

only the first route works this way and when i put
&lt;my_ip&gt;/v2

in the url bar i get
cannot get /v2
  

how do i configure the ingress so it hits the / route when no subpath is specified and the /v2 route when /v2 is specified?
if i change the first route to
backend:
          service:
            name: hello-world-2
            port:
              number: 5000

and get rid of the second one it works.
but if i change the route to /v2 it stops working?
***** edit *****
following this tutorial here ingress tut i tried changing the yaml so the different routes were on different ports and this breaks it. does anybody know why?
",<kubernetes><google-kubernetes-engine>,69714412,2,"by default, when you create an ingress in your cluster, gke creates an http(s) load balancer and configures it to route traffic to your application, as stated in the following document [1]. so, you should not be configuring your services as loadbalancer type, instead you need to configure them as nodeport.
here, you can follow an example of a complete implementation similar to what you want to accomplish:

create a manifest that runs the application container image in the specified port, for each version:

apiversion: apps/v1
kind: deployment
metadata:
  name: web1
  namespace: default
spec:
  selector:
    matchlabels:
      run: web1
  template:
    metadata:
      labels:
        run: web1
    spec:
      containers:
      - image: gcr.io/google-samples/hello-app:1.0
        imagepullpolicy: ifnotpresent
        name: web1
        ports:
        - containerport: 8000
          protocol: tcp

---
apiversion: apps/v1
kind: deployment
metadata:
  name: web2
  namespace: default
spec:
  selector:
    matchlabels:
      run: web2
  template:
    metadata:
      labels:
        run: web2
    spec:
      containers:
      - image: gcr.io/google-samples/hello-app:2.0
        imagepullpolicy: ifnotpresent
        name: web2
        ports:
        - containerport: 9000
          protocol: tcp


create two services (one for each version) as type nodeport. a very important note at this step, is that the targetport specified should be the one the application is listening on, in my case both services are pointing to port 8080 since i am using the same application but different versions:

apiversion: v1
kind: service
metadata:
  name: web1
  namespace: default
spec:
  ports:
  - port: 8000
    protocol: tcp
    targetport: 8080
  selector:
    run: web1
  type: nodeport

---
apiversion: v1
kind: service
metadata:
  name: web2
  namespace: default
spec:
  ports:
  - port: 9000
    protocol: tcp
    targetport: 8080
  selector:
    run: web2
  type: nodeport


finally, you need to create the ingress with the path rules:

apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: my-ingress
  annotations:
    kubernetes.io/ingress.class: gce
spec:
  defaultbackend:
    service:
      name: web1
      port:
        number: 8000
  rules:
  - http:
      paths:
      - path: /
        pathtype: implementationspecific
        backend:
          service:
            name: web1
            port:
              number: 8000
      - path: /v2
        pathtype: implementationspecific
        backend:
          service:
            name: web2
            port:
              number: 9000

if you configured everything correctly, the output of the command kubectl get ingress my-ingress should be something like this:
name         class    hosts   address          ports   age
my-ingress   &lt;none&gt;   *       &lt;external ip&gt;    80      149m

and, if your services are pointing to the correct ports, and your applications are listening on those ports, doing a curl to your external ip (curl external ip) should get you to the version one of your application, here is my example output:
hello, world!
version: 1.0.0
hostname: web1-xxxxxxxxxxxxxx

doing a curl to your external ip /v2 (curl external ip/v2) should get you to the version two of your application:
hello, world!
version: 2.0.0
hostname: web2-xxxxxxxxxxxxxx

[1] https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer
"
63197394,nfs-server-provisioner specify volume name,"when using nfs-server-provisioner is it possible to set a specific persistent volume for the nfs provisioner?
at present, i'm setting the storage class to use via helm:
helm install stable/nfs-server-provisioner \
--namespace &lt;chart-name&gt;-helm \
--name &lt;chart-name&gt;-nfs \
--set persistence.enabled=true \
--set persistence.storageclass=slow \
--set persistence.size=25gi \
--set storageclass.name=&lt;chart-name&gt;-nfs \
--set storageclass.reclaimpolicy=retain

and the storage class is built via:
apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  name: slow
provisioner: kubernetes.io/gce-pd
reclaimpolicy: retain
allowvolumeexpansion: true
parameters:
  type: pd-standard
  replication-type: none

this then generates the pv dynamically when requested by a pvc.
i'm using the pv to store files for a stateful cms, using nfs allows for multiple pods to connect to the same file store.
what i'd like to do now is move all those images to a new set of pods on a new cluster. rather than backing them up and going through the process of dynamically generating a pv and restoring the files to it, is it possible to retain the current pv and then connect the new pvc to it?
",<kubernetes><google-kubernetes-engine><kubernetes-helm>,63288732,2,"
when using nfs-server-provisioner is it possible to set a specific persistent volume for the nfs provisioner?

if we are talking if it's possible to retain the data from existing old pv which was the storage for the old nfs server and then use it with new nfs server, the answer is yes.
i've managed to find a way to do it. please remember that this is only a workaround.
steps:

create a snapshot out of existing old nfs server storage.
create a new disk where the source is previously created snapshot
create a pv and pvc for newly created nfs-server
pull the nfs-server-provisioner helm chart and edit it
spawn edited nfs-server-provisioner helm chart
create new pv's and pvc's with nfs-server-provisioner storageclass
attach newly created pvc's to workload

please remember that this solution is showing the way to create pv's and pvc's for a new workload manually.
i've included the whole process below.

create a snapshot out of existing old nfs server storage.
assuming that you created your old nfs-server with the gce-pd, you can access this disk via gcp cloud console to make a snapshot of it.
i've included here more safer approach which consists of creating a copy of the gce-pd with data inside of old nfs-server. this copy will be used in new nfs-server.
there is also a possibility to change the persistentvolumereclaimpolicy on existing old pv to not be deleted when the pvc of old nfs-server is deleted. in this way, you could reuse existing disk in new nfs-server.
please refer to official documentation how to create a snapshot out of persistent disks in gcp:

cloud.google.com: compute: disks: create snapshot


create a new disk where the source is previously created snapshot
you will need to create a new gce-pd disk for your new nfs-server. the earlier created snapshot will be the source for your new disk.
please refer to official documentation on how to create a new disk from existing snapshot:

cloud.google.com: compute: disks: restore and delete snapshots


create a pv and pvc for newly created nfs-server
to ensure that the gcp's disk will be bound to the newly created nfs-server you will need to create a pv and a pvc. you can use example below but please change it accordingly to your use case:
apiversion: v1
kind: persistentvolume
metadata:
  name: data-disk
spec:
  storageclassname: standard
  capacity:
    storage: 25g
  accessmodes:
    - readwriteonce
  claimref:
    namespace: default
    name: data-disk-pvc # reference to the pvc below
  gcepersistentdisk:
    pdname: old-data-disk # name of the disk created from snapshot in gcp
    fstype: ext4 
---
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: data-disk-pvc # reference to the pv above
spec:
  storageclassname: standard
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 25g

the way that this nfs-server works is that it creates a disk in the gcp infrastructure to store all the data saved on the nfs-server. further creation of pv's and pvc's with nfs storageclass will result in creation of a folder in the /export directory inside of a nfs-server pod.

pull the nfs-server-provisioner helm chart and edit it
you will need to pull the helm chart of the nfs-server-provisioner as it requires a reconfiguration. you can do it by invoking below command:

$ helm pull --untar stable/nfs-server-provisioner

the changes are following in the templates/statefulset.yaml file:

delete the parts responsible for handling persistence .values.persistence.enabled (on the bottom). this parts are responsible for creating storage which you already have.

      {{- if not .values.persistence.enabled }}
      volumes:
        - name: data
          emptydir: {}
      {{- end }}

  {{- if .values.persistence.enabled }}
  volumeclaimtemplates:
    - metadata:
        name: data
      spec:
        accessmodes: [ {{ .values.persistence.accessmode | quote }} ]
        {{- if .values.persistence.storageclass }}
        {{- if (eq &quot;-&quot; .values.persistence.storageclass) }}
        storageclassname: &quot;&quot;
        {{- else }}
        storageclassname: {{ .values.persistence.storageclass | quote }}
        {{- end }}
        {{- end }}
        resources:
          requests:
            storage: {{ .values.persistence.size | quote }}
  {{- end }}



add the volume definition to use previously created disk with the nfs data to the spec.template.spec part like here: kubernetes.io: configure persistent volume storage: create a pod

      volumes:
        - name: data
          persistentvolumeclaim:
            claimname: data-disk-pvc # name of the pvc created from the disk


spawn edited nfs-server-provisioner helm chart
you will need to run this helm chart from local storage instead of running it from the web. the command to run it will be following:

$ helm install kruk-nfs . --set storageclass.name=kruk-nfs --set storageclass.reclaimpolicy=retain


this syntax is specific to helm3

above parameters are necessary to specify the name of the storageclass as well it's reclaimpolicy.

create new pv's and pvc's with nfs-server-provisioner storageclass
example to create a pvc linked to the existing folder in nfs-server.
assuming that the /export directory looks like this:
bash-5.0# ls                 
ganesha.log
lost+found  
nfs-provisioner.identity  
pvc-2c16cccb-da67-41da-9986-a15f3f9e68cf # folder we will create a pv and pvc for
v4old  
v4recov  
vfs.conf


a tip!
when you create a pvc with a storageclass of this nfs-server it will create a folder with the name of this pvc.

you will need to create a pv for your share:
apiversion: v1
kind: persistentvolume
metadata:
  name: pv-example
spec:
  storageclassname: kruk-nfs
  capacity:
    storage: 100mi
  accessmodes:
    - readwriteonce
  volumemode: filesystem
  nfs:
    path: /export/pvc-2c16cccb-da67-41da-9986-a15f3f9e68cf # directory to mount pv
    server: 10.73.4.71  # clusterip of nfs-server-pod service

and pvc for your pv:
apiversion: &quot;v1&quot;
kind: &quot;persistentvolumeclaim&quot;
metadata:
  name: pv-example-claim
spec:
  storageclassname: kruk-nfs
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 100mi
  volumename: pv-example # name of the pv created for a folder

attach newly created pvc's to workload
above manifests will create a pvc with the name of pv-example-claim that will have the contents of the pvc-2c16cccb-da67-41da-9986-a15f3f9e68cf directory available for usage. you can mount this pvc to a pod by following this example:
piversion: v1
kind: pod
metadata:
  name: task-pv-pod
spec:
  volumes:
    - name: storage-mounting
      persistentvolumeclaim:
        claimname: pv-example-claim
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerport: 80
          name: &quot;http-server&quot;
      volumemounts:
        - mountpath: &quot;/storage&quot;
          name: storage-mounting

after that you should be able to check if you have the data in the folder specified in above manifest:
$ kubectl exec -it task-pv-pod -- cat /storage/hello                                                                      
hello there

"
69686202,kubernetes ingress objects return no response on windows 10 minikube,"i try to test kubernetes ingress on minikube. my os is windows 10. minikube is installed successfully as well as nginx ingress controller.
&gt; minikube addons enable ingress

below is my kubernetes manifest file:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: test-ingress
  namespace: ingress-nginx
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: 'nginx'
    nginx.ingress.kubernetes.io/default-backend: app-nginx-svc
spec:
  rules:
    - host: boot.aaa.com
      http:
        paths:
          - path: /path
            pathtype: prefix
            backend:
              service:
                name: app-nginx-svc
                port:
                  number: 80

---
apiversion: v1
kind: service
metadata:
  name: app-nginx-svc
  namespace: ingress-nginx
spec:
  type: nodeport  
  selector:
    app: test-nginx  
  ports:
  - name: http
    port: 80
    targetport: 80
    nodeport: 30000

---
apiversion: v1
kind: pod
metadata:
  name: app-nginx
  namespace: ingress-nginx  
  labels:
    app: test-nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports: 
    - containerport: 80

kubernetes pod and service are generated on minikube without errors. when i test service with the below commands, the pod shows the right values.
&gt; minikube service -n ingress-nginx app-nginx-svc --url
* app-nginx-svc    
|--------------------|---------------|-------------|------------------------|
|   namespace   |     name     | target port |      url           |
|--------------------|---------------|-------------|------------------------|
|  ingress-nginx  | app-nginx-svc |           | http://127.0.0.1:63623 |
|-------------------|---------------|-------------|------------------------|
http://127.0.0.1:63623

but the problem occurs in the ingress object. the minikube ingress generates the endpoint and host domain.

i type in the domain mapping hostname in windows 10 host file
192.168.49.2       boot.aaa.com

but i can not receive any response from nginx container:
http://boot.aaa.com/path
the above url does not work at all.
",<kubernetes><kubernetes-ingress><nginx-ingress>,69688243,2,"when you try to access http://boot.aaa.com/path - do you provide the port on which it listens? from what i see from the output of:
minikube service -n ingress-nginx app-nginx-svc --url
* app-nginx-svc    
|--------------------|---------------|-------------|------------------------|
|   namespace        |      name     | target port |          url           |
|--------------------|---------------|-------------|------------------------|
|  ingress-nginx     | app-nginx-svc |             | http://127.0.0.1:63623 |
|--------------------|---------------|-------------|------------------------|
==&gt; http://127.0.0.1:63623 &lt;==


i think that you need to make request on: http://boot.aaa.com:63623/path
if you don't want to use hostname in you ingress, just remove it from manifest.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: test-ingress
  namespace: ingress-nginx
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: 'nginx'
    nginx.ingress.kubernetes.io/default-backend: app-nginx-svc
spec:
  rules:
    - http:
        paths:
          - path: /path
            pathtype: prefix
            backend:
              service:
                name: app-nginx-svc
                port:
                  number: 80

you should be able then to access your pod by only http://{ip}:{port}/path
my additional questions:

are you trying to make request from the same os where the minikube is installed?
is the hostfile edited on the os you are making requests from?
if yes, is the windows firewall turned on?

also, i see that you service expose a nodeport directly to your app on port 30000 (it will not pass through ingress controller).
usually if we are setting up an ingress endpoint to a pod, we do it to avoid exposing it directly by the nodeport. using clusterip service type will do so.
apiversion: v1
kind: service
metadata:
  name: app-nginx-svc
  namespace: ingress-nginx
spec:
  type: clusterip
  selector:
    app: test-nginx  
  ports:
  - name: http
    port: 80
    targetport: 80


"
69682704,retrieve and write tls crt kubernetes secret to another pod in helm template,"i have a kubernetes cluster with elasticsearch currently deployed.
the elasticsearch coordinator node is accessible behind a service via a clusterip over https. it uses a self-signed tls certificate.
i can retrieve the value of the ca:
kubectl get secret \
    -n elasticsearch elasticsearch-coordinating-only-crt \
    -o jsonpath=&quot;{.data.ca\.crt}&quot; | base64 -d
-----begin certificate-----
miidijccagqgawibagirankax51s
...
...

i need to provide this as a ca.crt to other app deployments.

note: the elasticsearch deployment is an an elasticsearch kubernetes namespace. new deployments will be in different namespaces.

an example of this is a deployment of kafka that includes a kafka-connect-elasticsearch/ sink. the sink connector uses configuration such as:
apiversion: v1
kind: configmap
metadata:
  name: {{ include &quot;kafka.fullname&quot; . }}-connect
  labels: {{- include &quot;common.labels.standard&quot; . | nindent 4 }}
    app.kubernetes.io/component: connector
data:
  connect-standalone-custom.properties: |-
    bootstrap.servers={{ include &quot;kafka.fullname&quot; . }}-0.{{ include &quot;kafka.fullname&quot; . }}-headless.{{ .release.namespace }}.svc.{{ .values.clusterdomain }}:{{ .values.service.port }}
    key.converter.schemas.enable=false
    value.converter.schemas.enable=false
    offset.storage.file.filename=/tmp/connect.offsets
    offset.flush.interval.ms=10000
    key.converter=org.apache.kafka.connect.json.jsonconverter
    value.converter=org.apache.kafka.connect.json.jsonconverter
    plugin.path=/usr/local/share/kafka/plugins
  elasticsearch.properties: |-
    name=elasticsearch-sink
    connector.class=io.confluent.connect.elasticsearch.elasticsearchsinkconnector
    tasks.max=4
    topics=syslog,nginx
    key.ignore=true
    schema.ignore=true
    connection.url=https://elasticsearch-coordinating-only.elasticsearch:9200
    type.name=kafka-connect
    connection.username=elastic
    connection.password=xxxxxxxx
    elastic.security.protocol=ssl
    elastic.https.ssl.truststore.location=/etc/ssl/certs/elasticsearch-ca.crt
    elastic.https.ssl.truststore.type=pem

notice the elastic.https.ssl.truststore.location=/etc/ssl/certs/elasticsearch-ca.crt; that's the file i need to put inside the kafka-based container.
what's the optimal way to do that with helm templates?
currently i have a fork of https://github.com/bitnami/charts/tree/master/bitnami/kafka. it adds 3 new templates under templates/:

kafka-connect-elasticsearch-configmap.yaml
kafka-connect-svc.yaml
kafka-connect.yaml

the configmap is shown above.  the kafka-connect.yaml deployment looks like this:
apiversion: apps/v1
kind: deployment
metadata:
  name: {{ include &quot;kafka.fullname&quot; . }}-connect
  labels: {{- include &quot;common.labels.standard&quot; . | nindent 4 }}
    app.kubernetes.io/component: connector
spec:
  replicas: 1
  selector:
    matchlabels: {{- include &quot;common.labels.matchlabels&quot; . | nindent 6 }}
      app.kubernetes.io/component: connector
  template:
    metadata:
      labels: {{- include &quot;common.labels.standard&quot; . | nindent 8 }}
        app.kubernetes.io/component: connector
    spec:
      containers:
        - name: connect
          image: redacted.dkr.ecr.redacted.amazonaws.com/kafka-connect-elasticsearch
          imagepullpolicy: always
          command:
            - /bin/bash
            - -ec
            - bin/connect-standalone.sh custom-config/connect-standalone-custom.properties custom-config/elasticsearch.properties
          ports:
            - name: connector
              containerport: 8083
          volumemounts:
            - name: configuration
              mountpath: /opt/bitnami/kafka/custom-config
      imagepullsecrets:
        - name: regcred
      volumes:
        - name: configuration
          configmap:
            name: {{ include &quot;kafka.fullname&quot; . }}-connect

how can i modify these kafka helm charts to allow them to retrieve the value for kubectl get secret -n elasticsearch elasticsearch-coordinating-only-crt -o jsonpath=&quot;{.data.ca\.crt}&quot; | base64 -d and write its content to /etc/ssl/certs/elasticsearch-ca.crt ?
",<kubernetes><kubernetes-helm><apache-kafka-connect><kubernetes-secrets>,69683011,2,"got this working and learned a few things in the process:

secret resources reside in a namespace. secrets can only be referenced by pods in that same namespace. (ref). therefore, i switched to using a shared namespace for elasticsearch + kafka
the secret can be used in a straightforward way as documented at https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets. this is not a helm-specific but rather core kubernetes feature

in my case this looked like:
apiversion: apps/v1
kind: deployment
metadata:
  name: {{ include &quot;kafka.fullname&quot; . }}-connect
  labels: {{- include &quot;common.labels.standard&quot; . | nindent 4 }}
    app.kubernetes.io/component: connector
spec:
  replicas: 1
  selector:
    matchlabels: {{- include &quot;common.labels.matchlabels&quot; . | nindent 6 }}
      app.kubernetes.io/component: connector
  template:
    metadata:
      labels: {{- include &quot;common.labels.standard&quot; . | nindent 8 }}
        app.kubernetes.io/component: connector
    spec:
      containers:
        - name: connect
          image: redacted.dkr.ecr.redacted.amazonaws.com/kafka-connect-elasticsearch
          imagepullpolicy: always
          command:
            - /bin/bash
            - -ec
            - bin/connect-standalone.sh custom-config/connect-standalone-custom.properties custom-config/elasticsearch.properties
          ports:
            - name: connector
              containerport: 8083
          volumemounts:
            - name: configuration
              mountpath: /opt/bitnami/kafka/custom-config
            - name: ca
              mountpath: /etc/ssl/certs
              readonly: true
      imagepullsecrets:
        - name: regcred
      volumes:
        - name: configuration
          configmap:
            name: {{ include &quot;kafka.fullname&quot; . }}-connect
        - name: ca
          secret:
            secretname: elasticsearch-coordinating-only-crt

this gets the kafka-connect pod up and running, and i can validate the certs are written there also:
$ kubectl exec -it -n elasticsearch kafka-connect-c4f4d7dbd-wbxfq \
    -- ls -1 /etc/ssl/certs

ca.crt
tls.crt
tls.key

"
69765867,usage of command variable in kubernetes pod object,"i'm trying to understand what is the correct usage of command in pods. taking below example of my yaml. this is a working yaml. my doubts are
1&gt; the sleep command is issued for 3600 seconds, but my pod busybox2 is still running after few hours when i see pods via 'k get pods'. my current understanding is, the sleep should execute for 3600 seconds and the pod is supposed to die out after that, as there is no process running my pod (like httpd, nginx etc). not sure why this is
apiversion: v1
kind: pod
metadata:
  name: busybox2
  namespace: default
spec:
  containers:
  - name: busy
    image: busybox
    command:
      - sleep
      - &quot;3600&quot; 

2&gt; when checked on k8s docs, the usage shows a different way to write it. i understand that cmd and the args are separate things.. but can i not simply use both ways for all scenarios? like writing command: [&quot;sleep&quot;, &quot;3600&quot;] as first example, and command: - printenv \ - hostname as another way to write second yaml command section. can someone elaborate a bit.
apiversion: v1
kind: pod
metadata:
  name: command-demo
  labels:
    purpose: demonstrate-command
spec:
  containers:
  - name: command-demo-container
    image: debian
    command: [&quot;printenv&quot;]
    args: [&quot;hostname&quot;, &quot;kubernetes_port&quot;]
  restartpolicy: onfailure

",<kubernetes><kubernetes-pod><docker-desktop>,69767297,2,"...but my pod busybox2 is still running after few hours...
this is because the default value for restartpolicy is always. means after an hour, your pod actually restarted.
apiversion: v1
kind: pod
metadata:
  name: busybox2
  namespace: default
spec:
  restartpolicy: onfailure  # &lt;-- add this line and it will enter &quot;completed&quot; status.
  containers:
  - name: busy
    image: busybox
    command:
      - sleep
      - &quot;10&quot;  # &lt;-- 10 seconds will do to see the effect.

see here for how k8s treats entrypoint, command, args and cmd.
"
63432263,nginx ingress controller returns 404 kubernetes,"i am trying to create an ingress controller that points to a service that i have exposed via nodeport.
here is the yaml file for the ingress controller (taken from https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/):
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: example-ingress
spec:
  rules:
  - host: hello-world.info
    http:
      paths:
      - path: /
        backend:
          servicename: appname
          serviceport: 80


i can connect directly to the node port and the frontend is displayed.
please note that i am doing this because the frontend app is unable to connect to other deployments that i have created and i read that an ingress controller would be able to solve the issue. will i still have to add an nginx reverse proxy? if so how would i do that? i have tried adding this to the nginx config file but with no success.
location /middleware/ {
      proxy_pass http://middleware/;
   }

",<docker><nginx><kubernetes><kubernetes-ingress>,63432482,2,"you must use a proper hostname to reach the route defined in the ingress object. either update your /etc/hosts file or use curl -h &quot;hello-world.info&quot; localhost type command. alternatively, you can delete the host mapping and redirect all traffic to one default service.
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: example-ingress
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          servicename: appname
          serviceport: 80

"
69902046,amazon eks (nfs) to kubernetes pod. can't mount volume,"i'm working on attaching amazon eks (nfs) to kubernetes pod using terraform.
everything runs without an error and is created:

pod victoriametrics
storage classes
persistent volumes
persistent volume claims

however, the volume victoriametrics-data doesn't attach to the pod. anyway, i can't see one in the pod's shell.
could someone be so kind to help me understand where i'm wrong, please?
i have cut some unimportant code for the question to get code shorted.
resource &quot;kubernetes_deployment&quot; &quot;victoriametrics&quot; {
...
      spec {
        container {
          image = var.image
          name  = var.name
          ...
          volume_mount {
              mount_path        = &quot;/data&quot;
              mount_propagation = &quot;none&quot;
              name              = &quot;victoriametrics-data&quot;
              read_only         = false
            }
        }

        volume {
            name = &quot;victoriametrics-data&quot;
        }

      }
    }
...

}

resource &quot;kubernetes_csi_driver&quot; &quot;efs&quot; {
  metadata {
    name = &quot;${local.cluster_name}-${local.namespace}&quot;
    annotations = {
      name = &quot;for store data of ${local.namespace}.&quot;
    }
  }
  spec {
    attach_required        = true
    pod_info_on_mount      = true
    volume_lifecycle_modes = [&quot;persistent&quot;]
  }
}

resource &quot;kubernetes_storage_class&quot; &quot;efs&quot; {
  metadata {
    name = &quot;efs-sc&quot;
  }
  storage_provisioner = kubernetes_csi_driver.efs.id
  reclaim_policy      = &quot;retain&quot;
  mount_options       = [&quot;file_mode=0700&quot;, &quot;dir_mode=0777&quot;, &quot;mfsymlinks&quot;, &quot;uid=1000&quot;, &quot;gid=1000&quot;, &quot;nobrl&quot;, &quot;cache=none&quot;]
}

resource &quot;kubernetes_persistent_volume&quot; &quot;victoriametrics&quot; {
  metadata {
    name = &quot;${local.cluster_name}-${local.namespace}&quot;
  }
  spec {
    storage_class_name               = &quot;efs-sc&quot;
    persistent_volume_reclaim_policy = &quot;retain&quot;
    volume_mode                      = &quot;filesystem&quot;
    access_modes                     = [&quot;readwritemany&quot;]
    capacity = {
      storage = var.size_of_persistent_volume_claim
    }
    persistent_volume_source {
      nfs {
        path   = &quot;/&quot;
        server = local.eks_iput_target
      }
    }
  }
}

resource &quot;kubernetes_persistent_volume_claim&quot; &quot;victoriametrics&quot; {
  metadata {
    name      = local.name_persistent_volume_claim
    namespace = local.namespace
  }
  spec {
    access_modes       = [&quot;readwritemany&quot;]
    storage_class_name = &quot;efs-sc&quot;
    resources {
      requests = {
        storage = var.size_of_persistent_volume_claim
      }
    }
    volume_name = kubernetes_persistent_volume.victoriametrics.metadata.0.name
  }
}

kind: deployment
apiversion: apps/v1
metadata:
  name: victoriametrics
  namespace: victoriametrics
  labels:
    k8s-app: victoriametrics
    purpose: victoriametrics
  annotations:
    deployment.kubernetes.io/revision: '1'
    name: &gt;-
      victoriametrics - the high performance open source time series database &amp;
      monitoring solution.
spec:
  replicas: 1
  selector:
    matchlabels:
      k8s-app: victoriametrics
      purpose: victoriametrics
  template:
    metadata:
      name: victoriametrics
      creationtimestamp: null
      labels:
        k8s-app: victoriametrics
        purpose: victoriametrics
      annotations:
        name: &gt;-
          victoriametrics - the high performance open source time series
          database &amp; monitoring solution.
    spec:
      containers:
        - name: victoriametrics
          image: 714154805721.dkr.ecr.us-east-1.amazonaws.com/victoriametrics:v1.68.0
          ports:
            - containerport: 8428
              protocol: tcp
            - containerport: 2003
              protocol: tcp
            - containerport: 2003
              protocol: udp
          volumemounts:
            - mountpath: /data
              name: victoriametrics-data
            - mountpath: /var/log
              name: varlog
          env:
            - name: name
              value: victoriametrics
          resources:
            limits:
              cpu: '1'
              memory: 1gi
            requests:
              cpu: 500m
              memory: 1gi
          terminationmessagepath: /dev/termination-log
          terminationmessagepolicy: file
          imagepullpolicy: ifnotpresent
      volumes:
      - name: victoriametrics-data
        emptydir: {}
      - name: varlog
        emptydir: {}    
      restartpolicy: always
      terminationgraceperiodseconds: 30
      dnspolicy: clusterfirst
      automountserviceaccounttoken: true
      shareprocessnamespace: false
      securitycontext: {}
      schedulername: default-scheduler
      tolerations:
        - key: k8s-app
          operator: equal
          value: victoriametrics
          effect: noschedule
      enableservicelinks: true
  strategy:
    type: rollingupdate
    rollingupdate:
      maxunavailable: 25%
      maxsurge: 25%
  minreadyseconds: 15
  revisionhistorylimit: 10
  progressdeadlineseconds: 300

",<kubernetes><terraform><amazon-eks>,69907296,2,"you need to use the persistent volume claim that you have created instead of emptydir in your deployment:
kind: deployment
apiversion: apps/v1
metadata:
  name: victoriametrics
...
  volumes:
  - name: victoriametrics-data
      persistentvolumeclaim:
        claimname: &lt;value of local.name_persistent_volume_claim&gt;

"
62330073,gcp couldn't reach kubernetes external load balancer ip from outside,"i have a cluster created in the gcp cloud having a simple k8s yaml file.

apiversion: v1
kind: service
metadata:
  name: lb-svc
  labels:
    app: lb-demo
spec:
  type: loadbalancer
  ports:
  - port: 8080
  selector:
    app: np-demo

---

apiversion: apps/v1
kind: deployment
metadata:
  name: np-deploy
spec:
  replicas: 3
  selector:
    matchlabels:
      app: np-demo
  template:
    metadata:
      labels:
        app: np-demo
    spec:
      containers:
      - name: np-pod
        image: nigelpoulton/k8s-deep-dive:0.1
        imagepullpolicy: always
        ports:
        - containerport: 8080


now; this yaml configuration has a loadbalancer service which in return exposes an external ip address to the public.
thus we can see the external ip address using:

kubectl get svc


the issue is, i can easily access the load balancer using curl within the cloud shell but couldn't reach it when trying to access it from outside (example browser).

tried:

curl external-ip:8080


any help?
",<kubernetes><google-cloud-platform><kubectl>,62330526,2,"your service ip only accessible to local vpc, if you need to expose service or ingress you need reserve a static ip, read here to reserve a static ip https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address

to assign your static ip to service, you need set loadbalancerip on your service configuration

apiversion: v1
kind: service
metadata:
  name: lb-svc
  labels:
    app: lb-demo
spec:
  type: loadbalancer
  loadbalancerip: &lt;your reserved ip&gt;
  ports:
  - port: 8080
  selector:
    app: np-demo


to assign your ip to ingress

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-ingress
  annotations:
    kubernetes.io/ingress.global-static-ip-name: &lt;name of reserved static ip&gt;
  labels:
    app: my-app
spec:
  backend:
    servicename: lb-svc
    serviceport: 8080


read more here
"
70279875,to make a loadbalancer service internal on gke,"do you know what is the annotation that we can use it on gke to make a loadbalancer service internal?. for example azure (and aws) supports the following annotation (shown in the yaml code snippet) to make a loadbalancer service internal. i couldnt find equivalent of it on gke. for example naturally one may expect gcp-load-balancer-internal as the equivalent annotation on gke; unfortunately it is not. here is the azure and aws documentation for it, i am looking equivalent of it on gke.

azure: internal loadbalancer
aws: annotations

apiversion: v1
kind: service
metadata:
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-internal: &quot;true&quot;
    service.beta.kubernetes.io/azure-load-balancer-internal: &quot;true&quot;

",<kubernetes><google-kubernetes-engine>,70282105,2,"the equivalent can be found here.
apiversion: v1
kind: service
metadata:
  name: ilb-service
  annotations:
    networking.gke.io/load-balancer-type: &quot;internal&quot;
  labels:
    app: hello
spec:
  type: loadbalancer
  selector:
    app: hello
  ports:
  - port: 80
    targetport: 8080
    protocol: tcp

"
62255313,how to block traffic from specific namespace in kubernetes,"i have multiple namespaces in my k8 cluster. all i want is that pods in development namespace can communicate with all other pods in other namespaces but should not be able to communicate with resources in production namespace.

i was not able to find a document for this scenario. there is a deny all policy like this

---
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: default-deny-egress
spec:
  podselector: {}
  policytypes:
  - egress
  - ingress


but it applies at pod level in the same namespace.  how can i modify the same to meet my requirements? 
",<kubernetes><kubernetes-networkpolicy>,62255886,2,"referring from the docs here


label all namespaces in development environment with environment=dev
label all namespaces in production environment with environment=prod


then you can have network policy as below


default deny policy as you have already
have a policy to whitelist traffic to pods from namespaces with label environment=prod


as below

apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: access-nginx
spec:
  podselector: {}
  ingress:
  - from:
    - namespaceselector:
        matchlabels:
          environment: ""prod""

"
62161691,308 redirect loop with externalname service using ingress-nginx,"i'm using ingress-nginx-controller (0.32.0) and am attempting to point an externalname service at a url and yet its stuck in a loop of 308 redirects. i've seen plenty of issues out there and i figure theres just one thing off with my configuration. is there something really small that i'm missing here?

configmap for nginx configuration:

kind: configmap
apiversion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
data:
  use-proxy-protocol: ""true""
  use-forwarded-headers: ""true""
  proxy-real-ip-cidr: ""0.0.0.0/0"" # restrict this to the ip addresses of elb
  proxy-read-timeout: ""3600""
  proxy-send-timeout: ""3600""
  backend-protocol: ""https""
  ssl-redirect: ""false""
  http-snippet: |
    map true $pass_access_scheme {
      default ""https"";
    }
    map true $pass_port {
      default 443;
    }
    server {
      listen 8080 proxy_protocol;
      return 308 https://$host$request_uri;
    }


nginx service:

kind: service
apiversion: v1
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: ""xxx""
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: ""tcp""
    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: ""https""
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: ""3600""
    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: ""*""
spec:
  type: loadbalancer
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
  ports:
    - name: http
      port: 80
      targetport: 8080
    - name: https
      port: 443
      targetport: http


ingress definition:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-members-portal
  namespace: dev
  annotations:
    kubernetes.io/ingress.class: ""nginx""
    nginx.ingress.kubernetes.io/use-regex: ""true""
spec:
  rules:
  - host: foo-111.dev.bar.com
    http:
      paths:
      - path: /login
        backend:
          servicename: foo-service
          serviceport: 80


externalname service:

apiversion: v1
kind: service
metadata:
  name: foo-service
spec:
  type: externalname
  externalname: foo.staging.bar.com
selector:
  app: foo


edit

i figured it out! i wanted to point to a service in another namespace, so i changed the externalname service to this:

apiversion: v1
kind: service
metadata:
  name: foo-service
spec:
  type: externalname
  externalname: foo-service.staging.svc.cluster.local
ports:
- port: 80
  protocol: tcp
  targetport: 80
selector:
  app: foo

",<nginx><kubernetes><kubernetes-ingress><nginx-ingress>,62180030,2,"i believe the issue you're seeing is due to the fact that your external service isn't working as you think it is.  in your ingress definition, you are defining the service to utilize port 80 on your foo-service.  in theory, this would redirect you back to your ingress controller's elb, redirect your request to the https://foo.staging.bar.com address, and move on.

however, external services don't really work that way. essentially, all externalname will do is run a dns check with kubedns/coredns, and return the cname information on that request.  it doesn't handle redirects of any kind.

for example, in this case, foo.staging.bar.com:80 would return foo.staging.bar.com:443.  you are directing the request for that site to port 80, which in itself directs the request to port 8080 in the ingress controller, which then redirects that request back out to the elb's port 443.  that redirect logic doesn't coexist with the external service.

the problem here, then, is that your app will essentially then try to do this:

http://foo-service:80 --> http://foo.staging.bar.com:80/login --> https://foo.staging.bar.com:443

my expectation on this is that you never actually reach the third step.  why? well because foo-service:80 is not directing you to port 443, first of all, but second of all...all coredns is doing in the backend is running a dns check against the foo-service's external name, which is foo.staging.bar.com.  it's not handling any kind of redirection.  so depending on how your host from your app is returned, and handled, your app may never actually get to that site and port.  so rather than reach that site, you just have your app keep looping back to http://foo-service:80 for those requests, which will always result in a 308 loopback.

the key here is that foo-service is the host header being sent to the nginx ingress controller, not foo.staging.bar.com.  so on a redirect to 443, my expectation is that all that is happening, then, is you are hitting foo-service, and any redirects are being improperly sent back to foo-service:80. 

a good way to test this is to run curl -l -v http://foo-service:80 and see what happens.  that will follow all redirects from that service, and provide you context as to how your ingress controller is handling those requests.

it's really hard to give more information, as i don't have access to your setup directly.  however, if you know that your app is always going to be hitting port 443, it would probably be a good fix, in this case, to change your ingress and service to look something like this:


apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-members-portal
  namespace: dev
  annotations:
    kubernetes.io/ingress.class: ""nginx""
    nginx.ingress.kubernetes.io/use-regex: ""true""
spec:
  rules:
  - host: foo-111.dev.bar.com
    http:
      paths:
      - path: /login
        backend:
          servicename: foo-service
          serviceport: 443
---
apiversion: v1
kind: service
metadata:
  name: foo-service
spec:
  type: externalname
  externalname: foo.staging.bar.com


that should ensure you don't have any kind of https redirect.  unfortunately, this may also cause issues with ssl validation, but that would be another issue all together.  the last piece of i can recommend is to possibly just use foo.staging.bar.com itself, rather than an external service in this case.

for more information, see: https://kubernetes.io/docs/concepts/services-networking/service/#externalname

hope this helps!
"
62162964,helm import map and array from values.yaml,"i have created a helm chart using  helm create &lt;chart-name&gt;
in values.yaml i added the following map and array
nodeselector:
  instance-type: &quot;re&quot;

tolerations:
  - key: &quot;re&quot;
    operator: &quot;equal&quot;
    value: &quot;true&quot;
    effect: &quot;noschedule&quot;

i am trying to import these in templates/deployment.yaml the config over there looks like with the right indentation
apiversion: apps/v1beta2
kind: deployment
metadata:
  name: {{ include &quot;dummy-app.fullname&quot; . }}
  labels:
    app.kubernetes.io/name: {{ include &quot;dummy-app.name&quot; . }}
    helm.sh/chart: {{ include &quot;dummy-app.chart&quot; . }}
    app.kubernetes.io/instance: {{ .release.name }}
    app.kubernetes.io/managed-by: {{ .release.service }}
spec:
  replicas: {{ .values.replicacount }}
  selector:
    matchlabels:
      app.kubernetes.io/name: {{ include &quot;dummy-app.name&quot; . }}
      app.kubernetes.io/instance: {{ .release.name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include &quot;dummy-app.name&quot; . }}
        app.kubernetes.io/instance: {{ .release.name }}
        log_group_name: {{ .values.logging.log_group_name }}
      annotations:
        jitsi.io/metrics_path: {{.values.service.metricspath | default &quot;/actuator/prometheus&quot; | quote }}
        jitsi.io/scrape_port: {{.values.service.actuatorport | default &quot;8083&quot; | quote }}
        jitsi.io/should_be_scraped: {{.values.service.shouldscrapp | default &quot;true&quot; | quote}}
    spec:
      containers:
        - name: {{ .chart.name }}
          image: &quot;{{ .values.image.repository }}:{{ .values.image.tag }}&quot;
          imagepullpolicy: {{ .values.image.pullpolicy }}
          ports:
            - name: http
              containerport: {{.values.service.targetport}}
              protocol: tcp
            - name: http-actuator
              containerport: {{.values.service.actuatorport}}
              protocol: tcp
          livenessprobe:
            httpget:
              path: /actuator/health
              port: http-actuator
            initialdelayseconds: 30
          readinessprobe:
            httpget:
              path: /actuator/health
              port: http-actuator
            initialdelayseconds: 30
          env:
            - name: profiles
              value: {{ required &quot;environment name is required.&quot; .values.env.environment | quote }}
          resources:
            {{- toyaml .values.resources | nindent 12 }}
    {{- with .values.nodeselector }}
    nodeselector:
      {{- toyaml . | nindent 8 }}
    {{- end }}
    {{- with .values.affinity }}
    affinity:
      {{- toyaml . | nindent 8 }}
    {{- end }}
    {{- with .values.tolerations }}
    tolerations:
      {{- toyaml . | nindent 8 }}
    {{- end }}

when i run this i get:
error: validation failed: error validating &quot;&quot;: error validating data: [validationerror(deployment.spec.template): unknown field &quot;nodeselector&quot; in io.k8s.api.core.v1.podtemplatespec, validationerror(deployment.spec.template): unknown field &quot;tolerations&quot; in io.k8s.api.core.v1.podtemplatespec]

i tried many other ways but none seems to work. my guess is the array and map there is something i need to change in deployment.yaml but i can't figure out how
",<kubernetes><kubernetes-helm><go-templates><amazon-eks>,62166815,2,"it seems you're indenting wrongly affinity, nodeselector and tolerations:

apiversion: apps/v1beta2
kind: deployment
metadata:
  name: {{ include ""dummy-app.fullname"" . }}
  labels:
    ...
spec:
  replicas: {{ .values.replicacount }}
  selector:
    matchlabels:
      ...
  template:
    metadata:
      labels:
        ...
      annotations:
        ...
    spec:
      containers:
        - name: {{ .chart.name }}
          image: ""{{ .values.image.repository }}:{{ .values.image.tag }}""
          imagepullpolicy: {{ .values.image.pullpolicy }}
          ports:
            ...
          livenessprobe:
            ...
          readinessprobe:
            ...
          env:
            ...
          resources:
            ...
    nodeselector:  # &lt;&lt;&lt; this are at the same level of `spec`
      ...
    affinity:      # &lt;&lt;&lt; this are at the same level of `spec`
      ...
    tolerations:   # &lt;&lt;&lt; this are at the same level of `spec`
      ...


the following keys must be at the same level of containers, so indenting with additional two blank spaces should fix your issue.
"
62122359,path based routing with nginx controller not working,"i have been trying my nginx path-based routing to work, however, after spending almost 4 hours, i am failed to understand, why is it not working. i have gone through almost every possible answer on stackoverflow before anyone downgrades my question, but none worked for me.

so here what i did:


i installed nginx-ingress using helm 3 (https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-helm/) in a separate namespace - nginx-test:

helm install my-release nginx-stable/nginx-ingress


a version of the ingress controller (https://hub.helm.sh/charts/nginx-edge/nginx-ingress):

$ pod_name=$(kubectl get pods -l app=nginx-controller-nginx-ingress -o jsonpath='{.items[0].metadata.name}')
$ 
$ kubectl exec -it $pod_name -- /nginx-ingress --version
version=edge gitcommit=50e908aa
$ 



there are 2 basic nginx deployments, 2 services already configured in the same namespace, and working fine when i configure host-based routing for them.
below one works fine for me (when i define host-based routing and get the required page index.html when i run both individual urls):


apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: nginx-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
  - host: nginx1.example.com
    http:
      paths:
      - path: /
        backend:
          servicename: nginx1
          serviceport: 80

  - host: nginx2.example.com
    http:
      paths:
      - path: /
        backend:
          servicename: nginx2
          serviceport: 80


now i wanted to achieve the same result using path-based routing, where there will be 1 url and 2 paths /nginx1 (pointing to nginx1 service) and /nginx2 (pointing to nginx2 service). so i configured the below ingress resource (and many permutations and combinations i applied based on different examples on internet), none of them worked for me.

apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: ingress-path-based
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    kubernetes.io/ingress.class: ""nginx""
spec:
  rules:
  - host: nginx.example.com
    http:
      paths:
      - path: /nginx1
        backend:
          servicename: nginx1
          serviceport: 80
      - path: /nginx2
        backend:
          servicename: nginx2
          serviceport: 80


when i access services directly, it works fine, however when i try to access - curl http://nginx.example.com/nginx1 or curl http://nginx.example.com/nginx2 - i get 404 not found error.

i was expecting to receive the same response which i was getting for host-based routing. but it does not seem to work.
",<kubernetes><kubernetes-ingress><nginx-ingress>,62128601,2,"so finally i had to install the controller using manifests, instead of helm charts (edge version).

i installed it from here (https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal), changed nodeport to loadbalancer to get a loadbalancer ip. i am using metallb on baremetal.

$ pod_name=$(kubectl get pods -n $pod_namespace -l app.kubernetes.io/component=controller -o jsonpath='{.items[0].metadata.name}')
$ kubectl exec -it $pod_name -n $pod_namespace -- /nginx-ingress-controller --version
-------------------------------------------------------------------------------
nginx ingress controller
  release:       0.32.0
  build:         git-446845114
  repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.17.10

-------------------------------------------------------------------------------

$ 


my ingress resource looks like the same which i posted while asking the question.

apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: ingress-path-based
  annotations:
    kubernetes.io/ingress.class: ""nginx""
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
  - host: nginx.gofork8s.com
    http:
      paths:
      - path: /nginx1
        backend:
          servicename: nginx1
          serviceport: 80
      - path: /nginx2
        backend:
          servicename: nginx2
          serviceport: 80


modified the new loadbalancer ip in /etc/hosts file to get the domain work.

192.168.0.1 nginx.example.com


now i am able to access - http://nginx.example.com/nginx1 and http://nginx.example.com/nginx2.

i hope it will help someone. i still need to figure out settings with helm charts.
"
62117515,kubernetes pod definition file issue,"i can run the docker container for wso2 ei with following command.
docker run -it -p 8280:8280 -p 8243:8243 -p 9443:9443 -v wso2ei:/home/wso2carbon --name integrator wso2/wso2ei-integrator

i'm trying to create the pod definition file for the same. i don't know how to do port mapping and volume mapping in pod definition file. the following is the file i have created up to now. how can i complete the rest?
apiversion: v1
kind: pod
metadata:
    name: ei-pod
    labels:
        type: ei
        version: 6.6.0
spec:
    containers:
        - name: integrator
          image: wso2/wso2ei-integrator

",<kubernetes><kubernetes-pod><wso2-esb>,62117973,2,"here is yaml content which might work:
apiversion: v1
kind: pod
metadata:
  name: ei-pod
  labels:
    type: ei
    version: 6.6.0
spec:
  containers:
  - name: integrator
    image: wso2/wso2ei-integrator
    ports:
    - containerport: 8280
    volumemounts:
    - mountpath: /wso2carbon
      name: wso2ei
  volumes:
  - name: wso2ei
    hostpath:
      # directory location on host
      path: /home/wso2carbon

while the above yaml content is just a basic example, it's not recommended for production usage because of two reasons:

use deployment or statefulset or daemonset instead of pods directly.

hostpath volume is not sharable between nodes. so use external volumes such as nfs or block and mount it into the pod. also look at dynamic volume provisioning using storage class.


"
72659036,how to keep loadbalancer[alb] even after we delete ingress manifest in aws eks?,"when we launch the eks cluster using the below manifest, it is creating alb.  we have a default alb that we are using, let's call it eks-alb. the hosted zone is routing traffic to this eks-alb. we gave tag ingress.k8s.aws/resource:loadbalancer,  ingress.k8s.aws/stack:test-alb, elbv2.k8s.aws/cluster: eks. but when we delete the manifest, it is deleting the default alb and we need to reconfigure hosted zone again with new alb which will get created in next deployment. is there any way to block ingress-controller not deleting alb, but only deleting the listeners and target group?
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: test-nginx-rule
  namespace: test
  annotations:
    alb.ingress.kubernetes.io/group.name: test-alb
    alb.ingress.kubernetes.io/scheme: internal
    alb.ingress.kubernetes.io/target-type: instance
    alb.ingress.kubernetes.io/listen-ports: '[{&quot;http&quot;: 80}, {&quot;https&quot;: 443}]'
    alb.ingress.kubernetes.io/ssl-redirect: '443'
    alb.ingress.kubernetes.io/healthcheck-port: traffic-port
    alb.ingress.kubernetes.io/healthcheck-path: /index.html
    alb.ingress.kubernetes.io/success-codes: 200-399
    alb.ingress.kubernetes.io/security-groups: eks-test-alb-sg
spec:
  ingressclassname: alb
  rules:
  - host: test.eks.abc.com
    http:
      paths:
      - pathtype: prefix
        path: &quot;/&quot;
        backend:
          service:
            name: test-svc
            port:
              number: 5005
---
apiversion: apps/v1
kind: deployment
metadata:
  name: test-dep
  namespace: test
  labels:
    app: test
spec:
  replicas: 1
  restartpolicy:
  selector:
    matchlabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      containers:
      - name: test
        image: imagepath
        imagepullpolicy: ifnotpresent
        ports:
        - containerport: 5005
        resources:
          requests:
            memory: &quot;256mi&quot;
            cpu: &quot;500m&quot;
---
apiversion: v1
kind: service
metadata:
  name: test-svc
  namespace: test
  labels:
    app: test
spec:
  type: nodeport
  ports:
  - port: 5005
    targetport: 80
    protocol: tcp
  selector:
    app: test
---
apiversion: autoscaling/v1
kind: horizontalpodautoscaler
metadata:
  name: test-scaler
  namespace: test
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: test-dep
  minreplicas: 1
  maxreplicas: 5
  targetcpuutilizationpercentage: 60
---

",<amazon-web-services><kubernetes><kubernetes-ingress><amazon-eks>,73520169,2,"in order to achieve the existing alb not being deleted with group.name annotation enabled, we need to meet following conditions:

alb should be tagged with below 3 tags:

alb.ingress.kubernetes.io/group.name: test-alb
alb.ingress.kubernetes.io/scheme: internal
alb.ingress.kubernetes.io/target-type: instance


create a dummy ingress with the same group name with the below manifest.

apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: test-nginx-rule
  namespace: test
  annotations:
    alb.ingress.kubernetes.io/group.name: test-alb
    alb.ingress.kubernetes.io/scheme: internal
    alb.ingress.kubernetes.io/target-type: instance
    alb.ingress.kubernetes.io/listen-ports: '[{&quot;http&quot;: 80}, {&quot;https&quot;: 443}]'
    alb.ingress.kubernetes.io/ssl-redirect: '443'
    alb.ingress.kubernetes.io/healthcheck-port: traffic-port
    alb.ingress.kubernetes.io/healthcheck-path: /index.html
    alb.ingress.kubernetes.io/success-codes: 200-399
    alb.ingress.kubernetes.io/security-groups: eks-test-alb-sg
spec:
  ingressclassname: alb
  rules:
  - host: dummy.eks.abc.com
    http:
      paths:
      - pathtype: prefix
        path: &quot;/&quot;
        backend:
          service:
            name: test-svc
            port:
              number: 5005

after deploying the above manifest, an ingress will be created using the same alb and listener will have rule of if host is dummy.eks.abc.com, it will return 443. it's create and forget type of manifest, so after creating this ingress, even after we delete all the running deployment services (except the dummy manifest file above), the alb will remain.
"
61752221,not able to access kubernetes service locally,"i am not be able to access my service from my local machine. i had done my k8 deployment using minikube and created a service of nodeport type to access it.

c:\windows\system32&gt;kubectl get deployments
name                 ready   up-to-date   available   age
fromk8dashboard      2/2     2            2           25m


and the service 

c:\windows\system32&gt;kubectl get svc
name              type        cluster-ip       external-ip   port(s)           age
fromk8dashboard   nodeport    10.100.68.204    &lt;none&gt;        31000:32058/tcp   26m
kubernetes        clusterip   10.96.0.1        &lt;none&gt;        443/tcp           24h


when i run the command:

c:\windows\system32&gt;minikube service fromk8dashboard
|-----------|-----------------|--------------------------|----------------------------|
| namespace |      name       |       target port        |            url             |
|-----------|-----------------|--------------------------|----------------------------|
| default   | fromk8dashboard | tcp-31000-80-6bw28/31000 | http://172.26.75.182:32058 |
|-----------|-----------------|--------------------------|----------------------------|
* opening service default/fromk8dashboard in default browser...


this opens the url ""http://172.26.75.182:32058"" on browser but it shows the site cannot be reached. am i missing some config or proxy settings?

another query is, my kubernetes dashboard is running on ""http://127.0.0.1:61750/"", so shouldn't the service nodeport url be http://127.0.0.1:32058/ or localhost:32058 ?

describe service returns the below result:

c:\windows\system32&gt;kubectl describe svc fromk8dashboard
name:                     fromk8dashboard
namespace:                default
labels:                   k8s-app=fromk8dashboard
annotations:              &lt;none&gt;
selector:                 k8s-app=fromk8dashboard
type:                     nodeport
ip:                       10.100.68.204
port:                     tcp-31000-80-6bw28  31000/tcp
targetport:               80/tcp
nodeport:                 tcp-31000-80-6bw28  32058/tcp
endpoints:                172.17.0.7:80,172.17.0.8:80
session affinity:         none
external traffic policy:  cluster
events:                   &lt;none&gt;



c:\windows\system32&gt;kubectl describe pod fromk8dashboard-6f98f9468c-wjrd5
name:           fromk8dashboard-6f98f9468c-wjrd5
namespace:      default
priority:       0
node:           minikube/172.26.75.182
start time:     tue, 12 may 2020 17:43:05 +0530
labels:         k8s-app=fromk8dashboard
                pod-template-hash=6f98f9468c
annotations:    &lt;none&gt;
status:         running
ip:             172.17.0.8
controlled by:  replicaset/fromk8dashboard-6f98f9468c
containers:
  fromk8dashboard:
    container id:   docker://eef3ecc8e564d9439b66592aa2f1eebd0a886e6380986066ae61bb00193c3687
    image:          ranjit1827/niguel
    image id:       docker-pullable://ranjit1827/niguel@sha256:b9229b2fdc7e5e3e9872189b3840f3c6f23d312af1bff9bbf5a5c68428be5894
    port:           &lt;none&gt;
    host port:      &lt;none&gt;
    state:          running
      started:      tue, 12 may 2020 17:43:30 +0530
    ready:          true
    restart count:  0
    environment:    &lt;none&gt;
    mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rn7lm (ro)
conditions:
  type              status
  initialized       true
  ready             true
  containersready   true
  podscheduled      true
volumes:
  default-token-rn7lm:
    type:        secret (a volume populated by a secret)
    secretname:  default-token-rn7lm
    optional:    false
qos class:       besteffort
node-selectors:  &lt;none&gt;
tolerations:     node.kubernetes.io/not-ready:noexecute for 300s
                 node.kubernetes.io/unreachable:noexecute for 300s
events:          &lt;none&gt;


my service yaml below:

kind: service
apiversion: v1
metadata:
  name: fromk8dashboard
  namespace: default
  selflink: /api/v1/namespaces/default/services/fromk8dashboard
  uid: 406e99ce-dec1-4e80-91aa-84f75c2d9892
  resourceversion: '70115'
  creationtimestamp: '2020-05-12t12:13:05z'
  labels:
    k8s-app: fromk8dashboard
  managedfields:
    - manager: dashboard
      operation: update
      apiversion: v1
      time: '2020-05-12t12:13:41z'
      fieldstype: fieldsv1
      fieldsv1:
        'f:metadata':
          'f:labels':
            .: {}
            'f:k8s-app': {}
        'f:spec':
          'f:externaltrafficpolicy': {}
          'f:ports':
            .: {}
            'k:{""port"":31000,""protocol"":""tcp""}':
              .: {}
              'f:name': {}
              'f:port': {}
              'f:protocol': {}
              'f:targetport': {}
          'f:selector':
            .: {}
            'f:k8s-app': {}
          'f:sessionaffinity': {}
          'f:type': {}
spec:
  ports:
    - name: tcp-31000-80-6bw28
      protocol: tcp
      port: 31000
      targetport: 80
      nodeport: 32058
  selector:
    k8s-app: fromk8dashboard
  clusterip: 10.100.68.204
  type: nodeport
  sessionaffinity: none
  externaltrafficpolicy: cluster
status:
  loadbalancer: {}

",<kubernetes><kubectl><minikube><kubernetes-service>,61752394,2,"use below yaml and you should be to access it via http://172.26.75.182:30007/

apiversion: v1
kind: service
metadata:
  name: my-service
spec:
  type: nodeport
  selector:
    k8s-app: fromk8dashboard
  ports:
      # by default and for convenience, the `targetport` is set to the same value as the `port` field.
    - port: 80
      targetport: 80
      # optional field
      # by default and for convenience, the kubernetes control plane will allocate a port from a range (default: 30000-32767)
      nodeport: 30007


check the guide for exposing pod via nodeport service.
"
72922657,allow two ports on multi-region ingress for gke service,"i have two ports on node.js, 4000 and 5050, open for normal api and web socket (require session affinity for web socket) respectively.
below is my mcs.yaml
apiversion: networking.gke.io/v1
kind: multiclusterservice
metadata:
  name: terraback-mcs
  namespace: terraback
  labels:
    version: v1
spec:
  template:
    spec:
      selector:
        app: terraback
      ports:
        - name: web
          protocol: tcp
          port: 8080
          targetport: 4000

mci.yaml
apiversion: networking.gke.io/v1
kind: multiclusteringress
metadata:
  name: terraback-ingress
  namespace: terraback
  labels:
    version: v1
spec:
  template:
    spec:
      backend:
        servicename: terraback-mcs
        serviceport: 8080

deployment.yaml
# copyright 2021 google llc
#
# licensed under the apache license, version 2.0 (the &quot;license&quot;);
# you may not use this file except in compliance with the license.
# you may obtain a copy of the license at
#
#      http://www.apache.org/licenses/license-2.0
#
# unless required by applicable law or agreed to in writing, software
# distributed under the license is distributed on an &quot;as is&quot; basis,
# without warranties or conditions of any kind, either express or implied.
# see the license for the specific language governing permissions and
# limitations under the license.

apiversion: apps/v1
kind: deployment
metadata:
  name: terraback-instance
  namespace: terraback
  labels:
    app: terraback
spec:
  replicas: 1
  selector:
    matchlabels:
      app: terraback
  template:
    metadata:
      labels:
        app: terraback
    spec:
      containers:
      - name: terraback-app
        # replace $location with your artifact registry location (e.g., us-west1).
        # replace $gcloud_project with your project id.
        image: us-east1-docker.pkg.dev/terraback-v2/terraback-repo/terraback-instance:latest
        # this app listens on port 8080 for web traffic by default.
        imagepullpolicy: always
        ports:
        - containerport: 8080
        env:
          - name: port
            value: &quot;8080&quot;
---

how should i make changes to allow additional port 5050?
i have tried the method from zer0, with the modification of mci from validation error for multiingress yaml file for gke and it works.
",<kubernetes><google-cloud-platform><google-kubernetes-engine><load-balancing><gke-networking>,72922924,2,"you need to create an additional port in your terraback-mcs service. this will serve the web socket connections. the following changes will be required:
apiversion: networking.gke.io/v1
kind: multiclusterservice
metadata:
  name: terraback-mcs
  namespace: terraback
  labels:
    version: v1
spec:
  template:
    spec:
      selector:
        app: terraback
      ports:
        - name: web
          protocol: tcp
          port: 8080
          targetport: 4000
        - name: web-socket
          protocol: tcp
          port: 8081
          targetport: 5050


you will then need to add an additional backend to your multiclusteringress. this can be done as follows:
apiversion: networking.gke.io/v1
kind: multiclusteringress
metadata:
  name: terraback-ingress
  namespace: terraback
  labels:
    version: v1
spec:
  template:
    spec:
      backend:
        servicename: terraback-mcs
        serviceport: 8080 # &lt;==== this gets routed to service port 8080-&gt;4000
      rules:
      - host: web.socket.hostname #&lt;==== adjust this as needed
          http:
            paths:        
              - backend:
                servicename: terraback-mcs
                serviceport: 8081 # &lt;=== this gets routed to serviceport 8081 =&gt; 5050

this ingress will route incoming requests to the provided host if there is a match. otherwise, the connection will be forwarded to the default host, which will be your api service.
i hope this gives you an idea of how to configure multiple ports with multiclusterservice and multiclusteringress objects. i'm unsure what is the point of the terraback-instance deployment. i don't believe this is your node.js application, since it neither exposes port 5050 nor port 4000. it it is your application indeed, then you need to expose both ports, correctly as:
ports:
  - name: api-port
    containerport: 4000
  - name: web-socket-port
    containerport: 5050

summary
the traffic flow will be like this:

all incoming traffic on the hostname web.socket.hostname will be rerouted by the ingress to the service with port 8081. this, will be routed by the service, to the container port 5050.
all the incoming traffic other than the hostname above, will be rerouted by the ingress to the service on port 8080. this, will be routed by the service, to the container port 4000.

understand that ingress objects process the incoming traffic on the basis of a host and path matching. and forward to backend services on certain ports based on the result of this match. in the current configuration, the ingress rule is defining a default backend only, because of which all the traffic coming in, will be forwarded to the same backend service configured.
once you make these changes, the ingress will separate traffic based on the host and forward the traffic to the appropriate service backend.
hope this helps.
"
61742229,kubernetes: storageclass with local provisioner and statefulset kind,"one of my pods has 'statefulset' kind with volumeclaimtemplates section referring to a storageclass(sc) i created, see below.

sc:

apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumebindingmode: waitforfirstconsumer


statefulset yaml with reference to above created sc:

volumeclaimtemplates:
    - metadata:
        name: mydata
      spec:
        storageclassname: local-storage
        accessmodes:
          - readwriteonce
        resources:
          requests:
            storage: 2gi


as far as i am aware, a statefulset will create node specific pvcs without a need for explicit pv and pvc set up, i see that pv being created but the pod status is 'pending' with below warning.


  warning  failedscheduling  default-scheduler  0/4 nodes are available: 4 node(s) didn't find available persistent volumes to bind.


note that i don't have a default storageclass set up in the cluster, i believe that's not required for this scenario, is that correct?
do we need to enable or configure anything for 'local' provisioner to work in the cluster?

thanks
",<kubernetes><persistent-volume-claims><kubernetes-statefulset>,61764167,2,"found out hard way that missing piece for this to work was pv set up.

---
apiversion: v1
kind: persistentvolume
metadata:
  name: pv-loc-sc
spec:
  persistentvolumereclaimpolicy: delete
  storageclassname: local-storage 
  capacity:
    storage: 2gi
  accessmodes:
    - readwriteonce
  local:
    path: ""/var/lib/test""
  nodeaffinity:
    required:
      nodeselectorterms:
      - matchexpressions:
        - key: kubernetes.io/hostname
          operator: in
          values:
          - my-test-node-host-name


the failedscheduling warning went away with pv, sc, and reference to the sc in statefulset pod yaml.
"
61886582,how to add resource and limits on kubernetes engine on google cloud platform,"i am trying to add resource and limits to my deployment on kuberenetes engine since one of my deployment on the pod is continuously getting evicted with an error message the node was low on resource: memory. container model-run was using 1904944ki, which exceeds its request of 0. i assume that the issue could be resolved by adding resource requests. 

when i try to add resource requests and deploy, the deployment is successful but when i go back and and view detailed information about the pod, with the command
kubectl get pod default-pod-name --output=yaml --namespace=default
it still says the pod has request of cpu: 100m and without any mention of memory that i have allotted. i am guessing that the cpu request of 100m was a default one. please let me know how i can allot the requests and limits, the code i am using to deploy is as follows:

kubectl run model-run --image-pull-policy=always --overrides='
{
    ""apiversion"": ""apps/v1beta1"",
    ""kind"": ""deployment"",
    ""metadata"": {
        ""name"": ""model-run"",
        ""labels"": {
            ""app"": ""model-run""
        }
    },
    ""spec"": {
        ""selector"": {
            ""matchlabels"": {
                ""app"": ""model-run""
            }
        },
        ""template"": {
            ""metadata"": {
                ""labels"": {
                    ""app"": ""model-run""
                }
            },
            ""spec"": {
                ""containers"": [
                    {
                        ""name"": ""model-run"",
                        ""image"": ""gcr.io/some-project/news/model-run:development"",
                    ""imagepullpolicy"": ""always"",
                      ""resouces"": {
                        ""requests"": [
                          {
                            ""memory"": ""2048mi"",
                            ""cpu"": ""500m""
                          }
                        ],
                        ""limits"": [
                          {
                            ""memory"": ""2500mi"",
                            ""cpu"": ""750m""
                          }
                        ]
                      },
                        ""volumemounts"": [
                            {
                                ""name"": ""credentials"",
                                ""readonly"": true,
                                ""mountpath"":""/path/collection/keys""
                            }
                        ],
                        ""env"":[
                            {
                                ""name"":""google_application_credentials"",
                                ""value"":""/path/collection/keys/key.json""
                            }
                                ]
                    }
                ],
                ""volumes"": [
                    {
                        ""name"": ""credentials"",
                        ""secret"": {
                            ""secretname"": ""credentials""
                        }
                    }
                ]
            }
        }
    }
}
'  --image=gcr.io/some-project/news/model-run:development


any solution will be appreciated
",<kubernetes><google-cloud-platform><google-kubernetes-engine><kubernetes-pod>,61892368,2,"
  the node was low on resource: memory. container model-run was using 1904944ki, which exceeds its request of 0.


at first the message seems like there is a lack of resource in the node itself but the second part makes me believe you are correct in trying to raise the request limit for the container.

just keep in mind that if you still face errors after this change, you might need to add mode powerful node-pools to your cluster.

i went through your command, there is a few issues i'd like to highlight:


kubectl run was deprecated in 1.12 to all resources except for pods and it is retired in version 1.18.
apiversion"": ""apps/v1beta1 is deprecated, and starting on v 1.16 it is no longer be supported, i replaced with apps/v1. 
in spec.template.spec.container it's written ""resouces"" instead of ""resources""
after fixing the resources the next issue is that requests and limits are written in array format, but they need to be in a list, otherwise you get this error:


kubectl run --generator=deployment/apps.v1 is deprecated and will be removed in a future version. use kubectl run --generator=run-pod/v1 or kubectl create instead.
error: v1beta1.deployment.spec: v1beta1.deploymentspec.template: v1.podtemplatespec.spec: v1.podspec.containers: []v1.container: v1.container.resources: v1.resourcerequirements.limits: readmapcb: expect { or n, but found [, error found in #10 byte of ...|""limits"":[{""cpu"":""75|..., bigger context ...|always"",""name"":""model-run"",""resources"":{""limits"":[{""cpu"":""750m"",""memory"":""2500mi""}],""requests"":[{""cp|...



here is the fixed format of your command:


kubectl run model-run --image-pull-policy=always --overrides='{
  ""apiversion"": ""apps/v1"",
  ""kind"": ""deployment"",
  ""metadata"": {
    ""name"": ""model-run"",
    ""labels"": {
      ""app"": ""model-run""
    }
  },
  ""spec"": {
    ""selector"": {
      ""matchlabels"": {
        ""app"": ""model-run""
      }
    },
    ""template"": {
      ""metadata"": {
        ""labels"": {
          ""app"": ""model-run""
        }
      },
      ""spec"": {
        ""containers"": [
          {
            ""name"": ""model-run"",
            ""image"": ""nginx"",
            ""imagepullpolicy"": ""always"",
            ""resources"": {
              ""requests"": {
                ""memory"": ""2048mi"",
                ""cpu"": ""500m""
              },
              ""limits"": {
                ""memory"": ""2500mi"",
                ""cpu"": ""750m""
              }
            },
            ""volumemounts"": [
              {
                ""name"": ""credentials"",
                ""readonly"": true,
                ""mountpath"": ""/path/collection/keys""
              }
            ],
            ""env"": [
              {
                ""name"": ""google_application_credentials"",
                ""value"": ""/path/collection/keys/key.json""
              }
            ]
          }
        ],
        ""volumes"": [
          {
            ""name"": ""credentials"",
            ""secret"": {
              ""secretname"": ""credentials""
            }
          }
        ]
      }
    }
  }
}'  --image=gcr.io/some-project/news/model-run:development



now after aplying it on my kubernetes engine cluster v1.15.11-gke.13 , here is the output of kubectl get pod x -o yaml:


$ kubectl get pods
name                         ready   status    restarts   age
model-run-7bd8d79c7d-brmrw   1/1     running   0          17s

$ kubectl get pod model-run-7bd8d79c7d-brmrw -o yaml
apiversion: v1
kind: pod
metadata:
  labels:
    app: model-run
    pod-template-hash: 7bd8d79c7d
    run: model-run
  name: model-run-7bd8d79c7d-brmrw
  namespace: default
spec:
  containers:
  - env:
    - name: google_application_credentials
      value: /path/collection/keys/key.json
    image: nginx
    imagepullpolicy: always
    name: model-run
    resources:
      limits:
        cpu: 750m
        memory: 2500mi
      requests:
        cpu: 500m
        memory: 2gi
    volumemounts:
    - mountpath: /path/collection/keys
      name: credentials
      readonly: true
    - mountpath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-tjn5t
      readonly: true
  nodename: gke-cluster-115-default-pool-abca4833-4jtx
  restartpolicy: always
  volumes:
  - name: credentials
    secret:
      defaultmode: 420
      secretname: credentials




you can see that the resources limits and requests were set.


if you still have any question let me know in the comments!
"
61958879,how do you set up kubernetes rbac resources so that pods can access the api via a client?,"problem

i have a simple rbac configuration to access the kubernetes api in-cluster. however i am getting what appears to be conflicting information from kubectl. after deploying the manifest, it appears that rbac is set up properly.

$ kubectl exec -ti pod/controller -- kubectl auth can-i get namespaces
warning: resource 'namespaces' is not namespace scoped
yes


however, actually making the request yields a permission error

$ kubectl exec -ti pod/controller -- kubectl get namespaces
error from server (forbidden): namespaces is forbidden: user ""system:serviceaccount:default:controller"" cannot list resource ""namespaces"" in api group """" at the cluster scope
command terminated with exit code 1


manifest

apiversion: 'v1'
kind: 'serviceaccount'
metadata:
  name: 'controller'
---

apiversion: 'rbac.authorization.k8s.io/v1'
kind: 'role'
metadata:
  name: 'read-namespaces'
rules:
  - apigroups:
      - ''
    resources:
      - 'namespaces'
    verbs:
      - 'get'
      - 'watch'
      - 'list'
---

apiversion: 'rbac.authorization.k8s.io/v1'
kind: 'rolebinding'
metadata:
  name: 'read-namespaces'
roleref:
  apigroup: ''
  kind: 'role'
  name: 'read-namespaces'
subjects:
  - kind: 'serviceaccount'
    name: 'controller'
---

apiversion: 'v1'
kind: 'pod'
metadata:
  name: 'controller'
  labels:
    'app': 'controller'
spec:
  containers:
    - name: 'kubectl'
      image: 'bitnami/kubectl:latest'
      imagepullpolicy: 'always'
      command:
        - 'sleep'
        - '3600'
  serviceaccountname: 'controller'
---


other info

i've tried kubectl auth reconcile -f manifest.yaml as well as kubectl apply -f manifest.yaml and the results are the same.

i've also set ""read-namespaces"" rolebinding.subjects[0].namespace to the  proper namespace (""default"" in this case). no change in output.
",<kubernetes><kubectl><rbac>,61967635,2,"namespace is a cluster scoped resource. so you need a clusterrole and a clusterrolebinding.

apiversion: 'rbac.authorization.k8s.io/v1'
kind: 'clusterrole'
metadata:
  name: 'read-namespaces'
rules:
  - apigroups:
      - ''
    resources:
      - 'namespaces'
    verbs:
      - 'get'
      - 'watch'
      - 'list'
---

apiversion: 'rbac.authorization.k8s.io/v1'
kind: 'clusterrolebinding'
metadata:
  name: 'read-namespaces'
roleref:
  apigroup: 'rbac.authorization.k8s.io'
  kind: 'clusterrole'
  name: 'read-namespaces'
subjects:
  - kind: 'serviceaccount'
    name: 'controller'
--- 

"
73593125,receiving external request from internal ip on nginx pod in gke,"i have created nginx pod with load balancer service in gke. when it receive requests from external user, in nginx log it is showing the request is received from an internal ip (and this is happening randomly).
sample request
&quot;@timestamp&quot;: &quot;03/sep/2022:16:25:38 +0530&quot;, &quot;@fields&quot;: { &quot;remote_addr&quot;: &quot;10.160.0.30&quot;, &quot;remote_user&quot;: &quot;-&quot;, &quot;body_bytes_sent&quot;: &quot;3767&quot;, &quot;gzip_ratio&quot;: &quot;3.52&quot;,&quot;request_time&quot;: &quot;0.113&quot;, &quot;connection_requests&quot;: &quot;7&quot;,&quot;status&quot;: &quot;200&quot;, &quot;request&quot;: &quot;get /admin http/2.0&quot;, &quot;request_method&quot;: &quot;get&quot;

yaml files
apiversion: apps/v1
kind: deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: image_name
        ports:
        - containerport: 80
        - containerport: 443
        resources:
          requests:
            cpu: 2000m
            memory: 2gi
          limits:
            cpu: 2000m
            memory: 2gi

==========
apiversion: v1
kind: service
metadata:
  name: dev-nginx-lb-service
spec:
  loadbalancerip: ip_address
  type: loadbalancer
  selector:
    app: nginx
  ports:
  - name: https
    protocol: tcp
    port: 443
    targetport: 443
  - name: http
    protocol: tcp
    port: 80
    targetport: 80

",<nginx><kubernetes><google-kubernetes-engine>,73606749,2,"not sure why it would be random.  by default, gke nodes perform snat on packets received via load balancing. if you want to pass the client address directly to the backend pods, you can set externaltrafficpolicy: local in your service spec:
apiversion: v1
kind: service
metadata:
  name: dev-nginx-lb-service
spec:
  loadbalancerip: ip_address
  type: loadbalancer
  externaltrafficpolicy: local
  selector:
    app: nginx
  ports:
  - name: https
    protocol: tcp
    port: 443
    targetport: 443
  - name: http
    protocol: tcp
    port: 80
    targetport: 80

"
73595267,remove nodeselectorterms param in manifest deployment,"i use this manifest configuration to deploy a registry into 3 mode kubernetes cluster:
apiversion: v1
kind: persistentvolume
metadata:
  name: pv1
  namespace: registry-space
spec:
  capacity:
    storage: 5gi # specify your own size
  volumemode: filesystem
  persistentvolumereclaimpolicy: retain
  local:
    path: /opt/registry # can be any path
  nodeaffinity:
    required:
      nodeselectorterms:
      - matchexpressions:
        - key: kubernetes.io/hostname
          operator: in
          values:
          - kubernetes2
  accessmodes:
    - readwritemany # only 1 node will read/write on the path.
---
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: pv1-claim
  namespace: registry-space
spec: # should match specs added in the persistenvolume
  accessmodes:
    - readwritemany
  volumemode: filesystem
  resources:
    requests:
      storage: 5gi
---
apiversion: apps/v1
kind: deployment
metadata:
  name: private-repository-k8s
  namespace: registry-space
  labels:
    app: private-repository-k8s
spec:
  replicas: 1
  selector:
    matchlabels:
      app: private-repository-k8s
  template:
    metadata:
      labels:
        app: private-repository-k8s
    spec:
      volumes:
       - name: certs-vol
         hostpath:
          path: /opt/certs
          type: directory
       - name: task-pv-storage
         persistentvolumeclaim:
           claimname: pv1-claim # specify the pvc that you've created. pvc and deployment must be in same namespace.
      containers:
        - image: registry:2
          name: private-repository-k8s
          imagepullpolicy: ifnotpresent
          env:
          - name: registry_http_tls_certificate
            value: &quot;/opt/certs/registry.crt&quot;
          - name: registry_http_tls_key
            value: &quot;/opt/certs/registry.key&quot;
          ports:
            - containerport: 5000
          volumemounts:
          - name: certs-vol
            mountpath: /opt/certs
          - name: task-pv-storage
            mountpath: /opt/registry

i manually created directories on every node under /opt/certs and /opt/registry.
but when i try to deploy the manifest without hardcoded nodeselectorterms on tha control plane i get error:
kubernetes@kubernetes1:/opt/registry$ kubectl get pods --all-namespaces
namespace        name                                       ready   status    restarts      age
kube-system      calico-kube-controllers-58dbc876ff-fsjd5   1/1     running   1 (74m ago)   84m
kube-system      calico-node-5brzt                          1/1     running   1 (73m ago)   84m
kube-system      calico-node-nph9n                          1/1     running   1 (76m ago)   84m
kube-system      calico-node-pcd74                          1/1     running   1 (74m ago)   84m
kube-system      calico-node-ph2ht                          1/1     running   1 (76m ago)   84m
kube-system      coredns-565d847f94-7pswp                   1/1     running   1 (74m ago)   105m
kube-system      coredns-565d847f94-tlrfr                   1/1     running   1 (74m ago)   105m
kube-system      etcd-kubernetes1                           1/1     running   2 (74m ago)   105m
kube-system      kube-apiserver-kubernetes1                 1/1     running   2 (74m ago)   105m
kube-system      kube-controller-manager-kubernetes1        1/1     running   2 (74m ago)   105m
kube-system      kube-proxy-4slm4                           1/1     running   1 (76m ago)   86m
kube-system      kube-proxy-4tnx2                           1/1     running   2 (74m ago)   105m
kube-system      kube-proxy-9dgsj                           1/1     running   1 (73m ago)   85m
kube-system      kube-proxy-cgr44                           1/1     running   1 (76m ago)   86m
kube-system      kube-scheduler-kubernetes1                 1/1     running   2 (74m ago)   105m
registry-space   private-repository-k8s-6d5d954b4f-xkmj5    0/1     pending   0             4m55s
kubernetes@kubernetes1:/opt/registry$

do you know how i can let kubernetes to decide where to deploy the pod?
",<kubernetes><kubernetes-helm>,73596639,2,"it seems like your node has taints hence pods are not getting scheduled. can you try using this command to remove taints from your node ?
kubectl taint nodes  &lt;node-name&gt; node-role.kubernetes.io/master-

or
kubectl taint nodes --all node-role.kubernetes.io/master-

to get the node name use kubectl get nodes
user was able to get the pod scheduled after running below command:
kubectl taint nodes kubernetes1 node-role.kubernetes.io/control-plane:noschedule-

now pod is failing due to crashloopbackoff this implies the pod has been scheduled.
can you please check if this pod is getting scheduled and running properly ?
apiversion: v1
kind: pod
metadata:
  name: nginx1
  namespace: test
spec:
  containers:
  - name: webserver
    image: nginx:alpine
    ports:
    - containerport: 80
    resources:
      requests:
        memory: &quot;64mi&quot;
        cpu: &quot;200m&quot;
      limits:
        memory: &quot;128mi&quot;
        cpu: &quot;350m&quot;

"
73766937,"error: namespaces is forbidden: user ""system:serviceaccount:default:test"" cannot create resource ""namespaces"" in api group """"","i want to configure native kubernetes cluster using terraform script. i tried this terraform script:
terraform {
  required_providers {
    kubernetes = {
      source = &quot;hashicorp/kubernetes&quot;
      version = &quot;2.13.1&quot;
    }
    kubectl = {
      source = &quot;gavinbunney/kubectl&quot;
      version = &quot;1.14.0&quot;
    }
    helm = {
      source = &quot;hashicorp/helm&quot;
      version = &quot;2.6.0&quot;
    }
  }
}

provider &quot;kubectl&quot; {
  # run kubectl cluster-info to get expoint and port
  host = &quot;https://192.168.1.139:6443/&quot;
  token = &quot;eyjhbgcioijsuzi1niisimt.....&quot;
  insecure = &quot;true&quot;
}

provider &quot;kubernetes&quot; {
  # run kubectl cluster-info to get expoint and port
  host = &quot;https://192.168.1.139:6443/&quot;
  token = &quot;eyjhbgcioijsuzi1niisimt.....&quot;
  insecure = &quot;true&quot;
}

resource &quot;kubernetes_namespace&quot; &quot;example&quot; {
  metadata {
    annotations = {
      name = &quot;example-annotation&quot;
    }

    labels = {
      mylabel = &quot;label-value&quot;
    }

    name = &quot;terraform-example-namespace&quot;
  }
}

ref: https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs
https://registry.terraform.io/providers/gavinbunney/kubectl/latest/docs
i tried to create a user from this tutorial: https://killercoda.com/kimwuestkamp/scenario/k8s1.24-serviceaccount-secret-changes
kubectl create sa cicd

kubectl get sa,secret


cat &lt;&lt;eof | kubectl apply -f -
apiversion: v1
kind: pod
metadata:
  name: cicd
spec:
  serviceaccount: cicd
  containers:
  - image: nginx
    name: cicd
eof




kubectl exec cicd -- cat /run/secrets/kubernetes.io/serviceaccount/token &amp;&amp; echo

kubectl exec cicd cat /run/secrets/kubernetes.io/serviceaccount/token &amp;&amp; echo


kubectl create token cicd

kubectl create token cicd --duration=999999h



cat &lt;&lt;eof | kubectl apply -f -
apiversion: v1
kind: secret
type: kubernetes.io/service-account-token
metadata:
  name: cicd
  annotations:
    kubernetes.io/service-account.name: &quot;cicd&quot;
eof



kubectl get sa,secret

kubectl describe secret cicd

kubectl describe sa cicd

kubectl get sa cicd -oyaml

kubectl get sa,secret

when i run the terraform script i get error:
kubernetes_namespace.example: creating...

 error: namespaces is forbidden: user &quot;system:serviceaccount:default:cicd&quot; cannot create resource &quot;namespaces&quot; in api group &quot;&quot; at the cluster scope

   with kubernetes_namespace.example,
   on main.tf line 36, in resource &quot;kubernetes_namespace&quot; &quot;example&quot;:
   36: resource &quot;kubernetes_namespace&quot; &quot;example&quot; {

can you advise what user configuration i'm missing?
can you advise what is the proper way to implement this script and provision helm chart into native kubernetes.
",<kubernetes><terraform><kubernetes-helm><terraform-template-file>,73861572,2,"
error: namespaces is forbidden: user &quot;system:serviceaccount:default:cicd&quot; cannot create resource &quot;namespaces&quot; in api group &quot;&quot; at the cluster scope

the service account cicd in namespace default is lacked of permissions. you can first assign cluster-admin permissions to ensure your pipeline is functioning, then trim the permissions gradually according to your use case. apply the following spec before your pipeline starts:
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: &lt;of your own&gt;
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: cluster-admin
subjects:
- kind: serviceaccount
  name: cicd
  namespace: default

"
63185780,consul statefulset failing,"i am trying to deploy consul using kubernetes statefulset with following manifest
---

apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  name: consul
  labels:
    app: consul
rules:
  - apigroups: [&quot;&quot;]
    resources:
      - pods
    verbs:
      - get
      - list

---

apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: consul
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: consul
subjects:
  - kind: serviceaccount
    name: consul
    namespace: dev-ethernet

---

apiversion: v1
kind: serviceaccount
metadata:
  name: consul
  namespace: dev-ethernet
  labels:
    app: consul

---

apiversion: v1
kind: secret
metadata:
  name: consul-secret
  namespace: dev-ethernet
data:
  consul-gossip-encryption-key: &quot;airpnkht/8tkvf757sj2m5acrlorwngzcli4ylemx7m=&quot;

---

apiversion: v1
kind: configmap
metadata:
  name: consul-config
  namespace: dev-ethernet
data:
  server.json: |
    {
      &quot;bind_addr&quot;: &quot;0.0.0.0&quot;,
      &quot;client_addr&quot;: &quot;0.0.0.0&quot;,
      &quot;disable_host_node_id&quot;: true,
      &quot;data_dir&quot;: &quot;/consul/data&quot;,
      &quot;log_level&quot;: &quot;info&quot;,
      &quot;datacenter&quot;: &quot;us-west-2&quot;,
      &quot;domain&quot;: &quot;cluster.local&quot;,
      &quot;ports&quot;: {
        &quot;http&quot;: 8500
      },
      &quot;retry_join&quot;: [
        &quot;provider=k8s label_selector=\&quot;app=consul,component=server\&quot;&quot;
      ],
      &quot;server&quot;: true,
      &quot;telemetry&quot;: {
        &quot;prometheus_retention_time&quot;: &quot;5m&quot;
      },
      &quot;ui&quot;: true
    }

---

apiversion: apps/v1
kind: statefulset
metadata:
  name: consul
  namespace: dev-ethernet
spec:
  selector:
    matchlabels:
      app: consul
      component: server
  servicename: consul
  podmanagementpolicy: parallel
  replicas: 3
  updatestrategy:
    rollingupdate:
      partition: 0
    type: rollingupdate
  template:
    metadata:
      labels:
        app: consul
        component: server
      annotations:
        consul.hashicorp.com/connect-inject: &quot;false&quot;
    spec:
      serviceaccountname: consul
      affinity:
        podantiaffinity:
          requiredduringschedulingignoredduringexecution:
            - labelselector:
                matchexpressions:
                  - key: app
                    operator: in
                    values:
                      - consul
              topologykey: kubernetes.io/hostname
      terminationgraceperiodseconds: 10
      securitycontext:
        fsgroup: 1000
      containers:
        - name: consul
          image: &quot;consul:1.8&quot;
          args:
            - &quot;agent&quot;
            - &quot;-advertise=$(pod_ip)&quot;
            - &quot;-bootstrap-expect=3&quot;
            - &quot;-config-file=/etc/consul/config/server.json&quot;
            - &quot;-encrypt=$(gossip_encryption_key)&quot;
          env:
            - name: pod_ip
              valuefrom:
                fieldref:
                  fieldpath: status.podip
            - name: gossip_encryption_key
              valuefrom:
                secretkeyref:
                  name: consul-secret
                  key: consul-gossip-encryption-key
          volumemounts:
            - name: data
              mountpath: /consul/data
            - name: config
              mountpath: /etc/consul/config
          lifecycle:
            prestop:
              exec:
                command:
                - /bin/sh
                - -c
                - consul leave
          ports:
            - containerport: 8500
              name: ui-port
            - containerport: 8400
              name: alt-port
            - containerport: 53
              name: udp-port
            - containerport: 8080
              name: http-port
            - containerport: 8301
              name: serflan
            - containerport: 8302
              name: serfwan
            - containerport: 8600
              name: consuldns
            - containerport: 8300
              name: server
      volumes:
        - name: config
          configmap:
            name: consul-config
  volumeclaimtemplates:
  - metadata:
      name: data
      labels:
        app: consul
    spec:
      accessmodes: [ &quot;readwriteonce&quot; ]
      storageclassname: aws-gp2
      resources:
        requests:
          storage: 3gi

but gets ==&gt; encrypt has invalid key: illegal base64 data at input byte 1 when container starts.
i have generated consul-gossip-encryption-key locally using docker run -i -t consul keygen
anyone knows whats wrong here ?
",<kubernetes><consul><kubernetes-secrets><kubernetes-statefulset>,63185863,2,"secret.data must be base64 string.
try
kubectl create secret generic consul-gossip-encryption-key --from-literal=key=&quot;$(docker run -i -t consul keygen)&quot; --dry-run -o=yaml

and replace
apiversion: v1
kind: secret
metadata:
  name: consul-secret
  namespace: dev-ethernet
data:
  consul-gossip-encryption-key: &quot;airpnkht/8tkvf757sj2m5acrlorwngzcli4ylemx7m=&quot;

ref: https://www.consul.io/docs/k8s/helm#v-global-gossipencryption
"
73824972,what is the network security policy for kubectl to work on minikube?,"i have started to implement nsps for a project. we currently have the following nsps defined as stated below. next to a default deny all nsp.
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: kubectl-policy
  namespace: ns-where-the-pod-lives
spec:
  podselector:
    matchlabels:
      app: virtualdesktop
  policytypes:
    - ingress
    - egress
  ingress:
    - from:
        - namespaceselector:
            matchlabels:
              kubernetes.io/metadata.name: kube-system
          podselector: {}
        - namespaceselector:
            matchlabels:
              kubernetes.io/metadata.name: default
          podselector: {}
        - namespaceselector:
            matchlabels:
              kubernetes.io/metadata.name: ns-where-the-pod-lives
          podselector: {}
  egress:
    - to:
        - namespaceselector:
            matchlabels:
              kubernetes.io/metadata.name: kube-system
          podselector: {}
        - namespaceselector:
            matchlabels:
              kubernetes.io/metadata.name: ns-where-the-pod-lives
          podselector: {}

but i cannot reach the control plane from a pod unless i provide an nsp which allows for using 0.0.0.0 (internet). what nsp do i need to allow the pod to reach eks k8s control plane internally?
",<kubernetes><network-programming><kubectl><amazon-eks><minikube>,73825636,2,"since you've applied a default deny-all netpol, you have to explicitly allow communication to the api-server.
first find out what the ip('s) of your kubernetes api service:
kubectl get endpoints --namespace default kubernetes
name         endpoints                                         age
kubernetes   172.24.4.2:8443,172.24.4.3:8443,172.24.4.4:8443   69d

you might have a different port than mine (8443)!
then just create the netpol that allows pods in the given namespace to communicate with the api-server, make sure to amend the ip's and the port if necessary.
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: allow-api-server
  namespace: xxx
spec:
  egress:
  - ports:
    - port: 8443
      protocol: tcp
    to:
    - ipblock:
        cidr: 172.24.4.2/32
    - ipblock:
        cidr: 172.24.4.3/32
    - ipblock:
        cidr: 172.24.4.4/32
  podselector: {}
  policytypes:
  - egress

if your desired pod has a accessible shell with some tools installed you could try out the connection with curl:
bash-4.4# curl -vvv 172.24.4.2:8443
* rebuilt url to: 172.24.4.2:8443/
*   trying 172.24.4.2...
* tcp_nodelay set
* connected to 172.24.4.2 (172.24.4.2) port 8443 (#0)
&gt; get / http/1.1
&gt; host: 172.24.4.2:8443
&gt; user-agent: curl/7.61.1
&gt; accept: */*

"
73328578,any docs on what rights need to be given to do a thing on kubernetes?,"here my first serviceaccount, clusterrole, and clusterrolebinding
---
# create namespace
apiversion: v1
kind: namespace
metadata:
  name: devops-tools
---
# create service account
apiversion: v1
kind: serviceaccount
metadata:
  namespace: devops-tools
  name: bino

---
# set secrets for sa
# k8s &gt;= 1.24 need to manualy created
# https://stackoverflow.com/a/72258300
apiversion: v1
kind: secret
metadata:
  name: bino-token
  namespace: devops-tools
  annotations:
    kubernetes.io/service-account.name: bino
type: kubernetes.io/service-account-token

---
# create cluster role
# beware !!! this is cluster wide full rights
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  name: devops-tools-role
  namespace: devops-tools
rules:
  - apigroups:
        - &quot;&quot;
        - apps
        - autoscaling
        - batch
        - extensions
        - policy
        - networking.k8s.io
        - rbac.authorization.k8s.io
    resources:
      - pods
      - componentstatuses
      - configmaps
      - daemonsets
      - deployments
      - events
      - endpoints
      - horizontalpodautoscalers
      - ingress
      - jobs
      - limitranges
      - namespaces
      - nodes
      - pods
      - persistentvolumes
      - persistentvolumeclaims
      - resourcequotas
      - replicasets
      - replicationcontrollers
      - serviceaccounts
      - services
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]
---
# bind the sa to cluster role
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: devops-tools-role-binding
subjects:
- namespace: devops-tools
  kind: serviceaccount
  name: bino
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: devops-tools-role
---

it work when i use to create namespace, deployment, and service.
but it fail (complain about 'have no right') when i try to create kind: ingress.
then i try to add
---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: devops-tools-role-binding-admin
subjects:
- namespace: devops-tools
  kind: serviceaccount
  name: bino
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: cluster-admin

and now 'bino' can do all things.
my question is: is there any docs on what 'apigroups' and 'resources' need to be assigned so one service account can do some-things (not all-things)?
sincerely
-bino-
",<kubernetes><kubernetes-rbac>,73328748,2,"you can run this command to determine the apigroup of a resource:
kubectl api-resources

you will see something like:
name        shortnames    apiversion              namespaced   kind
ingresses   ing           networking.k8s.io/v1    true         ingress

so you would need to add this to the rules of your clusterrole:
- apigroups:
  - &quot;networking.k8s.io/v1&quot;
  resources:
  - &quot;ingresses&quot;
  verbs:
  - &quot;get&quot;

"
76188366,eks fargate pods unreachable from internet,"i am trying to create eks fargate cluster and deploy example spring boot application with 1 endpoint, i successfully create stack with following cloudformation script:
---
awstemplateformatversion: '2010-09-09'
description: 'aws cloudformation template for eks fargate managed kubernetes cluster with exposed endpoints'

resources:
  vpc:
    type: aws::ec2::vpc
    properties:
      cidrblock: 10.0.0.0/16
      enablednssupport: true
      enablednshostnames: true

  internetgateway:
    type: aws::ec2::internetgateway

  vpcgatewayattachment:
    type: aws::ec2::vpcgatewayattachment
    properties:
      vpcid: !ref vpc
      internetgatewayid: !ref internetgateway

  publicsubnet:
    type: aws::ec2::subnet
    properties:
      vpcid: !ref vpc
      cidrblock: 10.0.2.0/24
      mappubliciponlaunch: true
      availabilityzone: !select [ 0, !getazs '' ]

  privatesubneta:
    type: aws::ec2::subnet
    properties:
      vpcid: !ref vpc
      cidrblock: 10.0.0.0/24
      availabilityzone: !select [ 0, !getazs '' ]

  privatesubnetb:
    type: aws::ec2::subnet
    properties:
      vpcid: !ref vpc
      cidrblock: 10.0.1.0/24
      availabilityzone: !select [ 1, !getazs '' ]

  publicroutetable:
    type: aws::ec2::routetable
    properties:
      vpcid: !ref vpc

  publicroute:
    type: aws::ec2::route
    properties:
      routetableid: !ref publicroutetable
      destinationcidrblock: 0.0.0.0/0
      gatewayid: !ref internetgateway

  subnetroutetableassociationa:
    type: aws::ec2::subnetroutetableassociation
    properties:
      subnetid: !ref publicsubnet
      routetableid: !ref publicroutetable

  eip:
    type: aws::ec2::eip

  natgateway:
    type: aws::ec2::natgateway
    properties:
      subnetid: !ref publicsubnet
      allocationid: !getatt eip.allocationid

  privateroutetable:
    type: aws::ec2::routetable
    properties:
      vpcid: !ref vpc

  privateroute:
    type: aws::ec2::route
    properties:
      routetableid: !ref privateroutetable
      destinationcidrblock: 0.0.0.0/0
      natgatewayid: !ref natgateway

  privatesubnetroutetableassociationa:
    type: aws::ec2::subnetroutetableassociation
    properties:
      subnetid: !ref privatesubneta
      routetableid: !ref privateroutetable

  privatesubnetroutetableassociationb:
    type: aws::ec2::subnetroutetableassociation
    properties:
      subnetid: !ref privatesubnetb
      routetableid: !ref privateroutetable

  ekscluster:
    type: aws::eks::cluster
    properties:
      name: eksfargatecluster
      version: '1.26'
      resourcesvpcconfig:
        subnetids:
          - !ref privatesubneta
          - !ref privatesubnetb
      rolearn: !getatt eksclusterrole.arn

  fargateprofile:
    type: aws::eks::fargateprofile
    properties:
      clustername: !ref ekscluster
      fargateprofilename: fargateprofile
      podexecutionrolearn: !getatt fargatepodexecutionrole.arn
      selectors:
        - namespace: default
      subnets:
        - !ref privatesubneta
        - !ref privatesubnetb

  fargateprofilecoredns:
    type: aws::eks::fargateprofile
    properties:
      clustername: !ref ekscluster
      fargateprofilename: corednsprofile
      podexecutionrolearn: !getatt fargatepodexecutionrole.arn
      selectors:
        - namespace: kube-system
          labels:
            - key: k8s-app
              value: kube-dns
      subnets:
        - !ref privatesubneta
        - !ref privatesubnetb

  fargatepodexecutionrole:
    type: aws::iam::role
    properties:
      assumerolepolicydocument:
        version: '2012-10-17'
        statement:
          - effect: allow
            principal:
              service:
                - eks-fargate-pods.amazonaws.com
            action:
              - sts:assumerole
      managedpolicyarns:
        - arn:aws:iam::aws:policy/amazoneksfargatepodexecutionrolepolicy

  eksclusterrole:
    type: aws::iam::role
    properties:
      assumerolepolicydocument:
        version: '2012-10-17'
        statement:
          - effect: allow
            principal:
              service:
                - eks.amazonaws.com
            action:
              - sts:assumerole
      managedpolicyarns:
        - arn:aws:iam::aws:policy/amazoneksclusterpolicy
        - arn:aws:iam::aws:policy/amazoneksvpcresourcecontroller

i run following command to path the coredns for fargate:
kubectl patch deployment coredns \
    -n kube-system \
    --type json \
    -p='[{&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/spec/template/metadata/annotations/eks.amazonaws.com~1compute-type&quot;}]'

then i deploy my example application image from public ecr with following kubernetes manifest:
---
apiversion: apps/v1
kind: deployment
metadata:
  name: example-app
spec:
  replicas: 2
  selector:
    matchlabels:
      app: example-app
  template:
    metadata:
      labels:
        app: example-app
    spec:
      containers:
        - name: ventu
          image: public.ecr.aws/not_real_url/public_ecr_name:latest
          ports:
            - containerport: 8080

---
apiversion: v1
kind: service
metadata:
  name: example-service
spec:
  type: loadbalancer
  selector:
    app: example-app
  ports:
    - protocol: tcp
      port: 80
      targetport: 8080

then when i run:
kubectl get svc

i see result:
name              type           cluster-ip      external-ip                                                                  port(s)        age
example-service   loadbalancer   172.20.228.77   aa0116829ac2647a7bf39a97bffb0183-1208408433.eu-central-1.elb.amazonaws.com   80:31915/tcp   16m
kubernetes        clusterip      172.20.0.1      &lt;none&gt;                                                                       443/tcp        29m

however when i try to reach the external-ip on my loadbalancer example-service, i get empty response, i can't reach my application on only path defined in my spring boot application: /api/v1/info
server.port=8080
server.servlet.context-path=/api/v1

what am i missing?
couple of information:

my pods spin up successfully, i can see spring boot logging when i run kubectl logs pod-name
my coredns pods spin up correctly as well
i use busybox to test my cluster's dns, and everything seems to be working too

",<amazon-web-services><kubernetes><aws-cloudformation><amazon-eks><aws-fargate>,76247614,2,"i solved my issue, by following this guide
i then exported resulting stack into my cloudformation script.
then to deploy my application i updated my kubernetes manifest to:
---
apiversion: v1
kind: namespace
metadata:
  name: example
---
apiversion: apps/v1
kind: deployment
metadata:
  namespace: example
  name: deployment-example-be-app
spec:
  selector:
    matchlabels:
      app.kubernetes.io/name: example-be-app
  replicas: 2
  template:
    metadata:
      labels:
        app.kubernetes.io/name: example-be-app
    spec:
      containers:
        - name: example-be-app
          image: public.ecr.aws/fake_url/example:latest
          imagepullpolicy: always
          ports:
            - containerport: 8080
---
apiversion: v1
kind: service
metadata:
  namespace: example
  name: service-example-be-app
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
    service.beta.kubernetes.io/aws-load-balancer-type: external
    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
spec:
  type: loadbalancer
  ports:
    - port: 80
      targetport: 8080
      protocol: tcp
  selector:
    app.kubernetes.io/name: example-be-app

now i access my example application form browser.
"
62997054,use istio virtual service to expose 3 routes,"we switch now to istio and i need to expose my app to outside
in the app, i've only 3 routes


&quot;/&quot; root route
&quot;/login&quot;
&quot;static&quot;  - my app should serve some static files


we have gw and host but somehow i cannot access my app, any idea what am i doing wrong here?
vs-yaml
is there a way to expose all the routes, or should i define them explicitly, if so how as it a bit confusing with routes and match?
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: bher-virtualservice
  namespace: ba-trail
spec:
  gateways:
    - webb-system.svc.cluster.local
  hosts:
    - trialio.cloud.str
  http:
    - route:
        - destination:
            host: bsa
            port:
              number: 5000

",<azure><kubernetes><kubernetes-ingress><istio>,63007449,2,"
if so how as it a bit confusing with routes and match

i would suggest to take a look at istio documentation about virtual services, it's well described there.

let's start from the beginning, you have virtual service and gateway, they should be in the same namespace as your application, or you need to specify that in both of them.
as far as i can see your virtual service is incorrect, i have prepared example which should work for you. take a look at below example.
gateway
apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: bher-gateway
  namespace: ba-trail 
spec:
  selector:
    istio: ingressgateway # use the default ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: http
    hosts:
    - &quot;trialio.cloud.str&quot;

i see you have gateway which is already deployed, if its not in the same namespace as virtual service, you should add it like in below example.
check the spec.gateways section
apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
name: my-gateway
namespace: some-config-namespace


apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: bookinfo-mongo
  namespace: bookinfo-namespace
spec:
  gateways:
  - some-config-namespace/my-gateway # can omit the namespace if gateway is in same
                                       namespace as virtual service.


virtual service
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: bher-virtualservice
  namespace: ba-trail 
spec:
  gateways:
    - bher-gateway   // name of your gateway 
  hosts:
    - trialio.cloud.str
  http:
    - match:
      - uri:
          prefix: &quot;/&quot;
      - uri:
          prefix: &quot;/login&quot;
      - uri:
          prefix: &quot;/static&quot;
      - uri:
          regex: '^.*\.(ico|png|jpg)$'
      route:
      - destination:
          host: bsa.ba-trail.svc.cluster.local   // name_of_your service.namespace.svc.cluster.local
          port:
            number: 5000

take a look at this example

lets break down the requests that should be routed to frontend:
exact path / should be routed to frontend to get the index.html
prefix path /static/* should be routed to frontend to get any static files needed by the frontend, like cascading style sheets and javascript files.
paths matching the regex ^.*.(ico|png|jpg)$ should be routed to frontend as it is an image, that the page needs to show.

http:
  - match:
    - uri:
        exact: /
    - uri:
        exact: /callback
    - uri:
        prefix: /static
    - uri:
        regex: '^.*\.(ico|png|jpg)$'
    route:
    - destination:
        host: frontend             
        port:
          number: 80


hope you find this useful. if you have any questions let me know in the comments.
"
62972461,nginx redirection of jenkins on kubernetes,"i am trying to deploy jenkins on kubernetes. i have deployed it with clusterip along with nginx ingress controller on aks.
when i access the ip of the ingress-controller, the jenkins login url (http://externalip/login?from=%2f) comes up. however the ui of the jenkins page isn't coming up and there is a some sort of redirection happening and keeps growing (http://externalip/login?from=%2f%3ffrom%3d%252f%253ffrom%253d%25252f%25253f). i am very new to ingress controller and annotations. i am not able to figure on what's causing this redirection.
below are my configuration files. can anyone please help on what's going wrong ?
clusterip-service.yml
kind: service
apiversion: v1
metadata:
  name: jenkins-nodeport-svc
  namespace: jenkins
  labels:
    env: poc
    app: myapp_jenkins
spec:
  ports:
  - name: &quot;http&quot;
    port: 80
    targetport: 8080
  type: clusterip
  selector:
      app: myapp_jenkins

ingress.yml
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: jenkins-ingress
  namespace: jenkins
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/backend-protocol: &quot;http&quot;
    nginx.ingress.kubernetes.io/cors-allow-credentials: &quot;true&quot;
    nginx.ingress.kubernetes.io/cors-allow-headers: authorization, origin, accept
    nginx.ingress.kubernetes.io/cors-allow-methods: get, options
    nginx.ingress.kubernetes.io/enable-cors: &quot;true&quot;

spec:
  rules:
  - http:
      paths:
       - backend:
          servicename: jenkins-nodeport-svc
          serviceport: 80
         path: /(.*)

",<jenkins><kubernetes><kubernetes-ingress><azure-aks>,62973962,2,"there's something in your ingress:
path: /(.*)

is a regular expression with a single capturing group that match everything. for example with following url: http://externalip/login?from=myurl your capturing group $1 (the first and only one) would match login?from/myurl.
now the problem is that nginx.ingress.kubernetes.io/rewrite-target: /$2 annotation is rewriting your url with a non existing capturing group.
you don't need rewriting, you just need to plain forward every request to the service.
here you can find rewrite examples if you interested on it.
but in your case you can set:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: jenkins-ingress
  namespace: jenkins
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
    nginx.ingress.kubernetes.io/backend-protocol: &quot;http&quot;
    nginx.ingress.kubernetes.io/cors-allow-credentials: &quot;true&quot;
    nginx.ingress.kubernetes.io/cors-allow-headers: authorization, origin, accept
    nginx.ingress.kubernetes.io/cors-allow-methods: get, options
    nginx.ingress.kubernetes.io/enable-cors: &quot;true&quot;

spec:
  rules:
  - http:
      paths:
       - backend:
          servicename: jenkins-nodeport-svc
          serviceport: 80
         path: /

and you're good to go.
"
63030381,get access to variables from the redis container,"i create secret redis-secret with following command.
kubectl create secret generic redis-secret --from-literal=password=0123456

after that i create pod secrets-via-file, using the redis image, which mounts secret name redis-secret at /secrets.
kubectl run secret-via-file --image=redis --dry-run=client -o yaml &gt; pod.yaml

i edited the create pod.yaml file.
apiversion: v1
    kind: pod
metadata:
  labels:
    run: secret-via-file
  name: secret-via-file
spec:
  containers:
  - image: redis
    name: secret-via-file
    volumemounts:
    - name: redis-secret
      mountpath: /secrets
  volumes:
  - name: redis-secret
    secret:
      secretname: redis-secret

i created second pod name secret-via-env, using the redis image, which exports password as password.
kubectl run secret-via-env --image=redis --dry-run=client -o yaml &gt; pod2.yaml

i editedthe pod2.yaml file.
apiversion: v1
kind: pod
metadata:
  labels:
    run: secrets-via-env
  name: secrets-via-env
spec:
  containers:
  - image: redis
    name: secrets-via-env
    env:
      - name: password
        valuefrom:
          secretkeyref:
            name: redis-secret
            key: password

i connect to the pod secrets-via-env with following command.
kubectl exec -it secret-via-file -- redis-cli

i try to verify if the secret is mounted to the pods. in the second pod i want to use the variable password to retrieve the assigned value(0123456).i used the command below but it does not work.
secret get password 

",<kubernetes><redis><kubernetes-pod>,63031172,2,"tried as below. i see the password secret is getting listed as env inside pod
# create secret
kubectl create secret generic redis-secret --from-literal=password=0123456

# create pod
apiversion: v1
kind: pod
metadata:
  labels:
    run: secrets-via-env
  name: secrets-via-env
spec:
  containers:
  - image: redis
    name: secrets-via-env
    env:
    - name: password
      valuefrom:
        secretkeyref:
          name: redis-secret
          key: password

# check password secret
master $ kubectl exec -it secrets-via-env sh
kubectl exec [pod] [command] is deprecated and will be removed in a future version. use kubectl kubectl exec [pod] -- [command] instead.
# echo $password
0123456

# from first pod
---
apiversion: v1
kind: pod
metadata:
  labels:
    run: secret-via-file
  name: secret-via-file
spec:
  containers:
  - image: redis
    name: secret-via-file
    volumemounts:
    - name: redis-secret
      mountpath: /secrets
  volumes:
  - name: redis-secret
    secret:
      secretname: redis-secret

controlplane $ kubectl exec -it secret-via-file sh
kubectl exec [pod] [command] is deprecated and will be removed in a future version. use kubectl kubectl exec [pod] -- [command] instead.
# ls -l /secrets
total 0
lrwxrwxrwx 1 root root 15 jul 22 09:45 password -&gt; ..data/password
# cat /secrets/password
0123456#

"
62455233,how do i connect a helm package to a persistentstorage volume?,"i've seen this question come up often, and i've yet to find a clean, generic solution. i'm just learning kubernetes so maybe there's something basic i'm missing. but here's what i've done:


install docker-desktop with kubernetes
manually create a persistent-storage volume using a yaml file (shown below)
helm install redis dandydev/redis-ha


or you can use any other helm chart, be it elasticsearch, postgres, you name it. i always get pod has unbound immediate persistentvolumeclaims.

also when i run: kubectl get storageclasses.storage.k8s.io
i do have (default) storage:

name                 provisioner          age
hostpath (default)   docker.io/hostpath   3h8m


can anyone please help me fix this issue in a generic way? so that i can actually install helm charts and have them automatically connect to a persistent storage volume?

my volume.yaml:

kind: persistentvolume
apiversion: v1
metadata:
  name: redis-volume
  labels:
    type: local
    app: redis
spec:
  storageclassname: """"
  capacity:
    storage: 5gi
  accessmodes:
    - readwritemany
  hostpath:
    path: ""/mnt/redis""

",<kubernetes><redis><kubernetes-helm><persistent-storage><docker-desktop>,62456388,2,"ok so i looked more online among the various custom solutions, and one did work:
https://github.com/helm/charts/issues/12521#issuecomment-477834805
in addition this answer provides more details into how to enable dynamic provisioning locally:
pod has unbound persistentvolumeclaims
basically (in addition to having the volume created above) i need to manually:

create a storage class, via storage-class.yaml
add that storage class to helm in 'values.yaml'

# storage-class.yaml
apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  name: data-storage
provisioner: kubernetes.io/no-provisioner
volumebindingmode: waitforfirstconsumer

in addition, some charts running locally need you to customize their config, under &lt;your-helm&gt;/charts/&lt;chart-name&gt;/&lt;file-to-config.yaml&gt;, or via --set &lt;var&gt;=valuesince, most helm charts want you to use more nodes, and running locally you might only have a single node.
another option is to use helm install --set replicas=1 ... and some charts will work well with this.
hope this helps someone out there.
"
62466505,how to block traffic to db pod & service (dns) from other namespaces in kubernates?,"i have created 2 tenants(tenant1,tenant2) in 2 namespaces tenant1-namespace,tenant2-namespace

each tenant has db pod and its services

how to isolate db pods/service i.e. how to restrict pod/service from his namespace to access other tenants db pods ?

i have used service account for each tenant and applied network policies so that namespaces are isolated. 

kubectl get svc --all-namespaces

tenant1-namespace   grafana-app            loadbalancer   10.64.7.233    104.x.x.x   3000:31271/tcp   92m
tenant1-namespace   postgres-app           nodeport       10.64.2.80     &lt;none&gt;      5432:31679/tcp   92m
tenant2-namespace   grafana-app            loadbalancer   10.64.14.38    35.x.x.x    3000:32226/tcp   92m
tenant2-namespace   postgres-app           nodeport       10.64.2.143    &lt;none&gt;      5432:31912/tcp   92m


so

i want to restrict grafana-app to use only his postgres db in his namespace only, not in other namespace.

but problem is that using dns qualified service name (app-name.namespace-name.svc.cluster.local) 
its allowing to access each other db pods (grafana-app in namespace tenant1-namespace can have access to postgres db in other tenant2-namespace via postgres-app.tenant2-namespace.svc.cluster.local

updates : network policies

1) 

kind: networkpolicy
apiversion: networking.k8s.io/v1
metadata:
  name: deny-from-other-namespaces
spec:
  podselector:
    matchlabels:
  ingress:
  - from:
    - podselector: {}


2) 

kind: networkpolicy
apiversion: networking.k8s.io/v1
metadata:
  name: web-allow-external
spec:
  podselector:
    matchlabels:
      app: grafana-app
  ingress:
  - from: []

",<postgresql><kubernetes><google-kubernetes-engine><kubernetes-helm><kubernetes-ingress>,62475892,2,"
your networkpolicy objects are correct, i created an example with them and will demonstrate bellow.
if you still have access to the service on the other namespace using fqdn, your networkpolicy may not be fully enabled on your cluster.


run gcloud container clusters describe ""cluster_name"" --zone ""zone"" and look for these two snippets:


at the beggining of the description it shows if the networkpolicy plugin is enabled at master level, it should be like this:


addonsconfig:
networkpolicyconfig: {}



at the middle of the description, you can find if the networkpolicy is enabled on the nodes. it should look like this:


name: cluster-1
network: default
networkconfig:
  network: projects/myproject/global/networks/default
  subnetwork: projects/myproject/regions/us-central1/subnetworks/default
networkpolicy:
  enabled: true
  provider: calico



if any of the above is different, check here: how to enable network policy in gke




reproduction:


i'll create a simple example, i'll use gcr.io/google-samples/hello-app:1.0 image for tenant1 and gcr.io/google-samples/hello-app:2.0 for tenant2, so it's simplier to see where it's connecting but i'll use the names of your environment:


$ kubectl create namespace tenant1
namespace/tenant1 created
$ kubectl create namespace tenant2
namespace/tenant2 created

$ kubectl run -n tenant1 grafana-app --generator=run-pod/v1 --image=gcr.io/google-samples/hello-app:1.0 
pod/grafana-app created
$ kubectl run -n tenant1 postgres-app --generator=run-pod/v1 --image=gcr.io/google-samples/hello-app:1.0 
pod/postgres-app created

$ kubectl run -n tenant2 grafana-app --generator=run-pod/v1 --image=gcr.io/google-samples/hello-app:2.0 
pod/grafana-app created
$ kubectl run -n tenant2 postgres-app --generator=run-pod/v1 --image=gcr.io/google-samples/hello-app:2.0 
pod/postgres-app created

$ kubectl expose pod -n tenant1 grafana-app --port=8080 --type=loadbalancer
service/grafana-app exposed
$ kubectl expose pod -n tenant1 postgres-app --port=8080 --type=nodeport
service/postgres-app exposed

$ kubectl expose pod -n tenant2 grafana-app --port=8080 --type=loadbalancer
service/grafana-app exposed
$ kubectl expose pod -n tenant2 postgres-app --port=8080 --type=nodeport
service/postgres-app exposed

$ kubectl get all -o wide -n tenant1
name               ready   status    restarts   age    ip          node                                         
pod/grafana-app    1/1     running   0          100m   10.48.2.4   gke-cluster-114-default-pool-e5df7e35-ez7s
pod/postgres-app   1/1     running   0          100m   10.48.0.6   gke-cluster-114-default-pool-e5df7e35-c68o

name                   type           cluster-ip   external-ip     port(s)          age   selector
service/grafana-app    loadbalancer   10.1.23.39   34.72.118.149   8080:31604/tcp   77m   run=grafana-app
service/postgres-app   nodeport       10.1.20.92   &lt;none&gt;          8080:31033/tcp   77m   run=postgres-app

$ kubectl get all -o wide -n tenant2
name               ready   status    restarts   age    ip          node                                         
pod/grafana-app    1/1     running   0          76m    10.48.4.8   gke-cluster-114-default-pool-e5df7e35-ol8n
pod/postgres-app   1/1     running   0          100m   10.48.4.5   gke-cluster-114-default-pool-e5df7e35-ol8n

name                   type           cluster-ip    external-ip      port(s)          age   selector
service/grafana-app    loadbalancer   10.1.17.50    104.154.135.69   8080:30534/tcp   76m   run=grafana-app
service/postgres-app   nodeport       10.1.29.215   &lt;none&gt;           8080:31667/tcp   77m   run=postgres-app



now, let's deploy your two rules:  the first blocking all traffic from outside the namespace, the second allowing ingress the grafana-app from outside of the namespace:


$ cat default-deny-other-ns.yaml 
kind: networkpolicy
apiversion: networking.k8s.io/v1
metadata:
  name: deny-from-other-namespaces
spec:
  podselector:
    matchlabels:
  ingress:
  - from:
    - podselector: {}

$ cat allow-grafana-ingress.yaml 
kind: networkpolicy
apiversion: networking.k8s.io/v1
metadata:
  name: web-allow-external
spec:
  podselector:
    matchlabels:
      run: grafana-app
  ingress:
  - from: []



let's review the rules for network policy isolation:



  by default, pods are non-isolated; they accept traffic from any source.
  
  pods become isolated by having a networkpolicy that selects them. once there is any networkpolicy in a namespace selecting a particular pod, that pod will reject any connections that are not allowed by any networkpolicy. (other pods in the namespace that are not selected by any networkpolicy will continue to accept all traffic.)
  
  network policies do not conflict; they are additive. if any policy or policies select a pod, the pod is restricted to what is allowed by the union of those policies' ingress/egress rules. thus, order of evaluation does not affect the policy result.



then we will apply the rules on both namespaces because the scope of the rule is the namespace it's assigned to:


$ kubectl apply -n tenant1 -f default-deny-other-ns.yaml 
networkpolicy.networking.k8s.io/deny-from-other-namespaces created
$ kubectl apply -n tenant2 -f default-deny-other-ns.yaml 
networkpolicy.networking.k8s.io/deny-from-other-namespaces created

$ kubectl apply -n tenant1 -f allow-grafana-ingress.yaml 
networkpolicy.networking.k8s.io/web-allow-external created
$ kubectl apply -n tenant2 -f allow-grafana-ingress.yaml 
networkpolicy.networking.k8s.io/web-allow-external created



now for final testing, i'll log inside grafana-app in tenant1 and try to reach the postgres-app in both namespaces and check the output:


$ kubectl exec -n tenant1 -it grafana-app -- /bin/sh
/ ### postgres same namespace ###
/ # wget -o- postgres-app:8080
connecting to postgres-app:8080 (10.1.20.92:8080)
hello, world!
version: 1.0.0
hostname: postgres-app

/ ### grafana other namespace ###
/ # wget -o- --timeout=1 http://grafana-app.tenant2.svc.cluster.local:8080
connecting to grafana-app.tenant2.svc.cluster.local:8080 (10.1.17.50:8080)
hello, world!
version: 2.0.0
hostname: grafana-app

/ ### postgres other namespace ###
/ # wget -o- --timeout=1 http://postgres-app.tenant2.svc.cluster.local:8080
connecting to postgres-app.tenant2.svc.cluster.local:8080 (10.1.29.215:8080)
wget: download timed out



you can see that the dns is resolved, but the networkpolicy blocks the access to the backend pods.


if after double checking networkpolicy is enabled on master and nodes you still face the same issue let me know in the comments and we can dig further.
"
75554647,crashloopbackoff error while deploying postgresql to gke,"i am trying to deploy postgresql to gke and here is my persistentvolumeclaim definition:
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: postgres-pvc
  namespace: db
  labels:
    app: imgress-db
spec:
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 400mi

and this is deployment/service definition:
---
apiversion: apps/v1
kind: statefulset
metadata:
  name: imgress-db
  namespace: db
spec:
  servicename: imgress-db
  replicas: 1
  selector:
    matchlabels:
      app: imgress-db
  template:
    metadata:
      labels:
        app: imgress-db
    spec:
      containers:
        - name: imgress-db
          image: postgres
          env:
            - name: postgres_host
              valuefrom:
                configmapkeyref:
                  name: db-configmap
                  key: database_host
            - name: postgres_db
              valuefrom:
                configmapkeyref:
                  name: db-configmap
                  key: postgres_db
            - name: postgres_user
              valuefrom:
                configmapkeyref:
                  name: db-configmap
                  key: postgres_user
            - name: postgres_password
              valuefrom:
                secretkeyref:
                  name: db-secret
                  key: postgres_password
          ports:
            - containerport: 5432
          volumemounts:
            - name: postgres-data
              mountpath: /var/lib/postgresql/data
      volumes:
        - name: postgres-data
          persistentvolumeclaim:
            claimname: postgres-pvc
      restartpolicy: always
---
apiversion: v1
kind: service
metadata:
  name: imgress-db
  namespace: db
spec:
  selector:
    app: imgress-db
  ports:
    - name: postgres
      port: 5432

first i run:
kubectl apply -f postgres-pvc.yaml

and then:
kubectl apply -f postgres-deployment.yaml

but i get this notorious error when i run kubectl get pods -a:
namespace   name           ready   status             restarts        age
db          imgress-db-0   0/1     crashloopbackoff   6 (2m15s ago)   8m26s

for kubectl describe pvc postgres-pvc -n db i get this result:
name:          postgres-pvc
namespace:     db
storageclass:  standard
status:        bound
volume:        pvc-c6369764-1106-4a7d-887c-0e4009968115
labels:        app=imgress-db
annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
               volume.beta.kubernetes.io/storage-provisioner: pd.csi.storage.gke.io
               volume.kubernetes.io/storage-provisioner: pd.csi.storage.gke.io
finalizers:    [kubernetes.io/pvc-protection]
capacity:      1gi
access modes:  rwo
volumemode:    filesystem
used by:       imgress-db-0
events:
  type    reason                 age   from                                                                                              message
  ----    ------                 ----  ----                                                                                              -------
  normal  externalprovisioning   31m   persistentvolume-controller                                                                       waiting for a volume to be created, either by external provisioner &quot;pd.csi.storage.gke.io&quot; or manually created by system administrator
  normal  provisioning           31m   pd.csi.storage.gke.io_gke-e0f710dc594c4eb5ac14-5c62-e039-vm_ca2409ad-83a8-4139-93b4-4fffbacbf44f  external provisioner is provisioning volume for claim &quot;db/postgres-pvc&quot;
  normal  provisioningsucceeded  31m   pd.csi.storage.gke.io_gke-e0f710dc594c4eb5ac14-5c62-e039-vm_ca2409ad-83a8-4139-93b4-4fffbacbf44f  successfully provisioned volume pvc-c6369764-1106-4a7d-887c-0e4009968115

and for kubectl describe pod imgress-db-0 -n db i get this result (please pay attention to back-off restarting failed container on the last line):
name:             imgress-db-0
namespace:        db
priority:         0
service account:  default
node:             gke-imgress-default-pool-e9bdef38-hjhv/10.156.0.5
start time:       fri, 24 feb 2023 13:44:15 +0500
labels:           app=imgress-db
                  controller-revision-hash=imgress-db-7f557d4b88
                  statefulset.kubernetes.io/pod-name=imgress-db-0
annotations:      &lt;none&gt;
status:           running
ip:               10.84.2.49
ips:
  ip:           10.84.2.49
controlled by:  statefulset/imgress-db
containers:
  imgress-db:
    container id:   containerd://96140ec0b0e369ca97822361a770abcb82e27b7924bc90e17111ab354e51d6aa
    image:          postgres
    image id:       docker.io/library/postgres@sha256:901df890146ec46a5cab7a33f4ac84e81bac2fe92b2c9a14fd649502c4adf954
    port:           5432/tcp
    host port:      0/tcp
    state:          waiting
      reason:       crashloopbackoff
    last state:     terminated
      reason:       error
      exit code:    1
      started:      fri, 24 feb 2023 13:50:09 +0500
      finished:     fri, 24 feb 2023 13:50:11 +0500
    ready:          false
    restart count:  6
    environment:
      postgres_host:      &lt;set to the key 'database_host' of config map 'db-configmap'&gt;  optional: false
      postgres_db:        &lt;set to the key 'postgres_db' of config map 'db-configmap'&gt;    optional: false
      postgres_user:      &lt;set to the key 'postgres_user' of config map 'db-configmap'&gt;  optional: false
      postgres_password:  &lt;set to the key 'postgres_password' in secret 'db-secret'&gt;     optional: false
    mounts:
      /var/lib/postgresql/data from postgres-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tfsf9 (ro)
conditions:
  type              status
  initialized       true
  ready             false
  containersready   false
  podscheduled      true
volumes:
  postgres-data:
    type:       persistentvolumeclaim (a reference to a persistentvolumeclaim in the same namespace)
    claimname:  postgres-pvc
    readonly:   false
  kube-api-access-tfsf9:
    type:                    projected (a volume that contains injected data from multiple sources)
    tokenexpirationseconds:  3607
    configmapname:           kube-root-ca.crt
    configmapoptional:       &lt;nil&gt;
    downwardapi:             true
qos class:                   besteffort
node-selectors:              &lt;none&gt;
tolerations:                 node.kubernetes.io/not-ready:noexecute op=exists for 300s
                             node.kubernetes.io/unreachable:noexecute op=exists for 300s
events:
  type     reason                  age                    from                     message
  ----     ------                  ----                   ----                     -------
  normal   scheduled               6m51s                  default-scheduler        successfully assigned db/imgress-db-0 to gke-imgress-default-pool-e9bdef38-hjhv
  normal   successfulattachvolume  6m46s                  attachdetach-controller  attachvolume.attach succeeded for volume &quot;pvc-c6369764-1106-4a7d-887c-0e4009968115&quot;
  normal   pulled                  6m42s                  kubelet                  successfully pulled image &quot;postgres&quot; in 365.355391ms
  normal   pulled                  6m39s                  kubelet                  successfully pulled image &quot;postgres&quot; in 236.532115ms
  normal   pulled                  6m26s                  kubelet                  successfully pulled image &quot;postgres&quot; in 278.12995ms
  normal   created                 5m59s (x4 over 6m42s)  kubelet                  created container imgress-db
  normal   started                 5m59s (x4 over 6m41s)  kubelet                  started container imgress-db
  normal   pulled                  5m59s                  kubelet                  successfully pulled image &quot;postgres&quot; in 348.694043ms
  normal   pulling                 5m10s (x5 over 6m42s)  kubelet                  pulling image &quot;postgres&quot;
  normal   pulled                  5m9s                   kubelet                  successfully pulled image &quot;postgres&quot; in 288.848351ms
  warning  backoff                 95s (x25 over 6m38s)   kubelet                  back-off restarting failed container

result of kubectl logs imgress-db-0 -n db:
the files belonging to this database system will be owned by user &quot;postgres&quot;.
this user must also own the server process.

the database cluster will be initialized with locale &quot;en_us.utf8&quot;.
the default database encoding has accordingly been set to &quot;utf8&quot;.
the default text search configuration will be set to &quot;english&quot;.

data page checksums are disabled.

initdb: error: directory &quot;/var/lib/postgresql/data&quot; exists but is not empty
initdb: detail: it contains a lost+found directory, perhaps due to it being a mount point.
initdb: hint: using a mount point directly as the data directory is not recommended.
create a subdirectory under the mount point.

i've already seen some related questions asked on so but i guess it can be caused by different problems.
what i tried:

adding user and password as an env variables
deleting everything in the namespace and recreating
deleting livenessprobe and readinessprobe from the description

",<postgresql><kubernetes><google-cloud-platform><google-kubernetes-engine>,75555652,2,"i was able to solve the issue by adding two things to description:

pgdata env variable inside containers:
  env:
    - name: pgdata
      value: /var/lib/postgresql/data/pgdata


subpath: postgres under volumemounts


now my deployment file looks like this:
---
apiversion: apps/v1
kind: statefulset
metadata:
  name: imgress-db
  namespace: db
spec:
  servicename: imgress-db
  replicas: 1
  selector:
    matchlabels:
      app: imgress-db
  template:
    metadata:
      labels:
        app: imgress-db
    spec:
      containers:
        - name: imgress-db
          image: postgres
          env:
            - name: postgres_host
              valuefrom:
                configmapkeyref:
                  name: db-configmap
                  key: database_host
            - name: postgres_db
              valuefrom:
                configmapkeyref:
                  name: db-configmap
                  key: postgres_db
            - name: postgres_user
              valuefrom:
                configmapkeyref:
                  name: db-configmap
                  key: postgres_user
            - name: postgres_password
              valuefrom:
                secretkeyref:
                  name: db-secret
                  key: postgres_password
            - name: pgdata
              value: /var/lib/postgresql/data/pgdata
          ports:
            - containerport: 5432
          volumemounts:
            - name: postgres-data
              mountpath: /var/lib/postgresql/data
              subpath: postgres
      volumes:
        - name: postgres-data
          persistentvolumeclaim:
            claimname: postgres-pvc
      restartpolicy: always
---
apiversion: v1
kind: service
metadata:
  name: imgress-db
  namespace: db
spec:
  selector:
    app: imgress-db
  ports:
    - name: postgres
      port: 5432

"
74228996,how to use kubernetes secret to pull a private docker image from docker hub?,"i'm trying to run my kuberntes app using minikube on ubuntu20.04 and applied a secret to pull a private docker image from docker hub, but it doesn't seem to work correctly.

failed to pull image &quot;xxx/node-graphql:latest&quot;: rpc error: code
= unknown desc = error response from daemon: pull access denied for xxx/node-graphql, repository does not exist or may require
'docker login': denied: requested access to the resource is denied

here's the secret generated by
kubectl create secret generic regcred \                     
    --from-file=.dockerconfigjson=&lt;pathtofile&gt;.docker/config.json \
    --type=kubernetes.io/dockerconfigjson

and here's the secret yaml file i have created
apiversion: v1
data:
  .dockerconfigjson: xxx9trxpnakzcstbbaffrpt0icgkjfqojfqp9
kind: secret
metadata:
  name: node-graphql-secret
  uid: xxx-2e18-44eb-9719-xxx
type: kubernetes.io/dockerconfigjson

did anyone try to pull a private docker image into kubernetes using a secret? any kind of help would be appreciated. thank you!
",<kubernetes><kubectl><minikube><dockerhub><kubernetes-secrets>,74229322,2,"i managed to add the secrets config in the following steps.
first, you need to login to docker hub using:
docker login

next, you create a k8s secret running:
kubectl create secret generic &lt;your-secret-name&gt;\\n    --from-file=.dockerconfigjson=&lt;pathtoyourdockerconfigfile&gt;.docker/config.json \\n    --type=kubernetes.io/dockerconfigjson

and then get the secret in yaml format
kubectl get secret -o yaml

it should look like this:
apiversion: v1
items:
- apiversion: v1
  data:
    .dockerconfigjson: xxxewojimf1dghzijogewojcsjodhrwczovl2luzgv4lmrvy2tl
  kind: secret
  metadata:
    creationtimestamp: &quot;2022-10-27t23:06:01z&quot;
    name: &lt;your-secret-name&gt;
    namespace: default
    resourceversion: &quot;513&quot;
    uid: xxxx-0f12-4beb-be41-xxx
  type: kubernetes.io/dockerconfigjson
kind: list
metadata:
  resourceversion: &quot;&quot;

and i have copied the content for the secret in the secret yaml file:
apiversion: v1
data:
  .dockerconfigjson: xxxewojimf1dghzijogewojcsjodhrwczovl2luzgv4lmrvy2tlci
kind: secret
metadata:
  creationtimestamp: &quot;2022-10-27t23:06:01z&quot;
  name: &lt;your-secret-name&gt;
  namespace: default
  resourceversion: &quot;513&quot;
  uid: xxx-0f12-4beb-be41-xxx
type: kubernetes.io/dockerconfigjson

it works! this is a simple approach to using secret to pull a private docker image for k8s.
as a side note, to apply the secret, run kubectl apply -f secret.yml
hope it helps
"
64542199,issue in deploying a pod on master node parsing issue,"i am testing to deploy two pods one on master and on one worker node for learning purpose.
i have the following yaml file to deploy a pod on a cluster on master node.
apiversion: v1 
kind: pod
metadata:
name: edge-server
labels: 
    app: edge-server
spec:
 containers:
  - name:  tensor-keras
    image: tensor-keras:latest
    command: [&quot;sleep&quot;]
    args: [&quot;infinity&quot;]
    imagepullpolicy: never
    ports:
    - containerport: 31700
   nodeselector:
       node_type: &quot;kmaster&quot; #also tried without quotes 

i have labelled the master node as node_type kmaster, i am retrieving the below error
&quot;error parsing line 15 didnot find expected key&quot;
thank you very much for your help
",<docker><kubernetes><kubernetes-pod>,64542439,2,"it may not like your indentation. it is kind of out of wack. you have multiple levels of indentation in one document. i see single, double, and 4-space indentation. yaml indentation is very important otherwise the wrong json is generated. try this:
apiversion: v1 
kind: pod
metadata:
  name: edge-server
  labels: 
    app: edge-server
spec:
  containers:
    - name:  tensor-keras
      image: tensor-keras:latest
      command: [&quot;sleep&quot;]
      args: [&quot;infinity&quot;]
      imagepullpolicy: never
      ports:
      - containerport: 31700
  nodeselector:
    node_type: kmaster

"
73996233,kubernetes ingress same with with master-slave architecture,"i am trying to create a service which follows vertical replication-

in this architecture, requestes goes to the master node. for that i can use a kubernetes ingress.
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: example-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: / 
        backend:
          servicename: master-node
          serviceport: http

now my requirement is if master is down, then request should go to the slave node.
i can achieve that by creating three paths /master, /slave-1, /slave-2. but the constraint is that the path of request must remain same. so, the path must always be /
how can i create a ingress in such a way that if master-node is down, then all requests should forward to the slave-1-node?
i want to achieve something like below-
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: example-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: / 
        priority: 1
        backend:
          servicename: master-node
          serviceport: http
  - host: example.com
    http:
      paths:
      - path: / 
        priority: 2
        backend:
          servicename: slave-1-node
          serviceport: http
  - host: example.com
    http:
      paths:
      - path: / 
        priority: 3
        backend:
          servicename: slave-2-node
          serviceport: http

",<kubernetes><kubernetes-ingress><nginx-ingress><distributed-system><ingress-controller>,73997339,2,"i'm not sure how to do this using just an ingress resource, but it would be very easy if you were to deploy an haproxy pod in front of your services, so that your architecture looks like this:

using an haproxy configuration like this, you would get the behavior you want:
global
    log         stdout format raw local0
    maxconn     4000
    user        haproxy
    group       haproxy

defaults
    mode    http
    log global
    option  httplog
    option  dontlognull
    option  http-server-close
    option  forwardfor  except 127.0.0.0/8
    option  redispatch
    retries 3
    timeout connect     10s
    timeout client      1m
    timeout server      1m

frontend  example_fe
    bind 0.0.0.0:8080
    default_backend example_be

backend example_be
    option httpchk get /healthz

    server alpha example-alpha:80 check
    server beta example-beta:80 check backup
    server gamma example-gamma:80 check backup

this will send all requests to alpha as long it is running. if alpha is offline, requests will go to beta, and if beta is not running, requests will go to gamma. i found this article useful when looking for information about how to set this up.
you create a deployment that runs haproxy:
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: haproxy
  name: haproxy
spec:
  replicas: 1
  selector:
    matchlabels:
      app: haproxy
  template:
    metadata:
      labels:
        app: haproxy
    spec:
      containers:
      - image: docker.io/haproxy:latest
        name: haproxy
        ports:
        - containerport: 8080
          name: http
        volumemounts:
        - mountpath: /usr/local/etc/haproxy
          name: haproxy-config
      volumes:
      - configmap:
          name: haproxy-config-ddc898c5f5
        name: haproxy-config


a service pointing at that deployment:
apiversion: v1
kind: service
metadata:
  labels:
    app: haproxy
  name: haproxy
spec:
  ports:
  - name: http
    port: 80
    targetport: http
  selector:
    app: haproxy

and then point the ingress at that service:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: example
spec:
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          service:
            name: haproxy
            port:
              name: http
        path: /
        pathtype: prefix


i've put together a complete configuration here if you want to try this out.
"
64369784,google stackdriver - how can i use my kubernetes yaml labels for stackdriver log query?,"when using google stackdriver i can use the log query to find the exact log statements i am looking for.
this might look like this:
resource.type=&quot;k8s_container&quot;
resource.labels.project_id=&quot;my-project&quot;
resource.labels.location=&quot;europe-west3-a&quot;
resource.labels.cluster_name=&quot;my-cluster&quot;
resource.labels.namespace_name=&quot;dev&quot;
resource.labels.pod_name=&quot;my-app-pod-7f6cf95b6c-nkkbm&quot;
resource.labels.container_name=&quot;container&quot;

however as you can see in this query argument resource.labels.pod_name=&quot;my-app-pod-7f6cf95b6c-nkkbm&quot; that i am looking for a pod with the id 7f6cf95b6c-nkkbm. because of this i can not use this stackdriver view with this exact query if i deployed a new revision of my-app therefore having a new id and the one in the curreny query becomes invalid or not locatable.
now i don't always want to look for the new id every time i want to have the current view of my my-app logs. so i tried to add a special label stackdriver: my-app to my kubernetes yaml file.
apiversion: apps/v1
kind: deployment
metadata:
  name: my-app
spec:
  template:
    metadata:
      labels:
        stackdriver: my-app &lt;&lt;&lt;

revisiting my newly deployed pod i can assure that the label stackdriver: my-app is indeed existing.
now i want to add this new label to use as a query argument:
resource.type=&quot;k8s_container&quot;
resource.labels.project_id=&quot;my-project&quot;
resource.labels.location=&quot;europe-west3-a&quot;
resource.labels.cluster_name=&quot;my-cluster&quot;
resource.labels.namespace_name=&quot;dev&quot;
resource.labels.pod_name=&quot;my-app-pod-7f6cf95b6c-nkkbm&quot;
resource.labels.container_name=&quot;container&quot;
resource.labels.stackdriver=my-app &lt;&lt;&lt; the kubernetes label

as you can guess this did not work otherwise i'd have no reason to write this question ;)
any idea how the thing i am about to do can be achieved?
",<kubernetes><google-cloud-platform><yaml><google-kubernetes-engine><google-cloud-stackdriver>,64373798,2,"
any idea how the thing i am about to do can be achieved?

yes! in fact, i've prepared an example to show you the whole process :)
let's assume:

you have a gke cluster named: gke-label
you have a cloud operations for gke enabled (logging)
you have a deployment named nginx with a following label:

stackdriver: look_here_for_me



deployment.yaml:
apiversion: apps/v1
kind: deployment
metadata:
  name: nginx
spec:
  selector:
    matchlabels:
      app: nginx
      stackdriver: look_here_for_me
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
        stackdriver: look_here_for_me
    spec:
      containers:
      - name: nginx
        image: nginx

you can apply this definition and send some traffic from the other pod so that the logs could be generated. i've done it with:

$ kubectl run -it --rm --image=ubuntu ubuntu -- /bin/bash
$ apt update &amp;&amp; apt install -y curl
$ curl nginx_pod_ip_address/nonexisting # &lt;-- this path is only for better visibility

after that you can go to:

gcp cloud console (web ui) -&gt; logging (i used new version)

with the following query:
resource.type=&quot;k8s_container&quot;
resource.labels.cluster_name=&quot;gke-label&quot;
--&gt;labels.&quot;k8s-pod/stackdriver&quot;=&quot;look_here_for_me&quot;

you should be able to see the container logs as well it's label:

"
74312997,searching correct ingress definition for multiple services on different backends,"i need help with setting up tls termination (using ingress-nginx) for two services in different backends. basically, the idea is to go with:
https://some.url.here/serviceone/ -&gt; http://hostone/
https://some.url.here/servicetwo/ -&gt; http://hosttwo/someservicenamehere/

i tried doing it with rewrite-target: &quot;/&quot;, rewrite-target: &quot;/$1&quot;, and use-regexp: &quot;true&quot;, but connecting to http://hosttwo/someservicenamehere/ constantly fails.
my ingress definition looks like this:
kind: ingress
metadata:
  name: rp-some-url-here
  namespace: reverse-proxy
  annotations:
    kubernetes.io/ingress.class: nginx
    # ??? nginx.ingress.kubernetes.io/rewrite-target: &quot;/$1&quot;
    # ??? nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    nginx.ingress.kubernetes.io/backend-protocol: &quot;http&quot;
    nginx.ingress.kubernetes.io/proxy-ssl-verify: &quot;false&quot;
    nginx.ingress.kubernetes.io/upstream-vhost: &quot;some.url.here&quot;
    nginx.ingress.kubernetes.io/proxy-connect-timeout: &quot;600&quot;
    nginx.ingress.kubernetes.io/proxy-send-timeout: &quot;600&quot;
    nginx.ingress.kubernetes.io/proxy-read-timeout: &quot;600&quot;
    # proxy-buffering is now disabled by default,
    # just having it here for reference
    nginx.ingress.kubernetes.io/proxy-buffering: &quot;off&quot;
spec:
  tls:
  - hosts:
    - some.url.here
    secretname: my-tls-secret
  rules:
  - host: some-url-here
    http:
      paths:
      - backend:
          service:
            name: serviceone
            port:
              number: 80
        # path: /serviceone # using this with rewrite-target
        path: /serviceone/.*
        pathtype: prefix
      - backend:
          service:
            name: servicetwo
            port:
                number: 8180
        # path: /servicetwo # using this with rewrite-target
        path: /servicetwo/.*
        pathtype: prefix

service definition is as simply as
---
apiversion: v1
kind: service
metadata:
  name: service-one
  namespace: reverse-proxy
spec:
  type: externalname
  externalname: some.hostname.here
---
apiversion: v1
kind: service
metadata:
  name: service-two
  namespace: reverse-proxy
spec:
  type: externalname
  externalname: another.hostname.here

",<kubernetes><kubernetes-ingress><ingress-nginx>,74446238,2,"here is a quick example of how it should be done ,also make sure that your second service responds to the port that you specified (8180)
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  name: rewrite
  namespace: default
spec:
  ingressclassname: nginx
  rules:
  - host: rewrite.bar.com
    http:
      paths:
      - path: /something(/|$)(.*)
        pathtype: prefix
        backend:
          service:
            name: http-svc1
            port: 
              number: 80
      - path: /something2(/|$)(.*)
        pathtype: prefix
        backend:
          service:
            name: http-svc2
            port: 
              number: 8180


"
74314692,kubernetes ingress controller cannot find tls certificate secret,"i'm actually trying for a couple of days to setup tls on my ingress controller.
my kubernetes cluster is hosted on azure (aks).
when i check the ingress controller logs, i get this :

w1104 08:49:29.472478       7 backend_ssl.go:45] error obtaining x.509 certificate: unexpected error creating ssl cert: no valid pem formatted block found


w1104 08:49:29.472595       7 controller.go:1334] error getting ssl certificate &quot;myapp-test/myapp-tls&quot;: local ssl certificate myapp-test/myapp-tls was not found. using default certificate


w1104 08:49:29.472611       7 controller.go:1334] error getting ssl certificate &quot;myapp-test/myapp-tls&quot;: local ssl certificate myapp-test/myapp-tls was not found. using default certificate

here is my myapp-ingress.yml
kind: ingress
apiversion: networking.k8s.io/v1
metadata:
  name: myapp-test
  namespace: myapp-test
spec:
  ingressclassname: nginx
  tls:
    - hosts:
        - test-app.myapp.io
        - test-api.myapp.io
      secretname: myapp-tls
  rules:
    - host: test-app.myapp.io
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: myapp-frontend
                port:
                  number: 80
    - host: test-api.myapp.io
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: myapp-backend-monolith
                port:
                  number: 80

here is my secret.yml
kind: secret
apiversion: v1
metadata:
  name: myapp-tls
  namespace: myapp-test
data:
  tls.crt: &gt;-
    base64 encoded crt file content
  tls.key: &gt;-
    base64 encoded key file content
type: kubernetes.io/tls

i actully tried to create ingresses and/or secrets in every namespaces. but ingress controller still can't find ssl certificate.
",<kubernetes><ssl><kubernetes-ingress><azure-aks><nginx-ingress>,74325186,2,"based on below error, appears your cert format is not right.

no valid pem formatted block found

is your original cert in pem format? you can decode the cert data in secret and double check, using a command like below (you might need to install jq command, or you can copy the tls.crt data manually and decode it with base64 -d command):
kubectl get secret your-secret-name -n your-namespace -o json | jq '.&quot;data&quot;.&quot;tls.crt&quot;'| sed 's/&quot;//g'| base64 -d -

below is what i did using a self-signed test cert/key file.
 kubectl get secret mytest-ssl-secret -o json


{
    &quot;apiversion&quot;: &quot;v1&quot;,
    &quot;data&quot;: {
        &quot;tls.crt&quot;: &quot;ls0tls1crudjtibdrvjusuzjq0furs0tls0tck1jsuvgvendqxyyz0f3sujbz0lvwg12blrrcgtqmlhiqkx...tls0k&quot;,
        &quot;tls.key&quot;: &quot;ls0tls1crudjtibquklwqvrfietfws0tls0tck1jsuv2z0lcqurbtkjna3foa2lhoxcwqkfrruzbqvndqkt...tls0k&quot;
    },
    &quot;kind&quot;: &quot;secret&quot;,
    &quot;metadata&quot;: {
        &quot;creationtimestamp&quot;: &quot;2022-11-05t04:22:12z&quot;,
        &quot;name&quot;: &quot;mytest-ssl-secret&quot;,
        &quot;namespace&quot;: &quot;default&quot;,
        &quot;resourceversion&quot;: &quot;2024434&quot;,
        &quot;uid&quot;: &quot;d63dce3d-8e5c-478a-be9e-815e59e4bd21&quot;
    },
    &quot;type&quot;: &quot;kubernetes.io/tls&quot;
}


kubectl get secret mytest-ssl-secret -o json | jq '.&quot;data&quot;.&quot;tls.crt&quot;'| sed 's/&quot;//g'| base64 -d -


-----begin certificate-----
miieftccav2gawibagiuxmvntkpkj2xbblrjo+mpbfp4mvawdqyjkozihvcnaqel
bqawgzkxczajbgnvbaytaknomrawdgydvqqidadkawfuz3n1mq0wcwydvqqhdarx
dxhpmrowgaydvqqkdbfuzxn0igxpbwl0zwqgsw5jljelmakga1uecwwcsvqxhdaa
bgnvbamme3rlc3qwmdguzxhhbxbszs5jb20xijagbgkqhkig9w0bcqewe3rlc3qw
...
xo8b+zyfrp1pzncakeudvh6rpmbvhwvfm0qog4m736b9fk1vmjtg4do=
-----end certificate-----


"
64320685,passing minified yaml as an argument to a kubernetes job,"here's a simplified version of a kubernetes job yaml config i use commonly:
apiversion: batch/v1
kind: job
metadata:
  name: myjob
spec:
  template:
    spec:
      containers:
      - name: mycontainer
        image: me/mycontainer:latest
        command: [&quot;bash&quot;, &quot;-c&quot;]
        args:
          - python -u myscript.py
              --param1 abc
              --param2 xyz

the above works great, and is easy to maintain and read. but now one of my parameters needs some minified yaml:
apiversion: batch/v1
kind: job
metadata:
  name: myjob
spec:
  template:
    spec:
      containers:
      - name: mycontainer
        image: me/mycontainer:latest
        command: [&quot;bash&quot;, &quot;-c&quot;]
        args:
          - python -u myscript.py
              --param_minified_yaml &quot;{key: value}&quot;

this bit of embedded minified yaml is being parsed by kubectl and causing: error: error parsing stdin: error converting yaml to json: yaml: line 26: mapping values are not allowed in this context
how can the embedded yaml in args: be escaped such that it's passed as a pure text argument?
",<kubernetes><yaml><kubectl>,64321532,2,"if the minified yaml (or the args string in general) does not include single quotes, you can wrap the whole command line in them:
apiversion: batch/v1
kind: job
metadata:
  name: myjob
spec:
  template:
    spec:
      containers:
      - name: mycontainer
        image: me/mycontainer:latest
        command: [&quot;bash&quot;, &quot;-c&quot;]
        args:
          - 'python -u myscript.py
              --param_minified_yaml &quot;{key: value}&quot;'

if the arg string contains includes single quotes, the args string can be passed as a yaml multiline string:
apiversion: batch/v1
kind: job
metadata:
  name: myjob
spec:
  template:
    spec:
      containers:
      - name: mycontainer
        image: me/mycontainer:latest
        command: [&quot;bash&quot;, &quot;-c&quot;]
        args:
          - &gt;-
            python -u myscript.py
            --param_minified_yaml &quot;{key: 'value'}&quot;

"
74599464,how to get the k8s ingress wildcard in the host and put it on the url path,"i use k8s to deploy my services.
i hope when i access the ingress host foo.example.com and the ingress forward it to server:8000/proxy/foo. the subdomain foo is dynamic and can be changed to any word. the expected result is as below:

foo.example.com -&gt; server:8000/proxy/foo
bar.example.com -&gt; server:8000/proxy/bar
......

i knew the ingress host could use wildcards and use ingress-nginx rewrite can rewrite the url path. i use the ingress-nginx controller. the ingress yaml file like as below:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /proxy/${wildcard}/$1 # could i use the host wildcard here?
name: nginx-forward
spec:
  rules:
  - host: *.example.com # i want to get the subdomain wildcard
    http:
      paths:
      - backend:
        service:
          name: service
        port:
          number: 8080
      path: /(.*)
      pathtype: prefix
  tls: # update this attribute
  - hosts:
    - *.example.com 
    secretname: my-secret

how could i use k8s ingress or other things to get what i want? thanks.
",<kubernetes><kubernetes-ingress>,74601424,2,"you can use server snippet to get the subdomain:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/server-snippet: |
      server_name     ~^(?&lt;subdomain&gt;\w+)\.example\.com$;
    nginx.ingress.kubernetes.io/rewrite-target: /proxy/$subdomain/$1 
name: nginx-forward
spec:
  rules:
  - http:
      paths:
      - backend:
        service:
          name: service
        port:
          number: 8080
      path: /(.*)
      pathtype: prefix

"
64262229,aws alb ingress controller and ingress resource for python app,"i have created python app following this link https://kubernetes.io/blog/2019/07/23/get-started-with-kubernetes-using-python/. i want to configure aws alb ingress controller/nginx controller and ingress resource but i am unable to understand the file. i don't have a domain using kops on ec2-instance, want to configure it without any domain. any help would be appreciated.
deployment:
apiversion: apps/v1
kind: deployment
metadata:
  name: hello-python
spec:
  selector:
    matchlabels:
      app: hello-python
  template:
    metadata:
      labels:
        app: hello-python
    spec:
      containers:
      - name: hello-python
        image: hello-python:latest
        ports:
        - containerport: 5000

service
apiversion: v1
kind: service
metadata:
  name: hello-python-service
spec:
  selector:
    app: hello-python
  type: nodeport
  ports:
    - nodeport: 30010
      port: 6000
      targetport: 5000

",<amazon-web-services><kubernetes><kubernetes-ingress><nginx-ingress><aws-alb>,64262527,2,"an ingress binds a service like hello-python-service above to an alb through the nginx ingress controller.
it does so by mapping a virtual host to your service so that nginx knows how to route requests.
example:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: python-ingress
  namespace: default
  annotations:
    kubernetes.io/ingress.class: ingress-controller-nginx
    nginx.ingress.kubernetes.io/default-backend: hello-python-service
    nginx.ingress.kubernetes.io/rewrite-target: /$1

spec:
  rules:
    - host: python.trash.net
      http:
        paths:
          - path: /?(.*)
            backend:
              servicename: hello-python-service
              serviceport: 6000

would generate a resource of class ingress like the below:
python-ingress   python.trash.net   internal-0358a08f01385b2812-331143124.us-east-2.elb.amazonaws.com   80      10s

issuing a curl request like this:
curl -h &quot;host: python.trash.net&quot; http://internal-0358a08f01385b2812-331143124.us-east-2.elb.amazonaws.com

should get you a response from your hello python app.
this obviously depends on you having the nginx ingress controller deployed and configured in your eks cluster.
the key takeaway for an ingress is this:  it replaces the need for a dedicated service of type loadbalancer to provide external access to your service.  instead, ingress maps traffic from outside the cluster to services in the cluster through a virtual host that is configured in the ingress' manifest file.
"
74498508,how to create kubernetes secret with multiple values for one key?,"this is how i'm trying to create a secret for my kubernetes mongodb, which gets deployed using the bitnami mongodb helm chart:
apiversion: v1
kind: secret
metadata:
  name: mongodb-secret
  namespace: mongodb
  labels:
    app.kubernetes.io/component: mongodb
type: opaque
data: 
  mongodb-root-password: 'encoded value'
  mongodb-passwords: '???'
  mongodb-metrics-password: 'encoded value'
  mongodb-replica-set-key: 'encoded value'

the helm chart values.yaml says:
auth:
  ## mongodb(&amp;reg;) custom users and databases
  ## ref: https://github.com/bitnami/containers/tree/main/bitnami/mongodb#creating-a-user-and-database-on-first-run
  ## @param auth.usernames list of custom users to be created during the initialization
  ## @param auth.passwords list of passwords for the custom users set at `auth.usernames`
  ## @param auth.databases list of custom databases to be created during the initialization
  ##
  usernames: []
  passwords: []
  databases: []
  ## @param auth.existingsecret existing secret with mongodb(&amp;reg;) credentials (keys: `mongodb-passwords`, `mongodb-root-password`, `mongodb-metrics-password`, ` mongodb-replica-set-key`)
  ## note: when it's set the previous parameters are ignored.
  ##
  existingsecret: &quot;&quot;

so passwords is an array of strings for each username and each database.
how do i have to implement these multiple passwords in my secret?
the helm template should give me a hint, but i don't understand it: secret.yaml
or is it a simple string with all passwords separated by , and encoded?
",<mongodb><kubernetes><kubernetes-helm><kubernetes-secrets>,74505280,2,"should be something like:
auth:
  usernames: [&quot;bob&quot;, &quot;alice&quot;]
  passwords: [&quot;bobpass&quot;, &quot;alicepass&quot;]
  databases: [&quot;bobdb&quot;, &quot;alicedb&quot;]

if you want to pass those on the cli --set flag instead, you should be able to use curly braces as per this comment: https://github.com/helm/helm/issues/1987#issuecomment-280497496 - like:
--set auth.usernames={bob,alice},auth.passwords={bobpass,alicepass},auth.databases={bobdb,alicedb}

this would produce a secret like following - which you can check with helm template command:
---
# source: mongodb/templates/secrets.yaml
apiversion: v1
kind: secret
metadata:
  name: release-name-mongodb
  namespace: default
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-13.4.4
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/component: mongodb
type: opaque
data:
  mongodb-root-password: &quot;uk1tzthhyznfzg==&quot;
  mongodb-passwords: &quot;ym9icgfzcyxhbgljzxbhc3m=&quot;
---

you can decode mongodb-passwords, using:
echo -n ym9icgfzcyxhbgljzxbhc3m= | base64 -d

and notice that it looks as following: bobpass,alicepass
also note that there seems to be an option to have mongodb.createsecret flag set to false and creating that secret manually (which may be more secure depending on the exact workflow).
"
74770691,kubernetes ingress exact not prioritized over prefix,"in kubernetes we need a new service to handle the root path, but but still a catch everything else on our current frontend.
current frontend ingress
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: current-frontend
  labels:
    app: current-frontend
    tier: frontend
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  tls:
    - hosts:
      - my.domain.com
      secretname: tls-secret
  rules:
    - host: my.domain.com
      http:
        paths:
          - backend:
              service:
                name: current-frontend
                port:
                  number: 80
            path: /
            pathtype: prefix

new service ingress
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: new-service
  labels:
    app: new-service
    tier: frontend
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  tls:
  - hosts:
    - my.domain.com
    secretname: tls-secret
  rules:
  - host: my.domain.com
    http:
      paths:
      - backend:
          service:
            name: new-service
            port:
              number: 80
        path: /someendpoint
        pathtype: implementationspecific
      - backend:
          service:
            name: new-service
            port:
              number: 80
        path: /
        pathtype: exact

according to the documentation of kuberntes ingress, it should prioritize exact over prefix

if two paths are still equally matched, precedence will be given to paths with an exact path type over prefix path type.

https://kubernetes.io/docs/concepts/services-networking/ingress/#multiple-matches
the problem is that everything else then my.domain.com/someendpoint goes to the current-frontend, while the expected would be that my.domain.com/ would go to new-service.
how do i achieving this?

solution
i got it working, even if it doesn't seams to be the optimal solution (according to the documentation)
i followed hemanth kumar's answer and changed the current frontend to use regex, with (.+) instead of (.*) as i wanted at least one char after the slash for it to be hit.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: current-frontend
  labels:
    app: current-frontend
    tier: frontend
  annotations:
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    kubernetes.io/ingress.class: nginx
spec:
  tls:
    - hosts:
      - my.domain.com
      secretname: tls-secret
  rules:
    - host: my.domain.com
      http:
        paths:
          - backend:
              service:
                name: current-frontend
                port:
                  number: 80
            path: /(.+)
            pathtype: prefix

at the same time i needed to change the new service to use prefix instead of exact as it does not work, event if there is no other services to hit.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: new-service
  labels:
    app: new-service
    tier: frontend
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  tls:
  - hosts:
    - my.domain.com
    secretname: tls-secret
  rules:
  - host: my.domain.com
    http:
      paths:
      - backend:
          service:
            name: new-service
            port:
              number: 80
        path: /someendpoint
        pathtype: implementationspecific
      - backend:
          service:
            name: new-service
            port:
              number: 80
        path: /
        pathtype: prefix

",<nginx><kubernetes><kubernetes-ingress><nginx-ingress>,74771488,2,"ingress path matching can be enabled by setting the
nginx.ingress.kubernetes.io/use-regex annotation to true .
see the description of the use-regex annotation :
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: test-ingress
  annotations:
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
spec:
  ingressclassname: nginx
  rules:
  - host: test.com
    http:
      paths:
      - path: /foo/.*
        pathtype: prefix
        backend:
          service:
            name: test
            port:
              number: 80

refer to this ingress path matching for more information on path priority
"
64012806,can the same hostname be used in different namespaces in kubernetes?,"i want to use my hostname as test.com in first namespace and example.com/demo in another namespace. however i am unable to do that since (in my opinion) the nginx-ingress controller always points to the first website (example.com). my nginx-ingress controller is running in the default namespace
#namespace prod
kind: ingress
apiversion: extensions/v1beta1
metadata:
  name: ingress
  namespace: production
    annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: 20m
spec:
  rules:
    - host: example.com
      http:
        paths:
          - path: /static
            backend:
              servicename: app-svc
              serviceport: 80
          - path: /
            backend:
              servicename: app-svc
              serviceport: 8000
  tls:
  - hosts:
    - example.com
    secretname: cert
status:
  loadbalancer:
    ingress:
      - ip: xx.xxx.xx.xxx


#namespace dev
kind: ingress
apiversion: extensions/v1beta1
metadata:
  name: test-ingress
  namespace: dev-env
    annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: 20m
spec:
  rules:
    - host: example.com
      http:
        paths:
          - path: /static
            backend:
              servicename: app-svc
              serviceport: 80
          - path: /demo
            backend:
              servicename: app-svc
              serviceport: 8000
  tls:
  - hosts:
    - example.com
    secretname: cert
status:
  loadbalancer:
    ingress:
      - ip: xx.xxx.xx.xxx
 
 

",<kubernetes><kubernetes-ingress><nginx-ingress>,64013732,2,"you can use the same hostname/domain, but in order to access distinct backend services, you have to define distinct paths, in your case you can do something like this:
#namespace prod
kind: ingress
apiversion: extensions/v1beta1
metadata:
  name: test-ingress
  namespace: prod-env
    annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: 20m
spec:
  rules:
    - host: example.com
      http:
        paths:
          - path: /prod/static
            backend:
              servicename: app-svc
              serviceport: 80
          - path: /prod/
            backend:
              servicename: app-svc
              serviceport: 8000
  tls:
  - hosts:
    - example.com
    secretname: selfsigned-cert
status:
  loadbalancer:
    ingress:
      - ip: 80.180.31.153


#namespace dev
kind: ingress
apiversion: extensions/v1beta1
metadata:
  name: test-ingress
  namespace: dev-env
    annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: 20m
spec:
  rules:
    - host: example.com
      http:
        paths:
          - path: /dev/static
            backend:
              servicename: app-svc
              serviceport: 80
          - path: /demo
            backend:
              servicename: app-svc
              serviceport: 8000
  tls:
  - hosts:
    - example.com
    secretname: selfsigned-cert
status:
  loadbalancer:
    ingress:
      - ip: 80.180.31.153

like this you have:
example.com/prod/static -&gt; points to app-svc:80 on prod-env namespace  
example.com/prod/ -&gt; points to app-svc:8000 on prod-env namespace  

example.com/dev/static -&gt; points to app-svc:8000 on dev-env namespace  
example.com/demo -&gt; points to app-svc:8000 on dev-env namespace 

if you need to preserve the url after host, you should add a subdomain for each namespace:
dev.example.com/static -&gt; points to app-svc:80 on prod-env namespace  
prod.example.com/static -&gt; points to app-svc:8000 on dev-env namespace  

in order for this one to work you have to have defined this as an a record in your domain dns administration, pointing at the same ip as your loadbalancer (80.180.31.153 in this case):
example.com a -&gt; 80.180.31.153 (optional)   
dev.exmaple.com a -&gt; 80.180.31.153  
prod.example.com a -&gt; 80.180.31.153  

or the easiest one:
*.example.com a -&gt; 80.180.31.153  

"
64013958,why is my externalname type service configuration not working in kubernetes?,"i created two namespaces and services in each namespace:

namespace: app-layer

rest-app
db-service-externalname


namespace: data-layer

db-service



when i try to connect to the mysql database in db service from the rest-app, i get the error:

mysql.data.mysqlclient.mysqlexception (0x80004005): unable to connect to any of the specified mysql hosts.
---&gt; system.aggregateexception: one or more errors occurred. (name or service not known)

i printed out in logs, and it correctly has db-service as the service name, and has the right user/pass.
here's what i defined:
db-service
apiversion: v1
kind: service
metadata:
  name: db-service
  namespace: data-layer
spec:
  selector:
    app: db-service
  ports:
   - port: 3306
  clusterip: none

db-service-externalname
apiversion: v1
kind: service
metadata:
  name: db-service
  namespace: app-layer
spec:
  type: externalname
  externalname: db-service.data-layer.service.cluster.local
  ports:
    - port: 3306

rest-app
apiversion: apps/v1
kind: deployment
metadata:
  name: rest-app
  namespace: app-layer
  labels:
    app: rest-app
spec:
  replicas: 1
  selector:
    matchlabels:
      app: rest-app
  template:
    metadata:
      labels:
        app: rest-app
    spec:
      containers:
        - name: rest-app
          image: restapp:latest
          imagepullpolicy: always
          ports:
            - containerport: 5000
          env:
            # these are from a secret i defined, and the logs show
            # the rest app gets them correctly
            - name: mysql_root_username
              valuefrom:
                secretkeyref:
                  name: db-credentials
                  key: db-username
            - name: mysql_root_password
              valuefrom:
                secretkeyref:
                  name: db-credentials
                  key: db-password
            # i hard-coded this to the externalname i created. 
            # is that right?
            - name: mysql_url
              value: db-service

questions:

am i correct to make the externalname in the app-layer namespace?
do i need to define it differently?
can the c# app in the docker container not refer to db-service which is the name of the externalname service?

",<mysql><docker><kubernetes><kubernetes-networking><kubernetes-namespace>,64013991,2,"the externalname type service should be as below. notice usage of svc instead of service.
apiversion: v1
kind: service
metadata:
  name: db-service
  namespace: app-layer
spec:
  type: externalname
  externalname: db-service.data-layer.svc.cluster.local
  ports:
    - port: 3306

"
63963554,persistent volume storage always in lost phase,"i had a broken mongo container that was not initializing, and i suspected that i could fix it by cleaning all the storage. so, i deleted the persistent storage being used, but now, every new storage that i create is in the &quot;lost&quot; phase, and it's not found by in the mongo's pod creation.
this is the .yml file that i'm using to create the pv:
apiversion: v1
kind: persistentvolumeclaim
metadata:
  annotations:
    pv.kubernetes.io/bind-completed: &quot;yes&quot;
    pv.kubernetes.io/bound-by-controller: &quot;yes&quot;
    volume.beta.kubernetes.io/storage-class: standard
    volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd
  creationtimestamp: &quot;2018-08-11t09:19:29z&quot;
  finalizers:
  - kubernetes.io/pvc-protection
  labels:
    environment: test
    role: mongo
  name: mongo-persistent-storage-mongo-db-0
  namespace: default
  resourceversion: &quot;122299922&quot;
  selflink: /api/v1/namespaces/default/persistentvolumeclaims/mongo-persistent-storage-mongo-db-0
  uid: a68de459-9d47-11e8-84b1-42010a800138
spec:
  accessmodes:
  - readwriteonce
  resources:
    requests:
      storage: 20gi
  volumemode: filesystem
  volumename: pvc-a68de459-9d47-11e8-84b1-42010a800135
status:
  accessmodes:
  - readwriteonce
  capacity:
    storage: 20gi
  phase: bound

this is the error that i get when restarting the mongo pod:
error while running &quot;volumebinding&quot; filter plugin for pod &quot;mongo-db-0&quot;: could not find v1.persistentvolume &quot;pvc-a68de459-9d47-11e8-84b1-42010a800135&quot;

i already tryied changing the pv name and ids, but it didn't work.
",<kubernetes><google-kubernetes-engine><kubernetes-pod><persistent-volumes>,63966156,2,"
this is the .yml file that i'm using to create the pv

it looks like you use a manifest that binds to a specific pv.
how about if you remove the unique fields and let the cluster dynamically provision a new pv for you?
example:
apiversion: v1
kind: persistentvolumeclaim
metadata:
  labels:
    environment: test
    role: mongo
  name: mongo-persistent-storage-mongo-db-0
  namespace: default
spec:
  accessmodes:
  - readwriteonce
  resources:
    requests:
      storage: 20gi
  volumemode: filesystem

"
76733760,"unable to access my services deployed to amazon eks using nginx ingress controller , nodeport","this is my eks cluster details: kubectl get all
name                                            ready   status    restarts   age
pod/feed-4fqdrrf-fwcc3                          1/1     running   0          64m
pod/gst-7adn3njl-fg43                           1/1     running   0          71m
pod/ingress-nginx-controller-f567efvef-f653dc   1/1     running   0          9d
pod/app-24dfs2d-m2fdqw                          1/1     running   0          66m

name                                         type           cluster-ip       external-ip                                        port(s)                      age
service/feed                                 nodeport       10.100.24.643    &lt;none&gt;                                             8082:30002/tcp               64m
service/gst                                  nodeport       10.100.54.543    &lt;none&gt;                                             8081:30004/tcp               71m
service/ingress-nginx-controller             loadbalancer   10.100.643.256   ******************.&lt;region&gt;.elb.amazonaws.com      80:30622/tcp,443:30721/tcp   9d
service/ingress-nginx-controller-admission   clusterip      10.100.654.542   &lt;none&gt;                                             443/tcp                      9d
service/kubernetes                           clusterip      10.100.0.7       &lt;none&gt;                                             443/tcp                      14d
service/app                                  nodeport       10.100.456.34    &lt;none&gt;                                             3001:30003/tcp               66m

name                                       ready   up-to-date   available   age
deployment.apps/feed                       1/1     1            1           64m
deployment.apps/gst                        1/1     1            1           71m
deployment.apps/ingress-nginx-controller   1/1     1            1           9d
deployment.apps/app                        1/1     1            1           66m

name                                                  desired   current   ready   age
replicaset.apps/feed-4fqdrrf                         1         1         1       64m
replicaset.apps/gst-7adn3njl                         1         1         1       71m
replicaset.apps/ingress-nginx-controller-f567efvef   1         1         1       9d
replicaset.apps/app-m2fdqw                           1         1         1       66m

kubectl logs feed-4fqdrrf-fwcc3 
helloworld: listening on port 8082
~ % kubectl logs gst-7adn3njl-fg43
helloworld: listening on port 8081
~ % kubectl logs app-24dfs2d-m2fdqw 
helloworld: listening on port 3001

these are my deployment and service yamls:
apiversion: apps/v1
kind: deployment
metadata:
  name: feed
  labels:
    app: feed
spec:
  selector:
    matchlabels:
      app: feed
  template:
    metadata:
      labels:
        app: feed
    spec:
      containers:
      - name: feed-container
        image: **************.dkr.ecr.******.amazonaws.com/feed:latest
        ports:
        - containerport: 8082

---
apiversion: v1
kind: service
metadata:
  name: feed
spec:
  ports:
  - port: 8082 
    protocol: tcp
    targetport: 8082
  selector:
    app: feed
  type: nodeport

similar for other 2 services and this is my ingress yaml:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: my-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/enable-websocket: &quot;true&quot;
    nginx.org/websocket-services: app
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
spec:
  ingressclassname: nginx
  rules:
    - http:
        paths:
          - path: /feed
            pathtype: prefix
            backend:
              service:
                name: expertfeed
                port:
                  number: 8082
          - path: /app
            pathtype: prefix
            backend:
              service:
                name: wehealapp
                port:
                  number: 3001
          - path: /socket.io/app
            pathtype: prefix
            backend:
              service:
                name: app
                port:
                  number: 3001
          - path: /gst
            pathtype: prefix
            backend:
              service:
                name: gst
                port:
                  number: 8001

i have multiple get request in all 3 services and i have exposed them like this:
   app.get('/getrequest1', jsonparser, async (request, response) =&gt; {
    //my code
     )}
    app.get('/getrequest2', jsonparser, async (request, response) =&gt; {
    //my code
     )}
       .
       .
       .
        const port = process.env.port || 8082;
        app.listen(port, () =&gt; {
            console.log(`helloworld: listening on port ${port}`);
        });

similar pattern in all 3 node services.
i am getting 404 not found nginx when i hit:
******************.elb.amazonaws.com
and when i hit:
******************.elb.amazonaws.com/feed  getting this error:cannot get /
if i use this url ******************.elb.amazonaws.com/feed/getreqest1 still getting same error
",<amazon-web-services><kubernetes><amazon-eks><nginx-ingress>,76762002,2,"i found one way to do it that i add my nginx load balancer external-ip to my dns records and add a cname record to it. here is the yaml below:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: my-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.org/websocket-services: &quot; ************.svc.cluster.local&quot;
spec:
  rules:
    - host: expertfeed.*************
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: expertfeed
                port:
                  number: 8082
    - host: wehealapp.*************
      http:
        paths:
          - path: /socket.io
            pathtype: prefix
            backend:
              service:
                name: wehealapp
                port:
                  number: 3001
    - host: gst.************
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: gst
                port:
                  number: 8081

"
64128047,"migrating rom, rwo persistent volume claims from in-tree plugin to csi in gke","i currently have a rom, rwo persistent volume claim that i regularly use as a read only volume in a deployment that sporadically gets repopulated by some job using it as a read write volume while the deployment is scaled down to 0. however, since in-tree plugins will be deprecated in future versions of kubernetes, i'm planning to migrate this process to volumes using csi drivers.
in order to clarify my current use of this kind of volumes, i'll put a sample yaml configuration file using the basic idea:
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: test
spec:
  storageclassname: standard
  accessmodes:
  - readonlymany
  - readwriteonce
  resources:
    requests:
      storage: 1gi
---
apiversion: batch/v1
kind: job
metadata:
  name: test
spec:
  template:
    spec:
      containers:
      - name: test
        image: busybox
        # populate the volume
        command:
        - touch
        - /foo/bar
        volumemounts:
        - name: test
          mountpath: /foo/
          subpath: foo
      volumes:
      - name: test
        persistentvolumeclaim:
          claimname: test
      restartpolicy: never
---
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: test
  name: test
spec:
  replicas: 0
  selector:
    matchlabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      containers:
      - name: test
        image: busybox
        command:
        - sh
        - '-c'
        - |
          # check the volume has been populated
          ls /foo/
          # prevent the pod from exiting for a while
          sleep 3600
        volumemounts:
        - name: test
          mountpath: /foo/
          subpath: foo
      volumes:
      - name: test
        persistentvolumeclaim:
          claimname: test
          readonly: true

so the job populates the the volume and later the deployment is scaled up. however, replacing the storageclassname field standard in the persistent volume claim by singlewriter-standard does not even allow the job to run.
is this some kind of bug? is there some workaround to this using volumes using the csi driver?
if this is a bug, i'd plan to migrate to using sci drivers later; however, if this is not a bug, how should i migrate my current workflow since in-tree plugins will eventually be deprecated?
edit:
the version of the kubernetes server is 1.17.9-gke.1504. as for the storage classes, they are the standard and singlewriter-standard default storage classes:
apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  labels:
    addonmanager.kubernetes.io/mode: ensureexists
    kubernetes.io/cluster-service: &quot;true&quot;
  name: standard
parameters:
  type: pd-standard
provisioner: kubernetes.io/gce-pd
reclaimpolicy: delete
volumebindingmode: immediate
---
apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  annotations:
    components.gke.io/component-name: pdcsi-addon
    components.gke.io/component-version: 0.5.1
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
  labels:
    addonmanager.kubernetes.io/mode: ensureexists
  name: singlewriter-standard
parameters:
  type: pd-standard
provisioner: pd.csi.storage.gke.io
reclaimpolicy: delete
volumebindingmode: waitforfirstconsumer

while the error is not shown in the job but in the pod itself (this is just for the singlewriter-standard storage class):
warning failedattachvolume attachdetach-controller attachvolume.attach failed for volume &quot;...&quot; : csi does not support readonlymany and readwriteonce on the same persistentvolume
",<kubernetes><google-kubernetes-engine>,64318825,2,"the message you encountered:
warning failedattachvolume attachdetach-controller attachvolume.attach failed for volume &quot;...&quot; : csi does not support readonlymany and readwriteonce on the same persistentvolume

is not a bug. the attachdetach-controller is showing this error as it doesn't know in which accessmode it should mount the volume:

for [readonlymany, readwriteonce] pv, the external attacher simply does not know if the attachment is going to be consumed as read-only(-many) or as read-write(-once)
-- github.com: kubernetes csi: external attacher: issues: 153

i encourage you to check the link above for a full explanation.


i currently have a rom, rwo persistent volume claim that i regularly use as a read only volume in a deployment that sporadically gets repopulated by some job using it as a read write volume

you can combine the steps from below guides:

turn on the csi persistent disk driver in gke

cloud.google.com: kubernetes engine: how to: persistent volumes: gce-pd-csi-driver


create a pvc with pd.csi.storage.gke.io provisioner (you will need to modify yaml definitions with storageclassname: singlewriter-standard):

cloud.google.com: kubernetes engine: how to: persistent volumes: readonlymany disks



citing the documentation on steps to take (from readonlymany guide) that should fulfill the setup you've shown:

before using a persistent disk in read-only mode, you must format it.
to format your persistent disk:

create a persistent disk manually or by using dynamic provisioning.
format the disk and populate it with data. to format the disk, you can:

reference the disk as a readwriteonce volume in a pod. doing this results in gke automatically formatting the disk, and enables the pod to pre-populate the disk with data. when the pod starts, make sure the pod writes data to the disk.
manually mount the disk to a vm and format it. write any data to the disk that you want. for details, see persistent disk formatting.


unmount and detach the disk:

if you referenced the disk in a pod, delete the pod, wait for it to terminate, and wait for the disk to automatically detach from the node.
if you mounted the disk to a vm, detach the disk using gcloud compute instances detach-disk.


create pods that access the volume as readonlymany as shown in the following section.

-- cloud.google.com: kubernetes engine: how to: persistent volumes: readonlymany disks


additional resources:

github.com: kubernetes: design proposals: storage: csi
kubernetes.io: blog: container storage interface
kubernetes-csi.github.io: docs: drivers


edit
following the official documentation:

cloud.google.com: kubernetes engine: how to: persistent volumes: readonlymany disks

please treat it as an example.
dynamically create a pvc that will be used with readwriteonce accessmode:
pvc.yaml
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: pvc-rwo
spec:
  storageclassname: singlewriter-standard
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 81gi

run a pod with a pvc mounted to it:
pod.yaml
apiversion: v1
kind: pod
metadata:
  name: busybox-pvc
spec:
  containers:
  - image: k8s.gcr.io/busybox
    name: busybox
    command:
      - &quot;sleep&quot;
      - &quot;36000&quot;
    volumemounts:
    - mountpath: /test-mnt
      name: my-volume
  volumes:
  - name: my-volume
    persistentvolumeclaim:
      claimname: pvc-rwo

run following commands:

$ kubectl exec -it busybox-pvc -- /bin/sh
$ echo &quot;hello there!&quot; &gt; /test-mnt/hello.txt

delete the pod and wait for the drive to be unmounted. please do not delete pvc as deleting it:

when you delete a claim, the corresponding persistentvolume object and the provisioned compute engine persistent disk are also deleted.
-- cloud.google.com: kubernetes engine: persistent volumes: dynamic provisioning


get the name (it's in volume column) of the earlier created disk by running:

$ kubectl get pvc

name      status   volume                                     capacity   access modes   storageclass            age
pvc-rwo   bound    pvc-11111111-2222-3333-4444-555555555555   81gi       rwo            singlewriter-standard   52m

create a pv and pvc with following definition:
apiversion: v1
kind: persistentvolume
metadata:
  name: pv-rox
spec:
  storageclassname: singlewriter-standard
  capacity:
    storage: 81gi
  accessmodes:
    - readonlymany
  claimref:
    namespace: default
    name: pvc-rox # &lt;-- important
  gcepersistentdisk:
    pdname: &lt;insert here the disk name from earlier command&gt; 
    # pdname: pvc-11111111-2222-3333-4444-555555555555 &lt;- example
    fstype: ext4
    readonly: true
---
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: pvc-rox # &lt;-- important
spec:
  storageclassname: singlewriter-standard
  accessmodes:
    - readonlymany
  resources:
    requests:
      storage: 81gi

you can test if your disk is in rox accessmode when the spawned pods were scheduled on multiple nodes and all of them have the pvc mounted:
apiversion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: deployment
metadata:
  name: nginx
spec:
  selector:
    matchlabels:
      app: nginx
  replicas: 15
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        volumemounts:
        - mountpath: /test-mnt
          name: volume-ro
          readonly: true
      volumes:
      - name: volume-ro
        persistentvolumeclaim:
          claimname: pvc-rox
          readonly: true


$ kubectl get deployment nginx

name    ready   up-to-date   available   age
nginx   15/15   15           15          3m1s


$ kubectl exec -it nginx-6c77b8bf66-njhpm  -- cat /test-mnt/hello.txt

hello there!

"
76279076,problem in isolating specific pods using network policy from other namespaces,"i have the following pods in the default namespace:
web-test-pod-01             1/1     running   0              19m   app=web-test-pod-01
web-test-pod-02             1/1     running   0              18m   app=web-test-pod-02

and in another namespace called devwebapp i have the following
name            ready   status    restarts   age   labels
pod/webapp-01   1/1     running   0          47m   run=webapp-01

name                    type        cluster-ip     external-ip   port(s)   age   labels
service/svc-webapp-01   clusterip   10.109.4.169   &lt;none&gt;        80/tcp    46m   run=webapp-01

i also have network policy called np-webapp-01 and its yaml descriptor:
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: np-webapp-01
  namespace: devwebapp
spec:
  podselector:
    matchlabels:
      run: webapp-01
  policytypes:
    - ingress
  ingress:
    - from:
        - namespaceselector:
            matchlabels:
              kubernetes.io/metadata.name: default
        - podselector:
            matchlabels:
              app: web-test-pod-01
      ports:
        - protocol: tcp
          port: 80

i am trying to allow only the pod web-test-pod-01 in default namespace to access the svc-webapp-01 service but at the moment all pods in default namespace can access it.
$ k exec web-test-pod-01 -- curl -i svc-webapp-01.devwebapp.svc
  % total    % received % xferd  average speed   time    time     time  current
                                 dload  upload   total   spent    left  speed
  0http/1.1 200 ok 0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
   615    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
server: nginx/1.23.4
date: thu, 18 may 2023 08:32:34 gmt
content-type: text/html
content-length: 615
last-modified: tue, 28 mar 2023 15:01:54 gmt
connection: keep-alive
etag: &quot;64230162-267&quot;
accept-ranges: bytes

the following pod should not be able to access the service but as of now it can reach it!
$ k exec web-test-pod-02 -- curl -i svc-webapp-01.devwebapp.svc
  % total    % received % xferd  average speed   time    time     time  current
                                 dload  upload   total   spent    left  speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0http/1.1 200 ok
  0   615    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
server: nginx/1.23.4
date: thu, 18 may 2023 08:33:21 gmt
content-type: text/html
content-length: 615
last-modified: tue, 28 mar 2023 15:01:54 gmt
connection: keep-alive
etag: &quot;64230162-267&quot;
accept-ranges: bytes

i am not sure why podselector in the network policy is not taking effect.
",<kubernetes><kubernetes-networkpolicy>,76282720,2,"in network policy for selecting pod and namespace we have two conditions . you can find them in this git link.
this example below is or condition(policy is enforced based on namespaceselector or podselector)
ingress:
    - from:
      - namespaceselector:
          matchlabels:
            team: operations
      - podselector:           
          matchlabels:
            type: monitoring

you have used the above condition.
while this example is and condition
ingress:
    - from:
      - namespaceselector:     
          matchlabels:
            team: operations
        podselector:          
          matchlabels:
            type: monitoring

can you try the and condition and let me know if this works.
attaching a blog written by ashish choudhary for reference.
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
 name: np-webapp-01
 namespace: devwebapp
spec:
 podselector: {}
 policytypes:
   - ingress
 ingress:
   - from:
       - namespaceselector:
           matchlabels:
             kubernetes.io/metadata.name: default
         podselector:
           matchlabels:
             app: web-test-pod-01
     ports:
       - port: 80

"
65789968,microk8s + metallb + ingress,"im quite new to kubernetes and im trying to set up a microk8s test environment on a vps with centos.
what i did:
i set up the cluster, enabled the ingress and metallb
microk8s enable ingress
microk8s enable metallb

exposed the ingress-controller service:
apiversion: v1
kind: service
metadata:
  name: ingress
  namespace: ingress
spec:
  type: loadbalancer
  selector:
    name: nginx-ingress-microk8s 
  ports:
  - name: http
    protocol: tcp
    port: 80
    targetport: 80
  - name: https
    protocol: tcp
    port: 443
    targetport: 443

exposed an nginx deployment to test the ingress
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    run: nginx
  name: nginx-deploy
spec:
  replicas: 1
  selector:
    matchlabels:
      run: nginx-deploy
  template:
    metadata:
      labels:
        run: nginx-deploy
    spec:
      containers:
      - image: nginx
        name: nginx

this is the status of my cluster:
namespace            name                                             ready   status    restarts   age
kube-system          pod/hostpath-provisioner-5c65fbdb4f-m2xq6        1/1     running   3          41h
kube-system          pod/coredns-86f78bb79c-7p8bs                     1/1     running   3          41h
kube-system          pod/calico-node-g4ws4                            1/1     running   6          42h
kube-system          pod/calico-kube-controllers-847c8c99d-xhmd7      1/1     running   4          42h
kube-system          pod/metrics-server-8bbfb4bdb-ggvk7               1/1     running   0          41h
kube-system          pod/kubernetes-dashboard-7ffd448895-ktv8j        1/1     running   0          41h
kube-system          pod/dashboard-metrics-scraper-6c4568dc68-l4xmg   1/1     running   0          41h
container-registry   pod/registry-9b57d9df8-xjh8d                     1/1     running   0          38h
cert-manager         pod/cert-manager-cainjector-5c6cb79446-vv5j2     1/1     running   0          12h
cert-manager         pod/cert-manager-794657589-srrmr                 1/1     running   0          12h
cert-manager         pod/cert-manager-webhook-574c9758c9-9dwr6        1/1     running   0          12h
metallb-system       pod/speaker-9gjng                                1/1     running   0          97m
metallb-system       pod/controller-559b68bfd8-trk5z                  1/1     running   0          97m
ingress              pod/nginx-ingress-microk8s-controller-f6cdb      1/1     running   0          65m
default              pod/nginx-deploy-5797b88878-vgp7x                1/1     running   0          20m

namespace            name                                type           cluster-ip       external-ip    port(s)                      age
default              service/kubernetes                  clusterip      10.152.183.1     &lt;none&gt;         443/tcp                      42h
kube-system          service/kube-dns                    clusterip      10.152.183.10    &lt;none&gt;         53/udp,53/tcp,9153/tcp       41h
kube-system          service/metrics-server              clusterip      10.152.183.243   &lt;none&gt;         443/tcp                      41h
kube-system          service/kubernetes-dashboard        clusterip      10.152.183.225   &lt;none&gt;         443/tcp                      41h
kube-system          service/dashboard-metrics-scraper   clusterip      10.152.183.109   &lt;none&gt;         8000/tcp                     41h
container-registry   service/registry                    nodeport       10.152.183.44    &lt;none&gt;         5000:32000/tcp               38h
cert-manager         service/cert-manager                clusterip      10.152.183.183   &lt;none&gt;         9402/tcp                     12h
cert-manager         service/cert-manager-webhook        clusterip      10.152.183.99    &lt;none&gt;         443/tcp                      12h
echoserver           service/echoserver                  clusterip      10.152.183.110   &lt;none&gt;         80/tcp                       72m
ingress              service/ingress                     loadbalancer   10.152.183.4     192.168.0.11   80:32617/tcp,443:31867/tcp   64m
default              service/nginx-deploy                clusterip      10.152.183.149   &lt;none&gt;         80/tcp                       19m

namespace        name                                               desired   current   ready   up-to-date   available   node selector                 age
kube-system      daemonset.apps/calico-node                         1         1         1       1            1           kubernetes.io/os=linux        42h
metallb-system   daemonset.apps/speaker                             1         1         1       1            1           beta.kubernetes.io/os=linux   97m
ingress          daemonset.apps/nginx-ingress-microk8s-controller   1         1         1       1            1           &lt;none&gt;                        65m

namespace            name                                        ready   up-to-date   available   age
kube-system          deployment.apps/hostpath-provisioner        1/1     1            1           41h
kube-system          deployment.apps/coredns                     1/1     1            1           41h
kube-system          deployment.apps/calico-kube-controllers     1/1     1            1           42h
kube-system          deployment.apps/metrics-server              1/1     1            1           41h
kube-system          deployment.apps/dashboard-metrics-scraper   1/1     1            1           41h
kube-system          deployment.apps/kubernetes-dashboard        1/1     1            1           41h
container-registry   deployment.apps/registry                    1/1     1            1           38h
cert-manager         deployment.apps/cert-manager-cainjector     1/1     1            1           12h
cert-manager         deployment.apps/cert-manager                1/1     1            1           12h
cert-manager         deployment.apps/cert-manager-webhook        1/1     1            1           12h
metallb-system       deployment.apps/controller                  1/1     1            1           97m
default              deployment.apps/nginx-deploy                1/1     1            1           20m

namespace            name                                                   desired   current   ready   age
kube-system          replicaset.apps/hostpath-provisioner-5c65fbdb4f        1         1         1       41h
kube-system          replicaset.apps/coredns-86f78bb79c                     1         1         1       41h
kube-system          replicaset.apps/calico-kube-controllers-847c8c99d      1         1         1       42h
kube-system          replicaset.apps/metrics-server-8bbfb4bdb               1         1         1       41h
kube-system          replicaset.apps/kubernetes-dashboard-7ffd448895        1         1         1       41h
kube-system          replicaset.apps/dashboard-metrics-scraper-6c4568dc68   1         1         1       41h
container-registry   replicaset.apps/registry-9b57d9df8                     1         1         1       38h
cert-manager         replicaset.apps/cert-manager-cainjector-5c6cb79446     1         1         1       12h
cert-manager         replicaset.apps/cert-manager-794657589                 1         1         1       12h
cert-manager         replicaset.apps/cert-manager-webhook-574c9758c9        1         1         1       12h
metallb-system       replicaset.apps/controller-559b68bfd8                  1         1         1       97m
default              replicaset.apps/nginx-deploy-5797b88878                1         1         1       20m

it looks like metallb works, as the ingress services received an ip from the pool i specified.
now, when i try to deploy an ingress to reach the nginx deployment, i dont get the address:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
  name: ingress-nginx-deploy
spec:
  rules:
  - host: test.com
    http:
      paths:
      - backend:
          servicename: nginx-deploy
          serviceport: 80

namespace   name                   class    hosts                       address   ports   age
default     ingress-nginx-deploy   &lt;none&gt;   test.com                              80      13m

an help would be really appreciated. thank you!
",<kubernetes><kubernetes-ingress><nginx-ingress><microk8s><metallb>,65799332,2,"tl;dr
there are some ways to fix your ingress so that it would get the ip address.
you can either:

delete the kubernetes.io/ingress.class: nginx and add ingressclassname: public under spec section.
use the newer example (apiversion) from official documentation that by default will have assigned an ingressclass:

kubernetes.io: docs: concepts: services networking: ingress



example of ingress resource that will fix your issue:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-nginx-deploy
spec:
  ingressclassname: public 
  # above field is optional as microk8s default ingressclass will be assigned
  rules:
  - host: test.com
    http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: nginx-deploy
            port:
              number: 80

you can read more about ingressclass by following official documentation:

kubernetes.io: blog: improvements to the ingress api in kubernetes 1.18 

i've included more explanation that should shed some additional light on this particular setup.

after you apply above ingress resource the output of:

$ kubectl get ingress

will be following:
name                   class    hosts      address     ports   age
ingress-nginx-deploy   public   test.com   127.0.0.1   80      43s

as you can see the address contains 127.0.0.1. it's because this particular ingress controller enabled by an addon, binds to your host (microk8s node) to ports 80,443.
you can see it by running:

$ sudo microk8s kubectl get daemonset -n ingress nginx-ingress-microk8s-controller -o yaml


a side note!
look for hostport and securitycontext.capabilities.

the service of type loadbalancer created by you will work with your ingress controller but it will not be displayed under address in $ kubectl get ingress.

a side note!
please remember that in this particular setup you will need to connect to your ingress controller with a header host: test.com unless you have dns resolution configured to support your setup. otherwise you will get a 404.


additional resource:

github.com: ubuntu: microk8s: ingress with metallb issues
kubernetes.io: docs: concepts: configuration: overview

"
65471145,sharing non-persistent volume between containers in a pod,"i am trying to put two nodejs applications into the same pod, because normally they should sit in the same machine, and are unfortunately heavily coupled together in such a way that each of them is looking for the folder of the other (pos/app.js needs /pos-service, and pos-service/app.js needs /pos)
in the end, the folder is supposed to contain:
/pos 
/pos-service

their volume doesn't need to be persistent, so i tried sharing their volumes with an emptydir like the following:
apiversion: apps/v1
kind: deployment
metadata:
  name: pos-deployment
  labels:
    app: pos
spec:
  replicas: 1
  selector:
    matchlabels:
      app: pos
  template:
    metadata:
      labels:
        app: pos
    spec:
      volumes:
      - name: shared-data
        emptydir: {}
      containers:
      - name: pos-service
        image: pos-service:0.0.1
        volumemounts:
        - name: shared-data
          mountpath: /pos-service
      - name: pos
        image: pos:0.0.3
        volumemounts:
        - name: shared-data
          mountpath: /pos

however, when the pod is launched, and i exec into each of the containers, they still seem to be isolated and eachother's folders can't be seen
i would appereciate any help, thanks
",<kubernetes><kubernetes-pod><persistent-volumes><kubernetes-container>,65972111,2,"this is a community wiki answer so feel free to edit it and add any additional details you consider important.
since this issue has already been solved or rather clarified as in fact there is nothing to be solved here, let's post a community wiki answer as it's partially based on comments of a few different users.
as matt and david maze have already mentioned, it works as expected and in your example there is nothing that copies any content to your emptydir volume:

with just the yaml you've shown, nothing ever copies anything into the
emptydir volume, unless the images' startup knows to do that.  david
maze dec 28 '20 at 12:45

and as the name itselt may suggest, emptydir comes totally empty, so it's your task to pre-populate it with the desired data. it can be done with the init container by temporarily mounting your emptydir to a different mount point e.g. /mnt/my-epmty-dir and copying the content of specific directory or directories already present in your container e.g. /pos and /pos-service as in your example and then mounting it again to the desired location. take a look at this example, presented in one of my older answers as it can be done in the very same way. your deployment may look as follows:
apiversion: apps/v1
kind: deployment
metadata:
  name: pos-deployment
  labels:
    app: pos
spec:
  replicas: 1
  selector:
    matchlabels:
      app: pos
  template:
    metadata:
      labels:
        app: pos
    spec:
      volumes:
      - name: shared-data
        emptydir: {}
      initcontainers:
      - name: pre-populate-empty-dir-1
        image: pos-service:0.0.1
        command: ['sh', '-c', 'cp -a /pos-service/* /mnt/empty-dir-content/']
        volumemounts:
         - name: shared-data
           mountpath: &quot;/mnt/empty-dir-content/&quot;
      - name: pre-populate-empty-dir-2
        image: pos:0.0.3
        command: ['sh', '-c', 'cp -a /pos/* /mnt/empty-dir-content/']
        volumemounts:
         - name: shared-data
           mountpath: &quot;/mnt/empty-dir-content/&quot;
      containers:
      - name: pos-service
        image: pos-service:0.0.1
        volumemounts:
        - name: shared-data
          mountpath: /pos-service
      - name: pos
        image: pos:0.0.3
        volumemounts:
        - name: shared-data
          mountpath: /pos

it's worth mentioning that there is nothing surprising here as it is exacly how mount works on linux or other nix-based operating systems.
if you have e.g. /var/log/your-app on your main disk, populated with logs and then you mount a new, empty disk defining as its mountpoint /var/log/your-app, you won't see any content there. it won't be deleted from its original location on your main disc, it will simply become unavailable as in this location now you've mounted completely different volume (which happens to be empty or may have totally different content). when you unmount and visit your /var/log/your-app again, you'll see its original content. i hope it's all clear.
"
65659970,gke ingress unable to route traffic to services,"i am testing gke ingress to route traffic to two different services. my deployment consists of a basic web container that deploys a blue web page default and a green web page. i am able to get a response back essentially the &quot;/&quot; works with either blue or green deployment. but when i go to http:///green i get a 404 response. i have tested the same with &quot;/&quot; as green deployment and it displays a green web page. but if i go http:///blue it results in a 404 response,
i have verified my containers are working properly by attaching a load balancer directly to them. i am following this how to guide in gke to setup a similar environment. gke ingress how to  guide
any help on what i am missing would greatly help me understand better what is going on and why my gke load balancer is unable to route traffic.
green deployment file
apiversion: apps/v1
kind: deployment
metadata: 
  name: myapp-green
spec:
  replicas: 2
  selector:
    matchlabels:
      app: myapp
      version: green
  template:
    metadata:
      name: myapp
      labels:
        app: myapp
        version: green
    spec:
      containers:
        - name: myapp
          image: gcr.io/ultra-welder-300122/myapp:green
          imagepullpolicy: always
          resources:
            requests:
              cpu: &quot;100m&quot;
              memory: &quot;100mi&quot;
          ports:
          - containerport: 8080

blue  deployment file
apiversion: apps/v1
kind: deployment
metadata: 
  name: myapp-blue
spec:
  replicas: 2
  selector:
    matchlabels:
      app: myapp
      version: blue
  template:
    metadata:
      name: myapp
      labels:
        app: myapp
        version: blue
    spec:
      containers:
        - name: myapp
          image: gcr.io/ultra-welder-300122/myapp:blue
          imagepullpolicy: always
          resources:
            requests:
              cpu: &quot;100m&quot;
              memory: &quot;100mi&quot;
          ports:
          - containerport: 8888

my service yaml
apiversion: v1
kind: service
metadata: 
  name: myapp-blue-service
  labels:
    app: myapp
    version: blue
spec:
  type: nodeport
  selector:
    app: myapp
    version: blue
  ports: 
  - protocol: tcp
    port: 80
    targetport: 8888
---
apiversion: v1
kind: service
metadata: 
  name: myapp-green-service
  labels:
    app: myapp
    version: green
spec:
  type: nodeport
  selector:
    app: myapp
    version: green
  ports: 
  - protocol: tcp
    port: 80
    targetport: 8080

gke ingress yaml
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: myapp-ingress
  annotations:
    # if the class annotation is not specified it defaults to &quot;gce&quot;.
    kubernetes.io/ingress.class: &quot;gce&quot;
spec:
  rules:
  - http:
      paths:
      - path: /*
        backend:
          servicename: myapp-blue-service
          serviceport: 80
      - path: /green
        backend:
          servicename: myapp-green-service
          serviceport: 80

service status
name                  type        cluster-ip     external-ip   port(s)        age
kubernetes            clusterip   10.42.0.1      &lt;none&gt;        443/tcp        6h18m
myapp-blue-service    nodeport    10.42.81.106   &lt;none&gt;        80:31664/tcp   4h35m
myapp-green-service   nodeport    10.42.74.168   &lt;none&gt;        80:30246/tcp   4h35m

ingress status
kubectl describe ing myapp-ingress
name:             myapp-ingress
namespace:        default
address:          34.95.121.24
default backend:  default-http-backend:80 (10.41.0.9:8080)
rules:
  host        path  backends
  ----        ----  --------
  *
              /*        myapp-blue-service:80 (10.41.0.24:8888,10.41.1.16:8888)
              /green    myapp-green-service:80 (10.41.0.27:8080,10.41.1.19:8080)
annotations:  ingress.kubernetes.io/backends:
                {&quot;k8s-be-30192--a4913825f2ae16d4&quot;:&quot;healthy&quot;,&quot;k8s-be-30246--a4913825f2ae16d4&quot;:&quot;healthy&quot;,&quot;k8s-be-31664--a4913825f2ae16d4&quot;:&quot;healthy&quot;}
              ingress.kubernetes.io/forwarding-rule: k8s2-fr-dkti1gqp-default-myapp-ingress-yd16xk65
              ingress.kubernetes.io/target-proxy: k8s2-tp-dkti1gqp-default-myapp-ingress-yd16xk65
              ingress.kubernetes.io/url-map: k8s2-um-dkti1gqp-default-myapp-ingress-yd16xk65
              kubernetes.io/ingress.class: gce

deployment and pods status
name                                          ready   up-to-date   available   age
deployment.apps/myapp-blue                    2/2     2            2           4h18m
deployment.apps/myapp-green                   2/2     2            2           38m
name                                               ready   status    restarts   age
pod/myapp-blue-9c9cbf5b-c7v7c                      1/1     running   0          4h18m
pod/myapp-blue-9c9cbf5b-cpvgl                      1/1     running   0          4h18m
pod/myapp-green-7f56cc9496-hnfkz                   1/1     running   0          38m
pod/myapp-green-7f56cc9496-v8bb6                   1/1     running   0          38m

service status
name                  type        cluster-ip     external-ip   port(s)        age
kubernetes            clusterip   10.42.0.1      &lt;none&gt;        443/tcp        6h24m
myapp-blue-service    nodeport    10.42.81.106   &lt;none&gt;        80:31664/tcp   4h40m
myapp-green-service   nodeport    10.42.74.168   &lt;none&gt;        80:30246/tcp   4h40m

",<kubernetes><google-kubernetes-engine>,65765227,2,"i found the problem is with gke ingress controller. gke ingress controller doesn't provide rewrite-target to /. i am serving my green and blue web pages from the webroot. gke ingress controller is forwarding the requested url http://34.95.121.24/green to http://{backend
}/green. there is no page hosted on http://{backend
}/green as my default page is hosted on http://{backend}/.
there are two solutions to the problem.

use nginx-ingress controller with annotation to rewrite-target set to /.

apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: myapp-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /*
        backend:
          servicename: myapp-blue-service
          serviceport: 80
      - path: /green
        backend:
          servicename: myapp-green-service
          serviceport: 80



handle rewrite or redirect non-existing pages in the docker container. in my case it was a nginx webserver.

i created a default server configuration section to route any traffic hitting /green to index.html.
server {
        listen 80 default_server;
        listen [::]:80 default_server;

        root /var/www/html;
        server_name _;
        location / {
                # first attempt to serve request as file, then
                # as directory, then fall back to displaying a 404.
                index index.html
                try_files $uri $uri/ =404;
        }
        location /green {
                try_files $uri $uri/ /index.html;
        }


i copied default site configuration to /etc/nginx/sites-avaiable in my docker configuration file.
my sample dockerfile
from ubuntu
run apt-get update
run apt-get install nginx -y
copy index.html /var/www/html/
copy default /etc/nginx/sites-available/
expose 80
cmd [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;]

"
66144573,create an empty file inside a volume in kubernetes pod,"i have a legacy app which keep checking an empty file inside a directory and perform certain action if the file timestamp is changed.
i am migrating this app to kubernetes so i want to create an empty file inside the pod. i tried subpath like below but it doesn't create any file.
apiversion: v1
kind: pod
metadata:
  name: demo-pod
spec:
  containers:
    - name: demo
      image: alpine
      command: [&quot;sleep&quot;, &quot;3600&quot;]
      volumemounts:
      - name: volume-name
        mountpath: '/volume-name-path'
        subpath: emptyfile
  volumes:
    - name: volume-name
      emptydir: {} 

describe pods shows
containers:
  demo:
    container id:  containerd://0b824265e96d75c5f77918326195d6029e22d17478ac54329deb47866bf8192d
    image:         alpine
    image id:      docker.io/library/alpine@sha256:08d6ca16c60fe7490c03d10dc339d9fd8ea67c6466dea8d558526b1330a85930
    port:          &lt;none&gt;
    host port:     &lt;none&gt;
    command:
      sleep
      3600
    state:          running
      started:      wed, 10 feb 2021 12:23:43 -0800
    ready:          true
    restart count:  0
    environment:    &lt;none&gt;
    mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-4gp4x (ro)
      /volume-name-path from volume-name (rw,path=&quot;emptyfile&quot;)

ls on the volume also shows nothing.
k8 exec -it demo-pod -c demo ls /volume-name-path
any suggestion??
ps: i don't want to use a configmap and simply wants to create an empty file.
",<kubernetes><kubernetes-pod>,66145097,2,"if the objective is to create a empty file when the pod starts, then the most easy way is to either use the entrypoint of the docker image or an init container.
with the initcontainer, you could go with something like the following (or with a more complex init image which you build and execute a whole bash script or something similar):
apiversion: v1
kind: pod
metadata:
  name: demo-pod
spec:
  initcontainers:
    - name: create-empty-file
      image: alpine
      command: [&quot;touch&quot;, &quot;/path/to/the/directory/empty_file&quot;]
      volumemounts:
      - name: volume-name
        mountpath: /path/to/the/directory
  containers:
    - name: demo
      image: alpine
      command: [&quot;sleep&quot;, &quot;3600&quot;]
      volumemounts:
      - name: volume-name
        mountpath: /path/to/the/directory
  volumes:
    - name: volume-name
      emptydir: {}

basically the init container gets executed first, runs its command and if it is successful, then it terminates and the main container starts running. they share the same volumes (and they can also mount them at different paths) so in the example, the init container mount the emptydir volume, creates an empty file and then complete. when the main container starts, the file is already there.
regarding your legacy application which is getting ported on kubernetes:
if you have control of the dockerfile, you could simply change it create an empty file at the path you are expecting it to be, so that when the app starts, the file is already created there, empty, from the beginning, just exactly as you add the application to the container, you can add also other files.
for more info on init container, please check the documentation (https://kubernetes.io/docs/concepts/workloads/pods/init-containers/)
"
66696691,cronjob openshift not running a pod,"im trying to schedule a cronjob to launch a kubectl command. the cronjob does not start a pod.
this is my cronjob
apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: mariadump
  namespace: my-namespace
spec:
  schedule: &quot;*/1 * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          serviceaccountname: mariadbdumpsa
          containers:
          - name: kubectl
            image: garland/kubectl:1.10.4
            command:
            - /bin/sh
            - -c
            - kubectl get pods;echo 'ddd'
          restartpolicy: onfailure 

i create the cronjob on openshift by:
oc create -f .\cron.yaml

obtaining the following results
ps c:\users\mymachine&gt; oc create -f .\cron.yaml
cronjob.batch/mariadump created
ps c:\users\mymachine&gt; oc get cronjob -w
name        schedule      suspend   active   last schedule   age
mariadump   */1 * * * *   false     0        &lt;none&gt;          22s
mariadump   */1 * * * *   false     1        10s             40s
mariadump   */1 * * * *   false     0        20s             50s
ps c:\users\mymachine&gt; oc get pods -w
name                         ready   status       restarts   age


the cronjob does not start a pod, but if change to this cronjob(removing the serviceaccount)
apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: mariadump
  namespace: my-namespace
spec:
  schedule: &quot;*/1 * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          containers:
          - name: kubectl
            image: garland/kubectl:1.10.4
            command:
            - /bin/sh
            - -c
            - kubectl get pod;echo 'ddd'
          restartpolicy: onfailure 

it works as expected without having permissions.
ps c:\users\myuser&gt; oc get cronjob -w
name        schedule      suspend   active   last schedule   age
mariadump   */1 * * * *   false     0        &lt;none&gt;          8s
mariadump   */1 * * * *   false     1        3s              61s
ps c:\users\myuser&gt; oc get pods -w
name                         ready   status             restarts   age
mariadump-1616089500-mnfxs   0/1     crashloopbackoff   1          8s

ps c:\users\myuser&gt; oc logs mariadump-1616089500-mnfxs
error from server (forbidden): pods is forbidden: user &quot;system:serviceaccount:my-namespace:default&quot; cannot list resource &quot;pods&quot; in api group &quot;&quot; in the namespace &quot;my-namespace&quot;

for giving the cronjob the proper permissions i used this template to create the role, the rolebinding and the serviceaccount.
kind: role
apiversion: rbac.authorization.k8s.io/v1
metadata:
  namespace: my_namespace
  name: mariadbdump
rules:
- apigroups:
  - extensions
  - apps
  resources:
  - deployments
  - replicasets
  verbs:
  - 'patch'
  - 'get'

---
kind: rolebinding
apiversion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: mariadbdump
  namespace: my_namespace
subjects:
- kind: serviceaccount
  name: mariadbdumpsa
  namespace: my_namespace
roleref:
  kind: role
  name: mariadbdump
  apigroup: &quot;&quot;
  
---
apiversion: v1
kind: serviceaccount
metadata:
  name: mariadbdumpsa
  namespace: my_namespace


anyone can help me to know why the cronjob with the serviceaccount is not working?
thanks
",<kubernetes><cron><openshift><kubernetes-pod>,66777936,2,"with this yaml is actually working
kind: role
apiversion: rbac.authorization.k8s.io/v1
metadata:
  namespace: my-namespace
  name: mariadbdump
rules:
  - apigroups:
      - &quot;&quot;
      - ''
    resources:
      - deployments
      - replicasets
      - pods
      - pods/exec
    verbs:
      - 'watch'
      - 'get'
      - 'create'
      - 'list'
      
---
kind: rolebinding
apiversion: rbac.authorization.k8s.io/v1
metadata:
  name: mariadbdump
  namespace: my-namespace
subjects:
  - kind: serviceaccount
    name: mariadbdumpsa
    namespace: my-namespace
roleref:
  kind: role
  name: mariadbdump
  apigroup: &quot;&quot;
  
---
apiversion: v1
kind: serviceaccount
metadata:
  name: mariadbdumpsa
  namespace: my-namespace
---
apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: mariadump
  namespace: my-namespace
spec:
  schedule: &quot;*/1 * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          serviceaccountname: mariadbdumpsa
          containers:
          - name: kubectl
            image: garland/kubectl:1.10.4
            command:
            - /bin/sh
            - -c
            - kubectl exec $(kubectl get pods | grep running | grep 'mariadb' | awk '{print $1}') -- /opt/rh/rh-mariadb102/root/usr/bin/mysqldump --skip-lock-tables -h 127.0.0.1 -p 3306 -u userdb --password=userdbpass databasename &gt;/tmp/backup.sql;kubectl cp my-namespace/$(kubectl get pods | grep running | grep 'mariadbdump' | awk '{print $1}'):/tmp/backup.sql my-namespace/$(kubectl get pods | grep running | grep 'mariadb' | awk '{print $1}'):/tmp/backup.sql;echo 'backup done'
          restartpolicy: onfailure

"
66552585,kubernetes nginx ingress controller return 404,"following this guide, i created an ingress controller on my local kubernetes server, the only difference is that it is created as a nodeport.
i have done some test deployments, with respective services and everything works, here the file
deploy1:
apiversion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: deployment
metadata:
  name: helloworld1
spec:
  selector:
    matchlabels:
      app: helloworld1
  replicas: 1
  template:
    metadata:
      labels:
        app: helloworld1
    spec:
      containers:
      - name: hello
        image: gcr.io/google-samples/hello-app:1.0
        ports:
        - containerport: 8080

deploy2:
apiversion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: deployment
metadata:
  name: helloworld2
spec:
  selector:
    matchlabels:
      app: helloworld2
  replicas: 1
  template:
    metadata:
      labels:
        app: helloworld2
    spec:
      containers:
      - name: hello
        image: gcr.io/google-samples/hello-app:2.0
        ports:
        - containerport: 8080

deploy3:
apiversion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: deployment
metadata:
  name: geojson-example
spec:
  selector:
    matchlabels:
      app: geojson-example
  replicas: 1
  template:
    metadata:
      labels:
        app: geojson-example
    spec:
      containers:
        - name: geojson-container
          image: &quot;nmex87/geojsonexample:latest&quot;
          ports:
            - containerport: 8080

service1:
apiversion: v1
kind: service
metadata:
  name: helloworld1
spec:
#  type: nodeport
  ports:
  - port: 8080
  selector:
    app: helloworld1

service2:
apiversion: v1
kind: service
metadata:
  name: helloworld2
spec:
#  type: nodeport
  ports:
  - port: 8080
  selector:
    app: helloworld2

service3:
apiversion: v1
kind: service
metadata:
  name: geojson-example
spec:
  ports:
    - port: 8080
  selector:
    app: geojson-example

this is the ingress controller:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: test-ingress
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/default-backend: geojson-example
spec:
  rules:
    - http:
        paths:
          - path: /geo
            pathtype: prefix
            backend:
              service:
                name: geojson-example
                port:
                  number: 8080
          - path: /test1
            pathtype: prefix
            backend:
              service:
                name: helloworld1
                port:
                  number: 8080
          - path: /test2
            pathtype: prefix
            backend:
              service:
                name: helloworld2
                port:
                  number: 8080

when i do a get on myserver:myport/test1 or /test2 everything works, on /geo i get the following answer
{
    &quot;timestamp&quot;: &quot;2021-03-09t17:02:36.606+00:00&quot;,
    &quot;status&quot;: 404,
    &quot;error&quot;: &quot;not found&quot;,
    &quot;message&quot;: &quot;&quot;,
    &quot;path&quot;: &quot;/geo&quot;
}

why??
if i create a pod, and from inside the pod, i do a curl on geojson-example it works, but from the external, i obtain a 404 (i think by nginx ingress controller)
this is the log of nginx pod:
x.x.x.x - - [09/mar/2021:17:02:21 +0000] &quot;get /test1 http/1.1&quot; 200 68 &quot;-&quot; &quot;postmanruntime/7.26.8&quot; 234 0.006 [default-helloworld1-8080] [] 192.168.168.92:8080 68 0.008 200 

x.x.x.x - - [09/mar/2021:17:02:36 +0000] &quot;get /geo http/1.1&quot; 404 116 &quot;-&quot; &quot;postmanruntime/7.26.8&quot; 232 0.013 [default-geojson-example-8080] [] 192.168.168.109:8080 116 0.012 404 

what can i do?
",<nginx><kubernetes><kubernetes-ingress>,66553007,2,"as far the doc: this annotation is of the form nginx.ingress.kubernetes.io/default-backend: &lt;svc name&gt; to specify a custom default backend. this &lt;svc name&gt; is a reference to a service inside of the same namespace in which you are applying this annotation. this annotation overrides the global default backend.
this service will be handle the response when the service in the ingress rule does not have active endpoints.
you cannot use same service as default backend and also for a path. when you do this the path /geo became invalid. as we know default backend serves only the inactive endpoints. now if you tell that you want geojson-example as default backend(for inactive endpoints) again in the paths if you tell that use geojson-example for a valid path /geo then it became invalid as you are creating a deadlock type situation here.
you actually do not need to give this nginx.ingress.kubernetes.io/default-backend annotation.
your ingress should be like below without the default annotation, or you can use the annotation but in that case you need to remove geojson-example from using for any valid path in the paths, or need to use another service for the path /geo. options that you can use are given below:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: test-ingress
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
spec:
  rules:
    - http:
        paths:
          - path: /geo
            pathtype: prefix
            backend:
              service:
                name: geojson-example
                port:
                  number: 8080
          - path: /test1
            pathtype: prefix
            backend:
              service:
                name: helloworld1
                port:
                  number: 8080
          - path: /test2
            pathtype: prefix
            backend:
              service:
                name: helloworld2
                port:
                  number: 8080

or:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: test-ingress
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/default-backend: geojson-example
spec:
  rules:
    - http:
        paths:
          - path: /geo
            pathtype: prefix
            backend:
              service:
                name: &lt;any_other_service&gt;    # here use another service except `geojson-example`
                port:
                  number: 8080
          - path: /test1
            pathtype: prefix
            backend:
              service:
                name: helloworld1
                port:
                  number: 8080
          - path: /test2
            pathtype: prefix
            backend:
              service:
                name: helloworld2
                port:
                  number: 8080

or:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: test-ingress
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/default-backend: geojson-example
spec:
  rules:
    - http:
        paths:
          - path: /test1
            pathtype: prefix
            backend:
              service:
                name: helloworld1
                port:
                  number: 8080
          - path: /test2
            pathtype: prefix
            backend:
              service:
                name: helloworld2
                port:
                  number: 8080

"
66527084,why ingress rules are not followed? default backend is reached instead,"i have ha proxy ingress installed on kubernetes aks. i installed it using:
helm install ingress haproxy-ingress/haproxy-ingress

my ingress is this:
apiversion: networking.k8s.io/v1beta1
kind: ingress  
metadata:
  name: ravendb
  namespace: default
  labels:
    app: ravendb
  annotations:
    ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;
spec:
  rules:
  - host: a.raven.aedas-prev.inercya.com
    http:
      paths:
      - backend:
          servicename: ravendb-0
          serviceport: 443
        path: /
  - host: tcp-a.raven.aedas-prev.inercya.com
    http:
      paths:
      - backend:
          servicename: ravendb-0
          serviceport: 38888
        path: /
  - host: b.raven.aedas-prev.inercya.com
    http:
      paths:
      - backend:
          servicename: ravendb-1
          serviceport: 443
        path: /
  - host: tcp-b.raven.aedas-prev.inercya.com
    http:
      paths:
      - backend:
          servicename: ravendb-1
          serviceport: 38888
        path: /
  - host: c.raven.aedas-prev.inercya.com
    http:
      paths:
      - backend:
          servicename: ravendb-2
          serviceport: 443
        path: /
  - host: tcp-c.raven.aedas-prev.inercya.com
    http:
      paths:
      - backend:
          servicename: ravendb-2
          serviceport: 38888
        path: /

however when i point my browser to https://a.raven.aedas-prev.inercya.com i get the default backend. ha proxy doesn't reverse proxy the request to ravendb-0 service.
what i'm doing wrong? what can i do to make the ingress work?
pods are running:
haproxy-ingress-8548ff5ff4-9wmxv            1/1     running            0          137m
ingress-default-backend-b6f678779-9d88r     1/1     running            0          137m
ravendb-0                                   1/1     running            0          137m
ravendb-1                                   1/1     running            0          139m
ravendb-2                                   1/1     running            0          141m

and services are configured:
name                       type           cluster-ip     external-ip      port(s)                        age
haproxy-ingress            loadbalancer   10.0.166.252   xx.xx.xx.xx    443:30526/tcp,1936:32388/tcp   139m
ingress-default-backend    clusterip      10.0.102.165   &lt;none&gt;           8080/tcp                       139m
kubernetes                 clusterip      10.0.0.1       &lt;none&gt;           443/tcp                        412d
ravendb                    clusterip      none           &lt;none&gt;           443/tcp,38888/tcp,161/tcp      411d
ravendb-0                  clusterip      10.0.193.14    &lt;none&gt;           443/tcp,38888/tcp,161/tcp      411d
ravendb-1                  clusterip      10.0.156.73    &lt;none&gt;           443/tcp,38888/tcp,161/tcp      411d
ravendb-2                  clusterip      10.0.53.227    &lt;none&gt;           443/tcp,38888/tcp,161/tcp      411d

",<kubernetes><ravendb><kubernetes-ingress><azure-aks><haproxy-ingress>,66529942,2,"i finally figured out what i was missing. i added kubernetes.io/ingress.class: haproxy annotation and problem solved:
apiversion: networking.k8s.io/v1beta1
kind: ingress  
metadata:
  name: ravendb
  namespace: default
  labels:
    app: ravendb
  annotations:
    ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;
    kubernetes.io/ingress.class: haproxy
spec:
  rules:
  - host: a.raven.aedas-prev.inercya.com
    http:
      paths:
      - backend:
          servicename: ravendb-0
          serviceport: 443
        path: /
  - host: tcp-a.raven.aedas-prev.inercya.com
    http:
      paths:
      - backend:
          servicename: ravendb-0
          serviceport: 38888
        path: /
  - host: b.raven.aedas-prev.inercya.com
    http:
      paths:
      - backend:
          servicename: ravendb-1
          serviceport: 443
        path: /
  - host: tcp-b.raven.aedas-prev.inercya.com
    http:
      paths:
      - backend:
          servicename: ravendb-1
          serviceport: 38888
        path: /
  - host: c.raven.aedas-prev.inercya.com
    http:
      paths:
      - backend:
          servicename: ravendb-2
          serviceport: 443
        path: /
  - host: tcp-c.raven.aedas-prev.inercya.com
    http:
      paths:
      - backend:
          servicename: ravendb-2
          serviceport: 38888
        path: /

now haproxy ingress works as expected, reverse proxying external traffic to internal services.
"
66298747,how can i join a alb ingress group instead of overriding an existing one in eks?,"i am deploying k8s to aws eks cluster and use alb for the deployments. i'd like to use one alb for multiple services but i can't figure out how to share the same alb. every time i deploy a ingress it will override an existing one.
i have two config yaml file:
a.yaml
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: sample-ingress
  namespace: default
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/group.name: sample-ingress
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/group.order: '1'
spec:
  rules:
    - http:
        paths:
          - path: /sample-app/*
            backend:
              servicename: sample-entrypoint
              serviceport: 80

b.yaml
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: sample-ingress
  namespace: default
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/group.name: sample-ingress
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/group.order: '2'
spec:
  rules:
    - http:
        paths:
          - path: /sample-es/*
            backend:
              servicename: sample-es-entrypoint
              serviceport: 9200

i'd like both share the same alb so i specify the group name to be the same:
alb.ingress.kubernetes.io/group.name: sample-ingress
i also specify a different order in the two files.
but when i run kubectl apply -f a.yaml, it creates a alb with the rule i specified in the config file: /sample-app/*. but when i run kubectl apply -f b.yaml, it overrides the existing rule with /sample-es/*. so how can i make both share the same alb and allow them provide different rules?
",<amazon-web-services><kubernetes><amazon-eks>,66300916,2,"i guess you can create separate ingress and attach them to the same service configuration. point the service configuration with alb, and that should work. i have a configuration for internal-facing services, please see if this works for you.
apiversion: v1
kind: service
metadata:
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0
  labels:
    app.kubernetes.io/instance: goldendev-ingress-test
    app.kubernetes.io/managed-by: tiller
    app.kubernetes.io/name: ingress-test
    environment: dev
    helm.sh/chart: ingress-test
  name: ingress-test
  namespace: default
spec:
  externaltrafficpolicy: cluster
  ports:
  - name: http
    port: 80
    protocol: tcp
    targetport: 8080
  selector:
    app.kubernetes.io/instance: z1
    app.kubernetes.io/name: gunicorn
  sessionaffinity: none
  type: loadbalancer

ingress.yaml
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: sample-ingress
  namespace: default
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/group.name: sample-ingress
    alb.ingress.kubernetes.io/scheme: internal
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/group.order: '1'
spec:
  rules:
    - http:
        paths:
          - path: /mappings/v1/hello/*
            backend:
              servicename: ingress-test
              serviceport: 80
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: sample-ingress-1
  namespace: default
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/group.name: sample-ingress
    alb.ingress.kubernetes.io/scheme: internal
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/group.order: '2'
spec:
  rules:
    - http:
        paths:
          - path: /mappings/v1/teams/*
            backend:
              servicename: ingress-test-2
              serviceport: 80

i verified in the aws console, it has created only 1 load balancer with service configuration.
ingress list:
 kubectl get ingress
name               hosts   address   ports   age
sample-ingress     *                 80      19m
sample-ingress-1   *                 80      19m

let me know if this helps.
"
66197850,"in my helm template, why is pluck evaluating to a float64?","as a followup question to my post helm function to set value based on a variable?, and modifying the answer given in dynamically accessing values depending on variable values in a helm chart, i'm trying this
$ helm version --short
v3.5.2+g167aac7

values.yaml
-----------
env: sandbox
environments:
  sandbox: 0
  staging: 1
  production: 2
replicacount:
  - 1
  - 2
  - 4

templates/deployments.yaml
--------------------------
apiversion: apps/v1
kind: deployment
metadata:
...
spec:
  {{- if not .values.autoscaling.enabled }}
  replicas: {{ index .values.replicacount (pluck .values.env .values.environments | first | default .values.environments.sandbox) }}

but i get
$ helm template . --dry-run
error: template: guestbook/templates/deployment.yaml:10:15: executing &quot;guestbook/templates/deployment.yaml&quot; at &lt;index .values.replicacount (pluck .values.env .values.environments | first | default .values.environments.sandbox)&gt;: error calling index: cannot index slice/array with type float64

why is pluck returning a float64 instead of an integer, which i expect since my environments dictionary values are integers?
",<kubernetes><kubernetes-helm>,66201330,2,"if i do this, that is, pipe pluck with the int converter, it works, but it doesn't explain why pluck returns a float64 value.
templates/deployments.yaml
--------------------------
apiversion: apps/v1
kind: deployment
metadata:
...
spec:
  {{- if not .values.autoscaling.enabled }}
  replicas: {{ index .values.replicacount ((pluck .values.env .values.environments | first | default .values.environments.sandbox) | int) }}

update: it turns out to be a known bug. see https://github.com/kubernetes-sigs/yaml/issues/45
"
66410514,creating a kubernetes ingress pointing two services,"requirement: want to deploy minio and another backend service using an ingress with https (not for production purposes)
i have been trying to create an ingress to access two services externally from the kubernetes cluster in gke. these are the attempts i tried.
attempt one
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: lightning-ingress
  namespace: default
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
spec:
  rules:
  - http:
      paths:
        - path: /storage
          backend:
            servicename: minio
            serviceport: 9000
        - path: /portal
          backend:
            servicename: oscar
            serviceport: 8080

attempt two
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: oscar
  annotations:
    # nginx.ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - http:
      paths:
      - backend:
          servicename: oscar
          serviceport: 8080
  - host: storage.lightningfaas.tech
    http:
      paths:
      - backend:
          servicename: minio
          serviceport: 9000

attempt three
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: lightning-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    kubernetes.io/ingress.class: &quot;nginx&quot;
spec:
  rules:
  - http:
      paths:
      - backend:
          servicename: minio
          serviceport: 9000
        path: /minio(/|$)(.*)
      - backend:
          servicename: oscar
          serviceport: 8080
        path: /portal(/|$)(.*)

attempt four
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: minio-ingress
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
spec:
  rules:
  - host: minio.lightningfaas.tech
    http:
      paths:
      - backend:
          servicename: minio
          serviceport: 9000
  - host: portal.lightningfaas.tech
    http:
      paths:
      - backend:
          servicename: oscar
          serviceport: 8080

however, none of the above attempts suites for my requirement. either it gives a 404 0r a 503. but i can confirm that creating an individual ingress for each service works fine as below.
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: oscar
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - http:
      paths:
      - backend:
          servicename: oscar
          serviceport: 8080

apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: minio-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - http:
      paths:
      - backend:
          servicename: minio
          serviceport: 9000

changing domain servers takes a huge time to test as well, therefore creating hosts are very annoying since i have to wait a massive time to test my code. is there anything more that i can try?
something like below would be ideal:
https://34.452.234.45:9000 &gt; will access minio
https://34.452.234.45:8080 &gt; will access oscar
your suggestions and opinions will be really helpful for me.
minio helm chart: https://github.com/minio/charts
minio deployment
helm install --namespace oscar minio minio/minio --set accesskey=minio --set secretkey=password --set persistence.existingclaim=lightnig --set resources.requests.memory=256mi 

oscar helm chart: https://github.com/grycap/helm-charts/tree/master/oscar
oscar deployment
helm install --namespace=oscar oscar oscar --set authpass=password --set service.type=clusterip --set createingress=false --set volume.storageclassname=nfs --set minio.endpoint=http://104.197.173.174 --set minio.tlsverify=false --set minio.accesskey=minio --set minio.secretkey=password --set serverlessbackend=openfaas


",<kubernetes><google-kubernetes-engine><kubernetes-ingress><nginx-ingress><gke-networking>,66412333,2,"according to kubernetes doc, simple fan-out example should solve your problem.
a simple fan-out example is given below where same host has two different paths for two different services.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: simple-fanout-example
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /foo
        pathtype: prefix
        backend:
          service:
            name: service1
            port:
              number: 4200
      - path: /bar
        pathtype: prefix
        backend:
          service:
            name: service2
            port:
              number: 8080

so your manifest file might look like this:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
    name: lightning-ingress
    namespace: default
spec:
  rules:
  - host: [your host name here]
    http:
      paths:
      - path: /storage
        pathtype: prefix
        backend:
          service:
            name: minio
            port:
              number: 9000
      - path: /portal
        pathtype: prefix
        backend:
          service:
            name: oscar
            port:
              number: 8080

ref: kubernetes doc
"
66782739,how to replace the in-memory storage with persistent storage using kustomize,"i'm trying to replace the in-memory storage of grafana deployment with persistent storage using kustomize. what i'm trying to do is that i'm removing the in-memory storage and then mapping persistent storage. but when i'm deploying it then it is giving me an error.
error
the deployment &quot;grafana&quot; is invalid: spec.template.spec.containers[0].volumemounts[1].name: not found: &quot;grafana-storage&quot;
kustomize version
{version:kustomize/v4.0.5 gitcommit:9e8e7a7fe99ec9fbf801463e8607928322fc5245 builddate:2021-03-08t20:53:03z goos:linux goarch:amd64}
kustomization.yaml
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization

resources:
- https://github.com/prometheus-operator/kube-prometheus
- grafana-pvc.yaml
patchesstrategicmerge:
- grafana-patch.yaml


grafana-pvc.yaml
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: grafana-storage
  namespace: monitoring
  labels:
    billingtype: &quot;hourly&quot;
    region: sng01
    zone: sng01
spec:
  accessmodes:
    - readwritemany
  resources:
    requests:
      storage: 2gi
  storageclassname: ibmc-file-bronze

grafana-patch.yaml
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: grafana
  name: grafana
  namespace: monitoring
spec:
  template:
    spec:
      volumes:
        # use persistent storage for storing users instead of in-memory storage
        - $patch: delete  &lt;---- trying to remove the previous volume
          name: grafana-storage
        - name: grafana-storage
          persistentvolumeclaim:
            claimname: grafana-storage
      containers:
        - name: grafana
          volumemounts:
            - name: grafana-storage
              mountpath: /var/lib/grafana


please help.
",<kubernetes><kubernetes-pvc><kustomize>,66799699,2,"the $patch: delete doesn't seem to work as i would expect.
it may be nice to open an issue on kustomize github: https://github.com/kubernetes-sigs/kustomize/issues and ask developers about it.

although here is the patch i tried, and it seems to work:
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: grafana
  name: grafana
  namespace: monitoring
spec:
  template:
    spec:
      volumes:
        - name: grafana-storage
          emptydir: null
          persistentvolumeclaim:
            claimname: grafana-storage
      containers:
        - name: grafana
          volumemounts:
            - name: grafana-storage
              mountpath: /var/lib/grafana


based on https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/add-new-patchstrategy-to-clear-fields-not-present-in-patch.md
the following should also work in theory:
spec:
  volumes:
    - $retainkeys:
      - name
      - persistentvolumeclaim
      name: grafana-storage
      persistentvolumeclaim:
        claimname: grafana-storage

but in practise it doesn't, and i think that's because kustomize has its own implementaions of strategic merge (different that k8s).
"
67521033,determine service account for airflow in kubernetes,"i am running airflow in kubernetes
one pod, 2 containers - webserver and scheduler.
kubernetesexecutor in configs
but due to organizational settings, the scheduler with the default service account can't work, not enough roles. i can't change this setting
{&quot;kind&quot;:&quot;status&quot;,&quot;apiversion&quot;:&quot;v1&quot;,&quot;metadata&quot;:{},&quot;status&quot;:&quot;failure&quot;,&quot;message&quot;:&quot;pods is forbidden: user \&quot;system:serviceaccount:&lt;account_name&gt;:default\&quot; cannot list resource \&quot;pods\&quot; in api group \&quot;\&quot; in the namespace \&quot;&lt;namespace_name&gt;\&quot;&quot;,&quot;reason&quot;:&quot;forbidden&quot;,&quot;details&quot;:{&quot;kind&quot;:&quot;pods&quot;},&quot;code&quot;:403}

so i created service account with the needed roles, rolebinging and etc. how can i set airflow to run the scheduler with that sa?
",<kubernetes><airflow><google-kubernetes-engine><airflow-scheduler><kubernetes-operator>,67522207,2,"you can specify the desired sa to use in your pod spec as discussed in the link below:
https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
apiversion: v1
kind: pod
metadata:
  name: my-pod
spec:
  serviceaccountname: build-robot
  automountserviceaccounttoken: false
  ...

"
67315285,routing issues with nginx ingress controller and basic django app,"edit: i rewrote the question with a fully reproducible example.
i have a sample django app with the base getting started https://docs.djangoproject.com/en/3.2/intro/tutorial01/, that is, there are two paths /admin and /polls.
if i deploy the app using nodeport, i can access both paths without issues. however, i haven't managed to do the same with nginx ingress controller. i have tried several combinations of annotations and paths to no avail.
so, assume this yaml:
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: django
  name: django
spec:
  selector:
    matchlabels:
      app: django
  template:
    metadata:
      labels:
        app: django
    spec:
      containers:
      - image: &quot;ay0o/django-ingress:latest&quot;
        name: django
# ---
# apiversion: v1
# kind: service
# metadata:
#   name: django
# spec:
#   ports:
#   - nodeport: 31000
#     port: 8000
#     targetport: 8000
#   selector:
#     app: django
#   type: nodeport
---
apiversion: v1
kind: service
metadata:
  name: django
spec:
  ports:
  - port: 8000
  selector:
    app: django
---
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: django
spec:
  rules:
  - http:
      paths:
      - path: /django
        pathtype: prefix
        backend:
          service:
            name: django
            port:
              number: 8000


when i use nodeport, http://localhost:31000/admin and http://localhost:31000/polls work fine.
when i use ingress, http://localhost/django returns a 404 from django (because the path is neither admin nor polls), but http://localhost/django/admin and http://localhost/django/polls return a 404 from nginx, meaning the ingress is not properly routing.
so, what should i change so that http://localhost/django/admin and http://localhost/django/polls will not return 404?
",<django><kubernetes><kubernetes-ingress><nginx-ingress>,67335154,2,"i have come to the conclusion that it's not possible to use path-based ingress for a django app, probably due to something related to django's internals.
every example out there uses host-based rules, and indeed, that just work. for example, the ingress above can be changed to the following, and it will work.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: django
spec:
  rules:
  - host: localhost
    http:
      paths:
      - pathtype: prefix
        path: &quot;/&quot;
        backend:
          service:
            name: django
            port:
              number: 8000

if anyone come up with a solution using path-based routing, feel free to answer the question.
"
67246684,"wrong fs type, bad option, bad superblock on /dev/nvme1n1, missing codepage or helper program, or other error","i am trying to deploy mongodb to my kubernetes cluster. it automatically creates a pvc and pv based on the storage class name i specify. however the pod is stuck on containercreating because of the following error:

mountvolume.mountdevice failed for volume &quot;pvc-f88bdca6-7794-455a-872f-8230f1ce295d&quot; : mount failed: exit status 32 mounting command: systemd-run mounting arguments: --description=kubernetes transient mount for /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03 --scope -- mount -t xfs -o debug,defaults /dev/xvdbq /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03 output: running scope as unit run-4113.scope. mount: /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03: wrong fs type, bad option, bad superblock on /dev/nvme1n1, missing codepage or helper program, or other error.

i'm not sure what to do as this is pretty consistant no matter how many times i uninstall and resinstall the helm chart.
kubectl version
client version: version.info{major:&quot;1&quot;, minor:&quot;20&quot;, gitversion:&quot;v1.20.4&quot;, gitcommit:&quot;e87da0bd6e03ec3fea7933c4b5263d151aafd07c&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2021-02-18t16:12:00z&quot;, goversion:&quot;go1.15.8&quot;, compiler:&quot;gc&quot;, platform:&quot;linux/amd64&quot;}
server version: version.info{major:&quot;1&quot;, minor:&quot;19+&quot;, gitversion:&quot;v1.19.6-eks-49a6c0&quot;, gitcommit:&quot;49a6c0bf091506e7bafcdb1b142351b69363355a&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2020-12-23t22:10:21z&quot;, goversion:&quot;go1.15.5&quot;, compiler:&quot;gc&quot;, platform:&quot;linux/amd64&quot;}

storage class
apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  name: mongodbstorage
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  fstype: xfs
reclaimpolicy: retain
allowvolumeexpansion: true
mountoptions:
  - debug
volumebindingmode: waitforfirstconsumer

kubectl describe pod mongodb-prod-0 -n mongodb
name:           mongodb-prod-0
namespace:      mongodb
priority:       0
node:           ip-10-0-4-244.us-east-2.compute.internal/10.0.4.244
start time:     sat, 24 apr 2021 20:03:06 +0100
labels:         app.kubernetes.io/component=mongodb
                app.kubernetes.io/instance=mongodb-prod
                app.kubernetes.io/managed-by=helm
                app.kubernetes.io/name=mongodb
                controller-revision-hash=mongodb-prod-58c557d4fc
                helm.sh/chart=mongodb-10.12.5
                statefulset.kubernetes.io/pod-name=mongodb-prod-0
annotations:    kubernetes.io/psp: eks.privileged
status:         pending
ip:             
ips:            &lt;none&gt;
controlled by:  statefulset/mongodb-prod
containers:
  mongodb:
    container id:  
    image:         docker.io/bitnami/mongodb:4.4.5-debian-10-r0
    image id:      
    port:          27017/tcp
    host port:     0/tcp
    command:
      /scripts/setup.sh
    state:          waiting
      reason:       containercreating
    ready:          false
    restart count:  0
    liveness:       exec [mongo --disableimplicitsessions --eval db.admincommand('ping')] delay=30s timeout=5s period=10s #success=1 #failure=6
    readiness:      exec [bash -ec mongo --disableimplicitsessions $tls_options --eval 'db.hello().iswritableprimary || db.hello().secondary' | grep -q 'true'
] delay=5s timeout=5s period=10s #success=1 #failure=6
    environment:
      bitnami_debug:                    false
      my_pod_name:                      mongodb-prod-0 (v1:metadata.name)
      my_pod_namespace:                 mongodb (v1:metadata.namespace)
      k8s_service_name:                 mongodb-prod-headless
      mongodb_initial_primary_host:     mongodb-prod-0.$(k8s_service_name).$(my_pod_namespace).svc.cluster.local
      mongodb_replica_set_name:         rs0
      mongodb_root_password:            &lt;set to the key 'mongodb-root-password' in secret 'mongodb-prod'&gt;    optional: false
      mongodb_replica_set_key:          &lt;set to the key 'mongodb-replica-set-key' in secret 'mongodb-prod'&gt;  optional: false
      allow_empty_password:             no
      mongodb_system_log_verbosity:     0
      mongodb_disable_system_log:       no
      mongodb_disable_javascript:       no
      mongodb_enable_ipv6:              no
      mongodb_enable_directory_per_db:  no
    mounts:
      /bitnami/mongodb from datadir (rw)
      /scripts/setup.sh from scripts (rw,path=&quot;setup.sh&quot;)
      /var/run/secrets/kubernetes.io/serviceaccount from mongodb-prod-token-4kjjm (ro)
conditions:
  type              status
  initialized       true 
  ready             false 
  containersready   false 
  podscheduled      true 
volumes:
  datadir:
    type:       persistentvolumeclaim (a reference to a persistentvolumeclaim in the same namespace)
    claimname:  datadir-mongodb-prod-0
    readonly:   false
  scripts:
    type:      configmap (a volume populated by a configmap)
    name:      mongodb-prod-scripts
    optional:  false
  mongodb-prod-token-4kjjm:
    type:        secret (a volume populated by a secret)
    secretname:  mongodb-prod-token-4kjjm
    optional:    false
qos class:       besteffort
node-selectors:  geeiq/node-type=ops
tolerations:     node.kubernetes.io/not-ready:noexecute op=exists for 300s
                 node.kubernetes.io/unreachable:noexecute op=exists for 300s
events:
  type     reason                  age   from                     message
  ----     ------                  ----  ----                     -------
  normal   scheduled               18m   default-scheduler        successfully assigned mongodb/mongodb-prod-0 to ip-10-0-4-244.us-east-2.compute.internal
  normal   successfulattachvolume  18m   attachdetach-controller  attachvolume.attach succeeded for volume &quot;pvc-f88bdca6-7794-455a-872f-8230f1ce295d&quot;
  warning  failedmount             18m   kubelet                  mountvolume.mountdevice failed for volume &quot;pvc-f88bdca6-7794-455a-872f-8230f1ce295d&quot; : mount failed: exit status 32
mounting command: systemd-run
mounting arguments: --description=kubernetes transient mount for /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03 --scope -- mount -t xfs -o debug,defaults /dev/xvdbq /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03
output: running scope as unit run-4113.scope.
mount: /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03: wrong fs type, bad option, bad superblock on /dev/nvme1n1, missing codepage or helper program, or other error.
  warning  failedmount  18m  kubelet  mountvolume.mountdevice failed for volume &quot;pvc-f88bdca6-7794-455a-872f-8230f1ce295d&quot; : mount failed: exit status 32
mounting command: systemd-run
mounting arguments: --description=kubernetes transient mount for /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03 --scope -- mount -t xfs -o debug,defaults /dev/xvdbq /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03
output: running scope as unit run-4182.scope.
mount: /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03: wrong fs type, bad option, bad superblock on /dev/nvme1n1, missing codepage or helper program, or other error.
  warning  failedmount  18m  kubelet  mountvolume.mountdevice failed for volume &quot;pvc-f88bdca6-7794-455a-872f-8230f1ce295d&quot; : mount failed: exit status 32
mounting command: systemd-run
mounting arguments: --description=kubernetes transient mount for /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03 --scope -- mount -t xfs -o debug,defaults /dev/xvdbq /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03
output: running scope as unit run-4256.scope.
mount: /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03: wrong fs type, bad option, bad superblock on /dev/nvme1n1, missing codepage or helper program, or other error.
  warning  failedmount  18m  kubelet  mountvolume.mountdevice failed for volume &quot;pvc-f88bdca6-7794-455a-872f-8230f1ce295d&quot; : mount failed: exit status 32
mounting command: systemd-run
mounting arguments: --description=kubernetes transient mount for /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03 --scope -- mount -t xfs -o debug,defaults /dev/xvdbq /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03
output: running scope as unit run-4297.scope.
mount: /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03: wrong fs type, bad option, bad superblock on /dev/nvme1n1, missing codepage or helper program, or other error.
  warning  failedmount  18m  kubelet  mountvolume.mountdevice failed for volume &quot;pvc-f88bdca6-7794-455a-872f-8230f1ce295d&quot; : mount failed: exit status 32
mounting command: systemd-run
mounting arguments: --description=kubernetes transient mount for /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03 --scope -- mount -t xfs -o debug,defaults /dev/xvdbq /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03
output: running scope as unit run-4458.scope.
mount: /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03: wrong fs type, bad option, bad superblock on /dev/nvme1n1, missing codepage or helper program, or other error.
  warning  failedmount  18m  kubelet  mountvolume.mountdevice failed for volume &quot;pvc-f88bdca6-7794-455a-872f-8230f1ce295d&quot; : mount failed: exit status 32
mounting command: systemd-run
mounting arguments: --description=kubernetes transient mount for /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03 --scope -- mount -t xfs -o debug,defaults /dev/xvdbq /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03
output: running scope as unit run-4562.scope.
mount: /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03: wrong fs type, bad option, bad superblock on /dev/nvme1n1, missing codepage or helper program, or other error.
  warning  failedmount  17m  kubelet  mountvolume.mountdevice failed for volume &quot;pvc-f88bdca6-7794-455a-872f-8230f1ce295d&quot; : mount failed: exit status 32
mounting command: systemd-run
mounting arguments: --description=kubernetes transient mount for /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03 --scope -- mount -t xfs -o debug,defaults /dev/xvdbq /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03
output: running scope as unit run-4835.scope.
mount: /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03: wrong fs type, bad option, bad superblock on /dev/nvme1n1, missing codepage or helper program, or other error.
  warning  failedmount  17m  kubelet  mountvolume.mountdevice failed for volume &quot;pvc-f88bdca6-7794-455a-872f-8230f1ce295d&quot; : mount failed: exit status 32
mounting command: systemd-run
mounting arguments: --description=kubernetes transient mount for /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03 --scope -- mount -t xfs -o debug,defaults /dev/xvdbq /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03
output: running scope as unit run-5281.scope.
mount: /var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-2a/vol-087b3e95d1aa21e03: wrong fs type, bad option, bad superblock on /dev/nvme1n1, missing codepage or helper program, or other error.
  warning  failedmount  5m30s (x2 over 16m)  kubelet  unable to attach or mount volumes: unmounted volumes=[datadir], unattached volumes=[scripts mongodb-prod-token-4kjjm datadir]: timed out waiting for the condition
  warning  failedmount  3m12s (x3 over 10m)  kubelet  unable to attach or mount volumes: unmounted volumes=[datadir], unattached volumes=[datadir scripts mongodb-prod-token-4kjjm]: timed out waiting for the condition
  warning  failedmount  56s (x11 over 16m)   kubelet  (combined from similar events): unable to attach or mount volumes: unmounted volumes=[datadir], unattached volumes=[datadir scripts mongodb-prod-token-4kjjm]: timed out waiting for the condition

kubectl get pods,svc,pvc,pv -o wide --namespace mongodb
name                         ready   status              restarts   age   ip           node                                       nominated node   readiness gates
pod/mongodb-prod-0           0/1     containercreating   0          20m   &lt;none&gt;       ip-10-0-4-244.us-east-2.compute.internal   &lt;none&gt;           &lt;none&gt;
pod/mongodb-prod-arbiter-0   1/1     running             5          20m   10.0.4.132   ip-10-0-4-244.us-east-2.compute.internal   &lt;none&gt;           &lt;none&gt;

name                                    type        cluster-ip      external-ip   port(s)           age   selector
service/mongodb-prod-0-external         nodeport    172.20.91.18    &lt;none&gt;        27017:30001/tcp   20m   app.kubernetes.io/component=mongodb,app.kubernetes.io/instance=mongodb-prod,app.kubernetes.io/name=mongodb,statefulset.kubernetes.io/pod-name=mongodb-prod-0
service/mongodb-prod-1-external         nodeport    172.20.202.43   &lt;none&gt;        27017:30002/tcp   20m   app.kubernetes.io/component=mongodb,app.kubernetes.io/instance=mongodb-prod,app.kubernetes.io/name=mongodb,statefulset.kubernetes.io/pod-name=mongodb-prod-1
service/mongodb-prod-arbiter-headless   clusterip   none            &lt;none&gt;        27017/tcp         20m   app.kubernetes.io/component=arbiter,app.kubernetes.io/instance=mongodb-prod,app.kubernetes.io/name=mongodb
service/mongodb-prod-headless           clusterip   none            &lt;none&gt;        27017/tcp         20m   app.kubernetes.io/component=mongodb,app.kubernetes.io/instance=mongodb-prod,app.kubernetes.io/name=mongodb

name                                           status   volume                                     capacity   access modes   storageclass     age   volumemode
persistentvolumeclaim/datadir-mongodb-prod-0   bound    pvc-f88bdca6-7794-455a-872f-8230f1ce295d   100gi      rwo            mongodbstorage   20m   filesystem

name                                                        capacity   access modes   reclaim policy   status   claim                            storageclass     reason   age   volumemode
persistentvolume/pvc-f88bdca6-7794-455a-872f-8230f1ce295d   100gi      rwo            retain           bound    mongodb/datadir-mongodb-prod-0   mongodbstorage            20m   filesystem

update:
kubectl describe pv pvc-30f3ca78-134b-4b4d-bac9-385a71a6f7e0
name:              pvc-30f3ca78-134b-4b4d-bac9-385a71a6f7e0
labels:            failure-domain.beta.kubernetes.io/region=us-east-2
                   failure-domain.beta.kubernetes.io/zone=us-east-2c
annotations:       kubernetes.io/createdby: aws-ebs-dynamic-provisioner
                   pv.kubernetes.io/bound-by-controller: yes
                   pv.kubernetes.io/provisioned-by: kubernetes.io/aws-ebs
finalizers:        [kubernetes.io/pv-protection]
storageclass:      mongodbstorage
status:            bound
claim:             mongodb/datadir-mongodb-prod-0
reclaim policy:    retain
access modes:      rwo
volumemode:        filesystem
capacity:          100gi
node affinity:     
  required terms:  
    term 0:        failure-domain.beta.kubernetes.io/zone in [us-east-2c]
                   failure-domain.beta.kubernetes.io/region in [us-east-2]
message:           
source:
    type:       awselasticblockstore (a persistent disk resource in aws)
    volumeid:   aws://us-east-2c/vol-08aebae8e0d675c4d
    fstype:     xfs
    partition:  0
    readonly:   false
events:         &lt;none&gt;


",<kubernetes><amazon-ebs><kubernetes-pvc>,67251734,2,"i found what the problem was, once i removed the mount options from the storage class and recreated it, it mounted properly.
mountoptions:
 - debug

apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  name: mongodbstorage
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  fstype: xfs
reclaimpolicy: retain
allowvolumeexpansion: true
volumebindingmode: waitforfirstconsumer

"
67178086,two tls certificate for two nginx ingress controller azure k8s cluster,"i have two ingress controller one with default class nginx in default namespace, while the second ingress controller has a nginx class: nginx-devices.
cert-manager is already installed using helm.
i managed to get tls certificate from lets encrypt for the first controller, using clusterissuer and ingress resource rules for routing ingress.

apiversion: cert-manager.io/v1alpha2
kind: clusterissuer
metadata:
  # name: letsencrypt-staging
  name: letsencrypt-prod
spec:
  acme:
    email: xx
    # server: https://acme-staging-v02.api.letsencrypt.org/directory
    server: https://acme-v02.api.letsencrypt.org/directory
    privatekeysecretref:
      # name: letsencrypt-staging
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          class: nginx

ingress routing:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: servicea-ingress-rules
  namespace: default
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: &quot;letsencrypt-prod&quot;
    ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
  - hosts:
    - firstservice.cloudapp.azure.com
    secretname: tls-secret
  rules:
  - host: firstservice.cloudapp.azure.com
    http:
      paths:
      - path: /servicea
        backend:
          servicename: servicea
          serviceport: 80

however, for creating the second tls certificate for the second ingress controller, the tls secret is not created
clusterissuer
# k8s/cluster-issuer.yaml

apiversion: cert-manager.io/v1alpha2
kind: clusterissuer
metadata:
  # name: letsencrypt-staging
  name: letsencrypt-prod-devices
  namespace: ingress-nginx-devices # namespace where the second ingress controller is installed
spec:
  acme:
    email: xxx
    # server: https://acme-staging-v02.api.letsencrypt.org/directory
    server: https://acme-v02.api.letsencrypt.org/directory
    privatekeysecretref:
      # name: letsencrypt-staging
      name: letsencrypt-prod-devices
    solvers:
    - http01:
        ingress:
          class: nginx-devices # ingress class of the second ingress controller


ingress routing
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: devices-ingress-rules
  namespace: default # since all the services are in default namespace
  annotations:
    kubernetes.io/ingress.class: nginx-devices # ingress class of the second ingress controller
    cert-manager.io/cluster-issuer: &quot;letsencrypt-prod-devices&quot; 
    ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
  - hosts:
    - secondservice.cloudapp.azure.com
    secretname: tls-secret
  rules:
  - host: secondservice.cloudapp.azure.com
    http:
      paths:
      - path: /serviceb
        backend:
          servicename: serviceb
          serviceport: 80

by looking at the secret i can only see: kubectl get secrets -n ingress-nginx-devices
name                                          type                                  data   age
default-token-xzp95                           kubernetes.io/service-account-token   3      92m
nginx-ingress-devices-backend-token-pd4vf     kubernetes.io/service-account-token   3      64m
nginx-ingress-devices-token-qvvps             kubernetes.io/service-account-token   3      64m
sh.helm.release.v1.nginx-ingress-devices.v1   helm.sh/release.v1                    1      64m

while in default namespace:
tls-secret                                          kubernetes.io/tls                     2      134m

why the second tls-secret is not being generated ? what could go wrong here ?
any help is appreciated :)
",<kubernetes><ssl-certificate><kubernetes-ingress><nginx-ingress><kubernetes-secrets>,67178626,2,"your second cluster issuer namespace is : ingress-nginx-devices ideally it should be in the default namespace as your ingress is in the default namespace.
keep these three in same namespace :

ingress
clusterissuer
service

if everything will work well you will see the secret in default namespace
also in your yaml of clusterissuer
privatekeysecretref:
      # name: letsencrypt-staging
      name: letsencrypt-prod-devices

name of your secret is : letsencrypt-prod-devices
but in ingress it is : tls-secret
keep it same otherwise wont work
here sharing a full example of clusterissuer and ingress keep in the same namespace. you can change the secret name, clusterissuer name as per need. clusterissuer will create the secret automatically just give prover names of secret &amp; clusterissuer in ingress (matching).
apiversion: cert-manager.io/v1alpha2
kind: clusterissuer
metadata:
  name: cluster-issuer-name
  namespace: development
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: harsh@example.com
    privatekeysecretref:
      name: secret-name
    solvers:
    - http01:
        ingress:
          class: nginx-class-name
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx-class-name
    cert-manager.io/cluster-issuer: cluster-issuer-name
    nginx.ingress.kubernetes.io/rewrite-target: /
  name: example-ingress
spec:
  rules:
  - host: sub.example.com
    http:
      paths:
      - path: /api
        backend:
          servicename: service-name
          serviceport: 80
  tls:
  - hosts:
    - sub.example.com
    secretname: secret-name

"
67156127,kubernetes secret is not stored in encoded format in environment variables,"i am a beginner to kubernetes. i have created a secret file and referred it in deployment yaml file.
app-secret.yaml
apiversion: v1
kind: secret
metadata:
  name: app-secret
data:
  username: ywrtaw4=
  password: ywrtaw4=

deploy.yaml
env:
          - name: deploy_env
            value: ${env}
          - name: namespace_name
            valuefrom:
                fieldref:
                  fieldpath : metadata.namespace
          - name: app_username
            valuefrom:
                secretkeyref:
                  name: app-secret
                  key: username
          - name: app_password
            valuefrom:
                secretkeyref:
                  name: app-secret
                  key: password

while using the command kubectl get secret pod-54rfxd -n dev-ns -o json, it is printing the username and password in encoded format only. when i query for the environment variables list using the command kubectl exec pod-54rfxd -n dev-ns -- printenv, it was giving below result.
app_username=admin
app_password=admin

why it was not in encoded format in environment variables. could you please let me know the reason and is it possible to have it in encoded format?
",<kubernetes><kubernetes-secrets><kubernetes-security>,72971082,2,"you could use the stringdata format:
apiversion: v1
kind: secret
metadata:
  name: app-secret
stringdata:
  username: &quot;ywrtaw4=&quot;
  password: &quot;ywrtaw4=&quot;

from k8s doc:

k8s doc
"
67148209,kubernetess multiple deployments using one code base but different configuration (environement variables),"i have a project where we are consuming data from kafka and publishing to mongo. in fact the code base does only one task, may be mongo to kafka migration, kafka to mongo migration or something else.
we have to consume from different kafka topics and publish to different mongo collections. now these are parallel streams of work.
current design is to have one codebase which can consume from any topic and publish to any mongo collection which is configurable using environment variables. so we created one kubernetes pod and have multiple containers inside it. each container has different environment variables.
my questions:

is it wise to use multiple containers in one pod. easy to distinguish, but as they are tightly coupled , i am guessing high chance of failure and not actually proper microservice design.
should i create multiple deployments for each of these pipelines ? would be very difficult to maintain as each will have different deployment configs.
is there any better way to address this ?

sample of step 1:
apiversion: apps/v1
kind: deployment
metadata:
  annotations: {}
  name: test-raw-mongodb-sink-apps
  namespace: test-apps
spec:
  selector:
    matchlabels:
      app: test-raw-mongodb-sink-apps
  template:
    metadata:
      labels:
        app: test-raw-mongodb-sink-apps
    spec:
      containers:
      - env:
        - name: events_topic
          value: test.ops.proc-events
        - name: group_id
          value: test-mongodb-sink-alchemy
        - name: input_topic
          value: test.raw.ptv.alchemy
        - name: mongodb_auth_db
          value: admin
        - name: mongodb_host0
          value: test-mongodb-0.test-mongodb-headless.test-infra
        - name: mongodb_host1
          value: test-mongodb-1.test-mongodb-headless.test-infra
        - name: mongodb_port
          value: &quot;27017&quot;
        - name: mongodb_password
          value: test123
        - name: mongodb_username
          value: root
        - name: server_port
          value: &quot;8081&quot;
        - name: kafka_brokers
          value: kafka-cluster-kafka-bootstrap.kafka:9093
        - name: truststore_password
          valuefrom:
            secretkeyref:
              key: ca.password
              name: kafka-ca-cert
        - name: keystore_password
          valuefrom:
            secretkeyref:
              key: user.password
              name: kafka
        image: tools.testcompany.co.za:8093/dpl/tt--mongodb-map:0.0.7.0-snapshot
        name: test-mongodb-sink-alchemy
        securitycontext:
          allowprivilegeescalation: true
          privileged: true
        volumemounts:
        - mountpath: /app/resources
          name: properties
        - mountpath: /stores
          name: stores
          readonly: true
      - env:
        - name: events_topic
          value: test.ops.proc-events
        - name: group_id
          value: test-mongodb-sink-bloomberg
        - name: input_topic
          value: test.raw.pretrade.bloomberg
        - name: mongodb_auth_db
          value: admin
        - name: mongodb_host0
          value: test-mongodb-0.test-mongodb-headless.test-infra
        - name: mongodb_host1
          value: test-mongodb-1.test-mongodb-headless.test-infra
        - name: mongodb_password
          value: test123
        - name: mongodb_port
          value: &quot;27017&quot;
        - name: mongodb_username
          value: root
        - name: server_port
          value: &quot;8082&quot;
        - name: kafka_brokers
          value: kafka-cluster-kafka-bootstrap.kafka:9093
        - name: truststore_password
          valuefrom:
            secretkeyref:
              key: ca.password
              name: kafka-ca-cert
        - name: keystore_password
          valuefrom:
            secretkeyref:
              key: user.password
              name: kafka
        image: tools.testcompany.co.za:8093/local/tt--mongodb-map:0.0.7.0-snapshot
        name: test-mongodb-sink-bloomberg
        securitycontext:
          allowprivilegeescalation: true
          privileged: true
        volumemounts:
        - mountpath: /app/resources
          name: properties
        - mountpath: /stores
          name: stores
          readonly: true
      - env:
        - name: events_topic
          value: test.ops.proc-events
        - name: group_id
          value: test-mongodb-sink-calypso
        - name: input_topic
          value: test.raw.ptv.calypso
        - name: mongodb_auth_db
          value: admin
        - name: mongodb_host0
          value: test-mongodb-0.test-mongodb-headless.test-infra
        - name: mongodb_host1
          value: test-mongodb-1.test-mongodb-headless.test-infra
        - name: mongodb_password
          value: test123
        - name: mongodb_port
          value: &quot;27017&quot;
        - name: mongodb_username
          value: root
        - name: server_port
          value: &quot;8083&quot;
        - name: kafka_brokers
          value: kafka-cluster-kafka-bootstrap.kafka:9093
        - name: truststore_password
          valuefrom:
            secretkeyref:
              key: ca.password
              name: kafka-ca-cert
        - name: keystore_password
          valuefrom:
            secretkeyref:
              key: user.password
              name: kafka
        image: tools.testcompany.co.za:8093/local/tt--mongodb-map:0.0.7.0-snapshot
        name: test-mongodb-sink-calypso
        securitycontext:
          allowprivilegeescalation: true
          privileged: true
        volumemounts:
        - mountpath: /app/resources
          name: properties
        - mountpath: /stores
          name: stores
          readonly: true
      - env:
        - name: events_topic
          value: test.ops.proc-events
        - name: group_id
          value: test-mongodb-sink-dtres
        - name: input_topic
          value: test.raw.ptv.dtres
        - name: mongodb_auth_db
          value: admin
        - name: mongodb_host0
          value: test-mongodb-0.test-mongodb-headless.test-infra
        - name: mongodb_host1
          value: test-mongodb-1.test-mongodb-headless.test-infra
        - name: mongodb_password
          value: test123
        - name: mongodb_port
          value: &quot;27017&quot;
        - name: mongodb_username
          value: root
        - name: server_port
          value: &quot;8084&quot;
        - name: kafka_brokers
          value: kafka-cluster-kafka-bootstrap.kafka:9093
        - name: truststore_password
          valuefrom:
            secretkeyref:
              key: ca.password
              name: kafka-ca-cert
        - name: keystore_password
          valuefrom:
            secretkeyref:
              key: user.password
              name: kafka
        image: tools.testcompany.co.za:8093/local/tt--mongodb-map:0.0.7.0-snapshot
        name: test-mongodb-sink-dtres
        securitycontext:
          allowprivilegeescalation: true
          privileged: true
        volumemounts:
        - mountpath: /app/resources
          name: properties
        - mountpath: /stores
          name: stores
          readonly: true
      - env:
        - name: events_topic
          value: test.ops.proc-events
        - name: group_id
          value: test-mongodb-sink-feds
        - name: input_topic
          value: test.raw.ptv.feds
        - name: mongodb_auth_db
          value: admin
        - name: mongodb_host0
          value: test-mongodb-0.test-mongodb-headless.test-infra
        - name: mongodb_host1
          value: test-mongodb-1.test-mongodb-headless.test-infra
        - name: mongodb_password
          value: test123
        - name: mongodb_port
          value: &quot;27017&quot;
        - name: mongodb_username
          value: root
        - name: server_port
          value: &quot;8085&quot;
        - name: kafka_brokers
          value: kafka-cluster-kafka-bootstrap.kafka:9093
        - name: truststore_password
          valuefrom:
            secretkeyref:
              key: ca.password
              name: kafka-ca-cert
        - name: keystore_password
          valuefrom:
            secretkeyref:
              key: user.password
              name: kafka
        image: tools.testcompany.co.za:8093/local/tt--mongodb-map:0.0.7.0-snapshot
        name: test-mongodb-sink-feds
        securitycontext:
          allowprivilegeescalation: true
          privileged: true
        volumemounts:
        - mountpath: /app/resources
          name: properties
        - mountpath: /stores
          name: stores
          readonly: true
      - env:
        - name: events_topic
          value: test.ops.proc-events
        - name: group_id
          value: test-mongodb-sink-hoops
        - name: input_topic
          value: test.raw.ptv.hoops
        - name: mongodb_auth_db
          value: admin
        - name: mongodb_host0
          value: test-mongodb-0.test-mongodb-headless.test-infra
        - name: mongodb_host1
          value: test-mongodb-1.test-mongodb-headless.test-infra
        - name: mongodb_password
          value: test123
        - name: mongodb_port
          value: &quot;27017&quot;
        - name: mongodb_username
          value: root
        - name: server_port
          value: &quot;8086&quot;
        - name: kafka_brokers
          value: kafka-cluster-kafka-bootstrap.kafka:9093
        - name: truststore_password
          valuefrom:
            secretkeyref:
              key: ca.password
              name: kafka-ca-cert
        - name: keystore_password
          valuefrom:
            secretkeyref:
              key: user.password
              name: kafka
        image: tools.testcompany.co.za:8093/local/tt--mongodb-map:0.0.7.0-snapshot
        name: test-mongodb-sink-hoops
        securitycontext:
          allowprivilegeescalation: true
          privileged: true
        volumemounts:
        - mountpath: /app/resources
          name: properties
        - mountpath: /stores
          name: stores
          readonly: true
      - env:
        - name: events_topic
          value: test.ops.proc-events
        - name: group_id
          value: test-mongodb-sink-mxcore
        - name: input_topic
          value: test.raw.ptv.murex_core
        - name: mongodb_auth_db
          value: admin
        - name: mongodb_host0
          value: test-mongodb-0.test-mongodb-headless.test-infra
        - name: mongodb_host1
          value: test-mongodb-1.test-mongodb-headless.test-infra
        - name: mongodb_password
          value: test123
        - name: mongodb_port
          value: &quot;27017&quot;
        - name: mongodb_username
          value: root
        - name: server_port
          value: &quot;8087&quot;
        - name: kafka_brokers
          value: kafka-cluster-kafka-bootstrap.kafka:9093
        - name: truststore_password
          valuefrom:
            secretkeyref:
              key: ca.password
              name: kafka-ca-cert
        - name: keystore_password
          valuefrom:
            secretkeyref:
              key: user.password
              name: kafka
        image: tools.testcompany.co.za:8093/local/tt--mongodb-map:0.0.7.0-snapshot
        name: test-mongodb-sink-mxcore
        securitycontext:
          allowprivilegeescalation: true
          privileged: true
        volumemounts:
        - mountpath: /app/resources
          name: properties
        - mountpath: /stores
          name: stores
          readonly: true
      - env:
        - name: events_topic
          value: test.ops.proc-events
        - name: group_id
          value: test-mongodb-sink-mxeqd
        - name: input_topic
          value: test.raw.ptv.murex_eqd_sa
        - name: mongodb_auth_db
          value: admin
        - name: mongodb_host0
          value: test-mongodb-0.test-mongodb-headless.test-infra
        - name: mongodb_host1
          value: test-mongodb-1.test-mongodb-headless.test-infra
        - name: mongodb_password
          value: test123
        - name: mongodb_port
          value: &quot;27017&quot;
        - name: mongodb_username
          value: root
        - name: server_port
          value: &quot;8088&quot;
        - name: kafka_brokers
          value: kafka-cluster-kafka-bootstrap.kafka:9093
        - name: truststore_password
          valuefrom:
            secretkeyref:
              key: ca.password
              name: kafka-ca-cert
        - name: keystore_password
          valuefrom:
            secretkeyref:
              key: user.password
              name: kafka
        image: tools.testcompany.co.za:8093/local/tt--mongodb-map:0.0.7.0-snapshot
        name: test-mongodb-sink-mxeqd
        securitycontext:
          allowprivilegeescalation: true
          privileged: true
        volumemounts:
        - mountpath: /app/resources
          name: properties
        - mountpath: /stores
          name: stores
          readonly: true
      - env:
        - name: events_topic
          value: test.ops.proc-events
        - name: group_id
          value: test-mongodb-sink-mxgts
        - name: input_topic
          value: test.raw.ptv.murex_gts_sa
        - name: mongodb_auth_db
          value: admin
        - name: mongodb_host0
          value: test-mongodb-0.test-mongodb-headless.test-infra
        - name: mongodb_host1
          value: test-mongodb-1.test-mongodb-headless.test-infra
        - name: mongodb_password
          value: test123
        - name: mongodb_port
          value: &quot;27017&quot;
        - name: mongodb_username
          value: root
        - name: server_port
          value: &quot;8089&quot;
        - name: kafka_brokers
          value: kafka-cluster-kafka-bootstrap.kafka:9093
        - name: truststore_password
          valuefrom:
            secretkeyref:
              key: ca.password
              name: kafka-ca-cert
        - name: keystore_password
          valuefrom:
            secretkeyref:
              key: user.password
              name: kafka
        image: tools.testcompany.co.za:8093/local/tt--mongodb-map:0.0.7.0-snapshot
        name: test-mongodb-sink-mxgts
        securitycontext:
          allowprivilegeescalation: true
          privileged: true
        volumemounts:
        - mountpath: /app/resources
          name: properties
        - mountpath: /stores
          name: stores
          readonly: true
      - env:
        - name: events_topic
          value: test.ops.proc-events
        - name: group_id
          value: test-mongodb-sink-mxmr
        - name: input_topic
          value: test.raw.ptv.murex_mr
        - name: mongodb_auth_db
          value: admin
        - name: mongodb_host0
          value: test-mongodb-0.test-mongodb-headless.test-infra
        - name: mongodb_host1
          value: test-mongodb-1.test-mongodb-headless.test-infra
        - name: mongodb_password
          value: test123
        - name: mongodb_port
          value: &quot;27017&quot;
        - name: mongodb_username
          value: root
        - name: server_port
          value: &quot;8090&quot;
        - name: kafka_brokers
          value: kafka-cluster-kafka-bootstrap.kafka:9093
        - name: truststore_password
          valuefrom:
            secretkeyref:
              key: ca.password
              name: kafka-ca-cert
        - name: keystore_password
          valuefrom:
            secretkeyref:
              key: user.password
              name: kafka
        image: tools.testcompany.co.za:8093/local/tt--mongodb-map:0.0.7.0-snapshot
        name: test-mongodb-sink-mxmr
        securitycontext:
          allowprivilegeescalation: true
          privileged: true
        volumemounts:
        - mountpath: /app/resources
          name: properties
        - mountpath: /stores
          name: stores
          readonly: true
      - env:
        - name: events_topic
          value: test.ops.proc-events
        - name: group_id
          value: test-mongodb-sink-mxgtscf
        - name: input_topic
          value: test.raw.cashflow.murex_gts_sa
        - name: mongodb_auth_db
          value: admin
        - name: mongodb_host0
          value: test-mongodb-0.test-mongodb-headless.test-infra
        - name: mongodb_host1
          value: test-mongodb-1.test-mongodb-headless.test-infra
        - name: mongodb_password
          value: test123
        - name: mongodb_port
          value: &quot;27017&quot;
        - name: mongodb_username
          value: root
        - name: server_port
          value: &quot;8091&quot;
        - name: kafka_brokers
          value: kafka-cluster-kafka-bootstrap.kafka:9093
        - name: truststore_password
          valuefrom:
            secretkeyref:
              key: ca.password
              name: kafka-ca-cert
        - name: keystore_password
          valuefrom:
            secretkeyref:
              key: user.password
              name: kafka
        image: tools.testcompany.co.za:8093/local/tt--mongodb-map:0.0.7.0-snapshot
        name: test-mongodb-sink-mxgtscf
        securitycontext:
          allowprivilegeescalation: true
          privileged: true
        volumemounts:
        - mountpath: /app/resources
          name: properties
        - mountpath: /stores
          name: stores
          readonly: true
      - env:
        - name: events_topic
          value: test.ops.proc-events
        - name: group_id
          value: test-mongodb-sink-mxcoll
        - name: input_topic
          value: test.raw.collateral.mxcoll
        - name: mongodb_auth_db
          value: admin
        - name: mongodb_host0
          value: test-mongodb-0.test-mongodb-headless.test-infra
        - name: mongodb_host1
          value: test-mongodb-1.test-mongodb-headless.test-infra
        - name: mongodb_password
          value: test123
        - name: mongodb_port
          value: &quot;27017&quot;
        - name: mongodb_username
          value: root
        - name: server_port
          value: &quot;8092&quot;
        - name: kafka_brokers
          value: kafka-cluster-kafka-bootstrap.kafka:9093
        - name: truststore_password
          valuefrom:
            secretkeyref:
              key: ca.password
              name: kafka-ca-cert
        - name: keystore_password
          valuefrom:
            secretkeyref:
              key: user.password
              name: kafka
        image: tools.testcompany.co.za:8093/local/tt--mongodb-map:0.0.7.0-snapshot
        name: test-mongodb-sink-mxcoll
        securitycontext:
          allowprivilegeescalation: true
          privileged: true
        volumemounts:
        - mountpath: /app/resources
          name: properties
        - mountpath: /stores
          name: stores
          readonly: true
      - env:
        - name: events_topic
          value: test.ops.proc-events
        - name: group_id
          value: test-mongodb-sink-mxcoll-link
        - name: input_topic
          value: test.raw.collateral.mxcoll_link
        - name: mongodb_auth_db
          value: admin
        - name: mongodb_host0
          value: test-mongodb-0.test-mongodb-headless.test-infra
        - name: mongodb_host1
          value: test-mongodb-1.test-mongodb-headless.test-infra
        - name: mongodb_password
          value: test123
        - name: mongodb_port
          value: &quot;27017&quot;
        - name: mongodb_username
          value: root
        - name: server_port
          value: &quot;8093&quot;
        - name: kafka_brokers
          value: kafka-cluster-kafka-bootstrap.kafka:9093
        - name: truststore_password
          valuefrom:
            secretkeyref:
              key: ca.password
              name: kafka-ca-cert
        - name: keystore_password
          valuefrom:
            secretkeyref:
              key: user.password
              name: kafka
        image: tools.testcompany.co.za:8093/local/tt--mongodb-map:0.0.7.0-snapshot
        name: test-mongodb-sink-mxcoll-link
        securitycontext:
          allowprivilegeescalation: true
          privileged: true
        volumemounts:
        - mountpath: /app/resources
          name: properties
        - mountpath: /stores
          name: stores
          readonly: true
      - env:
        - name: events_topic
          value: test.ops.proc-events
        - name: group_id
          value: test-mongodb-sink-ost
        - name: input_topic
          value: test.raw.ptv.ost
        - name: mongodb_auth_db
          value: admin
        - name: mongodb_host0
          value: test-mongodb-0.test-mongodb-headless.test-infra
        - name: mongodb_host1
          value: test-mongodb-1.test-mongodb-headless.test-infra
        - name: mongodb_password
          value: test123
        - name: mongodb_port
          value: &quot;27017&quot;
        - name: mongodb_username
          value: root
        - name: server_port
          value: &quot;8094&quot;
        - name: kafka_brokers
          value: kafka-cluster-kafka-bootstrap.kafka:9093
        - name: truststore_password
          valuefrom:
            secretkeyref:
              key: ca.password
              name: kafka-ca-cert
        - name: keystore_password
          valuefrom:
            secretkeyref:
              key: user.password
              name: kafka
        image: tools.testcompany.co.za:8093/local/tt--mongodb-map:0.0.7.0-snapshot
        name: test-mongodb-sink-ost
        securitycontext:
          allowprivilegeescalation: true
          privileged: true
        volumemounts:
        - mountpath: /app/resources
          name: properties
        - mountpath: /stores
          name: stores
          readonly: true
      - env:
        - name: events_topic
          value: test.ops.proc-events
        - name: group_id
          value: test-mongodb-sink-posmon
        - name: input_topic
          value: test.raw.ptp.posmon
        - name: mongodb_auth_db
          value: admin
        - name: mongodb_host0
          value: test-mongodb-0.test-mongodb-headless.test-infra
        - name: mongodb_host1
          value: test-mongodb-1.test-mongodb-headless.test-infra
        - name: mongodb_password
          value: test123
        - name: mongodb_port
          value: &quot;27017&quot;
        - name: mongodb_username
          value: root
        - name: server_port
          value: &quot;8095&quot;
        - name: kafka_brokers
          value: kafka-cluster-kafka-bootstrap.kafka:9093
        - name: truststore_password
          valuefrom:
            secretkeyref:
              key: ca.password
              name: kafka-ca-cert
        - name: keystore_password
          valuefrom:
            secretkeyref:
              key: user.password
              name: kafka
        image: tools.testcompany.co.za:8093/local/tt--mongodb-map:0.0.7.0-snapshot
        name: test-mongodb-sink-posmon
        securitycontext:
          allowprivilegeescalation: true
          privileged: true
        volumemounts:
        - mountpath: /app/resources
          name: properties
        - mountpath: /stores
          name: stores
          readonly: true
      

thanks
",<kubernetes><kubernetes-pod><kubernetes-deployment>,67148453,2,"a templating tool like helm will let you fill in the environment-variable values from deploy-time settings.  in helm this would look like:
env:
  - name: events_topic
    value: {{ .values.eventstopic }}
  - name: group_id
    value: {{ .values.groupid }}
  - name: input_topic
    value: {{ .values.inputtopic }}

you could then deploy this multiple times with different sets of topics:
helm install alchemy . \
  --set eventstopic=test.ops.proc-events \
  --set groupid=test-mongodb-sink-alchemy \
  --set inputtopic=test.raw.ptv.alchemy
helm install bloomberg . \
  --set eventstopic=test.ops.proc-events \
  --set groupid=test-mongodb-sink-bloomberg \
  --set inputtopic=test.raw.pretrade.bloomberg

you could write the helm chart to be configured with a list of topic sets, too, and only deploy the set once:
{{- $top := . -}}{{-/* because &quot;range&quot; overwrites &quot;.&quot; */-}}
{{- $topic := range $topics -}}
---
apiversion: v1
kind: deployment
metadata:
  name: {{ $topic.name }}
spec:
  ...
    env:
      - name: event_topic
        value: {{ $top.values.eventtopic }}{{/* common to all deployments */}}
      - name: group_id
        value: test-mongodb-sink-{{ $topic.name }}
      - name: input_topic
        value: {{ $topic.inputtopic }}

write configuration like:
eventtopic: test.ops.proc-events
topics:
  - name: alchemy
    inputtopic: test.raw.ptv.alchemy
  - name: bloomberg
    inputtopic: test.raw.pretrade.bloomberg

and deploy like:
helm install connector . -f topic-listing.yaml

in any case, you will want only one container per pod.  there are a couple of reasons for this.  if the list of topics ever changes, this lets you create or delete deployments without interfering with the other topics; if everything was in a single pod, you'd have to stop and restart everything together, and it can take kafka a minute or two to figure out what happens.  in a kafka context, you can also run as many consumers as there are partitions on a topic, but not really more; if you have a very busy topic you can easily set that deployment's replicas: to have multiple consumers for multiple partitions, but if everything together is in one pod, your only choice is to scale everything together.
"
68289505,is namespace mandatory while defining claimref under k8s persistentvolume manifest file?,"below is my scenario.
i have an nfs setup and it will be used to create pv. and then use pvc to bind the volume.
now, consider i want to bind particular pv/pvc together irrespective of where pvc will be created. as far as i tried i could not bind pv/pvc without bringing namespace into the picture. since i use helm charts for deployment and the namespace can be anything (use can create/use any namespace) hence i do not want to restrict pv to look for pvc only in one namespace, rather bind to matching pvc from any namespace.
nfs-pv.yaml
apiversion: v1
kind: persistentvolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 1gi
  volumemode: filesystem
  accessmodes:
    - readwritemany
  persistentvolumereclaimpolicy: retain
  storageclassname: nfs
  claimref:
    name: nfs-pvc
    namespace: default   # this is something i wanna get rid off
  nfs:
    path: /apps/exports
    server: &lt;nfs-server-ip&gt;

nfs-pvc.yaml  #this one i should be able to create in any namespace and attach to the above pvc.
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: nfs-pvc
  namespace: fhir
spec:
  volumename: nfs-pv
  storageclassname: nfs
  accessmodes:
    - readwritemany
  resources:
    requests:
      storage: 1gi

i have tried without giving the namespace option in pv, but it didn't work.

any help on this would be much appreciated.
",<kubernetes><devops><kubernetes-helm><nfs><persistent-storage>,68291059,2,"i think that is not possible because :

pvc is a namespaced resource and pv is not a namespaced resource.

kubectl api-resources | grep 'pv\|pvc\|name'
name                              shortnames   apiversion                             namespaced   kind
persistentvolumeclaims            pvc          v1                                     true         persistentvolumeclaim
persistentvolumes                 pv           v1                                     false        persistentvolume


so there can be multiple pvcs with the same 'name' across multiple namespaces.
so when we are mentioning the name of the pvc under claimref we need to mention the name
space as well.

"
68748219,can't deploy kubernetes ingress apiversion networking.k8s.io/v1,"i'm preparing all the ingress manifest files to keep the latest apiversion (networking.k8s.io/v1) to upgrade my cluster from 1.19 to 1.22.
i'm deleting the previous ingress rule and then recreating:
k delete ingress/my-ingress
k create -f /tmp/ingress.yaml

unfortunately, the ingress is created but with apiversion extensions/v1beta1 that's different for what i have on my manifest:
$ k get ingress/my-ingress -o yaml
warning: extensions/v1beta1 ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 ingress
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;
  creationtimestamp: &quot;2021-08-11t19:42:08z&quot;

here is an example of the yaml i'm using:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;
  labels:
    app.kubernetes.io/instance: my-app
    app.kubernetes.io/name: my-app
  name: my-ingress
  namespace: default
spec:
  rules:
  - host: application.com
    http:
      paths:
        - path: /
          pathtype: implementationspecific
          backend:
            service:
              name: my-app
              port:
                number: 443

kubernetes version:
client version: version.info{major:&quot;1&quot;, minor:&quot;20&quot;, gitversion:&quot;v1.20.1&quot;, gitcommit:&quot;c4d752765b3bbac2237bf87cf0b1c2e307844666&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2020-12-18t12:09:25z&quot;, goversion:&quot;go1.15.5&quot;, compiler:&quot;gc&quot;, platform:&quot;linux/amd64&quot;}
server version: version.info{major:&quot;1&quot;, minor:&quot;19+&quot;, gitversion:&quot;v1.19.13-eks-8df270&quot;, gitcommit:&quot;8df2700a72a2598fa3a67c05126fa158fd839620&quot;, gittreestate:&quot;clean&quot;, builddate:&quot;2021-07-31t01:36:57z&quot;, goversion:&quot;go1.15.14&quot;, compiler:&quot;gc&quot;, platform:&quot;linux/amd64&quot;}

ingress controller version (i upgraded from 0.41 to avoid any kind of issues):
image: k8s.gcr.io/ingress-nginx/controller:v0.48.1@sha256:e9fb216ace49dfa4a5983b183067e97496e7a8b307d2093f4278cd550c303899

",<kubernetes><kubernetes-ingress><nginx-ingress>,68759873,2,"this is working as expected, in particular check github answer
when you create an ingress object, it can be read via any version - the server handles converting into the requested version.
in your request get ingress/my-ingress -o yaml you not specified version, which should be read. in such case kubectl searches documents returned by the server to find the first among them with requested resource. and it can be any version, as in your case.
that is why, if you want to check particular version, you can:

improve your request with adding your manifest file, since version specified in the file

    $ kubectl get -f ingress.yaml -o yaml                                                                                                                
    apiversion: networking.k8s.io/v1
    kind: ingress
    metadata:
      annotations:
        ...


other option is to qualify necessary version in get request:

    $ kubectl get ingresses.v1.networking.k8s.io
    name         class    hosts             address   ports   age
    my-ingress   &lt;none&gt;   application.com             80      12m

    $ kubectl get ingresses.v1beta1.networking.k8s.io
    warning: networking.k8s.io/v1beta1 ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 ingress
    name         class    hosts             address   ports   age
    my-ingress   &lt;none&gt;   application.com             80      13m

"
68665466,kong gateway how to retrieve client cert and set to header,"i am using kubernetes and kong ingress controller.
i have set up a client, kong gateway, and server.
the client connects kong gateway with mtls, kong gateway connects the server with plain http with a header contains the client cert pem.
in nginx i can simplely add this config.
server{
    location / {
        proxy_set_header ssl-client-cert $ssl_client_escaped_cert;
    }
}

in kong gateway, i try to set helm values.yaml
env.nginx_proxy_proxy_set_header: &quot;ssl-client-cert $ssl_client_escaped_cert&quot;

but in the container /etc/kong_prefix/nginx-kong.conf, it looks like this and the header is not sent out.
server {
    proxy_set_header ssl-client-cert $ssl_client_escaped_cert;
}

i try to use plugin request-transformer but it considers $ssl_client_escaped_cert as a plain string.
apiversion: configuration.konghq.com/v1
kind: kongclusterplugin
metadata:
  name: kong-plugin-client-cert-header
  annotations:
    kubernetes.io/ingress.class: kong
  labels:
    global: &quot;true&quot;
config: 
  add:
    headers:
    - ssl-client-cert: $ssl_client_escaped_cert;
plugin: request-transformer

how can i get the client cert and send to server as header?
",<ssl><kubernetes><kubernetes-ingress><kong><kong-ingress>,70848002,2,"you can use the kong function plugin to do custom transformation. from that plugin you can use all nginx variables available:

https://docs.konghq.com/hub/kong-inc/serverless-functions

for example, your plugin can be configured with something like:
apiversion: configuration.konghq.com/v1
kind: kongplugin
metadata:
  name: kong-plugin-client-cert-header
config:
  access:
  - |
    kong.service.request.set_header('ssl-client-cert', ('--' or ngx.var.ssl_client_escaped_cert))
plugin: post-function

"
68575605,kubernetes ingress with grpc and http,"i have an application deployed to kubernetes (aks) where i have a mix of grpc and http services.  i initially added the route for a new grpc service to the existing ingress which was previously serving only http.  that didn't work and digging into it, i read that we need to add the nginx.ingress.kubernetes.io/backend-protocol: grpc annotation, and that it applied to all routes, so we would need two separate ingress.  i'm currently getting an exception io.grpc.internal.managedchannelimpl$nameresolverlistener error trying to connect to the grpc service with message nodename nor servname provided, or not known.  i'm guessing that though when multiple paths within an ingress match a request, precedence is given first to the longest matching path, that doesn't apply across the both ingress.  so i would need to either use different hosts, or change the /* path so that it didn't also match /results?  or is there something else that i need to change in my configuration?
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: {{ .release.name }}-{{ .chart.name }}-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt
spec:
  tls:
  - hosts:
    - {{ .values.ingress.hosts.host }}
    secretname: {{ .values.ingress.tls.secretname }}
  rules:
  - host: {{ .values.ingress.hosts.host }}
    http:
      paths:
      - path: /graphql
        pathtype: prefix
        backend:
          service:
            name: {{ .release.name }}-{{ .values.services.graphqlserver.host }}
            port:
              number: 80
      - path: /graphql/*
        pathtype: prefix
        backend:
          service:
            name: {{ .release.name }}-{{ .values.services.graphqlserver.host }}
            port:
              number: 80
      - path: /
        pathtype: prefix
        backend:
          service:
            name: {{ .release.name }}-{{ .values.services.webuiserver.host }}
            port:
              number: 80
      - path: /*
        pathtype: prefix
        backend:
          service:
            name: {{ .release.name }}-{{ .values.services.webuiserver.host }}
            port:
              number: 80
---
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: {{ .release.name }}-{{ .chart.name }}-grpc
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/backend-protocol: grpc
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    cert-manager.io/cluster-issuer: letsencrypt
spec:
  tls:
  - hosts:
    - {{ .values.ingress.hosts.host }}
    secretname: {{ .values.ingress.tls.secretname }}
  rules:
  - host: {{ .values.ingress.hosts.host }}
    http:
      paths:
      - path: /results
        pathtype: prefix
        backend:
          service:
            name: {{ .release.name }}-{{ .values.services.externalresults.host }}
            port:
              number: 9000


",<nginx><kubernetes><grpc><kubernetes-ingress><nginx-ingress>,68763688,2,"this wound up being resolved by creating a second host name that pointed to our k8s cluster.  i changed the route for the grpc service to be the root path and pathtype of implementationspecific.
      - path: /
        pathtype: implementationspecific

both host names needed to be included in the tls section of both ingress.  i was getting an ssl exception after changing the route but not updating the hosts in the tls section of each ingress.
channel pipeline: [sslhandler#0, protocolnegotiators$clienttlshandler#0, writebufferingandexceptionhandler#0, defaultchannelpipeline$tailcontext#0]
    at io.grpc.status.asruntimeexception(status.java:533)
    at akka.grpc.internal.unarycalladapter.onclose(unarycalladapter.scala:40)
    at io.grpc.internal.clientcallimpl.closeobserver(clientcallimpl.java:413)
  | =&gt; cat io.grpc.internal.clientcallimpl.access$500(clientcallimpl.java:66)
    at io.grpc.internal.clientcallimpl$clientstreamlistenerimpl$1streamclosed.runinternal(clientcallimpl.java:742)
    at io.grpc.internal.clientcallimpl$clientstreamlistenerimpl$1streamclosed.runincontext(clientcallimpl.java:721)
    at io.grpc.internal.contextrunnable.run(contextrunnable.java:37)
stderr: 
    at io.grpc.internal.serializingexecutor.run(serializingexecutor.java:123)
    at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149)
    at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624)
    at java.lang.thread.run(thread.java:748)
caused by: javax.net.ssl.sslhandshakeexception: general opensslengine problem
    at io.grpc.netty.shaded.io.netty.handler.ssl.referencecountedopensslengine.handshakeexception(referencecountedopensslengine.java:1771)
    at io.grpc.netty.shaded.io.netty.handler.ssl.referencecountedopensslengine.wrap(referencecountedopensslengine.java:776)
    at javax.net.ssl.sslengine.wrap(sslengine.java:511)
    at io.grpc.netty.shaded.io.netty.handler.ssl.sslhandler.wrap(sslhandler.java:1079)
    at io.grpc.netty.shaded.io.netty.handler.ssl.sslhandler.wrapnonappdata(sslhandler.java:970)
    at io.grpc.netty.shaded.io.netty.handler.ssl.sslhandler.unwrap(sslhandler.java:1443)
    at io.grpc.netty.shaded.io.netty.handler.ssl.sslhandler.decodejdkcompatible(sslhandler.java:1275)
    at io.grpc.netty.shaded.io.netty.handler.ssl.sslhandler.decode(sslhandler.java:1322)
    at io.grpc.netty.shaded.io.netty.handler.codec.bytetomessagedecoder.decoderemovalreentryprotection(bytetomessagedecoder.java:501)
    at io.grpc.netty.shaded.io.netty.handler.codec.bytetomessagedecoder.calldecode(bytetomessagedecoder.java:440)
    at io.grpc.netty.shaded.io.netty.handler.codec.bytetomessagedecoder.channelread(bytetomessagedecoder.java:276)
    at io.grpc.netty.shaded.io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:379)
    at io.grpc.netty.shaded.io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:365)
    at io.grpc.netty.shaded.io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:357)
    at io.grpc.netty.shaded.io.netty.channel.defaultchannelpipeline$headcontext.channelread(defaultchannelpipeline.java:1410)
    at io.grpc.netty.shaded.io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:379)
    at io.grpc.netty.shaded.io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:365)
    at io.grpc.netty.shaded.io.netty.channel.defaultchannelpipeline.firechannelread(defaultchannelpipeline.java:919)
    at io.grpc.netty.shaded.io.netty.channel.nio.abstractniobytechannel$niobyteunsafe.read(abstractniobytechannel.java:163)
    at io.grpc.netty.shaded.io.netty.channel.nio.nioeventloop.processselectedkey(nioeventloop.java:714)
    at io.grpc.netty.shaded.io.netty.channel.nio.nioeventloop.processselectedkeysoptimized(nioeventloop.java:650)
    at io.grpc.netty.shaded.io.netty.channel.nio.nioeventloop.processselectedkeys(nioeventloop.java:576)
    at io.grpc.netty.shaded.io.netty.channel.nio.nioeventloop.run(nioeventloop.java:493)
stderr: 
    at io.grpc.netty.shaded.io.netty.util.concurrent.singlethreadeventexecutor$4.run(singlethreadeventexecutor.java:989)
stderr: 
    at io.grpc.netty.shaded.io.netty.util.internal.threadexecutormap$2.run(threadexecutormap.java:74)
    at io.grpc.netty.shaded.io.netty.util.concurrent.fastthreadlocalrunnable.run(fastthreadlocalrunnable.java:30)
    ... 1 more
caused by: java.security.cert.certificateexception: no subject alternative dns name matching grpc.aks.dev.app.cycleautomation.com found.
stderr: 
    at sun.security.util.hostnamechecker.matchdns(hostnamechecker.java:214)
    at sun.security.util.hostnamechecker.match(hostnamechecker.java:96)
    at sun.security.ssl.x509trustmanagerimpl.checkidentity(x509trustmanagerimpl.java:462)
    at sun.security.ssl.x509trustmanagerimpl.checkidentity(x509trustmanagerimpl.java:428)
    at sun.security.ssl.x509trustmanagerimpl.checktrusted(x509trustmanagerimpl.java:261)
    at sun.security.ssl.x509trustmanagerimpl.checkservertrusted(x509trustmanagerimpl.java:144)
    at io.grpc.netty.shaded.io.netty.handler.ssl.openssltlsv13x509extendedtrustmanager.checkservertrusted(openssltlsv13x509extendedtrustmanager.java:223)
    at io.grpc.netty.shaded.io.netty.handler.ssl.referencecountedopensslclientcontext$extendedtrustmanagerverifycallback.verify(referencecountedopensslclientcontext.java:261)
    at io.grpc.netty.shaded.io.netty.handler.ssl.referencecountedopensslcontext$abstractcertificateverifier.verify(referencecountedopensslcontext.java:700)
    at io.grpc.netty.shaded.io.netty.internal.tcnative.ssl.readfromssl(native method)
    at io.grpc.netty.shaded.io.netty.handler.ssl.referencecountedopensslengine.readplaintextdata(referencecountedopensslengine.java:595)
    at io.grpc.netty.shaded.io.netty.handler.ssl.referencecountedopensslengine.unwrap(referencecountedopensslengine.java:1202)
    at io.grpc.netty.shaded.io.netty.handler.ssl.referencecountedopensslengine.unwrap(referencecountedopensslengine.java:1324)
    at io.grpc.netty.shaded.io.netty.handler.ssl.referencecountedopensslengine.unwrap(referencecountedopensslengine.java:1367)
    at io.grpc.netty.shaded.io.netty.handler.ssl.sslhandler$sslenginetype$1.unwrap(sslhandler.java:206)
    at io.grpc.netty.shaded.io.netty.handler.ssl.sslhandler.unwrap(sslhandler.java:1380)
    ... 21 more
    suppressed: javax.net.ssl.sslhandshakeexception: error:1000007d:ssl routines:openssl_internal:certificate_verify_failed
        at io.grpc.netty.shaded.io.netty.handler.ssl.referencecountedopensslengine.sslreaderrorresult(referencecountedopensslengine.java:1287)
        at io.grpc.netty.shaded.io.netty.handler.ssl.referencecountedopensslengine.unwrap(referencecountedopensslengine.java:1248)
        ... 25 more

the final yaml looked like this:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: {{ .release.name }}-{{ .chart.name }}-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt
spec:
  tls:
  - hosts:
    - {{ .values.ingress.hosts.host }}
    - {{ .values.ingress.grpc.host }}
    secretname: {{ .values.ingress.tls.secretname }}
  rules:
  - host: {{ .values.ingress.hosts.host }}
    http:
      paths:
      - path: /graphql
        pathtype: prefix
        backend:
          service:
            name: {{ .release.name }}-{{ .values.services.graphqlserver.host }}
            port:
              number: 80
      - path: /graphql/*
        pathtype: prefix
        backend:
          service:
            name: {{ .release.name }}-{{ .values.services.graphqlserver.host }}
            port:
              number: 80
      - path: /
        pathtype: prefix
        backend:
          service:
            name: {{ .release.name }}-{{ .values.services.webuiserver.host }}
            port:
              number: 80
      - path: /*
        pathtype: prefix
        backend:
          service:
            name: {{ .release.name }}-{{ .values.services.webuiserver.host }}
            port:
              number: 80
---
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: {{ .release.name }}-{{ .chart.name }}-grpc
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/backend-protocol: grpc
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    cert-manager.io/cluster-issuer: letsencrypt
spec:
  tls:
  - hosts:
    - {{ .values.ingress.hosts.host }}
    - {{ .values.ingress.grpc.host }}
    secretname: {{ .values.ingress.tls.secretname }}
  rules:
  - host: {{ .values.ingress.hosts.host }}
    http:
      paths:
      - path: /
        pathtype: implementationspecific
        backend:
          service:
            name: {{ .release.name }}-{{ .values.services.externalresults.host }}
            port:
              number: 9000

then i was able to connect to the grpc service over port 443 w/ tls enabled and just using the host name with no path in my connection.
"
68939439,docker volume mount to kubernetes volume,"i am trying out to have a volume mount on kubernetes.
currently i have a docker image which i run like:
docker run --mount type=bind,source=&quot;$(pwd)&quot;&lt;host_dir&gt;,target=&lt;docker_dir&gt; container

to have this run on google kubernetes cluster, i have:

create a google compute disk
created a persistent volume which refers to the disk:


kind: persistentvolume
...
    namespace: default
    name: pvc
spec:
  claimref:
    namespace: default
    name: pvc
  gcepersistentdisk:
    pdname: disk-name
    fstype: ext4
---
...
kind: persistentvolumeclaim
metadata:
  name: pvc
spec:
  storageclassname: &quot;storage&quot;
...
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 2000gi


created pod with mount

kind: pod
apiversion: v1
metadata:
  name: k8s-pod
spec:
  volumes:
    - name: pvc
      persistentvolumeclaim:
       claimname: pvc
  containers:
    - name: image_name
      image: eu.gcr.io/container:latest
      volumemounts:
        - mountpath: &lt;docker_dir&gt;
          name: dir

i am missing out where the binding between the host and container/pod directories will take place. also where do i mention that binding in my yaml files.
i will appreciate any help :)
",<docker><kubernetes><google-kubernetes-engine>,68951469,2,"you are on the right path here.  in your pod spec, the name of the volumemount should match the name of the volumes.  so in your case,
volumes:
    - name: pvc
      persistentvolumeclaim:
       claimname: pvc

volume name is pvc. so your volumemount should be
volumemounts:
        - mountpath: &quot;/path/in/container&quot;
          name: pvc

so, for example, to mount this volume at /mydata in your container, your pod spec would look like
kind: pod
apiversion: v1
metadata:
  name: k8s-pod
spec:
  volumes:
    - name: pvc
      persistentvolumeclaim:
       claimname: pvc
  containers:
    - name: image_name
      image: eu.gcr.io/container:latest
      volumemounts:
        - mountpath: &quot;/mydata&quot;
          name: pvc

"
58812791,ingress with kubernetes only calling the index route and not other routes,"i have a flask application which has multiple routes including the default route '/'. i deployed this app on the kubernetes. and i am using minikube as a standalone cluster. i exposed the deployment as a nodeport service and then used ingress to map the external request to the application running in cluster. my ingress resource looks like this...

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: kubernetes-test-svc
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: \""false\""
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  backend:
    servicename: defualt-http-backend
    serviceport: 80
  rules:
  - host: kubernetes-test.info
    http:
      paths:
      - path: /*
        backend:
          servicename: kubernetes-test-svc
          serviceport: 80


and i also configured my /etc/hosts file to route request to this host. it looks something like this...

192.168.99.100  kubernetes-test.info


the problem is no matter which endpoint i call the ingress always redirects it to the default route '/'. my flask app looks like this...

@app.route('/')
def index():
    return ""root route""


@app.route('/route1')
def route1():
    return ""route 1""


@app.route('/route2')
def route2():
    params = request.args
    return make_response(jsonify({'param1': params['one'], 'param2': params['two']}))


so if i make request to kubernetes-test.info/route1 it will show me the text ""root route"" instead of ""route 1"".

but if i type 192.168.99.100/route1 it show ""route 1"". i dont know why this is happening? why it is working with minikube ip but not working with host i specified. 

service deployment looks like this:-

apiversion: v1
kind: service
metadata:
  name: kubernetes-test-svc
spec:
  type: nodeport
  ports:
  - port: 80
    targetport: 8080
    protocol: tcp
    name: http
  selector:
    app: kubernetes-test

",<flask><kubernetes><kubernetes-ingress>,58813012,1,"update your ingress

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: kubernetes-test-svc
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
spec:
  backend:
    servicename: defualt-http-backend
    serviceport: 80
  rules:
  - host: kubernetes-test.info
    http:
      paths:
      - path: /
        backend:
          servicename: kubernetes-test-svc
          serviceport: 80

"
60572620,helm override common template values,"first day helm user here. trying to understand how to build common templates for k8s resources.
let's say i have 10 cron jobs within single chart, all of them different by args and names only. today 10 full job manifests exists and 95% of manifest content is equal. i want to move common part in template and create 10 manifests where i will provide specific values for args and names.

so i defined template _cron-job.yaml

    {{- define ""common.cron-job""}}
apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: {{ include ""costing-report.name"" . }}-bom-{{ .values.env }}
  labels:
{{ include ""costing-report.labels"" . | indent 4 }}
spec:
  schedule: ""{{ .values.cronjob.schedulebom }}""
  suspend: {{ .values.cronjob.suspendbom }}
  {{- with .values.cronjob.concurrencypolicy }}
  concurrencypolicy: {{ . }}
  {{- end }}
  {{- with .values.cronjob.failedjobshistorylimit }}
  failedjobshistorylimit: {{ . }}
  {{- end }}
  {{- with .values.cronjob.successfuljobshistorylimit }}
  successfuljobshistorylimit: {{ . }}
  {{- end }}
  jobtemplate:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include ""costing-report.name"" . }}
        app.kubernetes.io/instance: {{ .release.name }}
    spec:
      template:
        spec:
          containers:
            - name: {{ .chart.name }}
              image: ""{{ .values.image.repository }}:{{ .values.image.tag }}""
              imagepullpolicy: {{ .values.image.pullpolicy }}
              args: [""--report=bom"",""--email={{ .values.configmap.service.email_bom }}""]
              env:
                - name: spring_profiles_active
                  value: ""{{ .values.env }}""
              envfrom:
                - configmapref:
                    name: {{ include ""costing-report.fullname"" . }}
                - secretref:
                    name: {{ .values.secrets.properties }}
          restartpolicy: never
          {{- with .values.imagepullsecrets }}
          imagepullsecrets:
            {{- toyaml . | nindent 8 }}
          {{- end }}    
{{- end -}}


and now i need to create job manifest that override name and args job1.yaml

{{- template ""common.cron-job"" . -}}
??? override ???
name: {{ include ""cost-report.name"" . }}-job1-{{ .values.env }}
jobtemplate:
spec:
  template:
    spec:
      containers:
        args: [""--report=bom"",""--email={{ .values.configmap.service.email_bom }}""]


is there any way to do this? i didn't find this in helm docs. i did find this https://github.com/helm/charts/tree/master/incubator/common but it didn't work as well and gave me error.

thanks.
",<templates><kubernetes><charts><overriding><kubernetes-helm>,60607991,1,"solution found

option 1
use example from helm github https://github.com/helm/charts/tree/master/incubator/common 
solution based on yaml merging and values override. pretty flexible, allow you to define common templates and the use them to compose final k8s manifest. 

option 2
define common template and pass parameters with desired values.
in my case it looks smth like this.

_common.cronjob.yaml

{{- define ""common.cronjob"" -}}
{{- $root := .root -}} 
{{- $name := .name -}} 
{{- $schedule := .schedule -}} 
{{- $suspend := .suspend -}} 
{{- $args := .args -}} 

apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: {{ $name }}
  labels:
{{ include ""costing-report.labels"" $root | indent 4 }}
spec:
  schedule: {{ $schedule }}
  suspend: {{ $suspend }}
  {{- with $root.values.cronjob.concurrencypolicy }}
  concurrencypolicy: {{ . }}
  {{- end }}
  {{- with $root.values.cronjob.failedjobshistorylimit }}
  failedjobshistorylimit: {{ . }}
  {{- end }}
  {{- with $root.values.cronjob.successfuljobshistorylimit }}
  successfuljobshistorylimit: {{ . }}
  {{- end }}
  jobtemplate:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include ""costing-report.name"" $root }}
        app.kubernetes.io/instance: {{ $root.release.name }}
    spec:
      template:
        spec:
          containers:
            - name: {{ $root.chart.name }}
              image: ""{{ $root.values.image.repository }}:{{ $root.values.image.tag }}""
              imagepullpolicy: {{ $root.values.image.pullpolicy }}
              args: {{ $args }}
              env:
                - name: spring_profiles_active
                  value: ""{{ $root.values.env }}""
              envfrom:
                - configmapref:
                    name: {{ include ""costing-report.fullname"" $root }}
                - secretref:
                    name: {{ $root.values.secrets.properties }}
          restartpolicy: never
          {{- with $root.values.imagepullsecrets }}
          imagepullsecrets:
            {{- toyaml . | nindent 8 }}
          {{- end }}
{{- end -}}


then create job manifest(s), define values to pass to common template

bom-cronjob.yaml

{{ $bucket := (printf ""%s%s%s"" ""\""--bucket=ll-costing-report-"" .values.env ""\"","" )}}
{{ $email := (printf ""%s%s%s"" ""\""--email="" .values.configmap.service.email_bom ""\"""") }}
{{ $args := (list ""\""--report=bom\"","" ""\""--reporttype=load\"","" ""\""--source=bamboorose\"","" $bucket ""\""--table=costing_bom\"","" ""\""--ignorelines=1\"","" ""\""--truncate=true\"","" $email )}}
{{ $name := (printf ""%s%s"" ""costing-report.name-bom-"" .values.env )}}
{{- template ""common.cronjob"" (dict ""root"" . ""name"" $name ""schedule"" .values.cronjob.schedulebom ""suspend"" .values.cronjob.suspendbom ""args"" $args) -}}


last line do the trick. trick is that you can pass only single argument to template, in my case it's dictionary with all values that i need on template side. you can omit defining template variables and use dict values right away. please note that i pass root context (scope) as ""root"" and prefix . with ""root"" in template.
"
60594652,rewrite context path for google kubernetes engine ingress controller,"i have an application running in the tomcat path /app1, how should i access this from the ingress path? 

when accessing ""/"", it gives the default tomcat 404 - not found page, and when accessing via /app1 it shows ""default backend -404"" 

what i wanna know is:
is there anyway to configure the context path without using ngnix's ingress controller? (just using gke's default ingress controller)

here is a sample of my ingress:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: gke-my-ingress-1
  annotations:
    kubernetes.io/ingress.global-static-ip-name: gke-my-static-ip
    networking.gke.io/managed-certificates: gke-my-certificate
spec:
  rules:
  - host: mydomain.web.com
    http:
      paths:
      - path: /
        backend:
          servicename: my-service
          serviceport: my-port


edit: service output

kubectl get svc
my-service   nodeport    &lt;ip_redacted&gt;   &lt;none&gt;        8080:30310/tcp   5d16h


kubectl describe svc my-service
name:                     my-service
namespace:                default
labels:                   &lt;none&gt;
annotations:              kubectl.kubernetes.io/last-applied-configuration:
                            {""apiversion"":""v1"",""kind"":""service"",""metadata"":{""annotations"":{},""name"":""my-service"",""namespace"":""default""},""spec"":{""ports"":[{""name""...
selector:                 app=my-deployment-1
type:                     nodeport
ip:                       &lt;ip_redacted&gt;
port:                     my-port  8080/tcp
targetport:               8080/tcp
nodeport:                 my-port  30310/tcp
endpoints:                &lt;ip_redacted&gt;:8080
session affinity:         none
external traffic policy:  cluster
events:                   &lt;none&gt;


and this is my node port service yaml:

---
apiversion: v1
kind: service
metadata:
  name: my-service
spec:
  ports:
  - name: my-port
    port: 8080
    protocol: tcp
    targetport: 8080
  selector:
    app: my-deployment-1
  type: nodeport

",<kubernetes><google-kubernetes-engine><kubernetes-ingress><nginx-ingress>,60634617,1,"unfortunately current implementation of default gke ingress controller doesn't support rewrite targets. there is still an open github issue that you can find here.

what you're trying to achieve is rewriting your ingress path to some specific path exposed by your application, in your case apache tomcat web server.

isn't there any possibility to reconfigure your app to be served from the main path by apache tomcat ? if so, you can make it available on &lt;ingressloadbalancerip&gt;/app1 by configuring the following path in your ingress resource like in the example below:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-ingress
spec:
  rules:
  - http:
      paths:
      - path: /*
        backend:
          servicename: default-backend
          serviceport: 8080
      - path: /app1
        backend:
          servicename: my-service
          serviceport: 8080


but unfortunately you cannot configure rewrite in the way that when you go to &lt;ingressloadbalancerip&gt;/app1 it rewrites to your &lt;my-service&gt;/app1.

it seems that for now the only solution is to install different ingress controller such as mentioned nginx insgress controller.
"
58809593,dynamic ingress to kubernetes services with nginx proxy,"i am trying to make a dynamic proxy_pass with nginx, doing something like that: 


requests to foo.mywebsite.com are forwarded to service with foo name
requests to bar.mywebsite.com are forwarded to service with bar name


my nginx.conf works with static values, but with regex, works for about 5 minutes and then errors start 

apiversion: v1
kind: configmap
metadata:
  name: nginx-config-dns-file
data:
  nginx.conf: |
    server {
      listen 80;
      server_name ~^(?&lt;subdomain&gt;.*?)\.;
      resolver kube-dns.kube-system.svc.cluster.local valid=5s;

      location /healthz {
        return 200;
      }

      location / {
        proxy_pass http://$subdomain.default.svc.cluster.local; 
      }
    }


my pod gets the service ip instead of the name, here are the logs


  2019/11/11 22:30:40 [error] 6#6: 163 10.default.svc.cluster.local
  could not be resolved (3: host not found), client: 10.142.0.34,
  server: ~^(?.?)., request: ""get / http/1.1"", host:
  ""10.142.0.34""


10. is the beginning of ip to the point.
i don't know what is going wrong, can anyone help me with this, thank you!!
",<nginx><kubernetes><kubernetes-ingress><nginx-ingress>,58856511,1,"fix with this 

apiversion: v1
kind: configmap
metadata:
  name: nginx-config-dns-file
data:
  nginx.conf: |
    server {
      listen 80;
      server_name ~^(?&lt;subdomain&gt;.*?)\.;
      resolver kube-dns.kube-system.svc.cluster.local valid=5s;

      location /healthz {
        return 200;
      }

      location / {
        proxy_set_header host $host
        proxy_pass http://$subdomain.default.svc.cluster.local; 
      }
    }

"
78265140,how to create prefix matches in istio virtualservice for substring paths,"i'm encountering a warning message about duplicate/overlapping matches in my istio virtualservice configuration, and i'm seeking assistance in resolving it. here's the warning message:
warning: virtualservice rule #1 match #0 of prefix /apparchived/ is not used (duplicate/overlapping match in rule #1 of prefix /app on #0)

i have a virtualservice configuration where i need to route requests for paths /app and /apparchived to different services while preserving the uri paths. however, i'm facing issues with overlapping matches in my configuration.
here's my current virtualservice configuration:
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: app
  namespace: ingress-routes
spec:
  hosts:
  - &quot;*&quot;
  gateways:
  - dev-app-gateway
  http:
  - match:
    - uri:
        prefix: &quot;/app/&quot;
    - uri:
        prefix: &quot;/app&quot;
    rewrite:
      uri: &quot;/&quot;        
    route:
    - destination:
        host: app.app.svc.cluster.local
        port:
          number: 80
  - match:
    - uri:
        prefix: &quot;/apparchived/&quot;
    - uri:
        prefix: &quot;/apparchived&quot;        
    rewrite:
      uri: &quot;/&quot;        
    route:
    - destination:
        host: apparchived.app.svc.cluster.locala
          number: 80

i'd like to achieve the following routing behavior:

requests to /app should be routed to service app.app.svc.cluster.local.
requests to /apparchived should be routed to service apparchived.app.svc.cluster.local.
at the same time /app/ &amp; /apparchived/ also should route to respective services.
any insights on how to resolve the warning and achieve the desired routing behavior would be greatly appreciated.

thank you!
",<kubernetes><kubernetes-ingress><istio><envoyproxy><istio-gateway>,78277594,1,"please use &quot;exact&quot; and &quot;prefix&quot; attributes combination to achieve desired routing you are looking for.  please use below virtual service definition to avoid &quot;duplicate/overlapping match in rule warning&quot;
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: app
  namespace: ingress-routes
spec:
  hosts:
  - &quot;*&quot;
  gateways:
  - dev-app-gateway
  http:
  - match:
    - uri:
        prefix: &quot;/app/&quot;
    - uri:
        exact: &quot;/app&quot;
    rewrite:
      uri: &quot;/&quot;        
    route:
    - destination:
        host: app.app.svc.cluster.local
        port:
          number: 80
  - match:
    - uri:
        prefix: &quot;/apparchived/&quot;
    - uri:
        exact: &quot;/apparchived&quot;        
    rewrite:
      uri: &quot;/&quot;        
    route:
    - destination:
        host: apparchived.app.svc.cluster.local
          number: 80

"
60536390,remote access zero to jupyterhub over ethernet with ingress in kubernetes,"context

i installed kubernetes on a bare-metal server (4 nodes) and deployed zero to jupyterhub to it.
this works fine; i can correctly access the hub from the master-node.

now i want to access the hub on the server from an external computer via ethernet. therefore, i followed the official instructions and installed metallb in order to provide an external ip for my proxy-public-service (which correctly sets).
additionally, i installed the nginx-ingress-controller in order to be able to do an ingress, which also successfully gets an external ip (little hint: use the helm-chart; i couldn't get the service running when applying the other recommended steps).

since i had a little trouble figuring out how to do this ingress, here is an example:

kubectl apply -f ingress.yaml --namespace jhub

#ingress.yaml:
#apiversion: networking.k8s.io/v1beta1
#kind: ingress
#metadata:
#  name: jupyterhub-ingress
#  annotations:
#    nginx.ingress.kubernetes.io/rewrite-target: /$1
#spec:
#  rules:
#  - host: jupyterhub.cluster
#    http:
#      paths:
#      - path: /
#        backend:
#          servicename: proxy-public
#          serviceport: 80


anyhow, i cannot open the external ip proxy-public provides (meaning i'm inserting the external ip in my browser).


question

how can i remotely access my jupyterhub over the external ip; what am i missing?
",<kubernetes><kubernetes-ingress><nginx-ingress><jupyterhub>,60536391,1,"i missed that this can be achieved in the same way as with the kubernetes-dashboard: you have to establish an open ssh-connection (hence, open a tunnel -> tunneling) from the external computer.
of course this is not the ""exernal"" access i had in mind, but a working and fast solution for my test-environment (and maybe yours).

how to establish this ssh-connect

first, get the external ip-address of your proxy-public:

$: kubectl get services --namespace jhub
name           type           cluster-ip       external-ip     port(s)                      age
hub            clusterip      10.99.241.72     &lt;none&gt;          8081/tcp                     95m
proxy-api      clusterip      10.107.175.27    &lt;none&gt;          8001/tcp                     95m
proxy-public   loadbalancer   10.102.171.162   192.168.1.240   80:31976/tcp,443:32568/tcp   95m


note: the range of the external ip was defined in my layer2 in my metallb-config.

using this information (and assuming you're on linux), open a terminal and use the following command:

$ ssh pi@10.10.10.2 -l 8000:192.168.1.240:80
# -l opens a localhost-connection
# pi@10.10.10.2 logs me into my second node with user pi


note1: that localhost:8000 is configured as targetport for proxy-public with http can also be seen when you describe the service and take a look at the specs respectively ports (you can also get the settings for https there):

kind: service
apiversion: v1
metadata:
  name: proxy-public
  namespace: jhub
  ...
spec:
  ports:
    - name: http
      protocol: tcp
      port: 80
      targetport: 8000
      nodeport: 31976
    - name: https
...


finally, type http://localhost:8000/ into your browser - et voila, you get to your jupyterhub login-page!
"
78181293,is it possible to create a k8s stateful set without a persistent volume mounted?,"i try to create the following stateful set but it fails in 10 seconds while trying to start its only pod.
apiversion: apps/v1
kind: statefulset
metadata:
  name: dbss
spec:
  selector:
    matchlabels:
      app: db-pod
  replicas: 1
  template:
    metadata:
      labels:
        app: db-pod
    spec:
      containers:
      - name: db-cont
        image: mysql:5
        ports:
        - containerport: 3306

the error message is not descriptive enough in my opinion:
events:
  type     reason     age                   from               message
  ----     ------     ----                  ----               -------
  normal   scheduled  5m9s                  default-scheduler  successfully assigned dbss-0 to minikube
  normal   pulled     3m16s (x5 over 5m9s)  kubelet            container image &quot;mysql:5&quot; already present on machine
  normal   created    3m16s (x5 over 5m9s)  kubelet            created container db-cont
  normal   started    3m16s (x5 over 5m9s)  kubelet            started container db-cont
  warning  backoff    2s (x21 over 4m55s)   kubelet            back-off restarting failed container db-cont in pod dbss-0_k8s-overview(d7c03bf1-051e-47f8-bb72-6b1669584011)

is it because there is no persistent volume mounted to the stateful set?
is there a way to see a more descriptive error message to see the actual reason?
i know of the following limitation from https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#limitations:

limitations
the storage for a given pod must either be provisioned by a persistentvolume provisioner based on the requested storage class, or pre-provisioned by an admin.

yet i am not sure i understand the quoted limitation correctly. especially after looking at the events of the failing pods.
",<kubernetes><persistent-volumes><kubernetes-statefulset>,78181448,1,"it is possible to create a statefulset without a persistent volume
apiversion: v1
kind: service
metadata:
 name: nginx-hellow-world
 labels:
   app: nginx-hello-world
spec:
 ports:
 - port: 80
   name: web
 clusterip: none
 selector:
   app: nginx-hello-world
---
apiversion: apps/v1
kind: statefulset
metadata:
 name: nginx-hello-world
spec:
 selector:
   matchlabels:
     app: nginx-hello-world # has to match .spec.template.metadata.labels
 servicename: &quot;nginx-hello-world&quot;
 template:
   metadata:
     labels:
       app: nginx-hello-world # has to match .spec.selector.matchlabels
   spec:
     terminationgraceperiodseconds: 10
     containers:
     - name: nginx-hello-world
       image: dockerbogo/docker-nginx-hello-world
       ports:
       - containerport: 80
         name: web

in the example on the kubernetes reference page the nginx image they use expects a volume whith the nginx views. that is why in the example they mount a volume.
the terminal responses you provided you're pods are already scheduled but a container in the pod fails to start and kubernetes tries to restart them that is what back-off restarting failed container means, see this documentation.
"
60530901,exposing loadbalancer service in minikube at arbitrary port?,"i have a minikube cluster with a running wordpress in one deployment, and mysql in another. both of the deployments have corresponding services. the definition for wordpress service looks like this:

apiversion: v1
kind: service
metadata:
    name: wordpress
spec: 
    selector:
        app: wordpress
    ports:
        - port: 80 
    type: loadbalancer


the service works fine, and minikube service gives me a nice path, with an address of minikube ip and a random high port. the problem is wordpress needs a full url in the name of the site. i'd rather not change it every single time and have local dns name for the cluster.

is there a way to expose the loadbalancer on an arbitrary port in minikube? i'll be fine with any port, as long as it's port is decided by me, and not minikube itself?
",<kubernetes><load-balancing><kubernetes-ingress><minikube><kubernetes-service>,60533435,1,"keep in mind that minikube is unable to provide real loadbalancer like different cloud providers and it merely simulates it by using simple nodeport service instead.

you can have full control over the port that is used. first of all you can specify it manually in the nodeport service specification (remember it should be within the default range: 30000-32767):


  if you want a specific port number, you can specify a value in the
  nodeport field. the control plane will either allocate you that port
  or report that the api transaction failed. this means that you need to
  take care of possible port collisions yourself. you also have to use a
  valid port number, one thats inside the range configured for nodeport
  use.


your example may look as follows:

apiversion: v1
kind: service
metadata:
    name: wordpress
spec: 
    selector:
        app: wordpress
    ports:
        - port: 80
          targetport: 80
          nodeport: 30000
    type: nodeport


you can also change this default range by providing your custom value after --service-node-port-range flag when starting your kube-apiserver.

when you use kubernetes cluster set up by kukbeadm tool (minikube also uses it as a default bootstrapper), you need to edit /etc/kubernetes/manifests/kube-apiserver.yaml file and provide the required flag with your custom port range.
"
37060905,how do i do this deployment by command line,"i can do a deploy like this, but cannot do it via command line.




i was looking at doing it like this

kubectl create -f kubernetes-rc.json




{
  ""kind"": ""replicationcontroller"",
  ""apiversion"": ""v1"",
  ""metadata"": {
    ""name"": ""foo-frontend-rc"",
    ""labels"": {
      ""www"": true
    },
    ""namespace"": ""foo""
  },
  ""spec"": {
    ""replicas"": 1,
    ""template"": {
      ""metadata"": {
        ""labels"": {
          ""app"": ""foo-frontend""
        }
      },
      ""spec"": {
        ""containers"": [
          {
            ""name"": ""foo-frontend"",
            ""image"": ""gcr.io/atomic-griffin-130023/foo-frontend:b3fc862"",
            ""ports"": [
              {
                ""containerport"": 3009,
                ""protocol"": ""tcp""
              }
            ],
            ""imagepullpolicy"": ""ifnotpresent""
          }
        ],
        ""restartpolicy"": ""always"",
        ""dnspolicy"": ""clusterfirst""
      }
    }
  }
}




and

kubectl create -f kubernetes-service.json




{
  ""kind"": ""service"",
  ""apiversion"": ""v1"",
  ""metadata"": {
    ""name"": ""foo-frontend-service""
  },
  ""spec"": {
    ""selector"": {
      ""app"": ""foo-frontend-rc""
    },
    ""ports"": [
      {
        ""protocol"": ""tcp"",
        ""port"": 80,
        ""targetport"": 3009
      }
    ]
  }
}


to no avail. it creates the rc, but it wont expose the service externally.
",<kubernetes><google-kubernetes-engine>,37061210,1,"your service's selector is wrong. it should be selecting a label from the pod template, not a label on the rc itself.

if you change the following in your service:

""selector"": {
  ""app"": ""foo-frontend-rc""
},


to:

""selector"": {
  ""app"": ""foo-frontend""
},


it should fix it.



update

change your service definition to

{
  ""kind"": ""service"",
  ""apiversion"": ""v1"",
  ""metadata"": {
    ""name"": ""foo-frontend-service""
  },
  ""spec"": {
    ""selector"": {
      ""app"": ""foo-frontend""
    },
    ""ports"": [
      {
        ""protocol"": ""tcp"",
        ""port"": 80,
        ""targetport"": 3009,
        ""nodeport"": 30009
      }
    ],
    ""type"": ""loadbalancer""
  }
}

"
60662344,exposing mail or ssh honeypot in kubernetes cluster,"i'm experimenting with smtp (mailoney) and ssh honeypots in a kubernetes cluster to be exposed to the big bad www. i cant seem to figure out how to get it working since i'm only beginning to understand kubernetes just recently. 

i've got some config now, for example my mailoney.yaml:

apiversion: apps/v1
kind: deployment
metadata:
  name: mailoney
spec:
  selector:
    matchlabels:
      app: mailoney
  template:
    metadata:
      labels:
        app: mailoney
    spec:
      containers:
      - name: mailoney
        image: dtagdevsec/mailoney:2006
        ports:
        - containerport: 25


and the service config:

apiversion: v1
kind: service
metadata:
  name: ingress-mailoney
  labels:
    name: mailoney
spec:
  type: loadbalancer
  ports:
    - name: smtp
      port: 25
      targetport: 25
      protocol: tcp
  selector:
    name: mailoney


but when the loadbalancer is configured, it exposes the services on port >30000, which i know is default behaviour for kubernetes. but how do i exactly configure the loadbalancer to allow connections on port 25 and 22 respectively and actually letting connections through to the honeypots?

am i overlooking something really obvious?

any help is appreciated. 
",<kubernetes><kubernetes-ingress>,60668844,1,"as @coderanger mentioned, your cloud provider will take care of everything and make the original port available.
reading your service manifest i could notice that your selector is wrong, it should point to app: mailoney instead of name:. i tested it and it's working with the correct selector.
here is how your manifest should look like:
apiversion: v1
kind: service
metadata:
  name: ingress-mailoney
  labels:
    name: mailoney
spec:
  type: loadbalancer
  ports:
    - name: smtp
      port: 25
      targetport: 25
      protocol: tcp
  selector:
    app: mailoney

after changing it to app: mailoney i have the following results:
$ kubectl get service ingress-mailoney -o wide
name               type           cluster-ip     external-ip      port(s)        age   selector
ingress-mailoney   loadbalancer   10.31.250.51   104.197.119.16   25:30601/tcp   44m   app=mailoney

$ telnet 104.197.119.16 25
trying 104.197.119.16...
connected to 104.197.119.16.
escape character is '^]'.
220 mailrelay.local esmtp exim 4.81 #1 thu, 29 jul 2010 05:13:48 -0700

as you can see, it's working as designed.
"
45964882,kubernetes - persistentvolumeclaim failed,"i have a gke based kubernetes setup and a pod that requires a storage volume. i attempt to use the config below:

kind: persistentvolumeclaim
apiversion: v1
metadata:
  name: my-scratch-space
spec:
  accessmodes:
  - readwriteonce
resources:
  requests:
    storage: 2000gi
storageclassname: standard


this pvc is not provisioned. i get the below error:

failed to provision volume with storageclass ""standard"": googleapi: error 503: the zone 'projects/p01/zones/europe-west2-b' does not have enough resources available to fulfill the request.  try a different zone, or try again later.


looking at gke quotas page, i don't see any issues. deleting other pvcs also is not solving the issue. can anyone help? thanks.
",<kubernetes><google-kubernetes-engine>,45967044,1,"there is no configuration problem at your side - there are actually not enough resources in the europe-west2-b zone to create a 2t persistent disk. either try for a smaller volume or use a different zone.

there is an example for gce in the docs. create a new storageclass specifying say the europe-west1-b zone (which is actually cheaper than europe-west2-b) like this:

kind: storageclass
apiversion: storage.k8s.io/v1
metadata:
  name: gce-pd-europe-west1-b
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
  zones: europe-west1-b


and modify your pvc:

kind: persistentvolumeclaim
apiversion: v1
metadata:
  name: my-scratch-space
spec:
  accessmodes:
  - readwriteonce
resources:
  requests:
    storage: 2000gi
storageclassname: gce-pd-europe-west1-b

"
78050359,how to setup kubernetes ingress behind nginx reverse proxy,"i have kubernetes master-node running on my linux machine and on the very same machine there is also standalone nginx running as a service.
my goal was to have some monitoring and databases running on the machine as well as kubernetes node. (i know it's a bad idea, but this is just for development phase) and cover everything with nginx reverse proxy.
i had no problem handling proxying to some services running on the machine, however whatever i try i cannot seem to find resolution how to pass requests to ingress controller that would further forward it to service running inside kubernetes.
my nginx config (that runs outside kubernetes and should be the reverse proxy)
server {
        listen 8080 default_server;
        root /var/www/html;
        server_name _;

        location / {
                try_files $uri $uri/ =404;
        }
}


server {
    listen 80;
    server_name nginx.k8s.domain.me;

    location / {
        proxy_pass http://127.0.0.1:8080;
        proxy_set_header host $host;
        proxy_set_header x-real-ip $remote_addr;
        proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;
        proxy_set_header x-forwarded-proto $scheme;
    }
}

server {
    listen 80;
    server_name dashboard.kubernetes.k8s.domain.me;
    location / {
        proxy_pass http://192.168.77.139:80;
        proxy_set_header host $host;
        proxy_set_header x-real-ip $remote_addr;
        proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;
        proxy_set_header x-forwarded-proto $scheme;
        proxy_buffer_size   128k;
        proxy_buffers   4 256k;
        proxy_busy_buffers_size   256k;
    }
}

when i try to enter nginx.k8s.domain.me i am greeted with default nginx landing page, so we can consider it a success. but trying to enter dashboard.kubernetes.k8s.domain.me ends up with 404.
i applied the dashboard deployment from kubernetes wiki and used this ingress:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: dashboard-ingress
  namespace: kubernetes-dashboard
spec:
  rules:
  - host: dashboard.kubernetes.k8s.domain.me
    http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: kubernetes-dashboard
            port:
              number: 80

some kubernetes info for reference:
kubectl get nodes
name          status   roles           age    version
master-node   ready    control-plane   141m   v1.28.2

kubectl get ingresses --all-namespaces -o wide
namespace              name                  class    hosts                                 address   ports   age
kubernetes-dashboard   dashboard-ingress     &lt;none&gt;   dashboard.kubernetes.k8s.domain.me               80      131m

kubectl get pods --all-namespaces -o wide
namespace              name                                         ready   status      restarts   age    ip               node          nominated node   readiness gates
calico-apiserver       calico-apiserver-5bd8dc87cf-4p5rc            1/1     running     0          141m   192.168.77.133   master-node   &lt;none&gt;           &lt;none&gt;
calico-apiserver       calico-apiserver-5bd8dc87cf-9szzg            1/1     running     0          141m   192.168.77.134   master-node   &lt;none&gt;           &lt;none&gt;
calico-system          calico-kube-controllers-685f7c9b88-6mls5     1/1     running     0          141m   192.168.77.131   master-node   &lt;none&gt;           &lt;none&gt;
calico-system          calico-node-4zqpw                            1/1     running     0          141m   &lt;hidden&gt;    master-node   &lt;none&gt;           &lt;none&gt;
calico-system          calico-typha-5d74745fdd-2cmnt                1/1     running     0          141m   &lt;hidden&gt;    master-node   &lt;none&gt;           &lt;none&gt;
calico-system          csi-node-driver-mfmql                        2/2     running     0          141m   192.168.77.130   master-node   &lt;none&gt;           &lt;none&gt;
default                hello-world-deployment-5cccb4989-8jdsw       1/1     running     0          41m    192.168.77.140   master-node   &lt;none&gt;           &lt;none&gt;
default                hello-world-deployment-5cccb4989-cnxzz       1/1     running     0          41m    192.168.77.142   master-node   &lt;none&gt;           &lt;none&gt;
default                hello-world-deployment-5cccb4989-l4ltg       1/1     running     0          41m    192.168.77.141   master-node   &lt;none&gt;           &lt;none&gt;
ingress-nginx          ingress-nginx-admission-create-kswjk         0/1     completed   0          135m   192.168.77.138   master-node   &lt;none&gt;           &lt;none&gt;
ingress-nginx          ingress-nginx-admission-patch-gsmjs          0/1     completed   1          135m   192.168.77.137   master-node   &lt;none&gt;           &lt;none&gt;
ingress-nginx          ingress-nginx-controller-68fb8cf9cc-9bkft    1/1     running     0          135m   192.168.77.139   master-node   &lt;none&gt;           &lt;none&gt;
kube-system            coredns-5dd5756b68-lvqsj                     1/1     running     0          142m   192.168.77.132   master-node   &lt;none&gt;           &lt;none&gt;
kube-system            coredns-5dd5756b68-t4ssf                     1/1     running     0          142m   192.168.77.129   master-node   &lt;none&gt;           &lt;none&gt;
kube-system            etcd-master-node                             1/1     running     2          143m   &lt;hidden&gt;    master-node   &lt;none&gt;           &lt;none&gt;
kube-system            kube-apiserver-master-node                   1/1     running     2          143m   &lt;hidden&gt;    master-node   &lt;none&gt;           &lt;none&gt;
kube-system            kube-controller-manager-master-node          1/1     running     1          143m   &lt;hidden&gt;    master-node   &lt;none&gt;           &lt;none&gt;
kube-system            kube-proxy-dlm74                             1/1     running     0          142m   &lt;hidden&gt;    master-node   &lt;none&gt;           &lt;none&gt;
kube-system            kube-scheduler-master-node                   1/1     running     2          143m   &lt;hidden&gt;    master-node   &lt;none&gt;           &lt;none&gt;
kubernetes-dashboard   dashboard-metrics-scraper-5657497c4c-qx5nr   1/1     running     0          137m   192.168.77.135   master-node   &lt;none&gt;           &lt;none&gt;
kubernetes-dashboard   kubernetes-dashboard-78f87ddfc-5vljg         1/1     running     0          137m   192.168.77.136   master-node   &lt;none&gt;           &lt;none&gt;
tigera-operator        tigera-operator-8547bd6cc6-5k6s7             1/1     running     0          142m   &lt;hidden&gt;    master-node   &lt;none&gt;           &lt;none&gt;

kubectl get services --all-namespaces -o wide
namespace              name                                 type           cluster-ip       external-ip   port(s)                      age    selector
calico-apiserver       calico-api                           clusterip      10.96.71.22      &lt;none&gt;        443/tcp                      142m   apiserver=true
calico-system          calico-kube-controllers-metrics      clusterip      none             &lt;none&gt;        9094/tcp                     142m   k8s-app=calico-kube-controllers
calico-system          calico-typha                         clusterip      10.101.144.111   &lt;none&gt;        5473/tcp                     142m   k8s-app=calico-typha
default                hello-world-service                  clusterip      10.106.93.244    &lt;none&gt;        80/tcp                       42m    app=hello-world
default                kubernetes                           clusterip      10.96.0.1        &lt;none&gt;        443/tcp                      144m   &lt;none&gt;
ingress-nginx          ingress-nginx-controller             loadbalancer   10.102.254.176   &lt;pending&gt;     80:31488/tcp,443:32443/tcp   136m   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx
ingress-nginx          ingress-nginx-controller-admission   clusterip      10.97.17.56      &lt;none&gt;        443/tcp                      136m   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx
kube-system            kube-dns                             clusterip      10.96.0.10       &lt;none&gt;        53/udp,53/tcp,9153/tcp       144m   k8s-app=kube-dns
kubernetes-dashboard   dashboard-metrics-scraper            clusterip      10.107.180.47    &lt;none&gt;        8000/tcp                     138m   k8s-app=dashboard-metrics-scraper
kubernetes-dashboard   kubernetes-dashboard                 clusterip      10.108.179.123   &lt;none&gt;        443/tcp                      138m   k8s-app=kubernetes-dashboard

i tried messing with nginx config and ingresses but they did not seem to work at all.
",<kubernetes><nginx><reverse-proxy><kubernetes-ingress><nginx-ingress>,78050445,1,"alright guys, that was pretty simple to be honest. i had my ingress config pointing to port 80, while dashboard exposes port 443. second thing that i had to change is adding annotation to the ingress, so final version looks like this
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: dashboard-ingress
  namespace: kubernetes-dashboard
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
spec:
  rules:
  - host: dashboard.kubernetes.k8s.domain.me
    http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: kubernetes-dashboard
            port:
              number: 443

"
60641653,how to configure ingress to deploy services to subdomains without creating a new loadbalancer,"i am new to kubernetes and have an application deployed via gke on mydomain.com and now want to add another service which should be available on api.mydomain.com without adding a new expensive load balancer. what should the new ingress file for api.mydomain look like? i read the documentation, but cannot figure out how to do this.

this is my first service running on mydomain.com:

kind: service
apiversion: v1
metadata:
  name: app-service
spec:
  selector:
    app: app
  ports:
  - protocol: tcp
    port: 80
    targetport: 80
  type: nodeport
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: app-ingress
  annotations:
    kubernetes.io/ingress.global-static-ip-name: ""ip""
    cert-manager.io/cluster-issuer: ""letsencrypt-prod""
    acme.cert-manager.io/http01-edit-in-place: ""true""
    kubernetes.io/tls-acme: ""true""
spec:
  rules:
  - host: mydomain.com
    http:
      paths:
      - backend:
          servicename: app-service
          serviceport: 80
  tls:
  - hosts:
    - mydomain.com
    secretname: my-certs


i tried to use the same configuration for the subdomain api.mydomain.com, but this does not work.

kind: service
apiversion: v1
metadata:
  name: api-service
spec:
  selector:
    app: api
  ports:
  - protocol: tcp
    port: 80
    targetport: 80
  type: nodeport
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: api-ingress
  annotations:
    kubernetes.io/ingress.global-static-ip-name: ""ip""
    cert-manager.io/cluster-issuer: ""letsencrypt-prod""
    acme.cert-manager.io/http01-edit-in-place: ""true""
    kubernetes.io/tls-acme: ""true""
spec:
  rules:
  - host: api.mydomain.com
    http:
      paths:
      - backend:
          servicename: api-service
          serviceport: 80
  tls:
  - hosts:
    - api.mydomain.com
    secretname: my-certs-api


maybe i'm approaching the problem in the wrong way, i'm new in gke, any suggestions?
",<kubernetes><google-kubernetes-engine><kubernetes-ingress>,60655586,1,"try ""nginx"" ingress.class . the config shall be like the following (i have removed tls part out it). 

nginx ingress controller 
is quite easy and functional. 

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: double-ingress
  annotations:
    kubernetes.io/ingress.class: ""nginx""
#    nginx.ingress.kubernetes.io/rewrite-target: /$2  ### you can use it to route different paths to different services under the same host
spec:
  rules:
  - host: mydomain.com
    http:
      paths:
      - path: /
#      - path: /doc/common(/|$)(.*)    ### if you are using rewrite
        backend:
          servicename: app-service
          serviceport: 80

  - host: api.mydomain.com
    http:
      paths:
      - path: /
        backend:
          servicename: api-service
          serviceport: 80
  tls:
  - hosts:
    - api.mydomain.com
    secretname: my-certs-api


hope that helps!
"
58622284,can i combine storageclass with persistentvolume in gke?,"i'm fairly new to kubernetes and find it difficult to get it working from documentation, kubenetes docs says that storageclass contains the fields provisioner, parameters, and reclaimpolicy, which are used when a persistentvolume belonging to the class needs to be dynamically provisioned however can i use storageclass with pv(not dynamic allocation) to specify high performance disk allocation such as ssd?

without storageclass it worked fine for me.

following is my manifest

kind: persistentvolume
metadata:
  name: gke-pv
  labels:
    app: test
spec:
  capacity:
    storage: 10gi
  accessmodes:
    - readwriteonce
  gcepersistentdisk:
    pdname: gce-disk
    fstype: ext4
---
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: gke-pvc
  labels:
    app: test
spec:
  accessmodes:
    - readwriteonce
  storageclassname: ssd-sc 
  resources:
    requests:
      storage: 2gi
  selector:
    matchlabels:
      app: test

",<kubernetes><google-cloud-platform><storage><google-kubernetes-engine>,58627188,1,"the problem that is going on here is that if you want to statically provision persistentvolumes, they don't have a storageclass.  however, gke clusters are created with a standard storageclass which is the default, and so the pvc gets confused and tries to dynamically allocate.

the solution is to have the pvc request an empty storage class, which forces it to look at the statically provisioned pvs.

so you'd use a sequence like this to create a pv and then get it bound to a pvc:


manually provision the ssd:


gcloud compute disks create --size=10gi --zone=[your zone] --type=pd-ssd already-created-ssd-disk


then apply a pv object that uses the statically provisioned disk, like so:


apiversion: v1
kind: persistentvolume
metadata:
  name: ssd-for-k8s-volume
spec:
  capacity:
    storage: 10gi
  volumemode: filesystem
  accessmodes:
    - readwriteonce
  gcepersistentdisk:
    pdname: already-created-ssd-disk
    fstype: ext4



then, you can claim it with a pvc like this:


apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: pvc-ssd-demo
spec:
  storageclassname: """"
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 10gi


you could also use labels to refine which pvs are selected, of course, for example if you have some that are ssd and others that are regular spinning metal.

note that the idea of using a storageclass for static provisioning isn't really the right thing, since storageclass is tied to how you describe storage for dynamic provisioning.
"
78130483,"horizontal pod autoscaler with stackdriver custom metric in gke fails with ""invalid metric name"" error","we're trying to use a custom jvm metric (jvm_memory_bytes_used{area=&quot;heap&quot;}) to scale a deployment in our gke cluster using a horizontal pod autoscaler (hpa).
setup:

enabled stackdriver managed prometheus
installed jmx exporter on jvms and configured it to export the desired metric
deployed the stackdriver adapter for custom metrics
created an hpa referencing the custom metric:

apiversion: autoscaling/v2
kind: horizontalpodautoscaler
metadata:
  name: my-autoscale
  namespace: somenamespace
spec:
  maxreplicas: 3
  metrics:
  - pods:
      metric:
        name: jvm_memory_bytes_used{area=&quot;heap&quot;}  # metric name in question
      target:
        averagevalue: 2g
        type: averagevalue
  type: pods
  minreplicas: 1
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: my-java-app

problem:
the hpa creation fails with the following error:
error 400: invalid metric name: custom.googleapis.com/jvm_memory_bytes_used{area=&quot;heap&quot;},

we've tried various combinations of quotes around the metric name and the area label, but none worked.
question:
is it possible to use this specific custom metric for hpa scaling in gke? if so, what's the correct way to specify it in the hpa configuration?
",<kubernetes><google-kubernetes-engine><prometheus><google-cloud-stackdriver><hpa>,78144107,1,"i had to change three things for getting it to work:

add proper prefix and suffix to the metric name;
use a selector clause for filtering metrics by label; and
add the proper prefix to the metric label name.

prometheus metrics have a prefix and a suffix
from horizontal pod autoscaling (hpa) | operations suite | google cloud

prometheus metrics are stored with the following conventions:

the prefix prometheus.googleapis.com.
this suffix is usually one of gauge, counter, summary, or histogram, although untyped metrics might have the unknown or unknown:counter suffix. to verify the suffix, look up the metric in cloud monitoring by using metrics explorer.


so i did it, went to metrics explorer, enabled the build and went to search for the metrics i wanted to use:

using a selector for metric labels
i wanted to filter by the area label, but we should not pass it in the name of the metrics. filtering by labels should be done by using a selector in the metric clause.
also...
metric label names have a prefix
we cannot just add a metric name to the matchlabels / matchexpressions. every metric label name should be prefixed by metric.labels.:
metric:
  name: prometheus.googleapis.com|jvm_memory_bytes_used|gauge
    selector:
      matchlabels:
        area: heap

the end result is this:
apiversion: autoscaling/v2
kind: horizontalpodautoscaler
metadata:
  name: my-autoscale
  namespace: somenamespace
spec:
  maxreplicas: 3
  metrics:
  - pods:
      metric:
        name: prometheus.googleapis.com|jvm_memory_bytes_used|gauge
        selector:
           matchlabels:
             metric.labels.area: heap
      target:
        averagevalue: 2g
        type: averagevalue
  type: pods
  minreplicas: 1
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: my-java-app

with that, i managed to make hpa respond to the custom metrics.
"
58561504,kubernetes logs don't print requests output when i use a port in an address,"i've created a cluster with minikube 

minikube start 


applied this yaml manifest:

apiversion: apps/v1
kind: deployment
metadata:
  name: gateway-deployment
spec:
  selector:
    matchlabels:
      app: gateway
  replicas: 1
  template:
    metadata:
      labels:
        app: gateway
    spec:
      containers:
      - name: gateway
        image: docker_gateway
        imagepullpolicy: never
        ports:
        - containerport: 4001
          protocol: tcp
---
apiversion: v1
kind: service
metadata:
  name: gateway
spec:
  selector:
    app: gateway
  ports:
  - protocol: tcp
    port: 4001


and my go app in the container docker_gateway is just a gin http server with one route

package main
import ""github.com/gin-gonic/gin""

func main() {
    r := gin.default()
    r.get(""/hello"", func(c *gin.context) {
        c.json(200, gin.h{
            ""message"": ""hello"",
        })
    })

    server = &amp;http.server{
        addr:    "":4001"",
        handler: r,
    }

    server.listenandserve()
}


in postman i make requests to 192.168.252.130:4001/hello and get a responses

but kubernetes pod's logs in kubernetes don't print those requests. i expect to get this:

[gin] 2019/10/25 - 14:17:20 | 200 |       1.115s |      192.168.252.1| get      /hello


but an interesting thing is when i add ingress

apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: ingress
spec:
  backend:
    servicename: gateway
    serviceport: 4001


i am able to make requests to 192.168.252.130/hello and 192.168.252.130:4001/hello
and without the port pod's logs print requests, but with the port - they don't.

[gin] 2019/10/25 - 14:19:13 | 200 |       2.433s |      192.168.252.1| get      /hello

",<go><kubernetes><kubernetes-ingress>,58562182,1,"it's because you cannot access a kubernetes service of clusterip type from outside(in your case, outside of minikube) of the cluster. 

learn more about service types here

to access your service from outside, change your service to nodeport type.

something like:

apiversion: v1
kind: service
metadata:
  name: gateway
spec:
  selector:
    app: gateway
  ports:
  - protocol: tcp
    nodeport: 30036
    port: 4001
  type: nodeport


then you will be able to access it at http://&lt;minikube-ip&gt;:30036/
"
44550647,can't mount secret to volume in /etc,"i have a k8s deployment that mounts a secret into /etc/google-cloud-account containing the google auth json file to use from the application. when i try to run the deployment, i get the following error from my pod:

1m  1m  1   kubelet, gke-development-cluster-default-pool-17f531d7-sj4x spec.containers{api}    normal  created     created container with docker id 36b85ec8415a; security:[seccomp=unconfined]
1m  1m  1   kubelet, gke-development-cluster-default-pool-17f531d7-sj4x spec.containers{api}    warning failed      failed to start container with docker id 36b85ec8415a with error: error response from daemon: rpc error: code = 2 desc = ""oci runtime error: could not synchronise with container process: mkdir /var/lib/docker/overlay/b4aa81194f72ccb54d88680e766a921ea26f7a4df0f4b32d6030123896b2b203/merged/etc/google-cloud-account: read-only file system""
1m  1m  1   kubelet, gke-development-cluster-default-pool-17f531d7-sj4x             warning failedsync  error syncing pod, skipping: failed to ""startcontainer"" for ""api"" with runcontainererror: ""runcontainer: error response from daemon: rpc error: code = 2 desc = \""oci runtime error: could not synchronise with container process: mkdir /var/lib/docker/overlay/b4aa81194f72ccb54d88680e766a921ea26f7a4df0f4b32d6030123896b2b203/merged/etc/google-cloud-account: read-only file system\""""

2m  13s 11  kubelet, gke-development-cluster-default-pool-17f531d7-sj4x spec.containers{api}    warning backoff     back-off restarting failed docker container


the deployment in question looks like:

apiversion: extensions/v1beta1
kind: deployment
metadata:
  # ...
spec:
  replicas: {{ .values.api.replicacount }}
  template:
    # ...
    spec:
      containers:
        - name: {{ .values.api.name }}
          # ...
          volumemounts:
            - name: google-cloud-account
              mountpath: /etc/google-cloud-account
      volumes:
        - name: google-cloud-account
          secret:
            secretname: {{ template ""fullname"" . }}
            items:
              - key: google-cloud-credentials
                path: credentials.json


i don't know how /etc in the container would be a read only file system and don't know how to change that.
",<kubernetes><google-kubernetes-engine><kubernetes-helm>,44557294,1,"as it turns out the error was caused by another volume mount. i left it out of the end code, but my deployment looked more like the following:

apiversion: extensions/v1beta1
kind: deployment
metadata:
  # ...
spec:
  replicas: {{ .values.api.replicacount }}
  template:
    # ...
    spec:
      containers:
        - name: {{ .values.api.name }}
          # ...
          volumemounts:
            - name: google-cloud-account
              mountpath: /etc/google-cloud-account
            - name: odbc
              mountpath: /etc
      volumes:
        - name: google-cloud-account
          secret:
            secretname: {{ template ""fullname"" . }}
            items:
              - key: google-cloud-credentials
                path: credentials.json
        - name: odbc
          configmap:
            name: {{ template ""fullname"" . }}
            items:
              - key: odbc.ini
                path: odbc.ini


mounting odbc took over the entire /etc directory. to fix it, i changed the odbc volumemount to:

- name: odbc
  mountpath: /etc/odbc.ini
  subpath: odbc.ini


which left everything else in /etc intact.
"
60719979,kubernetes: move from deployment to statefulset - set env from secrets,"kubernetes documentation says that for mysql pods we need to use stateful sets in order to avoid ""split brain"" situations when one pod dies, in other words, to declare one ""master"" node to which data will be written to, and if that pod dies, elect new master, that's why i want this deployment and service to transfer to stateful set:

  ---

    apiversion: apps/v1
    kind: deployment
    metadata:
      name: mysql-container
    spec:
      replicas: 3
      selector:
        matchlabels:
          app: mysql-container
      template:
        metadata:
          labels:
            app: mysql-container
        spec:
          containers:
          - name: mysql-container
            image: mysql:dev
            imagepullpolicy: ""ifnotpresent""
            envfrom:
              - secretref:
                 name: prod-secrets
            ports:
            - containerport: 3306
             # container (pod) path
            volumemounts:
              - name: mysql-persistent-storage
                mountpath: /data/db

          # minikube path
          volumes:
            - name: mysql-persistent-storage
              persistentvolumeclaim:
               claimname: mysql-pvc
            #resources:
            #  requests:
            #    memory: 300mi
            #    cpu: 400m
            #  limits:
            #    memory: 400mi
            #    cpu: 500m  
          restartpolicy: always

---

apiversion: v1
kind: service
metadata:
  name: mysql

spec:
  # open port 3306 only to pods in cluster
  selector:
    app: mysql-container

  ports:
    - name: mysql
      port: 3306
      protocol: tcp
      targetport: 3306
  type: clusterip


i created stateful set following: this guide

under containers section i specified environment variables from file, ie. removed

 env:
        - name: mysql_allow_empty_password
          value: ""1""


statefulset:

apiversion: apps/v1
kind: statefulset
metadata:
  name: mysql
spec:
  selector:
    matchlabels:
      app: mysql
  servicename: mysql
  replicas: 3
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initcontainers:
      - name: init-mysql
        image: mysql:5.7
        command:
        - bash
        - ""-c""
        - |
          set -ex
          # generate mysql server-id from pod ordinal index.
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${bash_rematch[1]}
          echo [mysqld] &gt; /mnt/conf.d/server-id.cnf
          # add an offset to avoid reserved server-id=0 value.
          echo server-id=$((100 + $ordinal)) &gt;&gt; /mnt/conf.d/server-id.cnf
          # copy appropriate conf.d files from config-map to emptydir.
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumemounts:
        - name: conf
          mountpath: /mnt/conf.d
        - name: config-map
          mountpath: /mnt/config-map
      - name: clone-mysql
        image: gcr.io/google-samples/xtrabackup:1.0
        command:
        - bash
        - ""-c""
        - |
          set -ex
          # skip the clone if data already exists.
          [[ -d /var/lib/mysql/mysql ]] &amp;&amp; exit 0
          # skip the clone on master (ordinal index 0).
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${bash_rematch[1]}
          [[ $ordinal -eq 0 ]] &amp;&amp; exit 0
          # clone data from previous peer.
          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -c /var/lib/mysql
          # prepare the backup.
          xtrabackup --prepare --target-dir=/var/lib/mysql
        volumemounts:
        - name: data
          mountpath: /var/lib/mysql
          subpath: mysql
        - name: conf
          mountpath: /etc/mysql/conf.d
      containers:
      - name: mysql
        image: mysql:dev
        imagepullpolicy: ""ifnotpresent""
        envfrom:
          - secretref:
             name: prod-secrets
        ports:
        - name: mysql
          containerport: 3306
        volumemounts:
        - name: data
          mountpath: /var/lib/mysql
          subpath: mysql
        - name: conf
          mountpath: /etc/mysql/conf.d
        resources:
          #requests:
           # cpu: 300m
           # memory: 1gi
        livenessprobe:
          exec:
            command: [""mysqladmin"", ""ping""]
          initialdelayseconds: 30
          periodseconds: 10
          timeoutseconds: 5
        readinessprobe:
          exec:
            # check we can execute queries over tcp (skip-networking is off).
            command: [""mysql"", ""-h"", ""127.0.0.1"", ""-e"", ""select 1""]
          initialdelayseconds: 5
          periodseconds: 2
          timeoutseconds: 1
      - name: xtrabackup
        image: gcr.io/google-samples/xtrabackup:1.0
        ports:
        - name: xtrabackup
          containerport: 3307
        command:
        - bash
        - ""-c""
        - |
          set -ex
          cd /var/lib/mysql

          # determine binlog position of cloned data, if any.
          if [[ -f xtrabackup_slave_info &amp;&amp; ""x$(&lt;xtrabackup_slave_info)"" != ""x"" ]]; then
            # xtrabackup already generated a partial ""change master to"" query
            # because we're cloning from an existing slave. (need to remove the tailing semicolon!)
            cat xtrabackup_slave_info | sed -e 's/;$//g' &gt; change_master_to.sql.in
            # ignore xtrabackup_binlog_info in this case (it's useless).
            rm -f xtrabackup_slave_info xtrabackup_binlog_info
          elif [[ -f xtrabackup_binlog_info ]]; then
            # we're cloning directly from master. parse binlog position.
            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
            rm -f xtrabackup_binlog_info xtrabackup_slave_info
            echo ""change master to master_log_file='${bash_rematch[1]}',\
                  master_log_pos=${bash_rematch[2]}"" &gt; change_master_to.sql.in
          fi

          # check if we need to complete a clone by starting replication.
          if [[ -f change_master_to.sql.in ]]; then
            echo ""waiting for mysqld to be ready (accepting connections)""
            until mysql -h 127.0.0.1 -e ""select 1""; do sleep 1; done

            echo ""initializing replication from clone position""
            mysql -h 127.0.0.1 \
                  -e ""$(&lt;change_master_to.sql.in), \
                          master_host='mysql-0.mysql', \
                          master_user='root', \
                          master_password='', \
                          master_connect_retry=10; \
                        start slave;"" || exit 1
            # in case of container restart, attempt this at-most-once.
            mv change_master_to.sql.in change_master_to.sql.orig
          fi

          # start a server to send backups when requested by peers.
          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
            ""xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root""
        volumemounts:
        - name: data
          mountpath: /var/lib/mysql
          subpath: mysql
        - name: conf
          mountpath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 100m
            memory: 100mi
      volumes:
      - name: conf
        emptydir: {}
      - name: config-map
        configmap:
          name: mysql
      - name: data
        persistentvolumeclaim:
            claimname: mysql-pvc


services:

# headless service for stable dns entries of statefulset members.
apiversion: v1
kind: service
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
    protocol: tcp
    targetport: 3306
  type: clusterip
  selector:
    app: mysql
---
# client service for connecting to any mysql instance for reads.
# for writes, you must instead connect to the master: mysql-0.mysql.
apiversion: v1
kind: service
metadata:
  name: mysql-read
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
    protocol: tcp
    targetport: 3306
  type: clusterip  
  selector:
    app: mysql


i have env file from which i created secret:

kubectl create secret prod-secrets \
       --from-env-file=env.example 


problem is that i can't access mysql (access denied), pods using credentials specified in secret, without stateful set, all works fine. all pods are running, no errors in logs

how to specify values in secrets into statefulset ?

i presume that i need somehow to pass those secrets to command section but have no idea how, example from kuberenets page assumes credentials are not used

if there is less complicated way to use stateful set for mysql,please let me know, thanks.
",<mysql><kubernetes><kubernetes-pod>,60780602,1,"at the end i managed to escape above complications by creating volume templates, created pv for each pod, both volumes are synchronized, no duplicate entries in database, and if one node fails,data are preserved

storage.yaml:

apiversion: storage.k8s.io/v1
kind: storageclass
metadata:

  name: localstorage

provisioner: kubernetes.io/no-provisioner
volumebindingmode: immediate
reclaimpolicy: delete
allowvolumeexpansion: true

---

kind: persistentvolume
apiversion: v1
metadata:
  name: mysql-01
  labels:
    type: local
spec:
  storageclassname: localstorage
  capacity:
    storage: 5gi
  accessmodes:
    - readwriteonce
  hostpath:
    path: ""/mnt/mysql01""

---
kind: persistentvolume
apiversion: v1
metadata:
  name: mysql-02
  labels:
    type: local
spec:
  storageclassname: localstorage
  capacity:
    storage: 5gi
  accessmodes:
    - readwriteonce
  hostpath:
    path: ""/mnt/mysql02""


statefulset:

apiversion: apps/v1
kind: statefulset
metadata:
  name: mysql-container
spec:
  servicename: mysql
  replicas: 2
  selector:
    matchlabels:
      app: mysql-container
  template:
    metadata:
      labels:
        app: mysql-container
    spec:
      containers:
      - name: mysql-container
        image: mysql:dev
        imagepullpolicy: ""ifnotpresent""
        envfrom:
          - secretref:
             name: prod-secrets
        ports:
        - containerport: 3306
        # container (pod) path
        volumemounts:
          - name: mysql-persistent-storage
            mountpath: /var/lib/mysql

        resources:
          requests:
            memory: 300mi
            cpu: 400m
          limits:
            memory: 400mi
            cpu: 500m
      restartpolicy: always

  volumeclaimtemplates:
    - metadata:
        name: mysql-persistent-storage
      spec:
        storageclassname: localstorage
        accessmodes: [""readwriteonce""]
        resources:
         requests:
          storage: 5gi
        selector:
         matchlabels:
          type: local

"
43118153,is it possible to attach google cloud persistent disk to a job in kubernetes?,"i searched the documents. i wrote a job descripiton and deployed it but the disk did not show up in kubernetes dashboard. 

is it possible to attach google cloud persistent disk to a job in kubernetes? 

update: here is the definition file. the disk site1-disk exists:

apiversion: batch/v1
kind: job
metadata:
  name: page-retriever
spec:
  template:
    metadata:
      name: page-retriever
    spec:
      containers:
      - name: page-retriever
        image: eu.gcr.io/crawler-162906/page-retriever:v1
      restartpolicy: onfailure
      volumes:
          - name: page-retriver-first-persistent-storage
            gcepersistentdisk:
              # this disk must already exist.
              pdname: site1-disk
              fstype: ext4

",<kubernetes><google-kubernetes-engine>,43124190,1,"you absolutely can. it looks like you're just missing the volumemounts section of your container def:

apiversion: batch/v1
kind: job
metadata:
  name: page-retriever
spec:
  template:
    metadata:
      name: page-retriever
    spec:
      containers:
      - name: page-retriever
        image: eu.gcr.io/crawler-162906/page-retriever:v1
        volumemounts:
        - name: page-retriver-first-persistent-storage
          mountpath: /path/to/where/the/disk/will/be/mounted/in/container

      restartpolicy: onfailure

      volumes:
          - name: page-retriver-first-persistent-storage
            gcepersistentdisk:
              # this disk must already exist.
              pdname: site1-disk
              fstype: ext4


without that, the volume 'page-retrier-first-persistent-storage' has no relationship to the container 'page-retriever'
"
77862098,kubernetes nginx ingress returning 400,"what i did:

install docker desktop for windows
enabled kubernetes (throught &quot;settings&quot; -&gt; &quot;kubernetes&quot; -&gt; &quot;enable kubernetes&quot;)
installed nginx ingress via yaml manifest: https://kubernetes.github.io/ingress-nginx/deploy/
installed kubernetes-dashboard via yaml manifest: https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#deploying-the-dashboard-ui
checked that dashboard services and pods working fine:

took a minimal ingress example (https://kubernetes.io/docs/concepts/services-networking/ingress/#the-ingress-resource) and created ingress manifest on it:

apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: root-ingress
  namespace: kubernetes-dashboard
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressclassname: nginx
  rules:
  - http:
      paths:
      - path: /dashboard
        pathtype: prefix
        backend:
          service:
            name: kubernetes-dashboard
            port:
              number: 443


ingress was succefuly started.


i am keep getting 400 response from that path.


curl -k -v https://localhost response on localhost:


ps c:\windows\system32&gt; curl.exe -k -v https://localhost
    *   trying [::1]:443...
    * connected to localhost (::1) port 443
    * schannel: disabled automatic use of client certificate
    * alpn: curl offers http/1.1
    * alpn: server accepted http/1.1
    * using http/1.1
    &gt; get / http/1.1
    &gt; host: localhost
    &gt; user-agent: curl/8.4.0
    &gt; accept: */*
    &gt;
    * schannel: remote party requests renegotiation
    * schannel: renegotiating ssl/tls connection
    * schannel: ssl/tls connection renegotiated
    * schannel: remote party requests renegotiation
    * schannel: renegotiating ssl/tls connection
    * schannel: ssl/tls connection renegotiated
    &lt; http/1.1 404 not found
    &lt; date: mon, 22 jan 2024 20:20:40 gmt
    &lt; content-type: text/html
    &lt; content-length: 146
    &lt; connection: keep-alive
    &lt; strict-transport-security: max-age=15724800; includesubdomains
    &lt;
    &lt;html&gt;
    &lt;head&gt;&lt;title&gt;404 not found&lt;/title&gt;&lt;/head&gt;
    &lt;body&gt;
    &lt;center&gt;&lt;h1&gt;404 not found&lt;/h1&gt;&lt;/center&gt;
    &lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
    &lt;/body&gt;
    &lt;/html&gt;
    * connection #0 to host localhost left intact


curl -k -v https://localhost response on localhost/dashboard:

    ps c:\windows\system32&gt; curl.exe -k -v https://localhost/dashboard
    *   trying [::1]:443...
    * connected to localhost (::1) port 443
    * schannel: disabled automatic use of client certificate
    * alpn: curl offers http/1.1
    * alpn: server accepted http/1.1
    * using http/1.1
    &gt; get /dashboard http/1.1
    &gt; host: localhost
    &gt; user-agent: curl/8.4.0
    &gt; accept: */*
    &gt;
    * schannel: remote party requests renegotiation
    * schannel: renegotiating ssl/tls connection
    * schannel: ssl/tls connection renegotiated
    * schannel: remote party requests renegotiation
    * schannel: renegotiating ssl/tls connection
    * schannel: ssl/tls connection renegotiated
    &lt; http/1.1 400 bad request
    &lt; date: mon, 22 jan 2024 20:23:04 gmt
    &lt; transfer-encoding: chunked
    &lt; connection: keep-alive
    &lt; strict-transport-security: max-age=15724800; includesubdomains
    &lt;
    client sent an http request to an https server.
    * connection #0 to host localhost left intact

",<kubernetes><kubernetes-ingress><docker-desktop>,77862761,1,"quick fix
just use nodeport
apiversion: v1
kind: service
metadata:
  name: kubernetes-dashboard-service
  namespace: kubernetes-dashboard
spec:
  selector:
    k8s-app: kubernetes-dashboard
  type: nodeport
  ports:
    - protocol: tcp
      port: 8443
      targetport: 8443
      nodeport: 31000

dashboard will be available at https://localhost:31000
root cause
what you are sending looks like this
browser ==https==&gt; ingress ==http==&gt; k8s-dashboard
but expected is
browser ==https==&gt; ingress ==https==&gt; k8s-dashboard
the issues it that k8s-dashboard service expects https request, this is the reason why you get client sent an http request to an https server.
it was decrypted by ingress and later on forward by using plain http
solution
1 export tcp port without using ingress
instead of using ingress you can use nodeport (i gave it in quick fix) or loadbalancer. idea is to work on iso layer 4(ip) instead of routing it using layer 7(http)
2 use ssl-passthrough from nginx-ingress
to make it work it needs two changes

modify nginx ingress controller using kubectl edit

kubectl -n ingress-nginx edit deployment ingress-nginx-controller

you have to add - --enable-ssl-passthrough(just one line) to the args
after change it should look like
...
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(pod_namespace)/ingress-nginx-controller
        - --election-id=ingress-nginx-leader
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(pod_namespace)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        - --enable-ssl-passthrough
...


add proper ingress route

when route is created you can't specify path like /dashboard because k8s dashboard doesn't know it's working in some subpath, so using rewrite will not resolve it as links generated by dashboard will be wrong(i wrote more about it here)
so we need ingress route like this (with ssl-passthrough and backend-protocol)
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: root-ingress
  namespace: kubernetes-dashboard
  annotations:
    nginx.ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;
    nginx.ingress.kubernetes.io/backend-protocol: https
spec:
  ingressclassname: nginx
  rules:
  - http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: kubernetes-dashboard
            port:
              number: 443

3 disable https in kubernetes dashboard (i didn't try)
when you disable https in kubernetes dashboard your flow will look like this
browser ==https==&gt; ingress ==http==&gt; k8s-dashboard
it will perfectly match what you have right now and should work without issues
as i didn't try it i will leave this article how to do it.
there is also raw manifest but maybe a little old
"
60263943,ability to exclude one page from https redirection in nginx ingress controller,"i have an app in kubernetes which is served over https. so now i would like to exclude one url from that rule and use http to serve it for performance reasons. i am struggling with that the whole day and it seems impossible.

these are my ingress yaml:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    field.cattle.io/publicendpoints: '[{""addresses"":[""172.31.1.11""],""port"":443,""protocol"":""https"",""servicename"":""myservice:myservice"",""ingressname"":""myservice:myservice"",""hostname"":""app.server.test.mycompany.com"",""path"":""/"",""allnodes"":true}]'
    kubernetes.io/ingress.class: nginx
  creationtimestamp: ""2020-02-17t13:14:19z""
  generation: 1
  labels:
    app-kubernetes-io/instance: mycompany
    app-kubernetes-io/managed-by: tiller
    app-kubernetes-io/name: mycompany
    helm.sh/chart: mycompany-1.0.0
    io.cattle.field/appid: mycompany
  name: mycompany
  namespace: mycompany
  resourceversion: ""565608""
  selflink: /apis/extensions/v1beta1/namespaces/mycompany/ingresses/mycompany
  uid: c6b93108-a28f-4de6-a62b-487708b3f5d1
spec:
  rules:
  - host: app.server.test.mycompany.com
    http:
      paths:
      - backend:
          servicename: mycompany
          serviceport: 80
        path: /
  tls:
  - hosts:
    - app.server.test.mycompany.com
    secretname: mycompany-tls-secret
status:
  loadbalancer:
    ingress:
    - ip: 172.31.1.11


apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    field.cattle.io/publicendpoints: '[{""addresses"":[""172.31.1.1""],""port"":80,""protocol"":""http"",""servicename"":""mycompany:mycompany"",""ingressname"":""mycompany:mycompany-particular-service"",""hostname"":""app.server.test.mycompany.com"",""path"":""/account_name/particular_service/"",""allnodes"":true}]'
    nginx.ingress.kubernetes.io/force-ssl-redirect: ""false""
    nginx.ingress.kubernetes.io/use-regex: ""true""
  creationtimestamp: ""2020-02-17t13:14:19z""
  generation: 1
  labels:
    app-kubernetes-io/instance: mycompany
    app-kubernetes-io/managed-by: tiller
    app-kubernetes-io/name: mycompany
    helm.sh/chart: mycompany-1.0.0
    io.cattle.field/appid: mycompany
  name: mycompany-particular-service
  namespace: mycompany
  resourceversion: ""565609""
  selflink: /apis/extensions/v1beta1/namespaces/mycompany/ingresses/mycompany-particular-service
  uid: 88127a02-e0d1-4b2f-b226-5e8d160c1654
spec:
  rules:
  - host: app.server.test.mycompany.com
    http:
      paths:
      - backend:
          servicename: mycompany
          serviceport: 80
        path: /account_name/particular_service/
status:
  loadbalancer:
    ingress:
    - ip: 172.31.1.11


so as you can see from above i would like to server /particular_service/ over http. ingress, however, redirects to https as tls is enabled for that host in the first ingress.

is there any way to disable tls just for that one specific path when the same host is being used for configuration?

in short summary i would like to have:

https://app.server.test.mycompany.com
but
http://app.server.test.mycompany.com/account_name/particular_service/

",<nginx><kubernetes><kubernetes-ingress><nginx-ingress>,60265474,1,"i've tested with 2 ingress of the same domain, the first one with tls enabled and the second without tls and it worked.
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
  name: echo-https
spec:
  tls:
  - hosts:
    - myapp.mydomain.com
    secretname: https-myapp.mydomain.com
  rules:
  - host: myapp.mydomain.com
    http:
      paths:
      - backend:
          servicename: echo-svc
          serviceport: 80
        path: /
---
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
  name: echo-http
spec:
  rules:
  - host: myapp.mydomain.com
    http:
      paths:
      - backend:
          servicename: echo-svc
          serviceport: 80
        path: /insecure

by the nginx docs:

by default the controller redirects http clients to the https port 443 using a 308 permanent redirect response if tls is enabled for that ingress.
this can be disabled globally using ssl-redirect: &quot;false&quot; in the nginx config map, or per-ingress with the nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot; annotation in the particular resource.

please let me if that helps.
"
41554845,kubernetes - ingress / service / lb,"i am new to k8s and this is my first time trying to get to grips with it. i am trying to set up a basic nodejs express api using this deployment.yml:

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: api
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: api
    spec:
      containers:
      - image: registry.gitlab.com/&lt;project&gt;/&lt;app&gt;:&lt;tag&gt;
        imagepullpolicy: always
        name: api
        env:
        - name: port
          value: ""8080""
        ports:
          - containerport: 8080
            hostport: 80
        livenessprobe:
          httpget:
            path: /healthz
            port: 8080
          initialdelayseconds: 30
          timeoutseconds: 1
        readinessprobe:
          httpget:
            path: /healthz
            port: 8080
          initialdelayseconds: 30
          timeoutseconds: 1
      imagepullsecrets:
        - name: registry.gitlab.com


which is being deployed via gitlab-ci. this is working and i have set up a service to expose it:

apiversion: v1
kind: service
metadata:
  name: api-svc
  labels:
    app: api-svc
spec:
  ports:
  - port: 80
    targetport: 80
    protocol: tcp
    name: http
  selector:
    app: api
  type: loadbalancer


but i have been looking into ingress to have a single point of entry for possibly multiple services. i have been reading through kubernetes guides and i read through this kubernetes ingress example and this is the ingress.yml i created:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress
spec:
  backend:
    servicename: api-svc
    serviceport: 80


but this did not work, when i visited the external ip address that was generated from the ingress and i just 502 error pages. 

could anyone point me in the right direction, what am i doing wrong or what am i missing? i see that in the example link above that there is an nginx-rc.yml which i deployed exactly like in the example and that was created but still got nothing from the endpoint. the api was accessible from the service external ip though..

many thanks
",<node.js><kubernetes><google-compute-engine><google-kubernetes-engine><gitlab-ci>,41634785,1,"thought i would post my working deployment/service/ingress

so after much effort in getting this working, here is what i used to get it working:

deployment

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: backend-api-v2
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: backend-api-v2
    spec:
      containers:
      - image: registry.gitlab.com/&lt;project&gt;/&lt;app&gt;:&lt;tag&gt;
        imagepullpolicy: always
        name: backend-api-v2
        env:
        - name: port
          value: ""8080""
        ports:
          - containerport: 8080
        livenessprobe:
          httpget:
            # path to probe; should be cheap, but representative of typical behavior
            path: /healthz
            port: 8080
          initialdelayseconds: 30
          timeoutseconds: 5
        readinessprobe:
          httpget:
            path: /healthz
            port: 8080
          initialdelayseconds: 30
          timeoutseconds: 5
      imagepullsecrets:
        - name: registry.gitlab.com


service

apiversion: v1
kind: service
metadata:
  name: api-svc-v2
  labels:
    app: api-svc-v2
spec:
  type: nodeport
  ports:
  - port: 80
    targetport: 8080
    nodeport: 31810
    protocol: tcp
    name: http
  selector:
    app: backend-api-v2


ingress

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: app-ingress
spec:
  rules:
  - host: api.foo.com
    http:
      paths:
      - path: /v1/*
        backend:
          servicename: api-svc
          serviceport: 80
      - path: /v2/*
        backend:
          servicename: api-svc-v2
          serviceport: 80


the important bits to notice as @tigraine pointed out is the service is using type: nodeport and not loadbalancer, i have also defined a nodeport but i believe it will create one if you leave it out. 

it will use the default-http-backend for any routes that don't match the rules this is a default container that gke runs in the kube-system namespace. so if i visited http://api.foo.com/bob i get the default response of default backend - 404.

hope this helps 
"
60184344,sonar cannot be access via istio virtual service but can be locally accessed after port forwarding,"i am trying to implement sonarqube in a kubernetes cluster. the deployment is running properly and is also exposed via a virtual service. i am able to open the ui via the localhost:port/sonar but i am not able to access it through my external ip. i understand that sonar binds to localhost and does not allow access from outside the remote server. i am running this on gke with a mysql database. here is my yaml file:

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: sonarqube
  namespace: sonar
  labels:
    service: sonarqube
    version: v1
spec:
  replicas: 1
  template:
    metadata:
      name: sonarqube
      labels:
        name: sonarqube
    spec:
      terminationgraceperiodseconds: 15
      initcontainers:
        - name: volume-permission
          image: busybox
          command:
            - sh
            - -c
            - sysctl -w vm.max_map_count=262144
          securitycontext:
            privileged: true
      containers:
        - name: sonarqube
          image: sonarqube:6.7
          resources:
            limits:
              memory: 4gi
              cpu: 2
            requests:
              memory: 2gi
              cpu: 1
          args:
            - -dsonar.web.context=/sonar
            - -dsonar.web.host=0.0.0.0
          env:
            - name: sonarqube_jdbc_username
              valuefrom:
                secretkeyref:
                  name: cloudsql-db-credentials
                  key: username
            - name: sonarqube_jdbc_password
              valuefrom:
                secretkeyref:
                  name: cloudsql-db-credentials
                  key: password
            - name: sonarqube_jdbc_url
              value: jdbc:mysql://***.***.**.*:3306/sonar?useunicode=true&amp;characterencoding=utf8
          ports:
            - containerport: 9000
              name: sonarqube-port
---
apiversion: v1
kind: service
metadata:
  labels:
    service: sonarqube
    version: v1
  name: sonarqube
  namespace: sonar
spec:
  selector:
    name: sonarqube
  ports:
    - name: http
      port: 80
      targetport: sonarqube-port
---
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: sonarqube-internal
  namespace: sonar
spec:
  hosts:
    - sonarqube.staging.jeet11.internal
    - sonarqube
  gateways:
    - default/ilb-gateway
    - mesh
  http:
    - route:
        - destination:
            host: sonarqube
---
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: sonarqube-external
  namespace: sonar
spec:
  hosts:
    - sonarqube.staging.jeet11.com
  gateways:
    - default/elb-gateway
  http:
    - route:
        - destination:
            host: sonarqube
---


the deployment completes successfully. my exposed services gives a public ip that has been mapped to the host url but i am unable to access the service at the host url. 

i need to change the mapping such that sonar binds with the server ip but i am unable to understand how to do that. i cannot bind it to my cluster ip, neither to my internal or external service ip. 

what should i do? please help!
",<kubernetes><google-cloud-platform><sonarqube><google-kubernetes-engine>,64311682,1,"i had the same issue recently and i managed to get this resolved today.
i hope the following solution will work for anyone facing the same issue!.
environment

cloud provider: azure - aks

this should work regardless of whatever provider you use.


istio version: 1.7.3
k8 version: 1.16.10

tools - debugging

kubectl logs -n istio-system -l app=istiod

logs from istiod and events happening in the control plane.


istioctl analyze -n &lt;namespace&gt;

this generally gives you any warnings and errors for a given namespace.
lets you know if things are misconfigured.


kiali - istioctl dashboard kiali

see if you are getting inbound traffic.
also, shows you any misconfigurations.


prometheus - istioctl dashboard prometheus

query metric - istio_requests_total. this shows you the traffic going into the service.
if there's any misconfiguration you will see the destination_app as unknown.



issue

unable to access sonarqube ui via external ip, but accessible via localhost (port-forward).
unable to route traffic via istio ingressgateway.

solution
sonarqube service manifest
apiversion: v1
kind: service
metadata:
  name: sonarqube
  namespace: sonarqube
  labels:
    name: sonarqube
spec:
  type: clusterip
  ports:
  - name: http
    port: 9000
    targetport: 9000
  selector:
    app: sonarqube
status:
  loadbalancer: {}


your targetport is the container port. to avoid any confusion just assign the service port number as same as the service targetport.
the port name is very important here. istio required the service ports to follow the naming form of protocol-suffix where the -suffix part is optional - kia0601 - port name must follow [-suffix] form

istio gateway and virtualservice manifest for sonarqube
apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: sonarqube-gateway
  namespace: sonarqube
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 9000
      name: http
      protocol: http
    hosts:
    - &quot;xxxx.xxxx.com.au&quot;
---
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: sonarqube
  namespace: sonarqube
spec:
  hosts:
  - &quot;xxxx.xxxx.com.au&quot;
  gateways:
  - sonarqube-gateway
  http:
  - route:
    - destination:
        host: sonarqube
        port:
          number: 9000


gateway protocol must be set to http.
gateway server port and virtualservice destination port is the same. if you have different app service port, then your virtualservice destination port number should match the app service port. the gateway server port should match the app service targetport.
now comes to the fun bit! the hosts. if you want to access the service outside of the cluster, then you need to have your host-name (whatever host-name that you want to map the sonarqube server) as an dns a record mapped to the external public ip address of the istio-ingressgateway.
to get the external-ip address of the ingressgateway, run kubectl -n istio-system get service istio-ingressgateway.
if you do a simple nslookup (run - nslookup &lt;hostname&gt;), the ip address you get must match with the ip address that is assigned to the istio-ingressgateway service.

expose a new port in the ingressgateway

note that your sonarqube gateway port is a new port that you are introducing to kubernetes and youre telling the cluster to listen on that port. but your load balancer doesnt know about this port. therefore, you need to open the specified gateway port on your kubernetes external load balancer. ref - info
you dont need to manually change your load balancer service. you just need to update the ingress gateway to include the new port, which will update the load balancer automatically.
you can identify if the port is causing issues by running istioctl analyze -n sonarqube. you should get the following warning;

[33mwarn[0m [ist0104] (gateway sonarqube-gateway.sonarqube) the gateway refers to a port that is not exposed on the workload (pod selector istio=ingressgateway; port 9000) error: analyzers found issues when analyzing namespace: sonarqube. see https://istio.io/docs/reference/config/analysis for more information about causes and resolutions.

you should get the corresponding error in the control plane. run kubectl logs -n istio-system -l app=istiod.
at this point you need to update the istio ingressgateway service to expose the new port. run kubectl edit svc istio-ingressgateway -n istio-system and add the following section to the ports.


bypass creating a new port

in the previous section you saw how to expose a new port. this is optional and depending on your use case.
in this section you will see how to use a port that is already exposed.
if you look at the service of the istio-ingressgateway. you can see that there are default ports exposed. here we are going to use port 80.



your setup will look like the following;




to void specifying the port with your host name just add match uri prefix, as shown in the virtualservice manifest.

time for testing

if everything works up to this point as expected, then you are good to go.



during testing i made one mistake by not specifying the port. if you get 404 status, which is still a good thing, in this way you can verify what server it is using. if you setup things correctly, it should use the istio-envoy server, not the nginx.



without specifiying the port. this will only work if you add the match uri prefix.


"
59124945,helm pre-install yaml for config,"ive dependency in priority class inside my k8s yaml configs files and i need to install before any of my yaml inside the template folder
the prio class

apiversion: scheduling.k8s.io/v1beta1
kind: priorityclass
metadata:
  name: ocritical
value: 1000
globaldefault: false


after reading the helm docs it seems that i can use the pre-install hook

ive changed my yaml and add anotiations section with pre-hook, and still it doesnt works, any idea what i miss here?  

apiversion: scheduling.k8s.io/v1beta1
kind: priorityclass
metadata:
  name: ocritical
  annotations:
    ""helm.sh/hook"": pre-install
value: 1000
globaldefault: false


the yaml is located inisde the template folder
",<kubernetes><kubernetes-helm>,59140025,1,"you put quotation marks for helm.sh/hook annotation which is incorrect - you can only add quotation marks for values of them.
you can add description field in your configuration file, remember that this field is an arbitrary string. it is meant to tell users of the cluster when they should use this priorityclass. 

your priorityclass should looks like this:

apiversion: scheduling.k8s.io/v1beta1
kind: priorityclass
metadata:
  name: ocritical
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
value: 1000
globaldefault: false
description: ""this priority class should be used for xyz service pods only.""


more information about proper configuration of priorityclass you can find here: priorityclass.
more information about installing hooks you can find here: helm-hooks.

i hope it helps.
"
59321083,helm-charts: share env var in sibling charts' deployment/configmaps,"files structure (minimized)

there is a charts folder containing multiple charts.

charts/
  foo-chart/
    templates/
       deployment.yml
       secrets.yml
  bar-chart/
    templates/
      configmaps/
        script.yml


secrets.yml

defines a token:

apiversion: v1
kind: secret
metadata:
  name: {{ .release.name }}-secret
  labels:
    app: {{ include ""metrics.name"" . }}
    chart: {{ include ""metrics.chart"" . }}
    release: {{ .release.name }}
    heritage: {{ .release.service }}
type: opaque
data:
  # note: service token has to fit the nist requirement
  servicetoken: {{ randascii 40 | b64enc }}


deployment.yml

runs a command which uses an environmental variable which uses a secret:

containers:
  command:
  - fancy-binary
  - -token
  - $(auth_token)
  env:
  - name: auth_token
  valuefrom:
    secretkeyref:
    name: {{ .release.name }}-secret
    key: servicetoken


script.yml

is supposed to run bash command (django admin-command) and use environmental variable as well:

# create a service token
django-admin service_token_add $(auth_token)




issues


is the auth_token going to be visible in script.yml?
does the env valuefrom auto-set the value of auth_token (is deployment going to work)?

",<kubernetes><kubernetes-helm>,59353940,1,"answering to your first question, environment variables passed through env field of a container will be visible everywhere in your container
so also in the script you run unless you explicitly unset it.

you can check it by creating this (you should be able to copypaste the example):

apiversion: v1
kind: secret
metadata:
  name: test-secret
type: opaque
data:
  servicetoken: mtizndu2nzg5mao=     # base64 encoded string: ""1234567890""

---
apiversion: v1
kind: pod
metadata:
  name: test
spec:
  containers:
  - args:
    - echo
    - hello
    - $(auth_token)
    name: test
    env:
    - name: auth_token
      valuefrom:
          secretkeyref:
          name: test-secret
          key: servicetoken
    image: centos:7
  restartpolicy: never


and then when pod completes, check logs and you will see your token:

$ kubectl logs test
hello 1234567890


the same applies to scripts.

answering you second question; as you probably already saw in example above, using env valuefrom will indeed auto-set your env to the value from secret.

let me know if it was helpful.
"
75483327,approach for configmap and secret for a yaml file,"i have a yaml file which needs to be loaded into my pods, this yaml file will have both sensitive and non-sensitive data, this yaml file need to be present in a path which i have included as env in containers.
        env:
          - name:  config_path
            value: /myapp/config/config.yaml

if my understanding is right, the configmap was the right choice, but i am forced to give the sensitive data like password as plain text in the values.yaml in helm chart.
apiversion: v1
kind: configmap
metadata:
  name: {{ .release.name }}-config
  labels:
    app: {{ .release.name }}-config
data:
  config.yaml: |

    configuration: 
      settings: 
        password: &quot;{{.values.config.password}}&quot;
        username: myuser

values.yaml
config:
  password: &quot;mypassword&quot;

mounted the above config map as follows
        volumemounts:
          - name:  {{ .release.name }}-config
            mountpath:  /myapp/config/

so i wanted to try secret, if i try secret, it is loading as environment variables inside pod, but it is not going into this config.yaml file.
if i convert the above yaml file into secret instead of configmap , should i convert the entire config.yaml into base64 secret? my yaml file has more entries and it will look cumbersome and i dont think it as a solution.
if i take secret as a stringdata then the base64 will be taken as it is.
how do i make sure that config.yaml loads into pods with passwords not exposed in the values.yaml is there a way to combine configmap and secret
i read about projected volumes, but i dont see a use case for merging configmap and secrets into single config.yaml
any help would be appreciated.
",<kubernetes><kubernetes-helm><configmap><kubernetes-secrets>,75483929,1,"kubernetes has no real way to construct files out of several parts.  you can embed an entire (small) file in a configmap or a secret, but you can't ask the cluster to assemble a file out of parts in multiple places.
in helm, one thing you can do is to put the configuration-file data into a helper template
{{- define &quot;config.yaml&quot; -}}
configuration: 
  settings: 
    password: &quot;{{.values.config.password}}&quot;
    username: myuser
{{ end -}}

in the configmap you can use this helper template rather than embedding the content directly
apiversion: v1
kind: configmap
metadata: { ... }
data:
  config.yaml: |
{{ include &quot;config.yaml&quot; . | indent 4 }}

if you move it to a secret you do in fact need to base64 encode it.  but with the helper template that's just a matter of invoking the template and encoding the result.
apiversion: v1
kind: secret
metadata: { ... }
data:
  config.yaml: {{ include &quot;config.yaml&quot; . | b64enc }}

if it's possible to set properties in this file directly via environment variables (like spring properties) or to insert environment-variable references in the file (like a ruby erb file) that could let you put the bulk of the file into a configmap, but use a secret for specific values; you would need a little more wiring to also make the environment variables available.
you briefly note a concern around passing the credential as a helm value.  this does in fact require having it in plain text at deploy time, and an operator could helm get values later to retrieve it.  if this is a problem, you'll need some other path to inject or retrieve the secret value.
"
54994813,k8s - ingress - route 2 different applications,"in my minikube k8s cluster, i have 2 applications/services. 

this is my ingress controller for the services i have. 

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-tutorial
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  backend:
    servicename: default-http-backend
    serviceport: 80
  rules:
  - host: mysite.com
    http:
      paths:
      - path: /sva
        backend:
          servicename: app-a
          serviceport: 8080
      - path: /svb
        backend:
          servicename: app-b
          serviceport: 8150


when i type mysite.com/sva or mysite.com/svb it routes to appropriate services. however, none of the static files (js, css, images etc) is loaded as they seem to request for the resources from mysite.com instead of mysite.com/sva/

how can i make it look for the resources under specific service?
",<nginx><kubernetes><kubernetes-ingress>,55006652,1,"based on harsh manvar answer, looks like rewrite does not work on the static resources as of now.

this is a workaround i am planning to follow to route diff apps like sva.mysite.com or svb.mysite.com. it works fine. just adding this as an answer if somebody faces similar problem.

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-tutorial
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  backend:
    servicename: default-http-backend
    serviceport: 80
  rules:
  - host: sva.mysite.com
    http:
      paths:
      - path: /
        backend:
          servicename: app-a
          serviceport: 8080
  - host: svb.mysite.com
    http:
      paths:
      - path: /
        backend:
          servicename: app-b
          serviceport: 8150

"
55009028,configure ssl certificates in kubernetes with cert-manager istio ingress and letsencrypt,"i'm trying to configure ssl certificates in kubernetes with cert-manager, istio ingress and letsencrypt. i have installed istio with helm, cert-manager, created clusterissuer and then i'm trying to create a certificate. the acme challenge can't be validated, i'm trying to do it with http01 and can't figure it out how to use istio ingress for this. istio is deployed with following options:



helm install --name istio install/kubernetes/helm/istio `
--namespace istio-system `
--set global.controlplanesecurityenabled=true `
--set grafana.enabled=true`
--set tracing.enabled=true 
--set kiali.enabled=true `
--set ingress.enabled=true




certificate configuration:



apiversion: certmanager.k8s.io/v1alpha1
kind: certificate
metadata:
  name: example.com
  namespace: istio-system
spec:
  secretname: example.com
  issuerref:
    name: letsencrypt-staging
    kind: clusterissuer
  commonname: 'example.com'
  dnsnames:
  - example.com
  acme:
    config:
    - http01:
        ingress: istio-ingress
      domains:
      - example.com




when trying this way, for some reason, istio-ingress can't be found, but when trying to specify ingressclass: some-name, instead of ingress: istio-ingress, i get 404 because example.com/.well-known/acme-challenge/token can't be reached.
how can this be solved? thank you!
",<ssl><kubernetes><kubernetes-ingress><istio><cert-manager>,55139188,1,"the solution was to move dns to azure and use dns validation for generating the certificate. i also used istio-1.1.0-rc.3 and configured the gateway in the following way:



apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: mygateway
spec:
  selector:
    istio: ingressgateway # use istio default ingress gateway
  servers:
  - hosts:
    - 'mydomain.com'
    port:
      name: http-bookinfo
      number: 80
      protocol: http
    tls:
      httpsredirect: true
  - hosts:
    - 'mydomain.com'
    port:
      name: https-bookinfo
      number: 443
      protocol: https
    tls:      
      mode: simple
      servercertificate: ""use sds"" #random string, because servercertificate and 
      #privatekey are required for tls.mode=simple
      privatekey: ""use sds"" 
      credentialname: ""istio-bookinfo-certs-staging"" #this must match the secret name 
      #from the certificate



in order to work enable sds at ingress gateway:



helm template install/kubernetes/helm/istio/ --name istio `
--namespace istio-system -x charts/gateways/templates/deployment.yaml `
--set gateways.istio-egressgateway.enabled=false `
--set gateways.istio-ingressgateway.sds.enabled=true &gt; `
$home/istio-ingressgateway.yaml

 kubectl apply -f $home/istio-ingressgateway.yaml



"
54942468,helm deployment status if tests failed,"is it possible to make helm charts deployment to fail if test which is run before installation fails? because now despite the test fails, status is 'deployed'.

my test, which checks if mongodb is deployed and is reachable:

apiversion: v1
kind: pod
metadata:
  name: ""{{ .release.name }}-database-connection-test""
  annotations:
    ""helm.sh/hook"": pre-install,test-success
    ""helm.sh/hook-delete-policy"": before-hook-creation
spec:
  containers:
  - name: {{ .release.name }}-database-connection-test
    image: {{ template ""mongo.image"" . }}
    imagepullpolicy: always
    env:
      - name: host
        value: {{ .values.mongo.host }}
      - name: port
        value: {{ .values.mongo.port | quote }}
      - name: database_name
        value: {{ .values.mongo.databasename }}
      - name: username
        value: {{ .values.mongo.username }}
      - name: password
        value: {{ .values.mongo.password }}
    command: [""sh"", ""-c"", ""mongo --username $username --password $password --authenticationdatabase $database_name --host $host --port $port""]
  restartpolicy: never

",<testing><kubernetes><kubernetes-helm>,54980079,1,"so in general this can be achieved setting resource type as job.
job will be blocking tiller until it will complete. 
there is a small issue here: if job will not complete it will be blocking helm chart deployment infinite amount of time. to avoid that, need to set spec.activedeadlineseconds. it will timeout the job if it will not complete until the set time limit.

apiversion: batch/v1
kind: job
metadata:
  name: ""{{ .release.name }}-database-connection-test""
  annotations:
    ""helm.sh/hook"": pre-install,test-success
    ""helm.sh/hook-delete-policy"": before-hook-creation
spec:
  ttlsecondsafterfinished: 300
  backoffpolicy: 1
  activedeadlineseconds: 100
  template:
    spec:
      containers:
      - name: {{ .release.name }}-database-connection-test
        image: {{ template ""mongo.image"" . }}
        imagepullpolicy: always
        env:
          - name: host
            value: {{ .values.mongo.host }}
          - name: port
            value: {{ .values.mongo.port | quote }}
          - name: database_name
            value: {{ .values.mongo.databasename }}
          - name: username
            value: {{ .values.mongo.username }}
          - name: password
            value: {{ .values.mongo.password }}
        command: [""sh"", ""-c"", ""mongo --username $username --password $password --authenticationdatabase $database_name --host $host --port $port""]
      restartpolicy: never


it's kind of a workaround, because initially helm test annotation shouldn't be used alongside other hooks.
"
60775608,error accessing external sql server from inside the minikube,"i have an external sql server, (on the internet accessible from my local system) that i am trying to call from inside the minikube. i am unable to do that. i have tried the 
calling an external service from within minikube

error that i am getting is ""sqlalchemy.exc.operationalerror: (pymssql.operationalerror) (20009, b'db-lib error message 20009, severity 9:\nunable to connect: adaptive server is unavailable or does not exist ""



i have already created pod --> service --> endpoints. all my clusters are under an ingress. please see the below code for the configuration that i have done. 

currently, i am passing the db host (1.1.1.1) as an environment variable to the pod and after this configuration, i am trying to pass the service name (sql-server) instead of db host name is this correct?
moreover, i am unable to ping the ip from inside the container. 

can anyone please help me. 

apiversion: v1
kind: endpoints
metadata:
  name: sql-server
subsets:
  - addresses:
      - ip: 1.1.1.1
    ports:
      - port: 1433


apiversion: v1
kind: service
metadata:
  name: sql-server
spec:
  type: clusterip
  ports:
    - port: 1433
      targetport: 1433

",<kubernetes><kubectl><kubernetes-ingress><minikube>,60777628,1,"i reproduced a similar scenario in my minikube system and this solution works as described. i will drive you through the setup and how to troubleshoot this issue. 

i have a linux server (hostname http-server) and i installed a http server (apache2) in on it that's serving a hello world message: 

user@http-server:~$ netstat -tan | grep ::80
tcp6       0      0 :::80                   :::*                    listen     
user@minikube-server:~$ curl 10.128.15.209
hello world!


now that we confirmed that my service is accessible from the machine where i have minikube installed, lets connect to minikube vm and check if i can access this http service: 

user@minikube-server:~$ minikube ssh
                         _             _            
            _         _ ( )           ( )           
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __  
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ curl 10.128.15.209
hello world!


great! this is looking good. if you can't access your service here, you have to check your network, something is preventing your minikube server from communicating with your service. 

now let's exit from this minikube ssh and create our endpoint: 

my endpoint manifest is looking like this: 

apiversion: v1
kind: endpoints
metadata:
  name: http-server
subsets:
  - addresses:
      - ip: 10.128.15.209
    ports:
      - port: 80


user@minikube-server:~$ kubectl apply -f http-server-endpoint.yaml
endpoints/http-server configured


let's create our service: 

apiversion: v1
kind: service
metadata:
  name: http-server
spec:
  ports:
    - port: 80
      targetport: 80


user@minikube-server:~$ kubectl apply -f http-server-service.yaml
service/http-server created


checking if our service exists and save it's clusterip for letter usage: 

user@minikube-server:~$$ kubectl get service
name          type        cluster-ip      external-ip   port(s)   age
http-server   clusterip   10.96.228.220   &lt;none&gt;        80/tcp    30m
kubernetes    clusterip   10.96.0.1       &lt;none&gt;        443/tcp   10d


now it's time to verify if we can access our service from a pod: 

kubectl run ubuntu -it --rm=true --restart=never --image=ubuntu bash


this command will create and open a bash session inside a ubuntu pod. 

in my case i'll install curl to be able to check if i can access my http server. you may need install mysql:

root@ubuntu:/# apt update; apt install -y curl


checking connectivity with my service using clusterip:

root@ubuntu:/# curl 10.128.15.209:80
hello world!


and finally using the service name (dns): 

root@ubuntu:/# curl http-server
hello world!


please run into all these steps and let me know if you have trouble on any and where.
"
74879854,helm - how to add a label block through values.yaml,"i have simple helm chart. i have a labels: block that i need to refer in a deployment
here's my values.yaml
labels:
    app: test-app
    group: test-group
    provider: test-provider

and in the templates/deployment.yaml i need to add the above whole labels block. so i did;
apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: {{ include &quot;accountmasterdata.fullname&quot; . }}
  namespace: {{ .values.namespace }}
  labels:
  {{ .values.labels | nindent 4 }}

  {{- include &quot;accountmasterdata.labels&quot; . | nindent 4 }} 

but i get the following error

wrong type for value; expected string; got map[string]interface {}

can someone help me with two things:

how can i solve this issue

and in the line where it says {{- include &quot;accountmasterdata.labels&quot; . | nindent 4 }} , where i can see the accountmasterdata.labels values? and how to override those?


thank you!
",<kubernetes><kubernetes-helm>,74880508,1,"iterating over a mapping is covered in the &quot;variables&quot; documentation:

for data structures that have both a key and a value, we can use range to get both. for example, we can loop through .values.favorite like this:
apiversion: v1
kind: configmap
metadata:
  name: {{ .release.name }}-configmap
data:
  myvalue: &quot;hello world&quot;
  {{- range $key, $val := .values.favorite }}
  {{ $key }}: {{ $val | quote }}
  {{- end }}


so in your template, you would handle the value of .values.labels like this:
  labels:
{{- range $name, $value := .values.labels }}
    {{ $name | quote }}: {{ $value | quote }}
{{- end -}}



and in the line where it says {{- include &quot;accountmasterdata.labels&quot; . | nindent 4 }} , where i can see the accountmasterdata.labels values? and how to override those?

is this a template you are writing? if so, where have you defined these values? presumably in your templates/ directory there exists a file that includes something like:
{{- define &quot;accountmasterdata.labels&quot; -}}
...
{{- end -}}

the contents of that block are what will get inserted at the point of reference.

lastly, in your template you have:
namespace: {{ .values.namespace }}

but you probably want to use .release.namespace instead:
namespace: {{ .release.namespace | quote }}


with the above changes in place, i end up with:
apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: {{ include &quot;accountmasterdata.fullname&quot; . }}
  namespace: {{ .release.namespace | quote }}
  labels:
{{- range $name, $value := .values.labels }}
    {{ $name | quote }}: {{ $value | quote }}
{{- end -}}
{{- include &quot;accountmasterdata.labels&quot; . | nindent 4 }} 

"
64950361,what is the best way to verify a deployment.yaml kubernetes file?,"i have the following deployment.yaml file in kuberentes:
apiversion: apps/v1
kind: deployment
metadata:
  name: basic-deployment
spec:
  replicas: 2
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: basic
    spec:
      containers:
      - name: basic
        image: nginx
        volumemounts:
        - name: config-volume
          mountpath: /etc/nginx/conf.d
      volumes:
      - name: config-volume
        configmap:
          name: basic-config


i am not sure how i can fix the following error when i run kubectl create -f basic-deployment.yaml:
the deployment &quot;basic-deployment&quot; is invalid: spec.template.metadata.labels: invalid value: map[string]string{&quot;app&quot;:&quot;basic&quot;}: selector does not match template labels
",<kubernetes><kubectl><kubernetes-deployment>,64950503,1,"apiversion: apps/v1
kind: deployment
metadata:
  name: basic-deployment
spec:
  replicas: 2
  selector:
    matchlabels:
      app: basic
  template:
    metadata:
      labels:
        app: basic
    spec:
      containers:
      - name: basic
        image: nginx
        volumemounts:
        - name: config-volume
          mountpath: /etc/nginx/conf.d
      volumes:
      - name: config-volume
        configmap:
          name: basic-config

basically, the selector match label in your deployment spec needs to match a label in your template. in your case, you have app: nginx as a matching label for the selector and you have app: basic in your template, so no match.
you would have to have something either one app: nginx or app: basic on both so that there is a match.
"
74964282,create kubernetes docker-registry secret from yaml file for each lookup namespaces?,"i am trying to dynamic lookup available namespaces and able to create secrets in the namespaces using below  helm chart.

templates/secrets.yaml

{{ range $index, $namespace := (lookup &quot;v1&quot; &quot;namespace&quot; &quot;&quot; &quot;&quot;).items }}
apiversion: v1
kind: secret
metadata:
  name: myregcred
  namespace: {{ $namespace.metadata.name }}
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: {{ template &quot;imagepullsecret&quot; . }}
{{- end}}


values.yaml

imagecredentials:
  registry: quay.io
  username: someone
  password: sillyness
  email: someone@host.com


_helpers.tpl

{{- define &quot;imagepullsecret&quot; }}
{{- with .values.imagecredentials }}
{{- printf &quot;{\&quot;auths\&quot;:{\&quot;%s\&quot;:{\&quot;username\&quot;:\&quot;%s\&quot;,\&quot;password\&quot;:\&quot;%s\&quot;,\&quot;email\&quot;:\&quot;%s\&quot;,\&quot;auth\&quot;:\&quot;%s\&quot;}}}&quot; .registry .username .password .email (printf &quot;%s:%s&quot; .username .password | b64enc) | b64enc }}
{{- end }}
{{- end }}

when  i run this helm chart, i get below error

error: installation failed: template: secrets/templates/_helpers.tpl:2:16: executing &quot;imagepullsecret&quot; at &lt;.values.imagecredentials&gt;: nil pointer evaluating interface {}.imagecredentials

i dont know what i am doing wrong here.
",<kubernetes><kubernetes-helm><go-templates>,74965784,1,"when you reference the named template &quot;imagepullsecret&quot; inside the range, the context &quot;.&quot; you are providing refers to the body of the loop, which does not have the &quot;values&quot; attribute.
try providing the root context instead:
{{ range $index, $namespace := (lookup &quot;v1&quot; &quot;namespace&quot; &quot;&quot; &quot;&quot;).items }}
apiversion: v1
kind: secret
metadata:
  name: myregcred
  namespace: {{ $namespace.metadata.name }}
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: {{ template &quot;imagepullsecret&quot; $ }}
---
{{- end}}

"
55093143,how to exclude customresourcedefinition from rendering in helm?,"i'm currently writing a helm chart for my multi-service application. in the application, i depend on customresources, which i apply before everything else with helm via the ""helm.sh/hook"": crd-install hook.

now i want to upgrade the application. helm fails because the crds are already installed.
in some gh issues, i read about the builtin .capabilities variable in helm templates. i want to wrap my crds with an ""if"" checking if the crd is already installed:  

{{- if (not (.capabilities.apiversions.has ""virtualmachineinstancepresets.kubevirt.io"")) }}


unfortunately, i misunderstood the apiversions property.
so my question is, does helm provide a way of checking whether a customapi is already installed, so i can exclude it from my helm pre-hook install?
",<kubernetes><kubernetes-helm>,59063154,1,"the simple answer for helm v2 is manually choose --no-crd-hook flag when running helm install.

the workaround by using builtin .capabilities variable can be a workaround. e.g., using this:

{{- if not (.capabilities.apiversions.has ""virtualmachineinstancepresets.kubevirt.io/v1beta1/myresource"") }}
apiversion: ...
{{- end}}


however, it also means you will never be able to manage the installed crds by helm again.

checkout a long answer from blog post helm v2 crd management which explained different approaches. however, i quote this:


  crd management in helm is, to be nice about it, utterly horrible.


personally i suggest managing crds via a separate chart out of app/library charts that depends on it, since they have totally different lifecycle.
"
55065273,inter container communication in kubernetes multi container pod,"i have a pod with 3 containers a, b, and c. i would like to access service in container a and b from c. neither localhost:&lt;port&gt; is working nor the 127.0.0.1.

my yaml 

apiversion: ""v1""
kind: pod
metadata:
  name: web3
  labels:
    name: web
    app: demo
spec:
  containers:
    - name: client
      image: ubuntu
      command: ['cat']
      tty: true
    - name: apache1
      image: nimmis/apache-php5
      ports:
        - containerport: 8080
          name: apacheport1
          protocol: tcp
    - name: apache2
      image: nimmis/apache-php5
      command: ['cat']
      tty: true
      ports:
        - containerport: 8088
          name: apacheport2
          protocol: tcp


what i am doing

kubectl apply -f example.yaml
kubectl exec -it web3 -c client bash


and then try to reach other 2 services

root@web3:/# curl http://localhost:8080
curl: (7) failed to connect to localhost port 8080: connection refused
root@web3:/# curl http://localhost:8088
curl: (7) failed to connect to localhost port 8088: connection refused
root@web3:/# curl http://localhost:80

&lt;!doctype html public ""-//w3c//dtd xhtml 1.0 transitional//en"" ""http://www.w3.org/tr/xhtml1/dtd/xhtml1-transitional.dtd""&gt;
&lt;html xmlns=""http://www.w3.org/1999/xhtml""&gt;
  &lt;!--
    modified from the debian original for ubuntu



questions
 how to make the first 2 curl work. (i do not want to use the service since my use case is only for testing purpose)
 why there is an open port 80 when i haven't exposed the port.
",<kubernetes><kubernetes-pod>,55068246,1,"the point is that with nimmis/apache-php5 apache is listening on port 80.
so, it's port 80 which is exposed.
through containerport: &lt;p&gt; you are not saying to expose container's port 80 to &lt;p&gt;, but rather exposing port &lt;p&gt; itself. also, as written in the docs, not specifying a port here does not prevent that port from being exposed..

i did not find a way to map an internal container port to a different port in the pod. however, you may map the internal container port to a host port through field hostport.

apiversion: ""v1""
kind: pod
metadata:
name: web3
labels:
name: web
app: demo
spec:
containers:
- name: client
  image: ubuntu
  command: ['cat']
  tty: true
- name: apache1
  image: nimmis/apache-php5
  ports:
    - containerport: 80
      name: apacheport1
      hostport: 8002
      protocol: tcp
- name: apache2
  image: nimmis/apache-php5
  command: ['cat']
  tty: true
  ports:
    - containerport: 80
      hostport: 8001
      name: apacheport2
      protocol: tcp


then you get the ip of the node, e.g., on minikube

$ minikube ip  # e.g., 192.168.97.100


and check that, from client, you can access the apache services:

$ kubectl exec -it web3 -c client bash
# apt-get update &amp;&amp; apt-get install curl
# curl 192.168.99.100:8002 

"
74975563,kubernetes ingress route one host,"in my azure aks cluster i was defined namespace dev with some services.my idea is connect to them from react static app under ssl.for this reason i maded ingress controller with certificate manager. the controller was work properly but with one unexpected bug.i use my own domain with dns zone and records.the idea is when i go to qa.mysite.com , ingress must go to payment-svc &quot;/api/health&quot; and if /web go to web-svc &quot;/&quot;.the problem is that when i use path prefix with &quot;/&quot; the controller does not route to any related service.the route only happens when i use deafult prefix &quot;/&quot;.the error is not found for nginx or browser not found
    apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-portals
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt
spec:
  tls:
    - hosts:
      - qa.mysite.com # update ip address here
      secretname: app-web-cert
  rules:
  - host: qa.mysite.com # update ip address here
    http:
      paths:
      - path: /web  # i tried with a lot of services and prefixes but same result
        pathtype: prefix
        backend:
          service:
            name: webapp1-svc # after i tried with the same service payment-servive but not work on qa.mysite.com/web/api/health
            port: 
              number: 80
      - path: /  # the route works only here qa.mysite.com/api/health
        pathtype: prefix
        backend:
          service:
            name: payment-service
            port: 
              number: 80


qa.mysite.com/api/healt works in this service i have endpoint /api/health

qa.mysite.com/web or anything else /health  qa.mysite.com/web/api/health .... did not


",<kubernetes><kubernetes-ingress><azure-aks>,74998798,1,"the path specified in the ingress is passed to the service in its entirety. so, your webapp1-svc should support endpoints starting /web. however, most likely you do not.
so, alternative is to rewrite the url when sending requests to webapp1-svc.
thus you have one ingress definition which sends requests to payment-service (and does not rewrite urls).
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-portals-payments
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt
spec:
  tls:
    - hosts:
      - qa.mysite.com
      secretname: app-web-cert
  rules:
  - host: qa.mysite.com
    http:
      paths:
      - path: /  
        pathtype: prefix
        backend:
          service:
            name: payment-service
            port: 
              number: 80

and a second one which rewrites urls to webapp1-svc. it will remove the preceding /web when sending requests to webapp1-svc.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-portals-web
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    cert-manager.io/cluster-issuer: letsencrypt
spec:
  tls:
    - hosts:
      - qa.mysite.com 
      secretname: app-web-cert
  rules:
  - host: qa.mysite.com 
    http:
      paths:
      - path: /web(/|$)(.*)
        pathtype: prefix
        backend:
          service:
            name: webapp1-svc 
            port: 
              number: 80

"
75364469,trying example cronjob from kubnetes with errors,"i am trying to use the example cronjob that is explained through kubernetes documentation here. however, when i check it on lens (a tool to display kubernetes info), i receive an error upon creating a pod. the only difference between the kubernetes example and my code is i added a namespace since i do not own the server i am working on. any help is appreciated. below is my error and yaml file.
error creating: pods &quot;hello-27928364--1-ftzjb&quot; is forbidden: exceeded quota: test-rq, requested: limits.cpu=16,limits.memory=64gi,requests.cpu=16,requests.memory=64gi, used: limits.cpu=1,limits.memory=2g,requests.cpu=1,requests.memory=2g, limited: limits.cpu=12,limits.memory=24gi,requests.cpu=12,requests.memory=24gi

this is my yaml file that i apply.
apiversion: batch/v1
kind: cronjob
metadata:
  name: hello
  namespace: test
spec:
  schedule: &quot;* * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          containers:
            - name: hello
              image: busybox:1.28
              imagepullpolicy: ifnotpresent
              command:
                - /bin/sh
                - c
                - date; echo hello from the kubernetes cluster
          restartpolicy: onfailure

",<kubernetes><kubernetes-cronjob>,75365278,1,"your namespace seems to have a quota configured. try to configure the resources on your cronjob, for example:
apiversion: batch/v1
kind: cronjob
metadata:
  name: hello
  namespace: test
spec:
  schedule: &quot;* * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          containers:
            - name: hello
              image: busybox:1.28
              imagepullpolicy: ifnotpresent
              command:
                - /bin/sh
                - c
                - date; echo hello from the kubernetes cluster
              resources:
                requests:
                  memory: &quot;64mi&quot;
                  cpu: &quot;250m&quot;
                limits:
                  memory: &quot;128mi&quot;
                  cpu: &quot;500m&quot;
          restartpolicy: onfailure

note the resources: and it's indentation.
"
64719717,google cloud gke horizontal pod autoscaling based on kubernetes metrics,"
i want to use the pod network received bytes count standard kubernetes metrics on hpa . using following yaml to accomplish that but getting errors like unable to fetch metrics from custom metrics api: no custom metrics api (custom.metrics.k8s.io) registered
apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  name: xxxx-hoa
  namespace: xxxxx
spec:
  scaletargetref:
    apiversion: apps/v1beta1
    kind: deployment
    name: xxxx-xxx
  minreplicas: 2
  maxreplicas: 6
  metrics:
  - type: pods
    pods:
      metricname: received_bytes_count
      targetaveragevalue: 20k

if anybody had an experience with same kind of metrics usage that would be greatly helpful

",<kubernetes><google-cloud-platform><google-kubernetes-engine><horizontal-pod-autoscaling>,64894748,1,"solution
to make it work, you need to deploy stackdriver custom metrics adapter. below commands to deploy it.
$ kubectl create clusterrolebinding cluster-admin-binding \
    --clusterrole cluster-admin --user &quot;$(gcloud config get-value account)&quot;

$ kubectl apply -f https://raw.githubusercontent.com/googlecloudplatform/k8s-stackdriver/master/custom-metrics-stackdriver-adapter/deploy/production/adapter_new_resource_model.yaml

later you need to use proper custom metric, in your case it should be kubernetes.io|pod|network|received_bytes_count
description
in custom and external metrics for autoscaling workloads documentation you have information that you need to deploy stackdriver adapter before you will be able to get custom metrics.

before you can use custom metrics, you must enable monitoring in your google cloud project and install the stackdriver adapter on your cluster.

next step is to deploy your application (i've used nginx deployment for test purpose) and create proper hpa.
in your hpa example, you have a few issues
apiversion: autoscaling/v2beta1 ## you can also use autoscaling/v2beta2 if you need more features, however for this scenario is ok
kind: horizontalpodautoscaler
metadata:
  name: xxxx-hoa
  namespace: xxxxx # hpa have namespace specified, deployment doesnt have
spec:
  scaletargetref:
    apiversion: apps/v1beta1 # apiversion: apps/v1beta1 is quite old. in kubernetes 1.16+ it was changed to apps/v1
    kind: deployment
    name: xxxx-xxx
  minreplicas: 2
  maxreplicas: 6
  metrics:
  - type: pods
    pods:
      metricname: received_bytes_count # this metrics should be replaced with kubernetes.io|pod|network|received_bytes_count
      targetaveragevalue: 20k

in gke you can choose between autoscaling/v2beta1 and autoscaling/v2beta2. your case will work with both apiversions, however if you would decide to use autoscaling/v2beta2 you will need to change manifest syntax.
why kubernetes.io/pod/network/received_bytes_count ?
you are refering to kubernetes metrics, and /pod/network/received_bytes_count is provided in this docs.
why | instead of /? if you will check stackdriver documentation on github you will find information.

stackdriver metrics have a form of paths separated by &quot;/&quot; character, but custom metrics api forbids using &quot;/&quot; character. when using custom metrics - stackdriver adapter either directly via custom metrics api or by specifying a custom metric in hpa, replace &quot;/&quot; character with &quot;|&quot;. for example, to use custom.googleapis.com/my/custom/metric, specify custom.googleapis.com|my|custom|metric.

proper configuration
for v2beta1
apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  name: xxxx-hoa
spec:
  scaletargetref:
    apiversion: apps/v1 # in your case should be apps/v1beta1 but my deployment was created with apps/v1 apiversion
    kind: deployment
    name: nginx
  minreplicas: 2
  maxreplicas: 6
  metrics:
  - type: pods
    pods:
      metricname: &quot;kubernetes.io|pod|network|received_bytes_count&quot;
      targetaveragevalue: 20k

for v2beta2
apiversion: autoscaling/v2beta2
kind: horizontalpodautoscaler
metadata:
  name: xxxx-hoa
  namespace: default
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: nginx
  minreplicas: 2
  maxreplicas: 6
  metrics:
  - type: pods
    pods:
      metric:
        name: &quot;kubernetes.io|pod|network|received_bytes_count&quot;
      target:
        type: averagevalue
        averagevalue: 20k

tests outputs
conditions:
  type            status  reason            message
  ----            ------  ------            -------
  abletoscale     true    succeededrescale  the hpa controller was able to update the target scale to 2
  scalingactive   true    validmetricfound  the hpa was able to successfully calculate a replica count from pods metric kubernetes.io|pod|network|received_bytes_count
  scalinglimited  true    toofewreplicas    the desired replica count is more than the maximum replica count
events:
  type    reason             age                 from                       message
  ----    ------             ----                ----                       -------
  normal  successfulrescale  8m18s               horizontal-pod-autoscaler  new size: 4; reason: pods metric kubernetes.io|pod|network|received_bytes_count above target
  normal  successfulrescale  8m9s                horizontal-pod-autoscaler  new size: 6; reason: pods metric kubernetes.io|pod|network|received_bytes_count above target
  normal  successfulrescale  17s                 horizontal-pod-autoscaler  new size: 5; reason: all metrics below target
  normal  successfulrescale  9s (x2 over 8m55s)  horizontal-pod-autoscaler  new size: 2; reason: all metrics below target

possible issues with your current configuration
in your hpa you have specified namespace but not in your target deployment. both, hpa and deployment should have the same namespace. with this mismatch configuration you can encounter issue below:
conditions:
  type         status  reason          message
  ----         ------  ------          -------
  abletoscale  false   failedgetscale  the hpa controller was unable to get the target's current scale: deployments/scale.apps &quot;nginx&quot; not found
events:
  type     reason          age                  from                       message
  ----     ------          ----                 ----                       -------
  warning  failedgetscale  94s (x264 over 76m)  horizontal-pod-autoscaler  deployments/scale.apps &quot;nginx&quot; not found

in kubernetes 1.16+, deployments are using apiversion: apps/v1, you won't be able to create deployment with apiversion: apps/v1beta1 in kubernets 1.16+
"
75331106,build kustomize with helm fails to build,"kustomize build --enable-helm .i have the following project structure:
project
  - helm-k8s
   - values.yml
   - chart.yml
   - templates
    - base
      - project-namespace.yml
      - grafana
        - grafana-service.yml
        - grafana-deployment.yml
        - grafana-datasource-config.yml
      - prometheus
        - prometheus-service.yml
        - prometheus-deployment.yml
        - prometheus-config.yml
        - prometheus-roles.yml
      - kustomization.yml
    - prod
      - kustomization.yml
    - test
      - kustomization.yml

i'm trying to build my kustomization file using helm like below:
project/helm-k8s/templates/base/$ kubectl kustomize build . --enable-helm -&gt; dummy.yml

i get an error message like this:
project/helm-k8s/templates/base$ kubectl kustomize . --enable-helm
error: accumulating resources: accumulation err='accumulating resources from 'project-namespace.yml': missing metadata.name in object {{v1 namespace} {{ } map[name:] map[]}}': must build at directory: '/home/my-user/project/helm-k8s/templates/base/project-namespace.yml': file is not directory

is it not possible for kustomize to use the values.yml which is located directly under helm-k8s folder and create the final manifest for my cluster? what am i doing wrong here?
edit: here is how my kustomization.yml looks like:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
metadata:
  name: open-electrons-monitoring-kustomization
resources:
  # 0. get the namespaces first
  - project-namespace.yml

  # 1. set up monitoring services (prometheus)
  #- monitoring/prometheus/prometheus-roles.yml
  - prometheus/prometheus-config.yml
  - prometheus/prometheus-roles.yml
  - prometheus/prometheus-deployment.yml
  - prometheus/prometheus-service.yml

  # 2. set up monitoring services (grafana)
  - grafana/grafana-datasource-config.yml
  - grafana/grafana-deployment.yml
  - grafana/grafana-service.yml

",<kubernetes><kubernetes-helm><kubectl><kustomize>,75335751,1,"i think you may have misunderstood the use of the --enable-helm parameter. it does not allow kustomize to perform helm-style templating on files, so when you write:
apiversion: v1
kind: namespace
metadata:
  name: {{ .values.app.namespace }}
  labels:
    name: {{ .values.app.namespace }}

that doesn't do anything useful. it just generates invalid yaml output.

the --enable-helm option allows you to explode helm charts using kustomize; see here for the documentation, but for example it allows you to process a kustomization.yaml file like this:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization

helmcharts:
  - name: traefik
    repo: https://helm.traefik.io/traefik
    includecrds: true
    releasename: example
    version: 20.8.0
    valuesinline:
      deployment:
        replicas: 3
      logs:
        access:
          enabled: true

running kubectl kustomize --enable-helm will cause kustomize to fetch the helm chart and run helm template on it, producing yaml manifests on stdout.
"
57868848,problems configuring ingress with cookie affinity,"i was looking for how to use cookie affinity in gke, using ingress for that.

i've found the following link to do it: https://cloud.google.com/kubernetes-engine/docs/how-to/configure-backend-service

i've created a yaml with the following:

---
apiversion: apps/v1
kind: deployment
metadata:
  name: my-bsc-deployment
spec:
  selector:
    matchlabels:
      purpose: bsc-config-demo
  replicas: 3
  template:
    metadata:
      labels:
        purpose: bsc-config-demo
    spec:
      containers:
      - name: hello-app-container
        image: gcr.io/google-samples/hello-app:1.0
---
apiversion: cloud.google.com/v1beta1
kind: backendconfig
metadata:
  name: my-bsc-backendconfig
spec:
  timeoutsec: 40
  connectiondraining:
    drainingtimeoutsec: 60
  sessionaffinity:
    affinitytype: ""generated_cookie""
    affinitycookiettlsec: 50
---
apiversion: v1
kind: service
metadata:
  name: my-bsc-service
  labels:
    purpose: bsc-config-demo
  annotations:
    beta.cloud.google.com/backend-config: '{""ports"": {""80"":""my-bsc-backendconfig""}}'
spec:
  type: nodeport
  selector:
    purpose: bsc-config-demo
  ports:
  - port: 80
    protocol: tcp
    targetport: 8080
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-bsc-ingress
spec:
  rules:
  - http:
      paths:
      - path: /*
        backend:
          servicename: my-bsc-service
          serviceport: 80
---


everything seems to go well. when i inspect the created ingress i see 2 backend services. one of them has the cookie configured, but the other doesn't.

if i create the deployment, and from gcp's console, create the service and ingress, only one backend service appears.

somebody knows why using a yaml i get 2, but doing it from console i only get one?

thanks in advance 

oscar
",<cookies><kubernetes><google-kubernetes-engine><kubernetes-ingress><affinity>,57907695,1,"your definition is good.

the reason you have two backend's is because your ingress does not define a default backend. gce lb require a default backend so during lb creation, a second backend is added to the lb to act as the default (this backend does nothing but serve 404 responses). the default backend does not use the backendconfig.

this shouldn't be a problem, but if you want to ensure only your backend is used, define a default backend value in your ingress definition by adding the spec.backend:


apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-bsc-ingress
spec:
  backend:
    servicename: my-bsc-service
    serviceport: 80
  rules:
  - http:
      paths:
      - path: /*
        backend:
          servicename: my-bsc-service
          serviceport: 80


but, like i said, you don't need to define this, the additional backend won't really come into play and no sessions affinity is required (there is only a single pod anyway). if you are curious, the default backend pod in question is called l7-default-backend-[replicaset_hash]-[pod_hash] in the kube-system namespace
"
64681254,errors form my k8s cornjob: pod errors: error with exit code 127,"i have backend service deployed on a private gke cluster, and i want to execute this corn job but everytime i get the following error: pod errors: error with exit code 127
   apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: call-callendar-api-demo
spec:
  
  schedule: &quot;*/15 * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          nodeselector:
            env: demo
          containers:
          - name: call-callendar-api-demo
            image: busybox
            command: [&quot;/bin/sh&quot;]
            args: [&quot;-c&quot;, 'curl -x post &quot;curl -x post &quot;https://x.x.x/api/v1/cal/add_link&quot; -h  &quot;accept: application/json&quot; -d &quot;&quot; &gt;/dev/null 2&gt;&amp;1&quot; -h  &quot;accept: application/json&quot; -d &quot;&quot; &gt;/dev/null 2&gt;&amp;1']
          restartpolicy: never

any suggestions why this  cornjob that is deployed on the same namespace with my backend service is giving me this error? also there is no logs in the container :(  btw i have basic auth, could that be a reason?
edit: logs from the pod after removing &gt;/dev/null/:
textpayload: &quot;curl: (3) url using bad/illegal format or missing url  

textpayload: &quot;
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) could not resolve host: application
&quot;

",<docker><kubernetes><google-kubernetes-engine>,64693818,1,"the command is wrong, and i changed the picture with one that implements curl it suppose to look like this.
apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: demo
spec:
  
  schedule: &quot;*/15 * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          nodeselector:
            env: demo
          containers:
          - name: -demo
            image: curlimages/curl #changed the picture 
            command: [&quot;/bin/sh&quot;]
            args: [&quot;-c&quot;, 'curl -x post &quot;https://x.x.x/api/v1/cal/addy_link&quot; -h  &quot;accept: application/json&quot; -d &quot;&quot; &gt;/dev/null 2&gt;&amp;1']
          restartpolicy: never

it solved my problem.
"
75407718,connection refused when trying to load keycloak on the browser after deployed it on kubernetes successfully,"i just follow the keycloak documentation for kubernetes.
https://www.keycloak.org/getting-started/getting-started-kube

but after deployed it like exactly how they are saying in the documentation.
when i try to load the keyclaok page, i'm getting this,

if you can give me a solution or explain why this is happening, really appreciate it!
my ingress config (keycloak-ingress.yaml) is,
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: keycloak
spec:
  tls:
    - hosts:
      - keycloak.192.168.49.2.nip.io
  rules:
  - host: keycloak.192.168.49.2.nip.io
    http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: keycloak
            port:
              number: 8080

",<kubernetes><keycloak><kubectl>,75407849,1,"make sure you have updated the ingress file with the proper ip of minikube.
also check with http instead https &amp; keycloak_hostname value
try below yaml :
apiversion: v1
kind: service
metadata:
  name: keycloak
  labels:
    app: keycloak
spec:
  ports:
  - name: http
    port: 8080
    targetport: 8080
  selector:
    app: keycloak
  type: loadbalancer
---
apiversion: apps/v1
kind: deployment
metadata:
  name: keycloak
  labels:
    app: keycloak
spec:
  replicas: 1
  selector:
    matchlabels:
      app: keycloak
  template:
    metadata:
      labels:
        app: keycloak
    spec:
      containers:
      - name: keycloak
        image: quay.io/keycloak/keycloak:20.0.3
        args: [&quot;start-dev&quot;]
        env:
        - name: keycloak_admin
          value: &quot;admin&quot;
        - name: keycloak_admin_password
          value: &quot;admin&quot;
        - name: kc_proxy
          value: &quot;edge&quot;
        ports:
        - name: http
          containerport: 8080
        readinessprobe:
          httpget:
            path: /realms/master
            port: 8080

it will creat the lb service for you so you will be able to access it without ingress config. run kubectl get svc -n &lt;namespace-name&gt; and check external ip and try opening that in browser.
extra :
you can refer to this yaml if the default one is not working. i am using postgres &amp; dpeloying the keycloak with that.
github repo path : https://github.com/harsh4870/keycloack-postgres-kubernetes-deployment
ref : https://faun.pub/keycloak-kubernetes-deployment-409d6ccd8a39
"
65268094,spring-boot running under istio sidecar proxy throw http 403 error,"there are servicea and serviceb services deployed under the same namespace. there is istio enabled for validating request authentication. any calls to the service needs to have 'authrization' header with valid jwt token. it get validated with requestauthenication along with authorizationpolicy set. it is working as expected and i can make http calls with valid auth token. now the servicea needs to talk to serviceb. i used the service-name serviceb..&lt;namespace-name&gt;.svc.cluster.local. the call is passed to serviceb but fails with http 403. it is expecting the auth token header.
how can i allow the calls between the services within the same namespace without auth token?
i am trying to find an example to customize the authorizationpolicy, so that it allows the calls with in the same namespace as trusted service without auth token. please let me know, whether it is possible or if there an alternate way.
all my pods running under services are spring-boot and using resttemplate for calling between services.
below is the istio auth policy used
apiversion: security.istio.io/v1beta1
kind: authorizationpolicy
metadata:
  name: service-auth-policy
  namespace: namespace-dev
spec:
  rules:
  - from:
      - source:
          requestprincipals: [&quot;*&quot;]

",<spring-boot><kubernetes><kubernetes-ingress><istio><istio-sidecar>,65268894,1,"i changed the original authorization policy from
apiversion: security.istio.io/v1beta1
kind: authorizationpolicy
metadata:
  name: service-auth-policy
  namespace: namespace-dev
spec:
  rules:
  - from:
      - source:
          requestprincipals: [&quot;*&quot;]

including the namespace as below
apiversion: security.istio.io/v1beta1
kind: authorizationpolicy
metadata:
  name: catalog-auth-policy
  namespace: namespace-dev
spec:
  rules:
  - from:
      - source:
          requestprincipals: [&quot;*&quot;]
      - source:
          namespaces: [&quot;namespace-dev&quot;]

and it worked as expected.
"
58473341,accessing bitnami/kafka outside the kubernetes cluster,"i am currently using bitnami/kafka image(https://hub.docker.com/r/bitnami/kafka) and deploying it on kubernetes. 


kubernetes master: 1
kubernetes workers: 3


within the cluster the other application are able to find kafka. the problem occurs when trying to access the kafka container from outside the cluster. when reading little bit i read that we need to set property ""advertised.listener=plainttext://hostname:port_number"" for external kafka clients. 

i am currently referencing ""https://github.com/bitnami/charts/tree/master/bitnami/kafka"". inside my values.yaml file i have added 

values.yaml


advertisedlisteners1: 10.21.0.191


and  statefulset.yaml

    - name: kafka_cfg_advertised_listeners
      value: 'plaintext://{{ .values.advertisedlisteners }}:9092' 


for a single kafka instance it is working fine.

but for 3 node kafka cluster, i changed some configuration like below:
values.yaml


advertisedlisteners1: 10.21.0.191 
advertisedlisteners2: 10.21.0.192
advertisedlisteners3: 10.21.0.193


and statefulset.yaml

    - name: kafka_cfg_advertised_listeners
      {{- if $my_pod_name := ""kafka-0"" }}
      value: 'plaintext://{{ .values.advertisedlisteners1 }}:9092'
      {{- else if $my_pod_name := ""kafka-1"" }}
      value: 'plaintext://{{ .values.advertisedlisteners2 }}:9092'
      {{- else if $my_pod_name := ""kafka-2"" }}
      value: 'plaintext://{{ .values.advertisedlisteners3 }}:9092'
      {{- end }}


expected result is that all the 3 kafka instances should get advertised.listener property set to worker nodes ip address.

example:


kafka-0 --> ""plaintext://10.21.0.191:9092""
kafka-1 --> ""plaintext://10.21.0.192:9092""
kafka-3 --> ""plaintext://10.21.0.193:9092""


currently only one kafka pod in up and running and the other two are going to crashloopbackoff state. 

and the other two pods are showing error as:

[2019-10-20 13:09:37,753] info [logdirfailurehandler]: starting (kafka.server.replicamanager$logdirfailurehandler)
[2019-10-20 13:09:37,786] error [kafkaserver id=1002] fatal error during kafkaserver startup. prepare to shutdown (kafka.server.kafkaserver)
java.lang.illegalargumentexception: requirement failed: configured end points 10.21.0.191:9092 in advertised listeners are already registered by broker 1001
    at scala.predef$.require(predef.scala:224)
    at kafka.server.kafkaserver$$anonfun$createbrokerinfo$2.apply(kafkaserver.scala:399)
    at kafka.server.kafkaserver$$anonfun$createbrokerinfo$2.apply(kafkaserver.scala:397)
    at scala.collection.mutable.resizablearray$class.foreach(resizablearray.scala:59)
    at scala.collection.mutable.arraybuffer.foreach(arraybuffer.scala:48)
    at kafka.server.kafkaserver.createbrokerinfo(kafkaserver.scala:397)
    at kafka.server.kafkaserver.startup(kafkaserver.scala:261)
    at kafka.server.kafkaserverstartable.startup(kafkaserverstartable.scala:38)
    at kafka.kafka$.main(kafka.scala:84)
    at kafka.kafka.main(kafka.scala)

that means the logic applied in statefulset.yaml is not working. 
can anyone help me in resolving this..? 

any help would be appreciated..

the output of kubectl get statefulset kafka -o yaml

apiversion: apps/v1
kind: statefulset
metadata:
  creationtimestamp: ""2019-10-29t07:04:12z""
  generation: 1
  labels:
    app.kubernetes.io/component: kafka
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: tiller
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-6.0.1
  name: kafka
  namespace: default
  resourceversion: ""12189730""
  selflink: /apis/apps/v1/namespaces/default/statefulsets/kafka
  uid: d40cfd5f-46a6-49d0-a9d3-e3a851356063
spec:
  podmanagementpolicy: parallel
  replicas: 3
  revisionhistorylimit: 10
  selector:
    matchlabels:
      app.kubernetes.io/component: kafka
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/name: kafka
  servicename: kafka-headless
  template:
    metadata:
      creationtimestamp: null
      labels:
        app.kubernetes.io/component: kafka
        app.kubernetes.io/instance: kafka
        app.kubernetes.io/managed-by: tiller
        app.kubernetes.io/name: kafka
        helm.sh/chart: kafka-6.0.1
      name: kafka
    spec:
      containers:
      - env:
        - name: my_pod_ip
          valuefrom:
            fieldref:
              apiversion: v1
              fieldpath: status.podip
        - name: my_pod_name
          valuefrom:
            fieldref:
              apiversion: v1
              fieldpath: metadata.name
        - name: kafka_cfg_zookeeper_connect
          value: kafka-zookeeper
        - name: kafka_port_number
          value: ""9092""
        - name: kafka_cfg_listeners
          value: plaintext://:$(kafka_port_number)
        - name: kafka_cfg_advertised_listeners
          value: plaintext://10.21.0.191:9092
        - name: allow_plaintext_listener
          value: ""yes""
        - name: kafka_cfg_broker_id
          value: ""-1""
        - name: kafka_cfg_delete_topic_enable
          value: ""false""
        - name: kafka_heap_opts
          value: -xmx1024m -xms1024m
        - name: kafka_cfg_log_flush_interval_messages
          value: ""10000""
        - name: kafka_cfg_log_flush_interval_ms
          value: ""1000""
        - name: kafka_cfg_log_retention_bytes
          value: ""1073741824""
        - name: kafka_cfg_log_retention_check_intervals_ms
          value: ""300000""
        - name: kafka_cfg_log_retention_hours
          value: ""168""
        - name: kafka_cfg_log_message_format_version
        - name: kafka_cfg_message_max_bytes
          value: ""1000012""
        - name: kafka_cfg_log_segment_bytes
          value: ""1073741824""
        - name: kafka_cfg_log_dirs
          value: /bitnami/kafka/data
        - name: kafka_cfg_default_replication_factor
          value: ""1""
        - name: kafka_cfg_offsets_topic_replication_factor
          value: ""1""
        - name: kafka_cfg_transaction_state_log_replication_factor
          value: ""1""
        - name: kafka_cfg_ssl_endpoint_identification_algorithm
          value: https
        - name: kafka_cfg_transaction_state_log_min_isr
          value: ""1""
        - name: kafka_cfg_num_io_threads
          value: ""8""
        - name: kafka_cfg_num_network_threads
          value: ""3""
        - name: kafka_cfg_num_partitions
          value: ""1""
        - name: kafka_cfg_num_recovery_threads_per_data_dir
          value: ""1""
        - name: kafka_cfg_socket_receive_buffer_bytes
          value: ""102400""
        - name: kafka_cfg_socket_request_max_bytes
          value: ""104857600""
        - name: kafka_cfg_socket_send_buffer_bytes
          value: ""102400""
        - name: kafka_cfg_zookeeper_connection_timeout_ms
          value: ""6000""
        image: docker.io/bitnami/kafka:2.3.0-debian-9-r88
        imagepullpolicy: ifnotpresent
        livenessprobe:
          failurethreshold: 2
          initialdelayseconds: 10
          periodseconds: 10
          successthreshold: 1
          tcpsocket:
            port: kafka
          timeoutseconds: 5
        name: kafka
        ports:
        - containerport: 9092
          name: kafka
          protocol: tcp
        readinessprobe:
          failurethreshold: 6
          initialdelayseconds: 5
          periodseconds: 10
          successthreshold: 1
          tcpsocket:
            port: kafka
          timeoutseconds: 5
        resources: {}
        terminationmessagepath: /dev/termination-log
        terminationmessagepolicy: file
        volumemounts:
        - mountpath: /bitnami/kafka
          name: data
      dnspolicy: clusterfirst
      restartpolicy: always
      schedulername: default-scheduler
      securitycontext:
        fsgroup: 1001
        runasuser: 1001
      terminationgraceperiodseconds: 30
  updatestrategy:
    type: rollingupdate
  volumeclaimtemplates:
  - metadata:
      creationtimestamp: null
      name: data
    spec:
      accessmodes:
      - readwriteonce
      resources:
        requests:
          storage: 8gi
      volumemode: filesystem
    status:
      phase: pending
status:
  collisioncount: 0
  currentreplicas: 3
  currentrevision: kafka-56ff499d74
  observedgeneration: 1
  readyreplicas: 1
  replicas: 3
  updaterevision: kafka-56ff499d74
  updatedreplicas: 3

",<kubernetes><apache-kafka><kubernetes-helm><bitnami><kubernetes-statefulset>,58626334,1,"i see you have some trouble with passing different environment variables for differents pods in a statefulset.

you are trying to achieve this using helm templates:

- name: kafka_cfg_advertised_listeners
  {{- if $my_pod_name := ""kafka-0"" }}
  value: 'plaintext://{{ .values.advertisedlisteners1 }}:9092'
  {{- else if $my_pod_name := ""kafka-1"" }}
  value: 'plaintext://{{ .values.advertisedlisteners2 }}:9092'
  {{- else if $my_pod_name := ""kafka-2"" }}
  value: 'plaintext://{{ .values.advertisedlisteners3 }}:9092'
  {{- end }}


in helm template guide documentation you can find this explaination:


  in helm templates, a variable is a named reference to another object.
  it follows the form $name. variables are assigned with a special assignment operator: :=.


now let's look at your code:

{{- if $my_pod_name := ""kafka-0"" }}


this is variable assignment, not comparasion and
after this assignment, if statement evaluates this expression to true and that's why in your
staefulset yaml manifest you see this as an output:

- name: kafka_cfg_advertised_listeners
    value: plaintext://10.21.0.191:9092




to make it work as expected, you shouldn't use helm templating. it's not going to work.

one way to do it would be to create separate enviroment variable for every kafka node
and pass all of these variables to all pods, like this:

- env:
  - name: my_pod_name
    valuefrom:
      fieldref:
        apiversion: v1
        fieldpath: metadata.name
  - name: kafka_0
      value: 10.21.0.191
  - name: kafka_1
      value: 10.21.0.192
  - name: kafka_2
      value: 10.21.0.193
#  - name: kafka_cfg_advertised_listeners
#      value: plaintext://$my_pod_name:9092


and also create your own docker image with modified starting script that will export kafka_cfg_advertised_listeners variable
with appropriate value depending on my_pod_name.

if you dont want to create your own image, you can create a configmap with modified entrypoint.sh and mount it
in place of old entrypoint.sh (you can also use any other file, just take a look here
for more information on how kafka image is built).

mounting configmap looks like this:

apiversion: v1
kind: pod
metadata:
  name: test
spec:
  containers:
    - name: test-container
      image: docker.io/bitnami/kafka:2.3.0-debian-9-r88
      volumemounts:
      - name: config-volume
        mountpath: /entrypoint.sh
        subpath: entrypoint.sh
  volumes:
    - name: config-volume
      configmap:
        # provide the name of the configmap containing the files you want
        # to add to the container
        name: kafka-entrypoint-config
        defaultmode: 0744 # remember to add proper (executable) permissions

apiversion: v1
kind: configmap
metadata:
  name: kafka-entrypoint-config
  namespace: default
data:
  entrypoint.sh: |
    #!/bin/bash
    # here add modified entrypoint script


please let me know if it helped.
"
75279605,populating a containers environment values with mounted configmap in kubernetes,"i'm currently learning kubernetes and recently learnt about using configmaps for a containers environment variables.
let's say i have the following simple configmap:
apiversion: v1
data:
  mysql_root_password: password
kind: configmap
metadata:
  creationtimestamp: null
  name: mycm

i know that a container of some deployment can consume this environment variable via:
kubectl set env deployment mydb --from=configmap/mycm
or by specifying it manually in the manifest like so:
containers:
- env:
  - name: mysql_root_password
    valuefrom:
      configmapkeyref:
        key: mysql_root_password
        name: mycm

however, this isn't what i am after, since i'd to manually change the environment variables each time the configmap changes.
i am aware that mounting a configmap to the pod's volume allows for the auto-updating of configmap values. i'm currently trying to find a way to set a container's environment variables to those stored in the mounted config map.
so far i have the following yaml manifest:
apiversion: apps/v1
kind: deployment
metadata:
  creationtimestamp: null
  labels:
    app: mydb
  name: mydb
spec:
  replicas: 1
  selector:
    matchlabels:
      app: mydb
  strategy: {}
  template:
    metadata:
      creationtimestamp: null
      labels:
        app: mydb
    spec:
      containers:
      - image: mariadb
        name: mariadb
        resources: {}
        args: [&quot;export mysql_root_password=$(cat /etc/config/mysql_root_password)&quot;]
        volumemounts:
        - name: config-volume
          mountpath: /etc/config
        env:
          - name: mysql_root_password
            value: temp
      volumes:
      - name: config-volume
        configmap:
          name: mycm
status: {}

i'm attempting to set the mysql_root_password to some temporary value, and then update it to mounted value as soon as the container starts via args: [&quot;export mysql_root_password=$(cat /etc/config/mysql_root_password)&quot;]
as i somewhat expected, this didn't work, resulting in the following error:
/usr/local/bin/docker-entrypoint.sh: line 539: /export mysql_root_password=$(cat /etc/config/mysql_root_password): no such file or directory
i assume this is because the volume is mounted after the entrypoint. i tried adding a readiness probe to wait for the mount but this didn't work either:
readinessprobe:
          exec:
            command: [&quot;sh&quot;, &quot;-c&quot;, &quot;test -f /etc/config/mysql_root_password&quot;]
          initialdelayseconds: 5
          periodseconds: 5

is there any easy way to achieve what i'm trying to do, or is it impossible?
",<kubernetes><kubernetes-deployment>,75291437,1,"so i managed to find a solution, with a lot of inspiration from this answer.
essentially, what i did was create a sidecar container based on the alpine k8s image that mounts the configmap and constantly watches for any changes, since the k8s api automatically updates the mounted configmap when the configmap is changed. this required the following script, watch_passwd.sh, which makes use of inotifywait to watch for changes and then uses the k8s api to rollout the changes accordingly:
update_passwd() {
    kubectl delete secret mysql-root-passwd &gt; /dev/null 2&gt;&amp;1
    kubectl create secret generic mysql-root-passwd --from-file=/etc/config/mysql_root_password
}

update_passwd

while true
do
    inotifywait -e modify &quot;/etc/config/mysql_root_password&quot;
    update_passwd
    kubectl rollout restart deployment $1
done


the dockerfile is then:
from docker.io/alpine/k8s:1.25.6
run apk update &amp;&amp; apk add inotify-tools
copy watch_passwd.sh .

after building the image (locally in this case) as mysidecar, i create the serviceaccount, role, and rolebinding outlined here, adding rules for deployments so that they can be restarted by the sidecar.
after this, i piece it all together to create the following yaml manifest (note that imagepullpolicy is set to never, since i created the image locally):
apiversion: apps/v1
kind: deployment
metadata:
  creationtimestamp: null
  labels:
    app: mydb
  name: mydb
spec:
  replicas: 3
  selector:
    matchlabels:
      app: mydb
  strategy: {}
  template:
    metadata:
      creationtimestamp: null
      labels:
        app: mydb
    spec:
      serviceaccountname: secretmaker
      containers:
      - image: mysidecar
        name: mysidecar
        imagepullpolicy: never
        command:
          - /bin/sh
          - -c
          - |
            ./watch_passwd.sh $(deployment_name)
        env:
          - name: deployment_name
            valuefrom:
              fieldref:
                fieldpath: metadata.labels['app']
        volumemounts:
        - name: config-volume
          mountpath: /etc/config
      - image: mariadb
        name: mariadb
        resources: {}
        envfrom:
        - secretref:
            name: mysql-root-passwd
      volumes:
      - name: config-volume
        configmap:
          name: mycm
status: {}
---
apiversion: v1
kind: serviceaccount
metadata:
  name: secretmaker
---
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  labels:
    app: mydb
  name: secretmaker
rules:
- apigroups: [&quot;&quot;]
  resources: [&quot;secrets&quot;]
  verbs: [&quot;create&quot;, &quot;get&quot;, &quot;delete&quot;, &quot;list&quot;]
- apigroups: [&quot;apps&quot;]
  resources: [&quot;deployments&quot;]
  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;, &quot;patch&quot;]
---
apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  labels:
    app: mydb
  name: secretmaker
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: secretmaker
subjects:
- kind: serviceaccount
  name: secretmaker
  namespace: default
---

it all works as expected! hopefully this is able to help someone out in the future. also, if anybody comes across this and has a better solution please feel free to let me know :)
"
58533832,badrequest error on deployment of yaml file,"getting a bad request error on deployment of yaml file in kubernetes cluster. 

error from server (badrequest): error when creating ""deployment.yaml"": service in version ""v1"" cannot be handled as a service: no kind ""service"" is registered for version ""v1"" in scheme ""k8s.io/kubernetes/pkg/api/legacyscheme/scheme.go:29""
error from server (badrequest): error when creating ""deployment.yaml"": deployment in version ""v1"" cannot be handled as a deployment: no kind ""deployment"" is registered for version ""apps/v1"" in scheme ""k8s.io/kubernetes/pkg/api/legacyscheme/scheme.go:29""



kubernetes cluster is version 1.14.7 with 2 nodepools. one is the default nodepool with a linux node and the other is a windows node pool (node count 1) for windows containers. i am logging into the azure cli within the portal itself and running the kubectl commands.

tried apiversion: apps/v1beta1 but no good. 

kubectl version

client version: version.info{major:""1"", minor:""16"", gitversion:""v1.16.0"", gitcommit:""2bd9643cee5b3b3a5ecbd3af49d09018f0773c77"", gittreestate:""clean"", builddate:""2019-09-18t14:36:53z"", goversion:""go1.12.9"", compiler:""gc"", platform:""linux/amd64""}
server version: version.info{major:""1"", minor:""14"", gitversion:""v1.14.6"", gitcommit:""96fac5cd13a5dc064f7d9f4f23030a6aeface6cc"", gittreestate:""clean"", builddate:""2019-08-19t11:05:16z"", goversion:""go1.12.9"", compiler:""gc"", platform:""linux/amd64""}


here is the deployment.yaml file

apiversion: v1
kind: service
metadata:  
  name: sampleapp
  labels:
    app: sampleapp
spec:
  type: loadbalancer  
  ports:
  - name: proxy
    protocol: tcp
    port: 9163
    targetport: 9163
  - name: banyan
    protocol: tcp
    port: 23010
    targetport: 23010      
  selector:
    app: sampleapp
---
apiversion: apps/v1
kind: deployment
metadata:
  name: sampleapp
  labels:
    app: sampleapp
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: sampleapp
    spec:
      nodeselector:
        ""beta.kubernetes.io/os"": windows
      imagepullsecrets:
        - name: docker-secret
      containers:
      - name: proxyservice
        image: docker.azurecr.io/proxyservice:326
        ports:
        - containerport: 9163        
        env:
        - name: sup_hostname
          value: ""xac-dev-docker4.eastus.cloudapp.azure.com""
      - name: syncservice
        image: docker.azurecr.io/syncservice:326
        ports:
        - containerport: 23010
        env:
        - name: broker_hostname
          value: """" 
  selector:
    matchlabels:
      app: sampleapp



expected result should be that the yaml file be deployed. 

not sure if this is related to indentation of the file. is the yaml file wrong or am i missing something?
",<kubernetes><azure-aks><kubernetes-deployment>,58535221,1,"as stated in the error description the issue is with the version compatibility of the deployment yaml. 

no kind ""service"" is registered for version ""v1"" 

this means the resources type service is not recognized by kubernetes api with version v1 as mentioned in the deployment yaml's apiversion: v1

try

this issue has the solution which is to use the appropriate apiversion in the deployment yaml. enter link description here

here are some references.


https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-groups
[for version 1.14] https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.14/




client - server version mismatch

also worth mentioning is the version mismatch between the client and server kubernetes version. kubernetes supports compatibility of master being behind the client by 1 minor version but that is not really recommended. since your versions are 2 minor versions apart i would recommend making the server to atelast match the client's version. 

reference - https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md#supported-releases-and-component-skew
"
65139122,attach iam role to serviceaccount from a pod in eks,"i am trying to attach an iam role to a pod's service account from within the pod in eks.
kubectl annotate serviceaccount -n $namespace $serviceaccount eks.amazonaws.com/role-arn=$arn

the current role attached to the $serviceaccountis outlined below:
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  name: common-role
rules:
  - apigroups: [&quot;&quot;]
    resources:
      - event
      - secrets
      - configmaps
      - serviceaccounts
     verbs:
      - get
      - create

however, when i execute the kubectl command i get the following:
error from server (forbidden): serviceaccounts $serviceaccount is forbidden: user &quot;system:servi....&quot; cannot get resource &quot;serviceaccounts&quot; in api group &quot;&quot; ...

is my role correct? why can't i modify the service account?
",<kubernetes><amazon-eks>,65143945,1,"kubernetes by default will run the pods with service account: default which don`t have the right permissions. since i cannot determine which one you are using for your pod i can only assume that you are using either default or some other created by you. in both cases the error suggest that the service account your are using to run your pod does not have proper rights.
if you run this pod with service account type default you will have add the appropriate rights to it. alternative way is to run your pod with another service account created for this purpose. here`s an example:
apiversion: v1  
kind: serviceaccount  
metadata:  
   name: run-kubectl-from-pod 

then you will have to create appropriate role (you can find full list of verbs here):
apiversion: rbac.authorization.k8s.io/v1  
kind: role  
metadata:  
  name: modify-service-accounts
rules:  
  - apigroups: [&quot;&quot;]  
    resources:  
      - serviceaccounts 
    verbs:  
      - get  
      - create 
      - patch 
      - list

i'm using here more verbs as a test. get and patch would be enough for this use case. i`m mentioning this since its best practice to provide as minimum rights as possible.
then create your role accordingly:
apiversion: rbac.authorization.k8s.io/v1  
kind: rolebinding  
metadata:  
name: modify-service-account-bind 
subjects:  
- kind: serviceaccount  
  name: run-kubectl-from-pod 
roleref:  
  kind: role  
  name: modify-service-accounts 
  apigroup: rbac.authorization.k8s.io

and now you just have reference that service account when your run your pod:
apiversion: v1  
kind: pod  
metadata:  
  name: run-kubectl-in-pod 
spec:  
  serviceaccountname: run-kubectl-from-pod  
  containers:  
    - name: kubectl-in-pod  
      image: bitnami/kubectl 
      command: 
      - sleep 
      - &quot;3600&quot; 

once that is done, you just exec into the pod:
  kubectl-pod kubectl exec -ti run-kubectl-in-pod sh  

and then annotate the service account:
$ kubectl get sa 
name                   secrets   age
default                1         19m
eks-sa                 1         36s
run-kubectl-from-pod   1         17m

$ kubectl annotate serviceaccount eks-sa eks.amazonaws.com/role-arn=$arn
serviceaccount/eks-sa annotated

$ kubectl describe sa eks-sa 
name:                eks-sa
namespace:           default
labels:              &lt;none&gt;
annotations:         eks.amazonaws.com/role-arn: 
image pull secrets:  &lt;none&gt;
mountable secrets:   eks-sa-token-sldnn
tokens:              &lt;none&gt;
events:              &lt;none&gt;

if you encounter any issues with request being refused please start with reviewing your request attributes and determine the appropriate request verb.
you can also check your access with kubectl auth can-i command:
kubectl-pod kubectl auth can-i patch serviceaccount 

api server will respond with simple yes or no.

please note that if you want to patch a service account to use an iam role you will have delete and re-create any existing pods that are assocaited with the service account to apply credentials environment variables. you can read more about it here.

"
56116503,block access between namespaces but allow access for external traffic,"i have two namespaces: prod and default.
i want to disable access for resources inside these namespaces (resources from default ns can't get access for resources from prod, and resources from prod can't get access for resources from default)
but allow the opportunity to access these resources for external traffic (ingresses).

# namespaces.yaml

---
kind: namespace
apiversion: v1
metadata:
  name: prod
  labels:
    tier: prod

---
kind: namespace
apiversion: v1
metadata:
  name: default
  labels:
    tier: infra


# network-policies.yaml

---
kind: networkpolicy
apiversion: networking.k8s.io/v1
metadata:
  name: network
  namespace: prod
spec:
  podselector: {}
  ingress:
    - from:
      - podselector: {}
---
kind: networkpolicy
apiversion: networking.k8s.io/v1
metadata:
  name: network
  namespace: default
spec:
  podselector: {}
  ingress:
    - from:
      - namespaceselector:
          matchlabels:
            tier: dev
      - namespaceselector:
          matchlabels:
            tier: rc


# services.yaml

---
apiversion: v1
kind: service
metadata:
  name: {{ include ""conference.appservice"" . }}
  labels:
    app: {{ include ""conference.name"" . }}
    release: {{ .release.name }}
spec:
  type: nodeport
  ports:
    - name: http
      port: 80
      targetport: http
      protocol: tcp
  selector:
    app: {{ include ""conference.name"" . }}
    release: {{ .release.name }}
    role: app



pods from prod have access to the other pods inside given namespace.
pods from default don't have access to the pods inside prod.

when i try to get access to the service from the browser it's blocked.
when i try to use port-forwarding to the service inside prod - all works fine.
",<kubernetes><google-kubernetes-engine><kubernetes-ingress><kubernetes-service>,56119661,1,"the problem was in from part in network policy.

---
kind: networkpolicy
apiversion: networking.k8s.io/v1
metadata:
  name: network
  namespace: default
spec:
  podselector: {}
  ingress:
    - from:
      - ipblock:
          cidr: 0.0.0.0/0
      - podselector: {}
      - namespaceselector:
          matchlabels:
            tier: dev
      - namespaceselector:
          matchlabels:
            tier: rc
  egress:
    - {}


the main idea of this selectors it:
podselector - for selecting the pods in current namespace
namespaceselector - for selecting the namespaces
namespaceselector.podselector - for selecting pods inside given namespace

and my problem:

ipblock - for selecting external ip addresses

it's doesn't work for internal ips so in my case 0.0.0.0/0 will be ok.
except will not disallow internal traffic.
"
78454313,kubernetes horizontal pod autoscaling (hpa) for specific container in the pod,"in our deployment, there are 4 containers in the pod. we have defined the resource limits (cpu and memory) only for container-1. the metrics server is installed in the cluster.
our requirement is to apply horizontal pod autoscaling (hpa) with the following requirements. :

need to check both cpu and memory for hpa
need to apply hpa (calculate memory and cpu) only for container-1 in the pod.

when we used following hpa yaml, it checked both cpu and memory. but it consider all containers in the pod. because when applying, it gave us an error. therefore, in this method we had to provide resource (cpu, memory) limits for other 3 containers as well. then it properly showed the percentage of the current memory and cpu usage.
error:

the hpa was unable to compute the replica count: failed to get cpu utilisation: missing request for cpu in container container-2 of pod test-pod

apiversion: autoscaling/v1
  kind: horizontalpodautoscaler
  metadata:
    annotations:
      autoscaling.alpha.kubernetes.io/metrics: '[{&quot;type&quot;:&quot;resource&quot;,&quot;resource&quot;:{&quot;name&quot;:&quot;memory&quot;,&quot;targetaverageutilization&quot;:75}}]'
    name: test-hpa
    namespace: ns
  spec:
    maxreplicas: 5
    minreplicas: 2
    scaletargetref:
      apiversion: apps/v1
      kind: deployment
      name: test-pod
    targetcpuutilizationpercentage: 65

the issue here is it doesn't only consider container-1 in the pod.

is there a way that we can give a specific container in the hpa yaml
fulfilling the mentioned requirements?
also, can we apply hpa without providing resource limits (cpu and
memory) to the other 3 containers in the pod?

i've tried several hpa yamls but it didn't fulfil the both requirements.
also note that &quot;apiversion: autoscaling/v2beta2&quot; is not working in our cluster because of the k8 version.
",<kubernetes><amazon-eks><hpa><horizontal-pod-autoscaling>,78458197,1,"after trying several methods the following code worked, and was able to get both memory and cpu in the pod and the container.
apiversion: autoscaling/v1
kind: horizontalpodautoscaler
metadata:
  annotations:
    autoscaling.alpha.kubernetes.io/metrics: |
      [
        {
          &quot;type&quot;: &quot;resource&quot;,
          &quot;resource&quot;: {
            &quot;name&quot;: &quot;memory&quot;,
            &quot;targetaverageutilization&quot;: 75
          }
        },
        {
          &quot;type&quot;: &quot;containerresource&quot;,
          &quot;containerresource&quot;: {
            &quot;name&quot;: &quot;cpu&quot;,
            &quot;container&quot;: &quot;container-1&quot;,
            &quot;targetaverageutilization&quot;: 65
          }
        },
        {
          &quot;type&quot;: &quot;containerresource&quot;,
          &quot;containerresource&quot;: {
            &quot;name&quot;: &quot;memory&quot;,
            &quot;container&quot;: &quot;container-1&quot;,
            &quot;targetaverageutilization&quot;: 75
          }
        }
      ]
  name: test-hpa
  namespace: ns
spec:
  maxreplicas: 5
  minreplicas: 2
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: test-pod
  targetcpuutilizationpercentage: 65

in here the following block in the annotation provide the memory metrics for the pod:
{
  &quot;type&quot;: &quot;resource&quot;,
  &quot;resource&quot;: {
    &quot;name&quot;: &quot;memory&quot;,
    &quot;targetaverageutilization&quot;: 75
  }
}

cpu metrics for the pod provided by:
targetcpuutilizationpercentage: 65
and the cpu and memory will be provided by following block in the annotation.
{
      &quot;type&quot;: &quot;containerresource&quot;,
      &quot;containerresource&quot;: {
        &quot;name&quot;: &quot;cpu&quot;,
        &quot;container&quot;: &quot;container-1&quot;,
        &quot;targetaverageutilization&quot;: 65
      }
    },
    {
      &quot;type&quot;: &quot;containerresource&quot;,
      &quot;containerresource&quot;: {
        &quot;name&quot;: &quot;memory&quot;,
        &quot;container&quot;: &quot;container-1&quot;,
        &quot;targetaverageutilization&quot;: 75
      }
    }

this provides the usage when i describe the hpa.
kubectl describe hpa -n ns


metrics:
( current / target )   resource memory on pods  (as a percentage of
request):                                 63% (5497292117333m) / 75%
resource cpu of container &quot;container-1&quot; on pods  (as a percentage of
request):     0% (17m) / 65%   resource memory of container
&quot;container-1&quot; on pods  (as a percentage of request):  124%
(2673916586666m) / 75%   resource cpu on pods  (as a percentage of
request):                                    1% (47m) / 65%

thanks for your input @zhakyp! it helped me resolve some issues. hope this helps someone else too. this may not be the best way to do this. but only this worked for our k8 version (1.29).
"
56833757,how to replace fields value of angular config.json using environment variable in kubernetes and nginx in ci &cd vsts,"i am trying to replace authenticationendpoint url and other configuation in the config.json of angular project using environment variable in kubernetes dynamically. for that configured in the helm chart for environment variable in the ci &amp; cd pipeline of vsts. but not sure how config.json field will be replaced with environment variable in kubernetes. could you please help me on this.?

    env in pods (kubernetes) ran printenv cmd

 authenticationendpoint=http://localhost:8888/security/auth


    config.json

 {
   ""authenticationendpoint"": ""http://localhost:8080/security/auth"",
   ""authenticationclientid"": ""my-project"",
   ""baseapiurl"": ""http://localhost:8080/"",
   ""homeurl"": ""http://localhost:4300/""
 }


   generated yaml file from helm chart

        # source: sample-web/templates/service.yaml
        apiversion: v1
        kind: service
        metadata:
          name: cloying-rattlesnake-sample-web
          labels:
            app.kubernetes.io/name: sample-web
            helm.sh/chart: sample-web-0.1.0
            app.kubernetes.io/instance: cloying-rattlesnake
            app.kubernetes.io/managed-by: tiller
        spec:
          type: clusterip
          ports:
            - port: 80
              targetport: 80
              protocol: tcp
              name: http
          selector:
            app.kubernetes.io/name: sample-web
            app.kubernetes.io/instance: cloying-rattlesnake
        ---
        # source: sample-web/templates/deployment.yaml
        apiversion: apps/v1
        kind: deployment
        metadata:
          name: cloying-rattlesnake-sample-web
          labels:
            app.kubernetes.io/name: sample-web
            helm.sh/chart: sample-web-0.1.0
            app.kubernetes.io/instance: cloying-rattlesnake
            app.kubernetes.io/managed-by: tiller
        spec:
          replicas: 1
          selector:
            matchlabels:
              app.kubernetes.io/name: sample-web
              app.kubernetes.io/instance: cloying-rattlesnake
          template:
            metadata:
              labels:
                app.kubernetes.io/name: sample-web
                app.kubernetes.io/instance: cloying-rattlesnake
            spec:
              containers:
                - name: sample-web
                  image: ""sample-web:stable""
                  imagepullpolicy: ifnotpresent
                  ports:
                    - name: http
                      containerport: 80
                      protocol: tcp
                  livenessprobe:
                    httpget:
                      path: /
                      port: http
                  readinessprobe:
                    httpget:
                      path: /
                      port: http
                  env:
                    - name: authenticationendpoint
                      value: ""http://localhost:8080/security/auth""
                  resources:
                    {}
        ---
        # source: sample-web/templates/ingress.yaml
        apiversion: extensions/v1beta1
        kind: ingress
        metadata:
          name: cloying-rattlesnake-sample-web
          labels:
            app.kubernetes.io/name: sample-web
            helm.sh/chart: sample-web-0.1.0
            app.kubernetes.io/instance: cloying-rattlesnake
            app.kubernetes.io/managed-by: tiller
          annotations:
            kubernetes.io/ingress.class: nginx
            nginx.ingress.kubernetes.io/rewrite-target: /$1
            nginx.ingress.kubernetes.io/ssl-redirect: ""false""

        spec:
          rules:
            - host: """"
              http:
                paths:
                  - path: /?(.*)
                    backend:
                      servicename: cloying-rattlesnake-sample-web
                      serviceport: 80


absolute path of config.json

ran shell cmd - kubectl exec -it sample-web-55b71d19c6-v82z4 /bin/sh

path: usr/share/nginx/html/config.json

",<docker><nginx><kubernetes><angular6><kubernetes-helm>,56834531,1,"use a init container to modify your config.json when the pod starts.

updated your deployment.yaml

    # source: sample-web/templates/deployment.yaml
    apiversion: apps/v1
    kind: deployment
    metadata:
      name: cloying-rattlesnake-sample-web
      labels:
        app.kubernetes.io/name: sample-web
        helm.sh/chart: sample-web-0.1.0
        app.kubernetes.io/instance: cloying-rattlesnake
        app.kubernetes.io/managed-by: tiller
    spec:
      replicas: 1
      selector:
        matchlabels:
          app.kubernetes.io/name: sample-web
          app.kubernetes.io/instance: cloying-rattlesnake
      template:
        metadata:
          labels:
            app.kubernetes.io/name: sample-web
            app.kubernetes.io/instance: cloying-rattlesnake
        spec:
          initcontainers:
            - name: init-myconfig
              image: busybox:1.28
              command: ['sh', '-c', 'cat /usr/share/nginx/html/config.json | sed -e ""s#\$authenticationendpoint#$authenticationendpoint#g"" &gt; /tmp/config.json &amp;&amp; cp /tmp/config.json /usr/share/nginx/html/config.json']
              env:
                - name: authenticationendpoint
                  value: ""http://localhost:8080/security/auth""
          containers:
            - name: sample-web
              image: ""sample-web:stable""
              imagepullpolicy: ifnotpresent
              ports:
                - name: http
                  containerport: 80
                  protocol: tcp
              livenessprobe:
                httpget:
                  path: /
                  port: http
              readinessprobe:
                httpget:
                  path: /
                  port: http
              env:
                - name: authenticationendpoint
                  value: ""http://localhost:8080/security/auth""
              volumemounts:
                - mountpath: /usr/share/nginx/html/config.json
                  name: config-volume
          volumes:
            - name: config-volume
              hostpath:
                path: /mnt/data.json # create this file in the host where the pod starts. content below.
                type: file


create /mnt/data.json file in the host where the pod starts

{
      ""authenticationendpoint"": ""$authenticationendpoint"",
      ""authenticationclientid"": ""my-project"",
      ""baseapiurl"": ""http://localhost:8080/"",
      ""homeurl"": ""http://localhost:4300/""
}

"
78915322,kubernetes 502 bad gateway error: connect() failed (111: connection refused) while connecting to upstream,"first of all i'm new to nginx, docker and kubernetes so i may made an obvious mistake but after a complete week of searching and trying stuffs, i'm facing the same error. also, i've seen other similar questions exists but unfortunately the provided solutions does not fix my problem.
when i access my url, i get a 502 bad gateway (nginx) message and when i log my nginx container from my pod, it returns this:
[error] 7#7: *102 connect() failed (111: connection refused) while connecting to upstream, client: &lt;ip_address&gt;, server: _, request: &quot;get / http/1.1&quot;, upstream: &quot;http://127.0.0.1:9000/&quot;, host: &lt;host_name&gt;

my project uses next js 14. i'm running 3 pods and each pod has two containers: my-project-nextjs and my-project-nginx (the default container).
for the docker part, i've tried different configurations. my application is in a monorepo using turbo so i follow this documentation to create my dockerfile:
from node:20.9.0-alpine as base

from base as builder
run apk update
run apk add --no-cache libc6-compat

workdir /app

run yarn global add turbo
copy . .

run turbo prune @monorepo/my-project --docker

from base as installer
run apk update
run apk add --no-cache libc6-compat
workdir /app

copy --from=builder /app/out/json/ .
run yarn install

copy --from=builder /app/out/full/ .
copy --from=builder /app/tsconfig.json ./tsconfig.json
run yarn turbo run build --filter=@monorepo/my-project

from base as runner
workdir /app

env node_env production

run addgroup --system --gid 1001 nodejs
run adduser --system --uid 1001 nextjs
user nextjs

copy --from=installer --chown=nextjs:nodejs /app/apps/my-project/.next/standalone ./
copy --from=installer --chown=nextjs:nodejs /app/apps/my-project/.next/static ./apps/my-project/.next/static
copy --from=installer --chown=nextjs:nodejs /app/apps/my-project/public ./apps/my-project/public

expose 9000

env port 9000

cmd node apps/my-project/server.js

for the dockerfile of nginx, i have tried debian:strech-line and nginx:stable-alpine: the result is the same, error 502. here is the debian version:
from debian:stretch-slim

env debian_frontend=&quot;noninteractive&quot; \
    term=&quot;xterm&quot;
run echo &quot;deb http://archive.debian.org/debian stretch main contrib non-free&quot; &gt; /etc/apt/sources.list
run apt-get update &amp;&amp; apt-get upgrade -y &amp;&amp;\
    apt-get install -y nginx-light

run echo &quot;acquire::http {no-cache=true;};&quot; &gt; /etc/apt/apt.conf.d/no-cache &amp;&amp;\
    apt-get -q update &amp;&amp; \
    apt-get -qy dist-upgrade &amp;&amp; \
    apt-get install -qy \
      nginx-light \
      nano \
    &amp;&amp; \
    apt-get -y autoremove &amp;&amp; \
    apt-get -y clean &amp;&amp; \
    rm -rf /var/lib/apt/lists/*

copy .docker/nginx/nginx.conf /etc/nginx/nginx.conf

run rm -v /etc/nginx/sites-enabled/default

add .docker/nginx/sites-enabled /etc/nginx/sites-enabled
add .docker/nginx/conf.d /etc/nginx/conf.d

copy . /app
workdir /app

cmd [&quot;nginx&quot;]

my application configuration is, i suppose, really basic:
server {
    listen 80;
    server_name _;

    root /app/apps/my-project/.next;

    location / {
        proxy_pass http://localhost:9000;
    }

    location /nginx-health {
        access_log off;
        return 200 &quot;ok\n&quot;;
    }
}

when i check my pods status, they are all running. my container my-project-nextjs return this log:
   next.js 14.2.3
  - local:        http://&lt;pod_name&gt;:9000
  - network:      http://&lt;ip_address&gt;:9000

  starting...
  ready in 161ms

so i suppose my next js docker image is correct.
my container my-project-nginx returns this log (after i tried to reach my website from its url):
[error] 7#7: *102 connect() failed (111: connection refused) while connecting to upstream, client: &lt;ip_address&gt;, server: _, request: &quot;get / http/1.1&quot;, upstream: &quot;http://127.0.0.1:9000/&quot;, host: &lt;host_name&gt;

from other solutions on so, i thought the problem comes from the port or the proxy_pass. the port looks good to me so i don't know what i could tried. for the proxy_pass, i've tried to replace my current value (proxy_pass http://localhost:9000;) with http://127.0.0.1:9000, http://0.0.0.0:9000 or create an upstream (but if i'm right, it's when the containers are not in the same pod).
do you have any ideas why i get a 502 error message?
edit and solution:
here is my deployment code:
apiversion: apps/v1
kind: deployment
metadata:
  name: my-project-deployment
  labels:
    app: my-project
  namespace: beta
spec:
  replicas: 2
  selector:
    matchlabels:
      name: my-project
  template:
    metadata:
      labels:
        name: my-project
    spec:
      containers:
        - name: my-project-nginx
          image: &lt;my_image&gt;
          imagepullpolicy: always
          ports:
          - containerport: 80
          env:
          - name: port
            value: &quot;80&quot;
          resources:
              limits:
                memory: 768mi
              requests:
                memory: 256mi
          readinessprobe:
            tcpsocket:
              port: 80 
            initialdelayseconds: 5
            periodseconds: 10
          livenessprobe:
            tcpsocket:
              port: 80
            initialdelayseconds: 15
            periodseconds: 20
            successthreshold: 1
            failurethreshold: 3
        - name: my-project-nextjs
          image: &lt;my_image&gt;
          imagepullpolicy: always
          ports:
          - containerport: 9000
          env:
          - name: port
            value: &quot;9000&quot;
          - name: dd_agent_host
            valuefrom:
              fieldref:
                fieldpath: status.hostip
          - name: dd_service_name
            value: &quot;my-project-beta&quot;
          resources:
              limits:
                memory: 1gi
              requests:
                memory: 512mi
          readinessprobe:
            tcpsocket:
              port: 9000
            initialdelayseconds: 5
            periodseconds: 10
          livenessprobe:
            tcpsocket:
              port: 9000
            initialdelayseconds: 15
            periodseconds: 20
            successthreshold: 1
            failurethreshold: 3
      dnspolicy: clusterfirst
      imagepullsecrets:
      - name: &lt;name&gt;

---
apiversion: v1
kind: service
metadata:
  name: my-project
  namespace: beta
  labels:
    name: my-project
spec:
  type: clusterip
  ports:
    - name: nginx-container
      port: 80
      targetport: 80 
    - name: nextjs-container
      port: 9000
      targetport: 9000
  selector:
    name: my-project

i manage to make the thing works by opening the port 9000. i've changed the proxy_pass to my service name my-project and add this part to fix my problem:
- name: nextjs-container
      port: 9000
      targetport: 9000

",<docker><kubernetes><nginx><next.js><kubernetes-ingress>,78917713,1,"in the configuration instead of localhost, use the kubernetes service name that exposes the my-project-nextjs container. update it as below
 location / {
        proxy_pass http://&lt;service_name&gt;:9000;
    }

the service should be something as below
apiversion: v1
kind: service
metadata:
  name: my-project-nextjs-service
spec:
  selector:
    app: my-project-nextjs
  ports:
    - protocol: tcp
      port: 9000
      targetport: 9000

also make sure that there is a kubernetes service that exposes the my-project-nextjs container.
and check with the below command, whether kubernetes service is reachable from the my-project-nginx container
kubectl exec -it &lt;nginx-pod-name&gt; -- curl http://&lt;service_name&gt;:9000

"
56585930,how to create a kubernetes ingress to work with sparkjava port 4567?,"i have created a java based web service which utilizes sparkjava.  by default this web service binds and listens to port 4567.  my company requested this be placed in a docker container.  i created a dockerfile and created the image, and when i run i expose port 4567...

docker run -d -p 4567:4567 -t myservice


i can invoke my web service for testing my calling a curl command...

curl -i -x ""post"" -h ""content-type: application/json"" -d ""{}"" ""http://localhost:4567/myservice""


...  and this is working.  my company then says it wants to put this in amazon eks kubernetes so i publish my docker image to the company's private dockerhub.  i create three yaml files...


deployment.yaml
service.yaml
ingress.yaml


i see my objects are created and i can get a /bin/bash command line to my container running in kubernetes and from there test localhost access to my service is working correctly including references to external web service resources, so i know my service is good.

i am confused by the ingress.  i need to expose a uri to get to my service and i am not sure how this is supposed to work.  many examples show using nginx, but i am not using nginx.

here are my files and what i have tested so far.  any guidance is appreciated.

service.yaml

kind: service
apiversion: v1
metadata:
  name: my-api-service
spec:
  selector:
    app: my-api
  ports:
    - name: main
      protocol: tcp
      port: 4567
      targetport: 4567


deployment.yaml

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: my-api-deployment
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: my-api
    spec:
      containers:
      - name: my-api-container
        image: hub.mycompany.net/myproject/my-api-service
        ports:
        - containerport: 4567


ingress.yaml

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-api-ingress
spec:
  backend:
    servicename: my-api-service
    serviceport: 4567


when i run the command ...

kubectl get ingress my-api-ingress


... shows ...

name                    hosts   address   ports   age
my-api-ingress          *                 80      9s


when i run the command ...

kubectl get service my-api-service


... shows ...

name                    type        cluster-ip       external-ip   port(s)    age
my-api-service          clusterip   172.20.247.225   &lt;none&gt;        4567/tcp   16h


when i run the following command...

kubectl cluster-info


... i see ...

kubernetes master is running at https://12ca0954ab5f8e1c52c3dd42a3dbe645.yl4.us-east-1.eks.amazonaws.com


as such i try to hit the end point using curl by issuing...

curl -i -x ""post"" -h ""content-type: application/json"" -d ""{}"" ""http://12ca0954ab5f8e1c52c3dd42a3dbe645.yl4.us-east-1.eks.amazonaws.com:4567/myservice""


after some time i receive a time-out error...

curl: (7) failed to connect to 12ca0954ab5f8e1c52c3dd42a3dbe645.yl4.us-east-1.eks.amazonaws.com port 4567: operation timed out


i believe my ingress is at fault but i am having difficulties finding non-nginx examples to compare.  

thoughts?
",<docker><kubernetes><kubernetes-ingress>,56636338,1,"barrypicker.

your service should be  ""type: nodeport""
this example is very similar (however tested in gke).

kind: service
apiversion: v1
metadata:
  name: my-api-service
spec:
  selector:
    app: my-api
  ports:
    - name: main
      protocol: tcp
      port: 4567
      targetport: 4567
  type: nodeport

---

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: my-api-deployment
spec:
  replicas: 1
  selector:
    matchlabels:
      app: my-api
  template:
    metadata:
      labels:
        app: my-api
    spec:
      containers:
      - name: my-api-container
        image: hashicorp/http-echo:0.2.1
        args = [""-listen=:4567"", ""-text='hello api'""]
        ports:
        - containerport: 4567
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-api-ingress
spec:
  backend:
    servicename: my-api-service
    serviceport: 4567


in your ingress kubectl get ingress &lt;your ingress&gt; you should see an external ip address. 

you can find specific aws implementation here. in addition more information about exposing services you can find here
"
56934639,routing ingress controller traffic based upon file type of path,"is it possible to route ingress controller traffic to different services/deployments based upon the file type in the path? for example if the path was:

domain.com/directory/hello.html -&gt; (frontend service)
domain.com/directory/hello.php -&gt; (backend service)


the architecture i have designed looks like this:


does this look suitable and is this possible, or is there a better way of achieving this?

my ingress controller looks like:

kind: ingress
apiversion: extensions/v1beta1
metadata:
  name: vote-ingress
  namespace: default
  selflink: /apis/extensions/v1beta1/namespaces/default/ingresses/vote-ingress
  uid: 597158e6-a0ce-11e9-b3b1-00155d599803
  resourceversion: '268064'
  generation: 1
  creationtimestamp: '2019-07-07t15:46:13z'
spec:
  rules:
    - host: localhost
      http:
        paths:
          - path: /*.php
            backend:
              servicename: website-backend
              serviceport: 80
          - path: /
            backend:
              servicename: website-frontend
              serviceport: 80
status:
  loadbalancer:
    ingress:
      - hostname: localhost

",<nginx><kubernetes><kubernetes-ingress><nginx-ingress>,57056406,1,"
  is it possible to route ingress controller traffic to different services/deployments based upon the file type in the path?


no in that way. you would need to route your request to nginx which will send request for *.php to a php_fpm. you can have that as separate containers inside one pod or as services. service example is nicely explained in how to deploy a php application with kubernetes on ubuntu 16.04

in this example you will have two containers nignx and php_fpm running on the pod.
php-fpm will handle dynamic php processing, and the nginx will act as a web server.

you will need to use custom nginx.conf configuration and in my opinion the easiest way would be using configmap.

your configmap might look like the following:

apiversion: v1
kind: configmap
metadata:
  name: nginx-config
data:
  config : |
    server {
      index index.php index.html;
      error_log  /var/log/nginx/error.log;
      access_log /var/log/nginx/access.log;
      root /;

      location / {
          try_files $uri $uri/ /index.php?$query_string;
      }

      location ~ \.php$ {
          try_files $uri =404;
          fastcgi_split_path_info ^(.+\.php)(/.+)$;
          fastcgi_pass 127.0.0.1:9000;
          fastcgi_index index.php;
          include fastcgi_params;
          fastcgi_param script_filename $document_root$fastcgi_script_name;
          fastcgi_param path_info $fastcgi_path_info;
        }
    }


nginx will catch and  send all *.php request via localhost:9000 to php-fpm container.

you can include the configmap in your pod using the following:

apiversion: v1
kind: pod
metadata:
  name: nginx
spec:
  containers:
    - name: nginx-container
      image: nginx:1.7.9
      volumemounts:
      - name: config-volume
        mountpath: /etc/nginx/nginx.conf
        subpath: nginx.conf
  volumes:
    - name: config-volume
      configmap:
        name: nginx-config
  restartpolicy: never

"
56862054,kubernetes ingress does not work with traefisk,"i created a kubernetes cluster in google cloud platform, after that, i installed helm/tiller on cluster, and after, i installed traefik with helm like oficial documentation says to do.

now i'm trying to create an ingress for a service, but if i put the annotation kubernetes.io/ingress.class: traefik, the load balancer for ingress is not created.
but without the annotation, it works with default ingress.
(the service type is nodeport)

edit: i also tried this example in a clean google cloud kubernetes cluster: https://supergiant.io/blog/using-traefik-as-ingress-controller-for-your-kubernetes-cluster/ but its the same, when i chose kubernetes.io/ingress.class: traefik, won't be created a load balancer for ingress.

my files are:
animals-svc.yaml:

---
apiversion: v1
kind: service
metadata:
  name: bear
spec:
  type: nodeport
  ports:
  - name: http
    targetport: 80
    port: 80
  selector:
    app: animals
    task: bear
---
apiversion: v1
kind: service
metadata:
  name: moose
spec:
  type: nodeport
  ports:
  - name: http
    targetport: 80
    port: 80
  selector:
    app: animals
    task: moose
---
apiversion: v1
kind: service
metadata:
  name: hare
  annotations:
    traefik.backend.circuitbreaker: ""networkerrorratio() &gt; 0.5""
spec:
  type: nodeport
  ports:
  - name: http
    targetport: 80
    port: 80
  selector:
    app: animals
    task: hare


animals-ingress.yaml:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: animals
  annotations:
    kubernetes.io/ingress.class: traefik
    # kubernetes.io/ingress.global-static-ip-name: ""my-reserved-global-ip""
    # traefik.ingress.kubernetes.io/frontend-entry-points: http
    # traefik.ingress.kubernetes.io/redirect-entry-point: http
    # traefik.ingress.kubernetes.io/redirect-permanent: ""true""
spec:
  rules:
  - host: hare.minikube
    http:
      paths:
      - path: /
        backend:
          servicename: hare
          serviceport: http
  - host: bear.minikube
    http:
      paths:
      - path: /
        backend:
          servicename: bear
          serviceport: http
  - host: moose.minikube
    http:
      paths:
      - path: /
        backend:
          servicename: moose
          serviceport: http


animals-deployment.yaml:

---
kind: deployment
apiversion: extensions/v1beta1
metadata:
  name: bear
  labels:
    app: animals
    animal: bear
spec:
  replicas: 2
  selector:
    matchlabels:
      app: animals
      task: bear
  template:
    metadata:
      labels:
        app: animals
        task: bear
        version: v0.0.1
    spec:
      containers:
      - name: bear
        image: supergiantkir/animals:bear
        ports:
        - containerport: 80
---
kind: deployment
apiversion: extensions/v1beta1
metadata:
  name: moose
  labels:
    app: animals
    animal: moose
spec:
  replicas: 2
  selector:
    matchlabels:
      app: animals
      task: moose
  template:
    metadata:
      labels:
        app: animals
        task: moose
        version: v0.0.1
    spec:
      containers:
      - name: moose
        image: supergiantkir/animals:moose
        ports:
        - containerport: 80
---
kind: deployment
apiversion: extensions/v1beta1
metadata:
  name: hare
  labels:
    app: animals
    animal: hare
spec:
  replicas: 2
  selector:
    matchlabels:
      app: animals
      task: hare
  template:
    metadata:
      labels:
        app: animals
        task: hare
        version: v0.0.1
    spec:
      containers:
      - name: hare
        image: supergiantkir/animals:hare
        ports:
        - containerport: 80


the services are created, but the ingress loadbalancer is not created:



but, if i remove the line kubernetes.io/ingress.class: traefik it works with the default ingress of kubernetes
",<kubernetes><kubernetes-ingress>,56871788,1,"traefik does not create a load balancer for you by default.

as http(s) load balancing with ingress documentation mention:


  when you create an ingress object, the gke ingress controller creates
  a google cloud platform http(s) load balancer and configures it
  according to the information in the ingress and its associated
  services.


this is all applicable for  gke ingress controller(gce) - more info about gce you can find here: https://github.com/kubernetes/ingress-gce

if you would like to use traefik as ingress  - you have to  expose traefik service with type: loadbalancer

example:

apiversion: v1
kind: service
metadata:
  name: traefik
spec:
  type: loadbalancer
  selector:
    k8s-app: traefik-ingress-lb
  ports:
  - port: 80
    targetport: 80


more info with a lot of explanation diagrams and real working example you can find in the exposing kubernetes services to the internet using traefik ingress controller article.

hope this help.
"
79021470,extract only secret values with eks + vault csi provider,"i am using the vault csi provider with eks to load secrets stored in vault kv into my deployment as a json file. here are the relevant configurations:
secretproviderclass configuration:
apiversion: secrets-store.csi.x-k8s.io/v1
kind: secretproviderclass
metadata:
  name: censored-secret-provider-class
  namespace: censored
spec:
  parameters:
    objects: |
      - objectname: &quot;censored&quot;
        secretpath: &quot;censored&quot;
        secretkey: &quot;&quot;
        template: |
          {
            &quot;bottoken&quot;: &quot;{{ .data.bottoken }}&quot;,
            &quot;mongo&quot;: {
              &quot;db&quot;: &quot;{{ .data.mongo.db }}&quot;,
              &quot;host&quot;: &quot;{{ .data.mongo.host }}&quot;,
              &quot;password&quot;: &quot;{{ .data.mongo.password }}&quot;,
              &quot;port&quot;: {{ .data.mongo.port }},
              &quot;username&quot;: &quot;{{ .data.mongo.username }}&quot;
            }
          }
    rolename: censored
    vaultaddress: censored
  provider: vault

secret stored in vault kv:
/ $ vault kv get censored
====== data ======
key         value
---         -----
bottoken    censored
mongo       map[db:censored host:censored password:censored port:censored username:censored]

deployment.yaml:
          volumemounts:
            - name: secret
              mountpath: censored
              readonly: true
      volumes:
      - name: secret
        csi:
          driver: secrets-store.csi.k8s.io
          readonly: true
          volumeattributes:
            secretproviderclass: &quot;censored&quot;
            secretkey: &quot;&quot;


my situation:
i am trying to use vault csi provider to load secrets stored in vault kv into my deployment.yaml. the service expects the secret in the form of an env.json file.
while i have successfully loaded the secrets from vault kv, i am encountering an issue where additional fields (like request_id, lease_id, etc.) are added to the json, making it difficult for me to access the desired data.
this is how the secret looks after being loaded:
as-is:
/censored # cat censored.json 
{
  &quot;request_id&quot;: &quot;...&quot;,
  &quot;lease_id&quot;: &quot;&quot;,
  &quot;lease_duration&quot;: 2764800,
  &quot;renewable&quot;: false,
  &quot;data&quot;: {
    &quot;bottoken&quot;: &quot;...&quot;,
    &quot;mongo&quot;: {
      &quot;db&quot;: &quot;...&quot;,
      &quot;host&quot;: &quot;...&quot;,
      &quot;password&quot;: &quot;...&quot;,
      &quot;port&quot;: ...,
      &quot;username&quot;: &quot;...&quot;
    }
  }
}

i want to extract only the values under the data field and remove the extra fields like request_id, lease_id, etc., so that the output looks like this:
to-be:
/censored # cat censored.json 
{
  &quot;bottoken&quot;: &quot;...&quot;,
  &quot;mongo&quot;: {
    &quot;db&quot;: &quot;...&quot;,
    &quot;host&quot;: &quot;...&quot;,
    &quot;password&quot;: &quot;...&quot;,
    &quot;port&quot;: ...,
    &quot;username&quot;: &quot;...&quot;
  }
}

question:
how can i modify my setup so that only the contents under the data field are included in the json file, without the additional metadata like request_id and lease_id?
",<kubernetes><amazon-eks><vault>,79022452,1,"when you set the secretkey: &quot;&quot;, it will have the metadata. the only solution is to copy all the needed data (as json object) to a new key (let's say appinfo in the same path) and retrieve that key:
vault kv put your_path_in_vault -&lt;&lt;eof
{
  &quot;appinfo&quot;: {
    &quot;bottoken&quot;: &quot;...&quot;,
    &quot;mongo&quot;: {
      &quot;db&quot;: &quot;...&quot;,
      &quot;host&quot;: &quot;...&quot;,
      &quot;password&quot;: &quot;...&quot;,
      &quot;port&quot;: &quot;...&quot;,
      &quot;username&quot;: &quot;...&quot;
    }
  }
}
eof

and then in your secretproviderclass configuration:
apiversion: secrets-store.csi.x-k8s.io/v1
kind: secretproviderclass
metadata:
  name: censored-secret-provider-class
  namespace: censored
spec:
  parameters:
    objects: |
      - objectname: &quot;censored&quot;
        secretpath: &quot;censored&quot;
        secretkey: &quot;appinfo&quot;
    rolename: censored
    vaultaddress: censored
  provider: vault

"
55406963,kubernetes volumemount folder and file permissions?,"trying to mount config files from a hostpath to a kubernetes container. this works using minikube and virtualbox shared folder, but i am unable to make this work on linux.

i making use of aws eks and the following architecture https://aws.amazon.com/quickstart/architecture/amazon-eks/. i think my problem is that the files need to live on each of the eks node instances.

here is the architecture diagram:


below is the deployment file.

apiversion: apps/v1
kind: deployment
metadata:
  name: openhim-core-deployment
spec:
  replicas: 1
  selector:
    matchlabels:
      component: openhim-core
  template:
    metadata:
      labels:
        component: openhim-core
    spec:
      volumes:
        - name: core-config
          hostpath:
            path: /var/config/openhim-core
      containers:
        - name: openhim-core
          image: jembi/openhim-core:5.rc
          ports:
            - containerport: 8080
            - containerport: 5000
            - containerport: 5001
          volumemounts:
            - name: core-config
              mountpath: /usr/src/app/config
          env:
            - name: node_env
              value: development

",<kubernetes><amazon-eks>,55474128,1,"after much pain i found that i am trying to place the configuration on the linux bastion host where i have access to kubectl but in fact this configuration will have to be on each of the ec2 instances in every availability zone.

the solution for me was to make use of a initcontainer.

apiversion: apps/v1
kind: deployment
metadata:
  name: openhim-core-deployment
spec:
  replicas: 1
  selector:
    matchlabels:
      component: openhim-core
  template:
    metadata:
      labels:
        component: openhim-core
    spec:
      volumes:
        - name: core-config
          hostpath:
            path: /var/config/openhim-core
      containers:
        - name: openhim-core
          image: jembi/openhim-core:5
          ports:
            - containerport: 8080
            - containerport: 5000
            - containerport: 5001
          volumemounts:
            - name: core-config
              mountpath: /usr/src/app/config
          env:
            - name: node_env
              value: development
      initcontainers:
        - name: install
          image: busybox
          command:
          - wget
          - ""-o""
          - ""/usr/src/app/config/development.json""
          - https://s3.eu-central-1.amazonaws.com/../development.json
          volumemounts:
            - name: core-config
              mountpath: ""/usr/src/app/config""      
      volumes:
        - name: core-config
          emptydir: {}       

"
55347313,i am trying to use externalname together with nodeport service for nginx-controller but i am getting 502 bad gateway,"environment:

i have:

1- nginx ingress controller version: 1.15.9, image: 0.23.0

2- kubernetes version:


  client version: version.info{major:""1"", minor:""13"",
  gitversion:""v1.13.4"",
  gitcommit:""c27b913fddd1a6c480c229191a087698aa92f0b1"",
  gittreestate:""clean"", builddate:""2019-02-28t13:37:52z"",
  goversion:""go1.11.5"", compiler:""gc"", platform:""linux/amd64""}
  
  server version: version.info{major:""1"", minor:""13"",
  gitversion:""v1.13.4"",
  gitcommit:""c27b913fddd1a6c480c229191a087698aa92f0b1"",
  gittreestate:""clean"", builddate:""2019-02-28t13:30:26z"",
  goversion:""go1.11.5"", compiler:""gc"", platform:""linux/amd64""}


cloud provider or hardware configuration: virtual machines on kvm

os (e.g. from /etc/os-release):


  name=""centos linux"" version=""7 (core)"" id=""centos"" id_like=""rhel
  fedora"" version_id=""7"" pretty_name=""centos linux 7 (core)""
  ansi_color=""0;31"" cpe_name=""cpe:/o:centos:centos:7""
  home_url=""https://www.centos.org/""
  bug_report_url=""https://bugs.centos.org/""
  
  centos_mantisbt_project=""centos-7"" centos_mantisbt_project_version=""7""
  redhat_support_product=""centos"" redhat_support_product_version=""7""


kernel (e.g. uname -a):


  linux node01 3.10.0-957.5.1.el7.x86_64 #1 smp fri feb 1 14:54:57 utc
  2019 x86_64 x86_64 x86_64 gnu/linux


install tools: kubeadm

more details:

cni : weave 

setup:


2 resilient ha proxy, 3 masters, 2 infra, and worker nodes.
i am exposing all the services as node ports, where the ha-proxy re-assign them to a public virtual ip.
dedicated project hosted on the infra node carrying the monitoring and logging tools (grafana, prometheus, efk, etc)
backend nfs storage as persistent storage


what happened:
i want to be able to use external name rather than node ports, so instead of accessing grafana for instance via vip + 3000 i want to access it via http://grafana.wild-card-dns-zone

deployment


i have created a new namespace called ingress 
i deployed it as follow:



apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  replicas: **2**
  selector:
    matchlabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
      annotations:
        prometheus.io/port: ""10254""
        prometheus.io/scrape: ""true""
      name: nginx-ingress
    spec:
      serviceaccountname: nginx-ingress-serviceaccount
      nodeselector:
        node-role.kubernetes.io/infra: infra
      terminationgraceperiodseconds: 60
      containers:
      - name: nginx-ingress-controller
        image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.23.0
        readinessprobe:
          httpget:
            path: /healthz
            port: 10254
            scheme: http
          periodseconds: 10
          successthreshold: 1
          timeoutseconds: 10
        livenessprobe:
          httpget:
            path: /healthz
            port: 10254
            scheme: http
          initialdelayseconds: 10
          periodseconds: 10
          successthreshold: 1
          timeoutseconds: 10
        args:
          - /nginx-ingress-controller
          - --default-backend-service=ingress/ingress-controller-nginx-ingress-default-backend
          - --configmap=$(pod_namespace)/nginx-configuration
          - --tcp-services-configmap=$(pod_namespace)/tcp-services
          - --udp-services-configmap=$(pod_namespace)/udp-services
          - --publish-service=$(pod_namespace)/ingress-nginx
          - --annotations-prefix=nginx.ingress.kubernetes.io
          - --v3
        securitycontext:
          allowprivilegeescalation: true
          capabilities:
            drop:
              - all
            add:
              - net_bind_service
          # www-data -&gt; 33
          runasuser: 33
        env:
          - name: pod_name
            valuefrom:
              fieldref:
                fieldpath: metadata.name
          - name: pod_namespace
            valuefrom:
              fieldref:
                fieldpath: metadata.namespace
        ports:
          - name: http
            containerport: 80
          - name: https
            containerport: 443
---
apiversion: extensions/v1beta1
kind: deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: ""1""
  generation: 1
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.3.1
    component: default-backend
  name: ingress-controller-nginx-ingress-default-backend
  namespace: ingress
spec:
  replicas: 1
  revisionhistorylimit: 10
  selector:
    matchlabels:
      app: nginx-ingress
      component: default-backend
      release: ingress-controller
  strategy:
    rollingupdate:
      maxsurge: 1
      maxunavailable: 1
    type: rollingupdate
  template:
    metadata:
      creationtimestamp: null
      labels:
        app: nginx-ingress
        component: default-backend
        release: ingress-controller
    spec:
      nodeselector:
        node-role.kubernetes.io/infra: infra
      containers:
      - image: k8s.gcr.io/defaultbackend:1.4
        imagepullpolicy: ifnotpresent
        livenessprobe:
          failurethreshold: 3
          httpget:
            path: /healthz
            port: 8080
            scheme: http
          initialdelayseconds: 30
          periodseconds: 10
          successthreshold: 1
          timeoutseconds: 5
        name: nginx-ingress-default-backend
        ports:
        - containerport: 8080
          name: http
          protocol: tcp
        resources: {}
        terminationmessagepath: /dev/termination-log
        terminationmessagepolicy: file
      dnspolicy: clusterfirst
      restartpolicy: always
      schedulername: default-scheduler
      securitycontext: {}
      terminationgraceperiodseconds: 60
---
apiversion: v1
kind: configmap
metadata:
  name: nginx-configuration
  namespace: ingress
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
---
kind: configmap
apiversion: v1
metadata:
  name: tcp-services
  namespace: ingress
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: configmap
apiversion: v1
metadata:
  name: udp-services
  namespace: ingress
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
---
apiversion: v1
kind: service
metadata:
  name: ingress-nginx
  namespace: ingress
spec:
  type: nodeport
  ports:
  - port: 80
    targetport: 80
    protocol: tcp
    name: http
  - port: 443
    targetport: 443
    protocol: tcp
    name: https
  selector:
    name: ingress-nginx
---
apiversion: v1
kind: service
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.3.1
    component: default-backend
  name: ingress-controller-nginx-ingress-default-backend
  namespace: ingress
spec:
  ports:
  - name: http
    port: 80
    protocol: tcp
    targetport: http
  selector:
    app: nginx-ingress
    component: default-backend
    release: ingress-controller
  sessionaffinity: none
  type: clusterip
---
apiversion: v1
kind: serviceaccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
---
apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrole
metadata:
  name: nginx-ingress-clusterrole
  namespace: ingress
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apigroups:
      - """"
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apigroups:
      - """"
    resources:
      - nodes
    verbs:
      - get
  - apigroups:
      - """"
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apigroups:
      - ""extensions""
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apigroups:
      - """"
    resources:
      - events
    verbs:
      - create
      - patch
  - apigroups:
      - ""extensions""
    resources:
      - ingresses/status
    verbs:
      - update
---
apiversion: rbac.authorization.k8s.io/v1beta1
kind: role
metadata:
  name: nginx-ingress-role
  namespace: ingress
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apigroups:
      - """"
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apigroups:
      - """"
    resources:
      - configmaps
    resourcenames:
      # defaults to ""&lt;election-id&gt;-&lt;ingress-class&gt;""
      # here: ""&lt;ingress-controller-leader&gt;-&lt;nginx&gt;""
      # this has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - ""ingress-controller-leader-nginx""
    verbs:
      - get
      - update
  - apigroups:
      - """"
    resources:
      - configmaps
    verbs:
      - create
  - apigroups:
      - """"
    resources:
      - endpoints
    verbs:
      - get
---
apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrolebinding
metadata:
  name: nginx-ingress-clusterrolebinding
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: nginx-ingress-clusterrole
subjects:
  - kind: serviceaccount
    name: nginx-ingress-serviceaccount
    namespace: ingress
---
apiversion: rbac.authorization.k8s.io/v1beta1
kind: rolebinding
metadata:
  name: nginx-ingress-rolebinding
  namespace: ingress
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: nginx-ingress-role
subjects:
  - kind: serviceaccount
    name: nginx-ingress-serviceaccount
    namespace: ingress



ingress setup:

services


# please edit the object below. lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. if an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiversion: v1
kind: service
metadata:
  creationtimestamp: ""2019-03-25t16:03:01z""
  labels:
    app: jaeger
    app.kubernetes.io/component: query
    app.kubernetes.io/instance: jeager
    app.kubernetes.io/managed-by: jaeger-operator
    app.kubernetes.io/name: jeager-query
    app.kubernetes.io/part-of: jaeger
  name: jeager-query
  namespace: monitoring-logging
  resourceversion: ""3055947""
  selflink: /api/v1/namespaces/monitoring-logging/services/jeager-query
  uid: 778550f0-4f17-11e9-9078-001a4a16021e
spec:
  externalname: jaeger.example.com
  ports:
  - port: 16686
    protocol: tcp
    targetport: 16686
  selector:
    app: jaeger
    app.kubernetes.io/component: query
    app.kubernetes.io/instance: jeager
    app.kubernetes.io/managed-by: jaeger-operator
    app.kubernetes.io/name: jeager-query
    app.kubernetes.io/part-of: jaeger
  sessionaffinity: none
  type: externalname
status:
  loadbalancer: {}
# please edit the object below. lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. if an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiversion: v1
kind: service
metadata:
  creationtimestamp: ""2019-03-25t15:40:30z""
  labels:
    app: grafana
    chart: grafana-2.2.4
    heritage: tiller
    release: grafana
  name: grafana
  namespace: monitoring-logging
  resourceversion: ""3053698""
  selflink: /api/v1/namespaces/monitoring-logging/services/grafana
  uid: 51b9d878-4f14-11e9-9078-001a4a16021e
spec:
  externalname: grafana.example.com
  ports:
  - name: http
    port: 3000
    protocol: tcp
    targetport: 3000
  selector:
    app: grafana
    release: grafana
  sessionaffinity: none
  type: externalname
status:
  loadbalancer: {}



ingress 

ingress 1

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    ingress.kubernetes.io/service-upstream: ""true""
  creationtimestamp: ""2019-03-25t21:13:56z""
  generation: 1
  labels:
    app: jaeger
    app.kubernetes.io/component: query-ingress
    app.kubernetes.io/instance: jeager
    app.kubernetes.io/managed-by: jaeger-operator
    app.kubernetes.io/name: jeager-query
    app.kubernetes.io/part-of: jaeger
  name: jaeger-query
  namespace: monitoring-logging
  resourceversion: ""3111683""
  selflink: /apis/extensions/v1beta1/namespaces/monitoring-logging/ingresses/jaeger-query
  uid: e6347f6b-4f42-11e9-9e8e-001a4a16021c
spec:
  rules:
  - host: jaeger.example.com
    http:
      paths:
      - backend:
          servicename: jeager-query
          serviceport: 16686
status:
  loadbalancer: {}


ingress 2

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {""apiversion"":""extensions/v1beta1"",""kind"":""ingress"",""metadata"":{""annotations"":{},""labels"":{""app"":""grafana""},""name"":""grafana"",""namespace"":""monitoring-logging""},""spec"":{""rules"":[{""host"":""grafana.example.com"",""http"":{""paths"":[{""backend"":{""servicename"":""grafana"",""serviceport"":3000}}]}}]}}
  creationtimestamp: ""2019-03-25t17:52:40z""
  generation: 1
  labels:
    app: grafana
  name: grafana
  namespace: monitoring-logging
  resourceversion: ""3071719""
  selflink: /apis/extensions/v1beta1/namespaces/monitoring-logging/ingresses/grafana
  uid: c89d7f34-4f26-11e9-8c10-001a4a16021d
spec:
  rules:
  - host: grafana.example.com
    http:
      paths:
      - backend:
          servicename: grafana
          serviceport: 3000
status:
  loadbalancer: {}


endpoints

endpoint 1

apiversion: v1
kind: endpoints
metadata:
  creationtimestamp: ""2019-03-25t15:40:30z""
  labels:
    app: grafana
    chart: grafana-2.2.4
    heritage: tiller
    release: grafana
  name: grafana
  namespace: monitoring-logging
  resourceversion: ""3050562""
  selflink: /api/v1/namespaces/monitoring-logging/endpoints/grafana
  uid: 51bb1f9c-4f14-11e9-9e8e-001a4a16021c
subsets:
- addresses:
  - ip: 10.42.0.15
    nodename: kuinfra01.example.com
    targetref:
      kind: pod
      name: grafana-b44b4f867-bcq2x
      namespace: monitoring-logging
      resourceversion: ""1386975""
      uid: 433e3d21-4827-11e9-9e8e-001a4a16021c
  ports:
  - name: http
    port: 3000
    protocol: tcp


endpoint 2

apiversion: v1
kind: endpoints
metadata:
  creationtimestamp: ""2019-03-25t16:03:01z""
  labels:
    app: jaeger
    app.kubernetes.io/component: service-query
    app.kubernetes.io/instance: jeager
    app.kubernetes.io/managed-by: jaeger-operator
    app.kubernetes.io/name: jeager-query
    app.kubernetes.io/part-of: jaeger
  name: jeager-query
  namespace: monitoring-logging
  resourceversion: ""3114702""
  selflink: /api/v1/namespaces/monitoring-logging/endpoints/jeager-query
  uid: 7786d833-4f17-11e9-9e8e-001a4a16021c
subsets:
- addresses:
  - ip: 10.35.0.3
    nodename: kunode02.example.com
    targetref:
      kind: pod
      name: jeager-query-7d9775d8f7-2hwdn
      namespace: monitoring-logging
      resourceversion: ""3114693""
      uid: fdac9771-4f49-11e9-9e8e-001a4a16021c
  ports:
  - name: query
    port: 16686
    protocol: tcp


i am able to curl the endpoints from inside the ingress-controller pod:

# kubectl exec -it nginx-ingress-controller-5dd67f88cc-z2g8s  -n ingress -- /bin/bash
www-data@nginx-ingress-controller-5dd67f88cc-z2g8s:/etc/nginx$ curl -k https://localhost
&lt;a href=""/login""&gt;found&lt;/a&gt;.

www-data@nginx-ingress-controller-5dd67f88cc-z2g8s:/etc/nginx$ curl http://localhost
&lt;html&gt;
&lt;head&gt;&lt;title&gt;308 permanent redirect&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;308 permanent redirect&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx/1.15.9&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
www-data@nginx-ingress-controller-5dd67f88cc-z2g8s:/etc/nginx$ exit


but from out side when i am trying to reach jaeger.example.com or grafana.example.com i am getting 502 bad gatway and the following error log:

10.39.0.0 - [10.39.0.0] - - [25/mar/2019:16:40:32 +0000] ""get /search http/1.1"" 502 559 ""-"" ""mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/73.0.3683.86 safari/537.36"" 514 0.001 [monitoring-logging-jeager-query-16686] vip:16686, vip:16686, vip:16686 0, 0, 0 0.001, 0.000, 0.000 502, 502, 502 b7c813286fccf27fffa03eb6564edfd1
2019/03/25 16:40:32 [error] 2816#2816: *4617326 connect() failed (111: connection refused) while connecting to upstream, client: 10.39.0.0, server: _, request: ""get /favicon.ico http/1.1"", upstream: ""http://vip:16686/favicon.ico"", host: ""jeager.example.com"", referrer: ""http://jeager.example.com/search""
2019/03/25 16:40:32 [error] 2816#2816: *4617326 connect() failed (111: connection refused) while connecting to upstream, client: 10.39.0.0, server: _, request: ""get /favicon.ico http/1.1"", upstream: ""http://vip:16686/favicon.ico"", host: ""jeager.example.com"", referrer: ""http://jeager.example.com/search""
2019/03/25 16:40:32 [error] 2816#2816: *4617326 connect() failed (111: connection refused) while connecting to upstream, client: 10.39.0.0, server: _, request: ""get /favicon.ico http/1.1"", upstream: ""http://vip:16686/favicon.ico"", host: ""jeager.example.com"", referrer: ""http://jeager.example.com/search""
10.39.0.0 - [10.39.0.0] - - [25/mar/2019:16:40:32 +0000] ""get /favicon.ico http/1.1"" 502 559 ""http://jeager.example.com/search"" ""mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/73.0.3683.86 safari/537.36"" 494 0.001 [monitoring-logging-jeager-query-16686] vip:16686, vip:16686, vip:16686 0, 0, 0 0.000, 0.001, 0.000 502, 502, 502 9e582912614e67dfee6be1f679de5933
i0325 16:40:32.497868       8 socket.go:225] skiping metric for host jeager.example.com that is not being served
i0325 16:40:32.497886       8 socket.go:225] skiping metric for host jeager.example.com that is not being served

",<kubernetes><kubernetes-ingress>,55397505,1,"first thanks for cookiedough for the clue to help regarding the service issue, but later i faced an issue to create service using external name but i found my mistake thanks for &quot;long&quot; user in the slack, the mistake is that i was using service of type externalname and it should be type cluster ip here are the steps to solve the problems (remark https issue is a separate problem):
1- create wild character dns zone pointing the public ip
1- for new service just create it of type clusterip
2- in the namespace for the service create an ingress using the following example (yaml):
apiversion: extensions/v1beta1
kind: ingress
metadata:
  labels:
    app: grafana
  name: grafana
  namespace: grafana-namespace
spec:
  rules:   
    host: grafana.example.com
    http:
      paths:
        backend:
          servicename: grafana
          serviceport: 3000

3- kubectl -f apply -f grafana-ingress.yaml
now you can reach your grafana on http://grafana.example,com
"
55393772,kubectl get pods shows crashloopbackoff,"im trying to create a pod using my local docker image as follow.

1.first i run this command in terminal 

eval $(minikube docker-env)


2.i created a docker image as follow

sudo docker image build -t my-first-image:3.0.0 .


3.i created the pod.yml as shown below and i run this command

kubectl -f create pod.yml.


4.then i tried to run this command

kubectl get pods


but it shows following error 


name                                  ready   status             restarts   age
multiplication-6b6d99554-d62kk        0/1     crashloopbackoff   9          22m
multiplication2019-5b4555bcf4-nsgkm   0/1     crashloopbackoff   8          17m
my-first-pod                          0/1     crashloopbackoff   4          2m51


5.i get the pods logs

kubectl describe pod my-first-pod 



events:
  type     reason     age                    from               message
  ----     ------     ----                   ----               -------
  normal   scheduled  6m22s                  default-scheduler  successfully assigned default/my-first-pod to minikube
  normal   pulled     5m20s (x4 over 6m17s)  kubelet, minikube  successfully pulled image ""docker77nira/myfirstimage:latest""
  normal   created    5m20s (x4 over 6m17s)  kubelet, minikube  created container
  normal   started    5m20s (x4 over 6m17s)  kubelet, minikube  started container
  normal   pulling    4m39s (x5 over 6m21s)  kubelet, minikube  pulling image ""docker77nira/myfirstimage:latest""
  warning  backoff    71s (x26 over 6m12s)   kubelet, minikube  back-off restarting failed container




dockerfile

    from node:carbon
    workdir /app
    copy . .
    cmd [ ""node"", ""index.js"" ]



pods.yml

    kind: pod
    apiversion: v1
    metadata:
     name: my-first-pod
    spec:
     containers:
     - name: my-first-container
       image: my-first-image:3.0.0



index.js

    var http = require('http');
    var server = http.createserver(function(request, response) {
     response.statuscode = 200;
     response.setheader('content-type', 'text/plain');
     response.end('welcome to the golden guide to kubernetes
    application development!');
    });
    server.listen(3000, function() {
     console.log('server running on port 3000');
    });


",<kubernetes><kubectl><minikube>,55400907,1,"i succeeded in running your image by performing these steps:

docker build -t foo .

then check if the container is working docker run -it foo 

/app/index.js:5
response.end('welcome to the golden guide to kubernetes
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

syntaxerror: invalid or unexpected token
    at createscript (vm.js:80:10)
    at object.runinthiscontext (vm.js:139:10)
    at module._compile (module.js:617:28)
    at object.module._extensions..js (module.js:664:10)
    at module.load (module.js:566:32)
    at trymoduleload (module.js:506:12)
    at function.module._load (module.js:498:3)
    at function.module.runmain (module.js:694:10)
    at startup (bootstrap_node.js:204:16)
    at bootstrap_node.js:625:3


not sure if this was the outcome you wanted to see, the container itself runs. but in kubernetes it gets into errimagepull 

then after editing your pod.yaml inspired by @harsh manvar it works fine with this. so the problem with exiting after completed command was just part of the problem. 

apiversion: v1
kind: pod
metadata:
  name: hello-pod
spec:
  restartpolicy: never
  containers:
  - name: hello
    image: ""foo""
    imagepullpolicy: never
    command: [ ""sleep"" ]
    args: [ ""infinity"" ]


this is minikube so you can reuse the images, but if you would have more nodes this might not work at all. you can find a good explanation about using local docker images with kubernetes here. 
"
55390348,kubernetes with helm on local persistent volume with docker for windows,"i tried to use helm on docker for windows on the local machine. when i used a storage class as local storage, persistent volume, and persistent volume claim without helm, it works fine. but when i used this setting with helm, crashloopbackoff happened.  

localstrageclass.yaml

kind: storageclass
apiversion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumebindingmode: waitforfirstconsumer


pv.yaml

apiversion: v1
kind: persistentvolume
metadata:
  name: local-pv002
  labels:
    type: local
spec:
  capacity:
    storage: 1gi
  accessmodes:
    - readwritemany
  #storageclassname: hostpath
  mountoptions:
    - hard
  persistentvolumereclaimpolicy: delete
  storageclassname: local-storage
  local:
    path: /c/k/share/mysql
  nodeaffinity:
    required:
      nodeselectorterms:
      - matchexpressions:
        #- key: docker.io/hostname
        - key: kubernetes.io/hostname
          operator: in
          values:
          - docker-desktop


pvc.yaml

apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: local-mysql-claim
spec:
  accessmodes:
    - readwritemany
  resources:
    requests:
      storage: 1gi
  storageclassname: local-storage


mysqlconf.yaml

persistence:
  enabled: true
  storageclass: local-storage
  existingclaim: local-mysql-claim
  accessmode: readwriteonce
  size: 1gi
  annotations: {}


$ helm install --name mysql stable/mysql -f mysqlconf.yaml
$ kubectl describe pod mysql

containers:
  mysql:
    container id:   docker://533e4569603b05fac83a0a701da97898b3190503618796678ac5db6340c4dce6
    image:          mysql:5.7.14
    image id:       docker-pullable://mysql@sha256:c8f03238ca1783d25af320877f063a36dbfce0daa56a7b4955e6c6e05ab5c70b
    port:           3306/tcp
    host port:      0/tcp
    state:          waiting
      reason:       crashloopbackoff
    last state:     terminated
      reason:       error
      exit code:    1
      started:      thu, 28 mar 2019 13:24:25 +0900
      finished:     thu, 28 mar 2019 13:24:25 +0900
    ready:          false
    restart count:  2
    requests:
      cpu:      100m
      memory:   256mi
    liveness:   exec [sh -c mysqladmin ping -u root -p${mysql_root_password}] delay=30s timeout=5s period=10s #success=1 #failure=3
    readiness:  exec [sh -c mysqladmin ping -u root -p${mysql_root_password}] delay=5s timeout=1s period=10s #success=1 #failure=3

    environment:
      mysql_root_password:  &lt;set to the key 'mysql-root-password' in secret 'mysql'&gt;  optional: false
      mysql_password:       &lt;set to the key 'mysql-password' in secret 'mysql'&gt;       optional: true
      mysql_user:
      mysql_database:
    mounts:
      /var/lib/mysql from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-dccpv (ro)
conditions:
  type              status
  initialized       true
  ready             false
  containersready   false
  podscheduled      true
volumes:
  data:
    type:       persistentvolumeclaim (a reference to a persistentvolumeclaim in the same namespace)
    claimname:  local-mysql-claim
    readonly:   false
  default-token-dccpv:
    type:        secret (a volume populated by a secret)
    secretname:  default-token-dccpv
    optional:    false
qos class:       burstable
node-selectors:  &lt;none&gt;
tolerations:     node.kubernetes.io/not-ready:noexecute for 300s
                 node.kubernetes.io/unreachable:noexecute for 300s
events:
  type     reason     age                from                     message
  ----     ------     ----               ----                     -------
  normal   scheduled  39s                default-scheduler        successfully assigned default/mysql-698897ff79-n768k to docker-desktop
  normal   pulled     38s                kubelet, docker-desktop  container image ""busybox:1.29.3"" already present on machine
  normal   created    38s                kubelet, docker-desktop  created container
  normal   started    38s                kubelet, docker-desktop  started container
  normal   pulled     18s (x3 over 37s)  kubelet, docker-desktop  container image ""mysql:5.7.14"" already present on machine
  normal   created    17s (x3 over 37s)  kubelet, docker-desktop  created container
  normal   started    17s (x3 over 37s)  kubelet, docker-desktop  started container
  warning  backoff    13s (x5 over 35s)  kubelet, docker-desktop  back-off restarting failed container


when storageclassname was hostpath or did not used the configuration file as
$ helm install --name mysql stable/mysql
it worked fine.  

please tell me how to fix this problem.
",<kubernetes><kubernetes-helm>,55397228,1,"i think you are having a mismatch of accessmodes between what you claim in pvc definition (readwriteonce) and what your storage class offers (readwritemany).

please mind also that persistentvolume(s) of hostpath type does not support readwritemany mode (see spec here).

i would propose you to create pv similar to this one:

# create pv of manual storageclass
kind: persistentvolume
apiversion: v1
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageclassname: manual
  capacity:
    storage: 10gi
  accessmodes:
    - readwriteonce
  hostpath:
    path: ""/c/users/k8s/mysql"" 


and override default pvc storageclassname configuration during helm install like this:

helm install --name my-sql stable/mysql --set persistence.storageclass=manual

"
55362271,helm init error: error installing: deployments.extensions is forbidden when run inside gitlab runner,"i have gitlab (11.8.1) (self-hosted) connected to self-hosted k8s cluster (1.13.4). there're 3 projects in gitlab name shipment, authentication_service and shipment_mobile_service.

all projects add the same k8s configuration exception project namespace.

the first project is successful when install helm tiller and gitlab runner in gitlab ui.

the second and third projects only install helm tiller success, gitlab runner error with log in install runner pod:

 client: &amp;version.version{semver:""v2.12.3"", gitcommit:""eecf22f77df5f65c823aacd2dbd30ae6c65f186e"", gittreestate:""clean""}
error: cannot connect to tiller
+ sleep 1s
+ echo 'retrying (30)...'
+ helm repo add runner https://charts.gitlab.io
retrying (30)...
""runner"" has been added to your repositories
+ helm repo update
hang tight while we grab the latest from your chart repositories...
...skip local chart repository
...successfully got an update from the ""runner"" chart repository
...successfully got an update from the ""stable"" chart repository
update complete.  happy helming! 
+ helm upgrade runner runner/gitlab-runner --install --reset-values --tls --tls-ca-cert /data/helm/runner/config/ca.pem --tls-cert /data/helm/runner/config/cert.pem --tls-key /data/helm/runner/config/key.pem --version 0.2.0 --set 'rbac.create=true,rbac.enabled=true' --namespace gitlab-managed-apps -f /data/helm/runner/config/values.yaml
error: upgrade failed: remote error: tls: bad certificate 


i don't config gitlab-ci with k8s cluster on first project, only setup for the second and third. the weird thing is with the same helm-data (only different by name), the second run success but the third is not.

and because there only one gitlab runner available (from the first project), i assign both 2nd and 3rd project to this runner.

i use this gitlab-ci.yml for both 2 projects with only different name in helm upgrade command.

stages:
  - test
  - build
  - deploy

variables:
  container_image: dockerhub.linhnh.vn/${ci_project_path}:${ci_pipeline_id}
  container_image_latest: dockerhub.linhnh.vn/${ci_project_path}:latest
  ci_registry: dockerhub.linhnh.vn
  docker_driver: overlay2
  docker_host: tcp://localhost:2375 # required when use dind

# test phase and build phase using docker:dind success

deploy_beta:
  stage: deploy
  image: alpine/helm
  script:
    - echo ""deploy test start ...""
    - helm init --upgrade
    - helm upgrade --install --force shipment-mobile-service --recreate-pods --set image.tag=${ci_pipeline_id} ./helm-data
    - echo ""deploy test completed!""
  environment:
    name: staging
  tags: [""kubernetes_beta""]
  only:
  - master


the helm-data is very simple so i think don't really need to paste here.
here is the log when second project deploy success:

running with gitlab-runner 11.7.0 (8bb608ff)
  on runner-gitlab-runner-6c8555c86b-gjt9f xrmajzy2
using kubernetes namespace: gitlab-managed-apps
using kubernetes executor with image linkyard/docker-helm ...
waiting for pod gitlab-managed-apps/runner-xrmajzy2-project-15-concurrent-0x2bms to be running, status is pending
waiting for pod gitlab-managed-apps/runner-xrmajzy2-project-15-concurrent-0x2bms to be running, status is pending
running on runner-xrmajzy2-project-15-concurrent-0x2bms via runner-gitlab-runner-6c8555c86b-gjt9f...
cloning into '/root/authentication_service'...
cloning repository...
checking out 5068bf1f as master...
skipping git submodules setup
$ echo ""deploy start ....""
deploy start ....
$ helm init --upgrade --dry-run --debug
---
apiversion: extensions/v1beta1
kind: deployment
metadata:
  creationtimestamp: null
  labels:
    app: helm
    name: tiller
  name: tiller-deploy
  namespace: kube-system
spec:
  replicas: 1
  strategy: {}
  template:
    metadata:
      creationtimestamp: null
      labels:
        app: helm
        name: tiller
    spec:
      automountserviceaccounttoken: true
      containers:
      - env:
        - name: tiller_namespace
          value: kube-system
        - name: tiller_history_max
          value: ""0""
        image: gcr.io/kubernetes-helm/tiller:v2.13.0
        imagepullpolicy: ifnotpresent
        livenessprobe:
          httpget:
            path: /liveness
            port: 44135
          initialdelayseconds: 1
          timeoutseconds: 1
        name: tiller
        ports:
        - containerport: 44134
          name: tiller
        - containerport: 44135
          name: http
        readinessprobe:
          httpget:
            path: /readiness
            port: 44135
          initialdelayseconds: 1
          timeoutseconds: 1
        resources: {}
status: {}

---
apiversion: v1
kind: service
metadata:
  creationtimestamp: null
  labels:
    app: helm
    name: tiller
  name: tiller-deploy
  namespace: kube-system
spec:
  ports:
  - name: tiller
    port: 44134
    targetport: tiller
  selector:
    app: helm
    name: tiller
  type: clusterip
status:
  loadbalancer: {}

...
$ helm upgrade --install --force authentication-service --recreate-pods --set image.tag=${ci_pipeline_id} ./helm-data
warning: namespace ""gitlab-managed-apps"" doesn't match with previous. release will be deployed to default
release ""authentication-service"" has been upgraded. happy helming!
last deployed: tue mar 26 05:27:51 2019
namespace: default
status: deployed

resources:
==&gt; v1/deployment
name                    ready  up-to-date  available  age
authentication-service  1/1    1           1          17d

==&gt; v1/pod(related)
name                                    ready  status       restarts  age
authentication-service-966c997c4-mglrb  0/1    pending      0         0s
authentication-service-966c997c4-wzrkj  1/1    terminating  0         49m

==&gt; v1/service
name                    type      cluster-ip     external-ip  port(s)       age
authentication-service  nodeport  10.108.64.133  &lt;none&gt;       80:31340/tcp  17d


notes:
1. get the application url by running these commands:
  export node_port=$(kubectl get --namespace default -o jsonpath=""{.spec.ports[0].nodeport}"" services authentication-service)
  echo http://$node_ip:$node_port
$ echo ""deploy completed""
deploy completed
job succeeded


and the third project fail:

running with gitlab-runner 11.7.0 (8bb608ff)
  on runner-gitlab-runner-6c8555c86b-gjt9f xrmajzy2
using kubernetes namespace: gitlab-managed-apps
using kubernetes executor with image alpine/helm ...
waiting for pod gitlab-managed-apps/runner-xrmajzy2-project-18-concurrent-0bv4bx to be running, status is pending
waiting for pod gitlab-managed-apps/runner-xrmajzy2-project-18-concurrent-0bv4bx to be running, status is pending
waiting for pod gitlab-managed-apps/runner-xrmajzy2-project-18-concurrent-0bv4bx to be running, status is pending
waiting for pod gitlab-managed-apps/runner-xrmajzy2-project-18-concurrent-0bv4bx to be running, status is pending
running on runner-xrmajzy2-project-18-concurrent-0bv4bx via runner-gitlab-runner-6c8555c86b-gjt9f...
cloning repository...
cloning into '/canhnv5/shipmentmobile'...
checking out 278cbd3d as master...
skipping git submodules setup
$ echo ""deploy test start ...""
deploy test start ...
$ helm init --upgrade
creating /root/.helm 
creating /root/.helm/repository 
creating /root/.helm/repository/cache 
creating /root/.helm/repository/local 
creating /root/.helm/plugins 
creating /root/.helm/starters 
creating /root/.helm/cache/archive 
creating /root/.helm/repository/repositories.yaml 
adding stable repo with url: https://kubernetes-charts.storage.googleapis.com 
adding local repo with url: http://127.0.0.1:8879/charts 
$helm_home has been configured at /root/.helm.
error: error installing: deployments.extensions is forbidden: user ""system:serviceaccount:shipment-mobile-service:shipment-mobile-service-service-account"" cannot create resource ""deployments"" in api group ""extensions"" in the namespace ""kube-system""
error: job failed: command terminated with exit code 1


i could see they use the same runner xrmajzy2 that i install in the first project, same k8s namespace gitlab-managed-apps.

i think they use privilege mode but don't know why the second can get the right permission, and the third can not? should i create user system:serviceaccount:shipment-mobile-service:shipment-mobile-service-service-account and assign to cluster-admin?

thanks to @cookiedough's instruction. i do these steps:


fork the canhv5/shipment-mobile-service into my root account root/shipment-mobile-service.
delete gitlab-managed-apps namespace without anything inside, run kubectl delete -f gitlab-admin-service-account.yaml.
apply this file then get the token as @cookiedough guide.
back to root/shipment-mobile-service in gitlab, remove previous cluster. add cluster back with new token. install helm tiller then gitlab runner in gitlab ui.
re run the job then the magic happens. but i still unclear why canhv5/shipment-mobile-service still get the same error.

",<kubernetes><gitlab-ci-runner><kubernetes-helm>,55362942,1,"before you do the following, delete the gitlab-managed-apps namespace:

kubectl delete namespace gitlab-managed-apps


reciting from the gitlab tutorial you will need to create a serviceaccount and clusterrolebinding got gitlab, and you will need the secret created as a result to connect your project to your cluster as a result.


  create a file called gitlab-admin-service-account.yaml with contents:


 apiversion: v1
 kind: serviceaccount
 metadata:
   name: gitlab-admin
   namespace: kube-system
 ---
 apiversion: rbac.authorization.k8s.io/v1beta1
 kind: clusterrolebinding
 metadata:
   name: gitlab-admin
 roleref:
   apigroup: rbac.authorization.k8s.io
   kind: clusterrole
   name: cluster-admin
 subjects:
 - kind: serviceaccount
   name: gitlab-admin
   namespace: kube-system



  apply the service account and cluster role binding to your cluster:


kubectl apply -f gitlab-admin-service-account.yaml



  output:


 serviceaccount ""gitlab-admin"" created
 clusterrolebinding ""gitlab-admin"" created



  retrieve the token for the gitlab-admin service account:


 kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep gitlab-admin | awk '{print $1}')


copy the &lt;authentication_token&gt; value from the output:

name:         gitlab-admin-token-b5zv4
namespace:    kube-system
labels:       &lt;none&gt;
annotations:  kubernetes.io/service-account.name=gitlab-admin
              kubernetes.io/service-account.uid=bcfe66ac-39be-11e8-97e8-026dce96b6e8

type:  kubernetes.io/service-account-token

data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      &lt;authentication_token&gt;


follow this tutorial to connect your cluster to the project, otherwise you will have to stitch up the same thing along the way with a lot more pain!
"
67614326,same hostname but different path with let's encrypt,"i've configured let's encrypt using cert-manager in my cluster and it works just fine for most of my use cases. however i have an application which is installed multiple times on the same hostname but with a different path.
my ingress is defined as below
{{- if .values.ingress.enabled -}}
{{- $fullname := include &quot;whoami-go.fullname&quot; . -}}
{{- $svcport := .values.service.port -}}
{{- $tls := haskey .values.ingress &quot;certissuer&quot; -}}
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: {{ $fullname }}
  labels:
    {{- include &quot;whoami-go.labels&quot; . | nindent 4 }}
  annotations:
  {{- if $tls }}
    cert-manager.io/cluster-issuer: {{ .values.ingress.certissuer | quote }}
    ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
  {{- end }}
spec:
  {{- if $tls }}
  tls:
    - secretname: {{ $fullname }}-tls
      hosts:
        - {{ .values.ingress.hostname | quote }}
  {{- end }}
  rules:
    - host: {{ .values.ingress.hostname | quote }}
      http:
        paths:
          - path: {{ .values.ingress.path }}
            pathtype: prefix
            backend:
              service:
                name: {{ $fullname }}
                port:
                  number: {{ $svcport }}
{{- end }}

and it's instantiated with values like below
ingress:
  enabled: true
  hostname: whoami-go.c.dhis2.org
  path: /something
  certissuer: letsencrypt-prod

where path is changed for each installation.
the problem...
e0520 03:13:49.242770 1 sync.go:210] cert-manager/controller/orders &quot;msg&quot;=&quot;failed to create order resource due to bad request, marking order as failed&quot; &quot;error&quot;=&quot;429 urn:ietf:params:acme:error:ratelimited: error creating new order :: too many certificates already issued for exact set of domains: whoami-go.c.dhis2.org: see https://letsencrypt.org/docs/rate-limits/&quot; &quot;resource_kind&quot;=&quot;order&quot; &quot;resource_name&quot;=&quot;finland-whoami-go-tls-tzvk6-4169341110&quot; &quot;resource_namespace&quot;=&quot;whoami&quot; &quot;resource_version&quot;=&quot;v1&quot;

since only the path is updated i hoped that cert-manager would reuse the certificate but that's obviously not the case. can i somehow configure my application to use the same certificate for the same hostname across multiple installations of the same chart?
",<kubernetes><kubernetes-ingress><lets-encrypt>,67614381,1,"error meaning
urn:ietf:params:acme:error:ratelimited: error creating new order :: too many certificates already issued for exact set of domains: whoami-go.c.dhis2.org:

we can only request a certain amount of ssl/tls certificate from the let's encrypt in week.
read more at : https://letsencrypt.org/docs/rate-limits/
due to that, it's showing the error of the rate-limiting. we can request for the 5 cert per week for duplicate cert.
you are using the certissuer: letsencrypt-prod or cluster issuer which will be storing the secret into the kubernetes secret.
while creating the ingress with different paths just change secret or add the secret to ingress as per need your ingress will be working with https.
while keep only one ingress with cluster issuer or issuer so if certificate getting explored it can auto-renew in to secret and that secret will be used by other ingress.
my simple ingress with he ssl/tls cert stored into secret.
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: sls-dev
    nginx.ingress.kubernetes.io/proxy-read-timeout: &quot;1800&quot;
    nginx.ingress.kubernetes.io/proxy-send-timeout: &quot;1800&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/proxy-body-size: &quot;15m&quot;
  name: sls-function-ingress
spec:
  rules:
  - host: app.dev.example.io
    http:
      paths:
      - path: /api/v1/
        backend:
          servicename: test-service
          serviceport: 80
  tls:
  - hosts:
    - app.dev.example.io
    secretname: sls-secret

you can keep cert-manager.io/cluster-issuer: sls-dev to one ingress while with other only secret need to attach.
"
55780971,kubernetes: mysql pod failed to open log file /var/log/pods/,"i am following the official tutorial here to run a stateful mysql pod on the kubernetes cluster which is already running on gcp. i have used the exact same commands to first create the persistent volume and persistent volume chain and then deployed the contents of the mysql yaml file as per the documentation. the mysql pod is not running and is in runcontainererror state. checking the logs of this mysql pod shows:

failed to open log file ""/var/log/pods/045cea87-6408-11e9-84d3-42010aa001c3/mysql/2.log"": open /var/log/pods/045cea87-6408-11e9-84d3-42010aa001c3/mysql/2.log: no such file or directory


update: as asked by @matthew in the comments, the result of kubectl describe pods -l app=mysql is provided here:

name:               mysql-fb75876c6-tk6ml
namespace:          default
priority:           0
priorityclassname:  &lt;none&gt;
node:               gke-mycluster-default-pool-b1c1d316-xv4v/10.160.0.13
start time:         tue, 23 apr 2019 13:36:04 +0530
labels:             app=mysql
                    pod-template-hash=963143272
annotations:        kubernetes.io/limit-ranger=limitranger plugin set: cpu request for container mysql
status:             running
ip:                 10.52.0.7
controlled by:      replicaset/mysql-fb75876c6
containers:
  mysql:
    container id:   docker://451ec5bf67f60269493b894004120b627d9a05f38e37cb50e9f283e58dbe6e56
    image:          mysql:5.6
    image id:       docker-pullable://mysql@sha256:5ab881bc5abe2ac734d9fb53d76d984cc04031159152ab42edcabbd377cc0859
    port:           3306/tcp
    host port:      0/tcp
    state:          waiting
      reason:       runcontainererror
    last state:     terminated
      reason:       containercannotrun
      message:      error while creating mount source path '/mnt/data': mkdir /mnt/data: read-only file system
      exit code:    128
      started:      tue, 23 apr 2019 13:36:18 +0530
      finished:     tue, 23 apr 2019 13:36:18 +0530
    ready:          false
    restart count:  1
    requests:
      cpu:  100m
    environment:
      mysql_root_password:  password
    mounts:
      /var/lib/mysql from mysql-persistent-storage (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jpkzg (ro)
conditions:
  type              status
  initialized       true
  ready             false
  containersready   false
  podscheduled      true
volumes:
  mysql-persistent-storage:
    type:       persistentvolumeclaim (a reference to a persistentvolumeclaim in the same namespace)
    claimname:  mysql-pv-claim
    readonly:   false
  default-token-jpkzg:
    type:        secret (a volume populated by a secret)
    secretname:  default-token-jpkzg
    optional:    false
qos class:       burstable
node-selectors:  &lt;none&gt;
tolerations:     node.kubernetes.io/not-ready:noexecute for 300s
                 node.kubernetes.io/unreachable:noexecute for 300s
events:
  type     reason     age               from                                               message
  ----     ------     ----              ----                                               -------
  normal   scheduled  32s               default-scheduler                                  successfully assigned default/mysql-fb75876c6-tk6ml to gke-mycluster-default-pool-b1c1d316-xv4v
  normal   pulling    31s               kubelet, gke-mycluster-default-pool-b1c1d316-xv4v  pulling image ""mysql:5.6""
  normal   pulled     22s               kubelet, gke-mycluster-default-pool-b1c1d316-xv4v  successfully pulled image ""mysql:5.6""
  normal   pulled     4s (x2 over 18s)  kubelet, gke-mycluster-default-pool-b1c1d316-xv4v  container image ""mysql:5.6"" already present on machine
  normal   created    3s (x3 over 18s)  kubelet, gke-mycluster-default-pool-b1c1d316-xv4v  created container
  warning  failed     3s (x3 over 18s)  kubelet, gke-mycluster-default-pool-b1c1d316-xv4v  error: failed to start container ""mysql"": error response from daemon: error while creating mount source path '/mnt/data': mkdir /mnt/data: read-only file system


as asked by @hanx:
result of kubectl describe pv mysql-pv-volume

name:            mysql-pv-volume
labels:          type=local
annotations:     kubectl.kubernetes.io/last-applied-configuration={""apiversion"":""v1"",""kind"":""persistentvolume"",""metadata"":{""annotations"":{},""labels"":{""type"":""local""},""name"":""mysql-pv-volume"",""namespace"":""""},""spec"":{""a...
                 pv.kubernetes.io/bound-by-controller=yes
finalizers:      [kubernetes.io/pv-protection]
storageclass:    manual
status:          bound
claim:           default/mysql-pv-claim
reclaim policy:  retain
access modes:    rwo
capacity:        1gi
node affinity:   &lt;none&gt;
message:
source:
    type:          hostpath (bare host directory volume)
    path:          /mnt/data
    hostpathtype:
events:            &lt;none&gt;


result of kubectl describe pvc mysql-pv-claim

name:          mysql-pv-claim
namespace:     default
storageclass:  manual
status:        bound
volume:        mysql-pv-volume
labels:        &lt;none&gt;
annotations:   kubectl.kubernetes.io/last-applied-configuration={""apiversion"":""v1"",""kind"":""persistentvolumeclaim"",""metadata"":{""annotations"":{},""name"":""mysql-pv-claim"",""namespace"":""default""},""spec"":{""accessmodes"":[""r...
               pv.kubernetes.io/bind-completed=yes
               pv.kubernetes.io/bound-by-controller=yes
finalizers:    [kubernetes.io/pvc-protection]
capacity:      1gi
access modes:  rwo
events:        &lt;none&gt;


mysql-pv.yaml

kind: persistentvolume
apiversion: v1
metadata:
  name: mysql-pv-volume
  labels:
    type: local
spec:
  storageclassname: manual
  capacity:
    storage: 20gi
  accessmodes:
    - readwriteonce
  hostpath:
    path: ""/mnt/data""
---
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: mysql-pv-claim
spec:
  storageclassname: manual
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 20gi


mysql.yaml

apiversion: v1
kind: service
metadata:
  name: mysql
spec:
  ports:
  - port: 3306
  selector:
    app: mysql
  clusterip: none
---
apiversion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: deployment
metadata:
  name: mysql
spec:
  selector:
    matchlabels:
      app: mysql
  strategy:
    type: recreate
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
          # use secret in real usage
        - name: mysql_root_password
          value: password
        ports:
        - containerport: 3306
          name: mysql
        volumemounts:
        - name: mysql-persistent-storage
          mountpath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentvolumeclaim:
          claimname: mysql-pv-claim

",<mysql><docker><kubernetes><google-kubernetes-engine>,55829064,1,"this is because you do not need to create those volumes and storageclasses on gke. those yaml files are completely  valid if you would want to use minikube or kubeadm, but not in case of gke which can take care of some of the manual steps on its own. 

you can use this official guide to run mysql on gke, or just use files edited by me and tested on gke.

kind: persistentvolumeclaim
apiversion: v1
metadata:
  name: mysql-volumeclaim
spec:
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 20gi


and mysql deployment: 

apiversion: v1
kind: service
metadata:
  name: mysql
spec:
  ports:
  - port: 3306
  selector:
    app: mysql
  clusterip: none
---
apiversion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: deployment
metadata:
  name: mysql
spec:
  selector:
    matchlabels:
      app: mysql
  strategy:
    type: recreate
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
          # use secret in real usage
        - name: mysql_root_password
          value: password
        ports:
        - containerport: 3306
          name: mysql
        volumemounts:
        - name: mysql-persistent-storage
          mountpath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentvolumeclaim:
          claimname: mysql-volumeclaim


make sure you read the linked guide as it explains the gke specific topics there. 
"
70364991,kubernetes yaml deployment - unable to create a symbolic link,"i am trying to make an nginx deployment and during the container creation, i want to create multiply symbolic links. but for some reason, it doesn't work and the container crashes.
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: tcc
    component: nginx
  name: tcc-nginx-deployment
  namespace: dev2
spec:
  replicas: 1
  selector:
    matchlabels:
      app: tcc
      component: nginx
  template:
    metadata:
      labels:
        app: tcc
        component: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        command:
              - /bin/sh
              - -c
              - |
                ln -s /shared/apps/ /var/www
                rm -r /etc/nginx/conf.d
                ln -s /shared/nginx-config/ /etc/nginx/conf.d

        ports:
        - containerport: 80
          protocol: tcp
        volumemounts:
        - mountpath: /shared
          name: efs-pvc
      volumes:
      - name: efs-pvc
        persistentvolumeclaim:
          claimname: tcc-efs-storage-claim

",<nginx><kubernetes><kubernetes-pod>,70377345,1,"the container is not running, because after the command block is executed, container is exiting, which is expected behaviour.
instead of playing with symbolic links in command in yaml template (which is not the best practice solution), why just don't use solution builtin kubernetes and do not use command block at all?
you should use subpath which is designed to share directories from one volume for multiple, different directories on the single pod:

sometimes, it is useful to share one volume for multiple uses in a single pod. the volumemounts.subpath property specifies a sub-path inside the referenced volume instead of its root.

in your case, the deployment yaml should look like this:
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: tcc
    component: nginx
  name: tcc-nginx-deployment
  namespace: dev2
spec:
  replicas: 1
  selector:
    matchlabels:
      app: tcc
      component: nginx
  template:
    metadata:
      labels:
        app: tcc
        component: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerport: 80
          protocol: tcp
        volumemounts:
        - mountpath: /shared
          name: efs-pvc
        - mountpath: /etc/nginx/conf.d
          name: efs-pvc
          subpath: nginx-config
        - mountpath: /var/www
          name: efs-pvc
          subpath: apps
      volumes:
      - name: efs-pvc
        persistentvolumeclaim:
          claimname: tcc-efs-storage-claim

also if you want to mount only config files for nginx, you may use configmap instead of volume - check this answer for more information.
"
57364093,how to fix `can't evaluate field extrahosts in type interface {}` in _helpers.tpl in helm,"i am trying to get some values from umbrella chart in helm in _helpers.tpl but i for some reason i am getting the error executing ""gluu.ldaplist"" at &lt;.values.ldap.extraho...&gt;: can't evaluate field extrahosts in type interface {}

this is what i am trying to do.
_helpers.ptl

{{- define ""gluu.ldaplist"" -}}
{{- $hosts := .values.ldap.extrahosts -}}
{{- $genldap := dict ""host"" (printf ""%s-%s"" .release.name .values.ldaptype) ""port"" .values.ldapport -}}
{{- $hosts := prepend $hosts $genldap -}}
{{- $local := dict ""first"" true -}}
{{- range $k, $v := $hosts -}}
{{- if not $local.first -}},{{- end -}}{{- printf ""%s:%.f"" $v.host $v.port -}}{{- $_ := set $local ""first"" false -}}
{{- end -}}
{{- end -}}


and this is part of values.yml for the umbrella chart
values.yml

ldap:
  enabled: true
  type: opendj
  extrahosts: [
    host: opendj,
    port: 3434
  ] #array of k,v e.g host: host1, port: port1


directory structure

helm/
  charts/
     chart_a/
       templates/
          configmap.yml -----&gt;&gt;&gt; this is where i want to use it
  templates/
     _helpers.tpl ----&gt;&gt;&gt;&gt; where the failing function is
  requirements.yml
  values.yml ----------&gt;&gt;&gt; where the ldap values are



the configmap.yml looks like below

apiversion: v1
kind: configmap
metadata:
  name: {{ template ""oxauth.fullname"" . }}-cm
data:
  gluu_config_adapter: {{ .values.global.configadaptername | quote }}
  gluu_ldap_url: {{ template ""gluu.ldaplist"" . }}


note: the _helpers.tpl is under the main/umbrella chart. chart_a is a subchart.

expected results are something like gluu_ldap_url:""opendj:3434""

helm version:  

client: &amp;version.version{semver:""v2.10.0"", gitcommit:""9ad53aac42165a5fadc6c87be0dea6b115f93090"", gittreestate:""clean""}
server: &amp;version.version{semver:""v2.10.0"", gitcommit:""9ad53aac42165a5fadc6c87be0dea6b115f93090"", gittreestate:""clean""}


expected result is that the function {{- define ""gluu.ldaplist"" -}} in _helpers.tpl completes without error even if no values are provided in the array. 
if there are values provided, the expected string is host:port as output.

if this can be done in another way, i welcome any suggestion.
",<go><kubernetes><kubernetes-helm><templating><gluu>,57393952,1,"this can be solved with global values which allow values in the parent chart to override (or supply unspecified) values in the child subcharts.

from the helm docs on subcharts and global values:


  
  a subchart is considered stand-alone, which means a subchart can never explicitly depend on its parent chart.
  for that reason, a subchart cannot access the values of its parent.
  a parent chart can override values for subcharts.
  helm has a concept of global values that can be accessed by all charts.
  


(at first i didn't think to search for ""helm subchart"" but once i did an internet search for that term, this was the first or second result)

here's a minimal example that solves your issue:

directory structure

helm
 chart.yaml
 charts
  chart_a
      chart.yaml
      templates
          configmap.yml
 templates
  _helpers.tpl
 values.yaml


note: i added chart.yaml files to make it actually work, renamed values.yml to values.yaml so that it works by default without extra flags, and removed requirements.yml since it wasn't necessary to reproduce the problem and solution.

values.yaml

global:
  ldap:
    enabled: true
    type: opendj
    extrahosts:
    - host: opendj
      port: 3434
  ldaptype: xxx
  ldapport: 123


the key was to nest what you had under a special global key. note, i also added ldaptype and ldapport since they were in your _helpers.tpl, and i fixed the yaml structure you had under extrahosts. what was there before didn't actually represent a list of maps with host and port keys. without this fix, the helm command doesn't fail but doesn't output what you want either.

result

$ helm template .
---
# source: helm/charts/chart_a/templates/configmap.yml
apiversion: v1
kind: configmap
metadata:
  name: cm
data:
  gluu_ldap_url: release-name-xxx:123,opendj:3434

"
68235637,traefik ingress for multiple ports,"i've deployed rabbitmq cluster in k3s cluster using rabbitmq rabbitmq cluster operator. as a result, it created a clusterip service looks like this:
name:              rabbitmq
namespace:         rabbits
labels:            app.kubernetes.io/component=rabbitmq
                   app.kubernetes.io/name=rabbitmq
                   app.kubernetes.io/part-of=rabbitmq
annotations:       &lt;none&gt;
selector:          app.kubernetes.io/name=rabbitmq
type:              clusterip
ip family policy:  singlestack
ip families:       ipv4
ip:                10.43.48.11
ips:               10.43.48.11
port:              amqp  5672/tcp
targetport:        5672/tcp
endpoints:         10.42.2.55:5672,10.42.2.56:5672
port:              management  15672/tcp
targetport:        15672/tcp
endpoints:         10.42.2.55:15672,10.42.2.56:15672
port:              mqtt  1883/tcp
targetport:        1883/tcp
endpoints:         10.42.2.55:1883,10.42.2.56:1883
port:              web-mqtt  15675/tcp
targetport:        15675/tcp
endpoints:         10.42.2.55:15675,10.42.2.56:15675
port:              stomp  61613/tcp
targetport:        61613/tcp
endpoints:         10.42.2.55:61613,10.42.2.56:61613
port:              web-stomp  15674/tcp
targetport:        15674/tcp
endpoints:         10.42.2.55:15674,10.42.2.56:15674
port:              prometheus  15692/tcp
targetport:        15692/tcp
endpoints:         10.42.2.55:15692,10.42.2.56:15692
session affinity:  none
events:            &lt;none&gt;

i want to open port 15672 for admin portal, and port 5672 for applications outside of the kubernetes cluster. i tried the following but didn't work:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: rabbitmq-admin-ingress
  annotations:
    kubernetes.io/ingress.class: &quot;traefik&quot;
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          servicename: rabbitmq
          serviceport: 15672
      - path : /
        backend:
          servicename: rabbitmq
          serviceport: 5672

anyone can help me, what i'm doing wrong?
thanks in advance.
",<kubernetes><rabbitmq><kubernetes-ingress><traefik><traefik-ingress>,68237309,1,"you can't have both paths pointing to the same location. also, you should set a fqdn for your ingress.
try using two ingresses, with different names - which would work in most cases:
---
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: rabbitmq-admin-ingress-one
  annotations:
    kubernetes.io/ingress.class: &quot;traefik&quot;
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          servicename: rabbitmq
          serviceport: 15672
    host: host1.example.com
---
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: rabbitmq-admin-ingress-two
  annotations:
    kubernetes.io/ingress.class: &quot;traefik&quot;
spec:
  rules:
  - http:
      paths:
      - path : /
        backend:
          servicename: rabbitmq
          serviceport: 5672
    host: host2.example.com

or use two paths on a single ingress - may involve some additional paths rewriting:
---
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: rabbitmq-admin-ingress
  annotations:
    kubernetes.io/ingress.class: &quot;traefik&quot;
spec:
  rules:
  - http:
      paths:
      - path: /one
        backend:
          servicename: rabbitmq
          serviceport: 15672
      - path : /two
        backend:
          servicename: rabbitmq
          serviceport: 5672
    host: host1.example.com

"
68209297,how to deploy .net core web and worker projects to kubernetes in single deployment?,"i am relatively new to docker and kubernetes technologies. my requirement is to deploy one web and one worker (.net background service) project in a single deployment.
this is how my deployment.yml file looks like :
apiversion : apps/v1
kind: deployment
metadata:
  name: worker
spec:
  progressdeadlineseconds: 3600
  replicas: 1
  selector:
    matchlabels:
      app: worker
  template:
    metadata:
      labels:
        app: worker
    spec:
      containers:
        - name: worker
          image: xxxxx.azurecr.io/worker:#{build.buildid}#
          #image: xxxxx.azurecr.io/web
          imagepullpolicy: always
          #ports:
          #- containerport: 80

apiversion : apps/v1
kind: deployment
metadata:
  name: web
spec:
  progressdeadlineseconds: 3600
  replicas: 1
  selector:
    matchlabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
        - name: web
          image: xxxxx.azurecr.io/web:#{build.buildid}#
          #image: xxxxx.azurecr.io/web
          imagepullpolicy: always
          ports:
          - containerport: 80

this is how my service.yml file looks like :
apiversion: v1
kind: service
metadata:
    name: worker
spec:
    type: loadbalancer
    ports:
    - port: 80 
    selector:
        app: worker
---
apiversion: v1
kind: service
metadata:
    name: web
spec:
    type: loadbalancer
    ports:
    - port: 80 
    selector:
        app: web

what i have found is if i keep both in service.yml file then its only deploying one in kubernetes and if i comment one and execute one by one then its deploying to kubernetes.
is there any rule that we cant have both in single file? any reason why its not working together however working individually?
one more ask is there any way we can look into worker service pod something like taking remote of that and see what exactly going on there....even if its a console application then anyway to read whats its printing on console after deployment.?
",<azure><docker><kubernetes><azure-aks><kubernetes-pod>,68343941,1,"this issue was resolved in the comments section and i decided to provide a community wiki answer just for better visibility to other community members.
it is possible to group multiple kubernetes resources in the same file, but it is important to separate them using three dashes (---).
it's also worth mentioning that resources will be created in the order they appear in the file.
for more information, see the organizing resource configurations documentation.

i've created an example to demonstrate how we can create a simple app-1 application (deployment + service) using a single manifest file:
$ cat app-1.yml
apiversion: v1
kind: service
metadata:
  labels:
    app: app-1
  name: app-1
spec:
  ports:
  - port: 80
    protocol: tcp
    targetport: 80
  selector:
    app: app-1
---
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: app-1
  name: app-1
spec:
  replicas: 1
  selector:
    matchlabels:
      app: app-1
  template:
    metadata:
      labels:
        app: app-1
    spec:
      containers:
      - image: nginx
        name: nginx

note: resources are created in the order they appear in the file:
$ kubectl apply -f app-1.yml
service/app-1 created
deployment.apps/app-1 created

$ kubectl get deploy,svc
name                    ready   up-to-date   
deployment.apps/app-1   1/1     1            

name                 type        cluster-ip    external-ip   port(s)   
service/app-1        clusterip   10.8.14.179   &lt;none&gt;        80/tcp  

"
68200734,ingress not showing up in namespace,"i have used the official kubernetes-dashboard and was trying to setup an ingress for the dashboard. i have tried to put the ingress into the kubernetes-dashboard namespace, but the ingress won't show up when i tried kubectl get all -n kubernetes-dashboard. i could still describe the ingress in the same namespace using kubectl get ingress -n kubernetes-dashboard, but the annotations shown up &lt;none&gt;. can anyone please help me?
here is my yaml file for the ingress:
apiversion: networking.k8s.io/v1
kind: ingress
metadata: 
  name: dashborad-ingress
  namespace: kubernetes-dashboard
spec:
  rules:
  - host: dashboard.com
    http:
      paths:
      - path: /testpath
        pathtype: prefix
        backend:
          service: 
            name: kubernetes-dashboard
            port: 
              number: 443

and the output of kubectl desribe ingress -n kubernetes-dashboard
name:             dashborad-ingress
namespace:        kubernetes-dashboard
address:          192.168.49.2
default backend:  default-http-backend:80 (&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;)
rules:
  host           path  backends
  ----           ----  --------
  dashboard.com  
                    kubernetes-dashboard:443 (172.17.0.6:8443)
annotations:     &lt;none&gt;
events:
  type    reason  age                 from                      message
  ----    ------  ----                ----                      -------
  normal  sync    4m7s (x5 over 24m)  nginx-ingress-controller  scheduled for sync

any helps would be appreciated.
",<kubernetes><kubernetes-ingress>,68207241,1,"annotations displayed as &lt;none&gt; appear to correct behaviour. you haven't defined any in the attached yaml file, so  &lt;none&gt; should be displayed.
i have create simple ingress with one annotation to show you how it works. look at my metadata section:
metadata:
  name: my-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1

when i describe my ingress by using command: kubectl describe ingress i have this line in the output:
annotations:  nginx.ingress.kubernetes.io/rewrite-target: /$1

however, i see the problem in a completely different place. when you describe ingress you have this line:
default backend:  default-http-backend:80 (&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;)

the default-backend is responsible to deliver the 404-page-not-found error page when you are trying to access a service that does not have a rule defined or a service that has not been configured correctly.
depending on how you set up the cluster, the solutions may be different. however, if you are using bare metal and minikube you can try to enable ingress addon:
minikube addons enable ingress

or
minikube addons enable ingress --alsologtostderr


another solution to this addons:
kubectl apply -f https://raw.githubusercontent.com/roelal/minikube/5093d8b21c0931a6c63fa448538761b4bf100ee0/deploy/addons/ingress/ingress-rc.yaml
kubectl apply -f https://raw.githubusercontent.com/roelal/minikube/5093d8b21c0931a6c63fa448538761b4bf100ee0/deploy/addons/ingress/ingress-svc.yaml


you can also add  defaultbackend  as a part of your ingress definition like so
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: default-backend-ingress-example
spec:
  defaultbackend:
    service:
      name: hotel-svc
      port:
        number: 80


see also:

question on stackoverflow about default http backed
same problem on github
another one
similar topic on linuxfoundation.org
troubleshoouting guide from official documentation

"
57516110,how do i get more control over the nodeports my service assigns to my deployment?,"i have a deployment with 5 replicas. all have ssh and telnet. they should not be load balanced. i would like each to select from a predictable list of 5.

here is my deployment

apiversion: apps/v1
kind: deployment
metadata:
  name: myapp
spec:
  selector:
    matchlabels:
      app: myapp
  replicas: 5
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: myapp:1.0
        ports:
        - name: ssh
          protocol: tcp
          containerport: 22
        - name: telnet
          protocol: tcp
          containerport: 23


this is my service with invalid nodeport values for illustrative purposes.

apiversion: v1
kind: service
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  type: nodeport
  ports:
  - name: ssh
    port: 22
    nodeport: [30022, 30122, 30222, 30322, 30422, 30522]
  - name: telnet
    port: 23
    nodeport: [30023, 30123, 30223, 30323, 30423, 30523]


i am hoping to be able to accomplish 2 things:


each pod replica instance will only get an ssh port from [30022, 30122, 30222, 30322, 30422, 30522] and a telnet port from [30023, 30123, 30223, 30323, 30423, 30523]
a pod replica instance that gets an ssh port of 30022 also gets the telnet port 30023. a pod replica instance that gets an ssh port of 30122 gets a telnet port of 30123 and so on.


thank you!
",<kubernetes><kubernetes-service><kubernetes-deployment>,57567103,1,"you can use a statefulset instead of a deployment:


  like a deployment, a statefulset manages pods that are based on an identical container spec. unlike a deployment, a statefulset maintains a sticky identity for each of their pods. these pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.


the particularly useful feature of statefulsets is that you will get a unique label predictably generated for each pod:


  when the statefulset controller creates a pod, it adds a label, statefulset.kubernetes.io/pod-name, that is set to the name of the pod. this label allows you to attach a service to a specific pod in the statefulset. [source]


then you would create five distinct services, one for each pod, of the following form:

apiversion: v1
kind: service
metadata:
  name: myapp-${n}
  labels: { ... } # whatever you want
spec:
  type: nodeport
  selector: 
    statefulset.kubernetes.io/pod-name: myapp-${n} # assuming you keep the name
                                                   # ""myapp"" and just switch kind
                                                   # from deployment to statefulset
  ports:
  - name: ssh
    port: 22
    nodeport: 30${n}22
  - name: telnet
    port: 23
    nodeport: 30${n}23


replacing ${n} with 0 through 4.
"
70847668,kubernetes ignoring persistentvolume,"i have created a persistent volume:
apiversion: v1
kind: persistentvolume
metadata:
  name: pv-volume
  labels:
    type: local
spec:
  storageclassname: manual
  capacity:
    storage: 5gi
  accessmodes:
    - readwriteonce
  hostpath:
    path: &quot;c:/users/xxx/desktop/pv&quot;

and want to make save mysql statefulset pods things on it.
so, i wrote the volumeclaimtemplate:
  volumeclaimtemplates:
  - metadata:
      name: data
    spec:
      accessmodes: [&quot;readwriteonce&quot;]
      resources:
        requests:
          storage: 1gi

thinking this would request the persistent storage from the only persistent volume i have. instead, this is what happens:

",<kubernetes><persistent-volumes><kubernetes-pvc>,70847935,1,"statefulsets requires you to use storage classes in order to bind the correct pvs with the correct pvcs.
the correct way to make statefulsets mount local storage is by using local type of volumes, take a look at the procedure below.

first, you create a storage class for the local volumes. something like the following:
apiversion: storage.k8s.io/v1
kind: storageclass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumebindingmode: waitforfirstconsumer

it has no-provisioner so it will not be able to automatically provision pvs, you'll need to create them manually, but that's exactly what you want for local storage.
second, you create your local pv, something as the following:
apiversion: v1
kind: persistentvolume
metadata:
  name: pv-volume
spec:
  capacity:
    storage: 5gi
  volumemode: filesystem
  accessmodes:
  - readwriteonce
  persistentvolumereclaimpolicy: retain
  storageclassname: local-storage
  local:
    path: &quot;c:/users/xxx/desktop/pv&quot;
  nodeaffinity:
    required:
      nodeselectorterms:
      - matchexpressions:
        - key: kubernetes.io/hostname
          operator: in
          values:
          - the-node-hostname-on-which-the-storage-is-located

this definition tells the local path on the node, but also forces the pv to be used on a specific node (which match the nodeselectorterms).
it also links this pv to the storage class created earlier. this means that now, if a statefulsets requires a storage with that storage class, it will receive this disk (if the space required is less or equal, of course)
third, you can now link the statefulset:
volumeclaimtemplates:
  - metadata:
      name: data
    spec:
      accessmodes: [ &quot;readwriteonce&quot; ]
      storageclassname: &quot;local-storage&quot;
      resources:
        requests:
          storage: 5gi


when the statefulset pod will need to be scheduled for the first time, the following will happen:

a pvc will be created and it will go bound with the pv you just created
the pod will be scheduled to run on the node on which the bounded pv is restricted to run


update:
in case you want to use hostpath storage instead of local storage (because for example you are on minikube and that is supported out of the box directly, so it's more easy) you need to change the pv declaration a bit, something like the following:
apiversion: v1
kind: persistentvolume
metadata:
  name: pv-volume
spec:
  storageclassname: local-storage
  accessmodes:
    - readwriteonce
  capacity:
    storage: 5gi
  hostpath:
    path: /data/pv0001/

now, the /data directory and all its content is persisted on the host (so if minikube gets restarted, it's still there) but if you want to mount specific directories of your host, you need to use minikube mount, for example:
minikube mount &lt;source directory&gt;:&lt;target directory&gt;

for example, you could do:
minikube mount c:/users/xxx/desktop/pv:/host/my-special-pv
and then you could use /host/my-special-pv as the hostpath inside the pv declaration.
more info can be read in the docs.
"
57554890,can't send log into graylog kubernetes,"how to expose node port on ingress?

name                         type        cluster-ip    external-ip   port(s)                                       age
logs-graylog                 nodeport    10.20.8.187   &lt;none&gt;        80:31300/tcp,12201:31301/udp,1514:31302/tcp   5d3h
logs-graylog-elasticsearch   clusterip   none          &lt;none&gt;        9200/tcp,9300/tcp                             5d3h
logs-graylog-master          clusterip   none          &lt;none&gt;        9000/tcp                                      5d3h
logs-graylog-slave           clusterip   none          &lt;none&gt;        9000/tcp                                      5d3h
logs-mongodb-replicaset      clusterip   none          &lt;none&gt;        27017/tcp                                     5d3h


this is how my service look like where there are some node ports.
 graylog web interface is expose on port 80.

but i am not able to send logs on url. my graylog weburl is https://logs.example.com

it's running on https cert-manager is there on kubernertes ingress.

i am not able to send glef udp logs on url. am i missing something to open port from ingress or udp filter something ?

this is my ingress :

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: logs-graylog-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    certmanager.k8s.io/cluster-issuer: graylog
    nginx.ingress.kubernetes.io/ssl-redirect: ""true""

spec:
  tls:
  - hosts:
    - logs.example.io
    secretname: graylog
  rules:
  - host: logs.example.io
    http:
      paths:
      - backend:
          servicename: logs-graylog
          serviceport: 80
      - backend:
          servicename: logs-graylog
          serviceport: 12201
      - backend:
          servicename: logs-graylog
          serviceport: 31301


service : 

apiversion: v1
kind: service
metadata:

  labels:
    app: graylog
    chart: graylog-0.1.0
    component: graylog-service
    heritage: tiller
    name: graylog
    release: logs
  name: logs-graylog

spec:
  clusterip: 10.20.8.187
  externaltrafficpolicy: cluster
  ports:
  - name: http
    nodeport: 31300
    port: 80
    protocol: tcp
    targetport: 9000
  - name: udp-input
    nodeport: 31301
    port: 12201
    protocol: udp
    targetport: 12201
  - name: tcp-input
    nodeport: 31302
    port: 1514
    protocol: tcp
    targetport: 1514
  selector:
    graylog: ""true""
  sessionaffinity: none
  type: nodeport
status:
  loadbalancer: {}

",<kubernetes><kubernetes-ingress><nginx-ingress><graylog3>,57566630,1,"udp services aren't normally exposed via an ingress controller like tcp http(s) services are. i'm not sure any ingress controllers even support udp, certainly not with 3 protocols combined in a single ingress definition. 

if the cluster is hosted on a cloud service, most support a service with type loadbalancer to map external connections into a cluster. 

apiversion: v1
kind: service
metadata:
  name: logs-direct-graylog
spec:
  selector:
    graylog: ""true""
  ports:
  - name: udp-input
    port: 12201
    protocol: udp
    targetport: 12201
  - name: tcp-input
    port: 1514
    protocol: tcp
    targetport: 1514
  type: loadbalancer


if service of type loadbalancer is not available in your environment you can use the nodeport service. the nodeports you have defined will be available on the external ip of each of your nodes. 

a nodeport is not strictly required for the http port, as the nginx ingress controller takes care of that for you elsewhere in it's own service. 

apiversion: v1
kind: service
metadata:
  name: logs-graylog
spec:
  selector:
    graylog: ""true""
  ports:
  - name: http
    port: 80
    protocol: tcp
    targetport: 9000


the ports other than 80 can be removed from your ingress definition. 
"
70884109,contacting service in kubernetes,"i thought i understood how kubernetes services work, and have always seen them like a way to &quot;group&quot; several pods, in order to make it able to contact the service instead of the single pods. however, it seems like i am wrong. i created a mysql deployment (with only one pod) and a service in order to reach out to the service if i want to use the mysql connection from other pods(other microservices). this is the service i made:
apiversion: v1
kind: service
metadata:
  name: mysql
  labels:
    run: mysql
spec:
  ports:
  - port: 3306
    targetport: 3306
    protocol: tcp
  selector:
    run: mysql

i hoped this would have allowed me to connect to the mysql pod by reaching the &lt;clusterip&gt;:&lt;targetport&gt;, but the connection is refused whenever i try to connect. i tried reading online and initially thought nodeport service type was a good idea, but the kubernetes website tells that the service is than reachable by &lt;nodeip&gt;:&lt;nodeport&gt; so this got me confused. mysql should be reachable only inside the cluster by other nodes. how can i make this happen?
notes:

i am working on minikube.
here is how i try connecting from one pod to the mysql service on python:

service = api.read_namespaced_service(name=&quot;mysql&quot;, namespace=&quot;default&quot;)

mydb = mysql.connector.connect(host=service.spec.cluster_ip, user=&quot;root&quot;,
                                 password=&quot;password&quot;, database=&quot;db_name&quot;,
                                 auth_plugin='mysql_native_password')

this is the error i get:
traceback (most recent call last):
  file &quot;/init/db_init.py&quot;, line 10, in &lt;module&gt;
    mydb = mysql.connector.connect(host=service.spec.cluster_ip, user=&quot;root&quot;,
  file &quot;/usr/local/lib/python3.9/site-packages/mysql/connector/__init__.py&quot;, line 272, in connect
    return cmysqlconnection(*args, **kwargs)
  file &quot;/usr/local/lib/python3.9/site-packages/mysql/connector/connection_cext.py&quot;, line 85, in __init__
    self.connect(**kwargs)
  file &quot;/usr/local/lib/python3.9/site-packages/mysql/connector/abstracts.py&quot;, line 1028, in connect
    self._open_connection()
  file &quot;/usr/local/lib/python3.9/site-packages/mysql/connector/connection_cext.py&quot;, line 241, in _open_connection
    raise errors.get_mysql_exception(msg=exc.msg, errno=exc.errno,
mysql.connector.errors.databaseerror: 2003 (hy000): can't connect to mysql server on '10.107.203.112:3306' (111)

update
as requested, here is the whole log of the mysql pod:
2022-01-27 17:57:14+00:00 [note] [entrypoint]: entrypoint script for mysql server 8.0.28-1debian10 started.
2022-01-27 17:57:15+00:00 [note] [entrypoint]: switching to dedicated user 'mysql'
2022-01-27 17:57:15+00:00 [note] [entrypoint]: entrypoint script for mysql server 8.0.28-1debian10 started.
2022-01-27 17:57:15+00:00 [note] [entrypoint]: initializing database files
2022-01-27t17:57:15.090697z 0 [system] [my-013169] [server] /usr/sbin/mysqld (mysqld 8.0.28) initializing of server in progress as process 43
2022-01-27t17:57:15.105399z 1 [system] [my-013576] [innodb] innodb initialization has started.
2022-01-27t17:57:16.522380z 1 [system] [my-013577] [innodb] innodb initialization has ended.
2022-01-27t17:57:20.805814z 6 [warning] [my-010453] [server] root@localhost is created with an empty password ! please consider switching off the --initialize-insecure option.
2022-01-27 17:57:29+00:00 [note] [entrypoint]: database files initialized
2022-01-27 17:57:29+00:00 [note] [entrypoint]: starting temporary server
2022-01-27t17:57:29.868217z 0 [system] [my-010116] [server] /usr/sbin/mysqld (mysqld 8.0.28) starting as process 92
2022-01-27t17:57:29.892649z 1 [system] [my-013576] [innodb] innodb initialization has started.
2022-01-27t17:57:30.100941z 1 [system] [my-013577] [innodb] innodb initialization has ended.
2022-01-27t17:57:30.398700z 0 [warning] [my-010068] [server] ca certificate ca.pem is self signed.
2022-01-27t17:57:30.398743z 0 [system] [my-013602] [server] channel mysql_main configured to support tls. encrypted connections are now supported for this channel.
2022-01-27t17:57:30.419293z 0 [warning] [my-011810] [server] insecure configuration for --pid-file: location '/var/run/mysqld' in the path is accessible to all os users. consider choosing a different directory.
2022-01-27t17:57:30.430833z 0 [system] [my-011323] [server] x plugin ready for connections. socket: /var/run/mysqld/mysqlx.sock
2022-01-27t17:57:30.430879z 0 [system] [my-010931] [server] /usr/sbin/mysqld: ready for connections. version: '8.0.28'  socket: '/var/run/mysqld/mysqld.sock'  port: 0  mysql community server - gpl.
2022-01-27 17:57:30+00:00 [note] [entrypoint]: temporary server started.
warning: unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. skipping it.
warning: unable to load '/usr/share/zoneinfo/leap-seconds.list' as time zone. skipping it.
warning: unable to load '/usr/share/zoneinfo/zone.tab' as time zone. skipping it.
warning: unable to load '/usr/share/zoneinfo/zone1970.tab' as time zone. skipping it.
2022-01-27 17:57:32+00:00 [note] [entrypoint]: creating database football

2022-01-27 17:57:32+00:00 [note] [entrypoint]: /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/sql.sql


2022-01-27 17:57:33+00:00 [note] [entrypoint]: stopping temporary server
2022-01-27t17:57:33.143178z 12 [system] [my-013172] [server] received shutdown from user root. shutting down mysqld (version: 8.0.28).
2022-01-27t17:57:36.222404z 0 [system] [my-010910] [server] /usr/sbin/mysqld: shutdown complete (mysqld 8.0.28)  mysql community server - gpl.
2022-01-27 17:57:37+00:00 [note] [entrypoint]: temporary server stopped

2022-01-27 17:57:37+00:00 [note] [entrypoint]: mysql init process done. ready for start up.

2022-01-27t17:57:37.329690z 0 [system] [my-010116] [server] /usr/sbin/mysqld (mysqld 8.0.28) starting as process 1
2022-01-27t17:57:37.336444z 1 [system] [my-013576] [innodb] innodb initialization has started.
2022-01-27t17:57:37.525143z 1 [system] [my-013577] [innodb] innodb initialization has ended.
2022-01-27t17:57:37.738175z 0 [warning] [my-010068] [server] ca certificate ca.pem is self signed.
2022-01-27t17:57:37.738216z 0 [system] [my-013602] [server] channel mysql_main configured to support tls. encrypted connections are now supported for this channel.
2022-01-27t17:57:37.745722z 0 [warning] [my-011810] [server] insecure configuration for --pid-file: location '/var/run/mysqld' in the path is accessible to all os users. consider choosing a different directory.
2022-01-27t17:57:37.757638z 0 [system] [my-011323] [server] x plugin ready for connections. bind-address: '::' port: 33060, socket: /var/run/mysqld/mysqlx.sock
2022-01-27t17:57:37.757679z 0 [system] [my-010931] [server] /usr/sbin/mysqld: ready for connections. version: '8.0.28'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  mysql community server - gpl.

also, here is the deployment i used:
apiversion: apps/v1
kind: deployment
metadata:
  name: mysql
spec:
  selector:
    matchlabels:
      app: mysql
  strategy:
    type: recreate
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - image: repo/football-mysql
        name: mysql
        env:
          # use secret in real usage
        - name: mysql_root_password
          value: password
        ports:
        - containerport: 3306
          name: mysql
        volumemounts:
        - name: mysql-persistent-storage
          mountpath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentvolumeclaim:
          claimname: mysql-pv-claim

",<python><mysql><kubernetes><kubernetes-pod>,70884917,1,"there is a label mismatch between the service and the pod:
# service
spec:
  selector:
    run: mysql  # &lt;- this

vs
# deployment
spec:
  template:
    metadata:
      labels:
        app: mysql  # &lt;- and this

because of it, the service targets some other pod, not the one created by the deployment. in other words, the service looks for pods with run label value equal to mysql, while your mysql pod has app label with mysql in it.
to make it work, at least one pair of labels has to be completely equal on both sides for the service and the pod. in this case, replacing run for app should be enough:
apiversion: v1
kind: service
metadata:
  name: mysql
  labels:
    run: mysql  # changing here isn't mandatory, up to you
spec:
  ports:
  - port: 3306
    targetport: 3306
    protocol: tcp
  selector:
    app: mysql  # here is the required change

"
48155041,kubernetes ingress rule,"i have a k8s 1.9.0 cluster and following is my ingress rule.

apiversion: extensions/v1beta1
kind: ingress
metadata:
 name: my-ingress
 labels:
  app: report
annotations:
  ingress.kubernetes.io/rewrite-target: /
spec:
 rules:
  - host: ""gayan.test.com""
    http:
     paths:
      - path: /report
        backend:
         servicename: qc-report-svc
         serviceport: 80
     - path: /report/*
        backend:
         servicename: qc-report-svc
         serviceport: 80


so i have two requests.

request one - https://gayan.test.com/report/ping this request hit the pod and return the response.
(get /ping 200 302.079 ms - 63)

request two - wss://gayan.test.com/report/socket.io/?eio=3&amp;transport=websocket.
this request doesn't even hit the server. i think this is related to ingress rule.

my question is how can i send all the /report traffic to qc-report-svc service?
",<docker><websocket><kubernetes><kubectl><docker-ingress>,48155862,1,"assuming you are using the nginx ingress controller you need to add the nginx.org/websocket-services annotation to enable websocket support.

apiversion: extensions/v1beta1
kind: ingress
metadata:
 name: my-ingress
 labels:
   app: report
annotations:
  ingress.kubernetes.io/rewrite-target: /
  nginx.org/websocket-services: ""qc-report-svc""
spec:
 rules:
  - host: ""gayan.test.com""
    http:
     paths:
      - path: /report
        backend:
         servicename: qc-report-svc
         serviceport: 80

"
48248522,unhealthy load balancer on gce,"i have a couple of services and the loadbalancers work fine. now i keep facing an issue with a service that runs fine, but when a loadbalancer is applied i cannot get it to work, because one service seams to be unhealty, but i cannot figure out why. how can i get that service healthy?



here are my k8s yaml.
deployment:

kind: deployment
apiversion: extensions/v1beta1
metadata:
  name: api-production
spec:
  replicas: 1
  template:
    metadata:
      name: api
      labels:
        app: api
        role: backend
        env: production
    spec:
      containers:
      - name: api
        image: eu.gcr.io/foobar/api:1.0.0
        livenessprobe:
          httpget:
            path: /readinez
            port: 8080
          initialdelayseconds: 45
          periodseconds: 10
        readinessprobe:
          httpget:
            path: /healthz
            port: 8080
        env:
        - name: environment
          value: ""production""
        - name: gin_mode
          value: ""release""
        resources:
          limits:
            memory: ""500mi""
            cpu: ""100m""
        imagepullpolicy: always
        ports:
        - name: api
          containerport: 8080


service.yaml

kind: service
apiversion: v1
metadata:
  name: api
spec:
  selector:
    app: api
    role: backend
  type: nodeport
  ports:
  - name: http
    port: 8080
  - name: external
    port: 80
    targetport: 80


ingress.yaml

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: api
  namespace: production
  annotations:
    kubernetes.io/tls-acme: ""true""
    kubernetes.io/ingress.class: ""gce""
spec:
  tls:
  - hosts:
    - foo.bar.io
    secretname: api-tls
  rules:
  - host: foo.bar.io
    http:
      paths:
      - path: /*
        backend:
          servicename: api
          serviceport: 80

",<kubernetes><google-cloud-platform><google-kubernetes-engine>,48347326,1,"the problem was solved by configuring the ports in the correct way. container, service and lb need (obviously) to be aligned. i also added the initialdelayseconds.

lb:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: api
  namespace: production
  annotations:
    # kubernetes.io/ingress.allow-http: ""false""
    kubernetes.io/tls-acme: ""true""
    kubernetes.io/ingress.class: ""gce""
spec:
  tls:
  - hosts:
    - api.foo.io
    secretname: api-tls
  rules:
  - host: api.foo.io
    http:
      paths:
      - path: /*
        backend:
          servicename: api
          serviceport: 8080 


service:

kind: service
apiversion: v1
metadata:
  name: api
spec:
  selector:
    app: api
    role: backend
  type: nodeport
  ports:
    - protocol: tcp
      port: 8080
      targetport: 8080
      name: http 


deployment:

kind: deployment
apiversion: extensions/v1beta1
metadata:
  name: api-production
spec:
  replicas: 1
  template:
    metadata:
      name: api
      labels:
        app: api
        role: backend
        env: production
    spec:
      containers:
      - name: api
        image: eu.gcr.io/foobarbar/api:1.0.0
        livenessprobe:
          httpget:
            path: /readinez
            port: 8080
          initialdelayseconds: 45
          periodseconds: 10
        readinessprobe:
          httpget:
            path: /healthz
            port: 8080
          initialdelayseconds: 45
        env:
         - name: environment
          value: ""production""
        - name: gin_mode
          value: ""release""
        resources:
          limits:
            memory: ""500mi""
            cpu: ""100m""
        imagepullpolicy: always
        ports:
        - containerport: 8080

"
70676003,specify [mysqld] in kubernetes deployment,"this is my configmap. i'm trying to specify [mysqld] config, but when i use this file alone with
helm upgrade -i eramba bitnami/mariadb --set auth.rootpassword=eramba,auth.database=erambadb,initdbscriptsconfigmap=eramba,volumepermissions.enabled=true,primary.persistence.existingclaim=eramba-storage --namespace eramba-1 --set mariadb.volumepermissions.enabled=true

i don't see the specified configurations in my db pod; however, i do see the c2.8.1.sql file applied tho.
apiversion: v1
kind: configmap
metadata:
  name: eramba
  namespace: eramba-1
data:
  my.cnf: |-
    [mysqld] 
    max_connections = 2000
    sql_mode=&quot;&quot;
    max_allowed_packet=&quot;128000000&quot;
    innodb_lock_wait_timeout=&quot;200&quot;
  c2.8.1.sql: |
    create database if not exists erambadb;
    #create user 'erambauser'@'eramba-mariadb' identified by 'erambapassword';
    #grant all on erambadb.* to 'erambauser'@'eramba-mariadb';
    #flush privileges;
    use erambadb;
    #
    # sql export
    # created by querious (201067)
    # created: 22 october 2019 at 17:39:48 cest
    # encoding: unicode (utf-8)
    #
    
    set @previous_foreign_key_checks = @@foreign_key_checks;
    set foreign_key_checks = 0;
    .....

",<kubernetes><kubernetes-helm>,70700497,1,"if you look at values.yaml file for mariadb helm chart, you can see 3 types of configmap:

initdbscriptsconfigmap - to supply init scripts to be run at first boot of db instance
primary.existingconfigmap - to control mariadb primary instance configuration
secondary.existingconfigmap - to control mariadb secondary instance configuration

thus, each of them is intended for the specific purpose and it is not a good idea to mix these settings in one configmap.
i recommend you to create new configmap eramba2 for custom my.cnf with all necessary values (not only new) as below.
    apiversion: v1
    kind: configmap
    metadata:
      name: eramba2
      namespace: eramba-1
    data:
      my.cnf: |-
        [mysqld]
        skip-name-resolve
        explicit_defaults_for_timestamp
        max_connections = 2000
        sql_mode=&quot;&quot;
        innodb_lock_wait_timeout=&quot;200&quot;
        basedir=/opt/bitnami/mariadb
        plugin_dir=/opt/bitnami/mariadb/plugin
        port=3306
        socket=/opt/bitnami/mariadb/tmp/mysql.sock
        tmpdir=/opt/bitnami/mariadb/tmp
        max_allowed_packet=128000000
        bind-address=::
        pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid
        log-error=/opt/bitnami/mariadb/logs/mysqld.log
        character-set-server=utf8
        collation-server=utf8_general_ci
    
        [client]
        port=3306
        socket=/opt/bitnami/mariadb/tmp/mysql.sock
        default-character-set=utf8
        plugin_dir=/opt/bitnami/mariadb/plugin
    
        [manager]
        port=3306
        socket=/opt/bitnami/mariadb/tmp/mysql.sock
        pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid

create eramba2 configmap:
kubectl create -f eramba2.yaml

and then create mariadb with helm using new configmap eramba2:
helm upgrade -i eramba bitnami/mariadb --set auth.rootpassword=eramba,auth.database=erambadb,initdbscriptsconfigmap=eramba,volumepermissions.enabled=true,primary.persistence.existingclaim=eramba-storage,mariadb.volumepermissions.enabled=true,primary.existingconfigmap=eramba2 --namespace eramba-1

connect to pod:
kubectl exec -it eramba-mariadb-0 -- /bin/bash

check my.cnf file:
cat /opt/bitnami/mariadb/conf/my.cnf

"
70656699,pathprefixstrip is ignored on ingress,"traefik version 2.5.6
i have the following ingress settings:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  kubernetes.io/ingress.class: traefik
  traefik.ingress.kubernetes.io/app-root: /users
  traefik.ingress.kubernetes.io/rule-type: pathprefixstrip

  name: users
spec:
  rules:
  - host: dev.[reducted]
    http:
      paths:
      - backend:
          service:
            name: users-service
            port:
              number: 80
        path: /users
        pathtype: prefix

but when i call:
curl -i http://dev.[reducted]/users/this-shoud-be-root

i get in the pod, serving the service:
error: get /users/this-shoud-be-root 404

what can be the reason for that?
",<kubernetes><kubernetes-ingress><traefik><traefik-ingress>,70864947,1,"try to use traefik routers as in the example below:
apiversion: traefik.containo.us/v1alpha1
kind: ingressroute
metadata:
  name: users
  namespace: default
spec:
  entrypoints:
    - web
  routes:
  - match: host(`dev.[reducted]`) &amp;&amp; pathprefix(`/users`)
    kind: rule
    services:
    - name: users-service
      port: 80

"
61500755,call to kubernetes service failed,"kubernetes version: 1.5.2
os: centos 7
etcd version: 3.4.0  

first, i create an etcd pod, the etcd dockerfile and etcd pod yaml file like this:
etcd dockerfile:

from alpine

copy . /usr/bin
workdir /usr/bin

cmd etcd --listen-client-urls http://0.0.0.0:2379 --advertise-client-urls http://0.0.0.0:2379

expose 2379


pod yaml file

apiversion: v1
kind: pod
metadata:
  name: etcd
  namespace: storageplatform
  labels:
    app: etcd
spec:
  containers:
    - name: etcd
      image: ""karldoenitz/etcd:3.4.0""
      ports:
        - containerport: 2379
          hostport: 12379


after created the docker image and push to dockerhub, i run the command kubectl apply -f etcd.yaml to create the etcd pod.
the ip of etcd pod is 10.254.140.117, i ran the command use etcdctl_api=3 etcdctl --endpoints=175.24.47.64:12379 put 1 1 and got ok.
my service yaml:  

apiversion: v1
kind: service
metadata:
  name: storageservice
  namespace: storageplatform
spec:
  type: nodeport
  ports:
    - port: 12379
      targetport: 12379
      nodeport: 32379
  selector:
    app: etcd


apply the yaml file to create the service.run the command kubectl get services -n storageplatform, i got these infomation.  

namespace         name                   cluster-ip       external-ip   port(s)           age
storageplatform   storageservice         10.254.140.117   &lt;nodes&gt;       12379:32379/tcp   51s


after all, i run the command 

etcdctl_api=3 etcdctl --endpoints=10.254.140.117:32379 get 1


or

etcdctl_api=3 etcdctl --endpoints={host-ip}:32379 get 1


i got error: context deadline exceeded.

what's the matter? how to make the service useful?
",<kubernetes><kubernetes-pod><kubernetes-service>,61503473,1,"change the service to refer to containerport instead of hostport

apiversion: v1
kind: service
metadata:
  name: storageservice
  namespace: storageplatform
spec:
  type: nodeport
  ports:
    - port: 2379
      targetport: 2379
      nodeport: 32379
  selector:
    app: etcd

"
46927643,error generating ca certificate and private key using cfssl and kubernetes,"i am using cfssl to generate csr.

i have below json format

{
""cn"": ""ambika"",
""key"": {
  ""algo"": ""ecdsa"",
  ""size"": 256
},
""names"": [
   {
       ""o"": ""system:masters""
   }
 ]
}

root@vagrant-xenial64:~/bin# cat csr.json | cfssl genkey - | cfssljson  -bare server
2017/10/25 08:28:07 [info] generate received request
2017/10/25 08:28:07 [info] received csr
2017/10/25 08:28:07 [info] generating key: ecdsa-256
2017/10/25 08:28:07 [info] encoded csr


in the next step 
generate a csr yaml blob and send it to the apiserver by running the following command:

root@vagrant-xenial64:~/bin# cat csr.yaml
apiversion: certificates.k8s.io/v1beta1
kind: certificatesigningrequest
metadata:
 name: ambika
spec:
  groups:
    - system:masters
    request: $(cat server.csr | base64 | tr -d ""\n"")
usages:
 - digital signature
- key encipherment
- client auth

root@vagrant-xenial64:~/bin# kubectl create -f csr.yaml
error from server (badrequest): error when creating ""stdin"": certificatesigningrequest in version ""v1beta1"" cannot be handled as a certificatesigningrequest: [pos 684]: json: error decoding base64 binary 'ls0tls1crudjtibdrvjusuzjq0fursbsrvfvrvnuls0tls0ktulibe1jr01bz0vbtunveez6qvzcz05wqkfvverutjvjm1jsylrwdflytjbawep6tve4d0rrwurwuvferxdaaapiv0pwytjfd1duqvrcz2nxagtqt1brsujcz2dxagtqt1brtujcd05dquftdxvibwf4bpadu1tkphzhh1cmn1clq1ou9yu0hewuq0d1j2s055njlmoekwl3rxtjf3wmrmckpydssxsdn5zgp2n0fmaujosuh6ahrmmhd6qun4b3akb0fbd0nwullb1pjemowruf3surtquf3ulfjaefotk9xaxfda1lsywgxqufoudq1bwnzb09qm2p4rjivdtb6zgphzw9bamheqkfpqu55mezksu9vrehlzdfgd3uvtwfbditmwgjxn3v6rudcqm9zuufsuwfycee9pqotls0tluvorcbdrvjusuzjq0fursbsrvfvrvnuls0tls0k': illegal base64 data at input byte 512


i am following this link
manage tls certificates in a cluster
",<ssl><ssl-certificate><kubernetes><client-certificates><kubernetes-security>,46933994,1,"since your running this in yaml file, you need to include based64 encoded value int he yaml file. $(cat server.csr | base64 | tr -d ""\n"")

in the example page they are running it in the shell directly.

cat server.csr | base64 | tr -d '\n' &gt; o encode like this and include the value  in the yaml file   it will work.

apiversion: certificates.k8s.io/v1beta1
kind: certificatesigningrequest
metadata:
  name: ambika
spec:
  groups:
  - system:masters
  request: ls0tls1crudjtibdrvjusuzjq0fursbsrvfvrvnuls0tls0ktulibu1jr01bz0vbtunveez6qvzcz05wqkfvverutjvjm1jsylrwdflytjbawep6tve4d0rrwurwuvferxdaaapiv0pwytjfd1duqvrcz2nxagtqt1brsujcz2dxagtqt1brtujcd05dquftvwvlymdpkzzhvhlhqlhddk40s29sclrqmmfmrluvvvzxv1z5whhjsnlmczhrae5pzdlrqnp0bzzhcdzesnywtddst1juznnxr2m2n0vvexuzdu5lz00kb0fbd0nnwullb1pjemowruf3surtuuf3umdjaefjekkxowxur3zueg1la1jym28vsdi1zwyyvklowurlmkjzrqpazujiahn1c0fprueyrxnwue0xv2huzumymvfel3zibxnnvfzqwwdvchrpu3dsq2lhswzqekhjpqotls0tluvorcbdrvjusuzjq0fursbsrvfvrvnuls0tls0k
  usages:
  - digital signature
  - key encipherment
  - server auth

"
61510006,istio reachable from browser but not from curl,"so i have successfully deployed istio, atleast i think so, everything seems to work fine. i have deployed my api in istio and i can reach it through my browser. i can even test my api using postman, but when i try to reach my api through curl it says the remote name could not be resolved: 'api.localhost'. that was the first red flag but i ignored it. now i'm trying to reach my api from my webapp but chrome responds with net:err_failed.

it seems like my services are only available for the host, which is me, and nothing else. i can't seem to find a solution for this on the internet so i hope someone has expirience and knows a fix.

thanks!



edit: more information

my infrastructure is all local, docker for desktop with kubernetes. the istio version i'm using is 1.5.0.

gateway:

apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: api-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
    - port:
        number: 80
        name: http-api
        protocol: http
      hosts:
        - ""api.localhost""


virtual service:

apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: pb-api
spec:
  gateways:
    - api-gateway
  hosts:
    - ""*""
  http:
    - match:
        - uri:
            prefix: /
      rewrite:
        uri: /
      route:
        - destination:
            host: pb-api
            port:
                number: 3001


when i try to do curl http://api.localhost/user/me i expect a 401, but instead i get the remote name could not be resolved: 'api.localhost' as stated above. that error is just the same as when i turn off docker for desktop and try again. through postman and the browser it works fine, but curl and my react webapp can't reach it.
",<kubernetes><kubernetes-ingress><istio>,61522566,1,"as i mentioned in the comments the curl should look like this


  curl -v -h ""host: api.localhost"" istio-ingressgateway-external-ip/


you can check istio-ingressgateway-external ip with

kubectl get svc istio-ingressgateway -n istio-system


as @sjaakvbrabant mentioned 


  external ip is localhost so i tried this command curl -v -h ""host: api.localhost"" localhost/user/me which gave me 401




ubuntu minikube example

additionally if you would like to curl api.localhost itself then you would have to configure your hosts locally, i'm not sure how this would work in your situation since your external ip is localhost.

but if you want, you can use metallb which is a loadbalancer, so your istio-ingressgateway would get an ip which could be configured in etc/hosts.

yamls

piversion: apps/v1
kind: deployment
metadata:
  name: nginx
  namespace: demo
spec:
  selector:
    matchlabels:
      app: demo
  replicas: 1
  template:
    metadata:
      labels:
        app: demo
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerport: 80
        lifecycle:
          poststart:
            exec:
              command: [""/bin/sh"", ""-c"", ""echo hello nginx1 &gt; /usr/share/nginx/html/index.html""]

---

apiversion: v1
kind: service
metadata:
  name: demo
  namespace: demo
  labels:
    app: demo
spec:
  ports:
  - name: http-demo
    port: 80
    protocol: tcp
  selector:
    app: demo


---

apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: demo-gw
  namespace: demo
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      name: http
      number: 80
      protocol: http
    hosts:
    - ""example.com""
---
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: demo-vs
  namespace: demo
spec:
  gateways:
  - demo-gw
  hosts:
  - ""example.com""
  http:
  - match:
    - uri:
        prefix: /
    rewrite:
      uri: /
    route:
    - destination:
        host: demo.demo.svc.cluster.local
        port:
          number: 80


etc/hosts

127.0.0.1       localhost
10.101.143.xxx  example.com


testing

curl -v -h ""host: example.com"" http://10.101.143.xxx/

&lt; http/1.1 200 ok


curl -v example.com

&lt; http/1.1 200 ok




hope you find this useful.
"
77490368,aks error when trying to install a private marketplace app,"i have created an app that is in a private store on azure.  i am trying to install the app from the marketplace when i get the following error:
error: [ innererror: [helm installation failed : unable to build the kubernetes resources in the extension based on the cluster : innererror [unable to build kubernetes objects from release manifest: error validating &quot;&quot;: error validating data: failed to check crd: failed to list crds: customresourcedefinitions.apiextensions.k8s.io is forbidden: user &quot;system:serviceaccount:kube-system:ext-installer-ssdeploy1&quot; cannot list resource &quot;customresourcedefinitions&quot; in api group &quot;apiextensions.k8s.io&quot; at the cluster scope]]] occurred while doing the operation : [patch] on the config, for general troubleshooting visit: https://aka.ms/k8s-extensions-tsg

here are the role and role binding yamls i tried but that did not work.  i am new to k8s so there are things i do not understand about it.
role.yaml
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  namespace: default
  name: default
rules:
- apigroups: [&quot;apiextensions.k8s.io&quot;] 
  resources: [&quot;customresourcedefinitions&quot;]
  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]

role-binding.yaml
apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: ext-installer-ssdeploy1
  namespace: default
subjects:
- kind: serviceaccount
  name: default
  namespace: default
roleref:
  kind: role 
  name: ext-installer-ssdeploy1
  apigroup: rbac.authorization.k8s.io

any help is greatly appreciated.
",<kubernetes><kubernetes-helm><azure-aks>,77543822,1,"your error indicates that the service account ext-installer-ssdeploy1 does not have necessary permissions to list custom resource definitions in the apiextensions.k8s.io api group. to fix this issue, you need to grant the necessary rbac permissions to the service account. also modify the name attribute in your role.yaml
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  namespace: default
  name: ext-installer-ssdeploy1
rules:
- apigroups: [&quot;apiextensions.k8s.io&quot;] 
  resources: [&quot;customresourcedefinitions&quot;]
  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;]


kubectl apply -f role.yaml

after granting the necessary permissions, attempt to install the helm chart again, it should work.
reference doc-
official k8s role example
"
49920869,spring boot on azure internal server error,"i have a very simple ""hello"" spring-boot application 

@restcontroller
public class helloworld {
    @requestmapping(""/"")
    public string sayhello() {
        return ""hello spring boot!!"";
    }
}


i packaged dockerfile

from java:8
copy ./springsimple-1.0-snapshot.jar /users/a/documents/dev/intellij/dockerimages/
workdir /users/a/documents/dev/intellij/dockerimages/  
expose 8090
cmd [""java"", ""-jar"", ""springsimple-1.0-snapshot.jar""]


and pulled into my container registry and deployed it

amhg$ kubectl run testproject --image acontainerregistry.azurecr.io/hellospring:v1 
    deployment.apps ""testproject"" created
amhg$ kubectl expose deployments testproject --port=5000 --type=loadbalancer
    service ""testproject"" exposed


command kubectl get pods

name                               ready     status             restarts   age
    testproject-bdf5b54d-gkk92         1/1       running            0          41s


however when i try the command  (starting to serve on 127.0.0.1:8001) i got the error:

 amhg$ curl http://127.0.0.1:8001/api/v1/proxy/namespaces/default/pods/testproject-bdf5b54d-gkk92/
    internal server error


what is missing?

the description of the pod is

amhg$ kubectl describe pod testproject-bdf5b54d-gkk92
name:           testproject-bdf5b54d-gkk92
namespace:      default
node:           aks-nodepool1-39744669-0/10.240.0.4
start time:     thu, 19 apr 2018 13:13:20 +0200
labels:         pod-template-hash=68916108
                run=testproject
annotations:    kubernetes.io/created-by={""kind"":""serializedreference"",""apiversion"":""v1"",""reference"":{""kind"":""replicaset"",""namespace"":""default"",""name"":""testproject-bdf5b54d"",""uid"":""aa99808e-43c2-11e8-9537-0a58ac1f0f4...
status:         running
ip:             10.244.0.40
controlled by:  replicaset/testproject-bdf5b54d
containers:
  testproject:
    container id:   docker://6ed3878fa4476a5d2e56f0ba70908742702709c7505c7b19989efc6ff658ea55
    image:          acontainerregistry.azurecr.io/hellospring:v1
    image id:       docker-pullable://acontainerregistry.azurecr.io/azure-vote-front@sha256:e2af252d275c99b802e21b3b469c75b256d7812ee71d7582cd759bd4faf5a6ec
    port:           &lt;none&gt;
    host port:      &lt;none&gt;
    state:          running
      started:      thu, 19 apr 2018 13:13:21 +0200
    ready:          true
    restart count:  0
    environment:    &lt;none&gt;
    mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vkpjm (ro)
conditions:
  type           status
  initialized    true 
  ready          true 
  podscheduled   true 
volumes:
  default-token-vkpjm:
    type:        secret (a volume populated by a secret)
    secretname:  default-token-vkpjm
    optional:    false
qos class:       besteffort
node-selectors:  &lt;none&gt;
tolerations:     node.alpha.kubernetes.io/notready:noexecute for 300s
                 node.alpha.kubernetes.io/unreachable:noexecute for 300s
events:
  type    reason                 age   from                               message
  ----    ------                 ----  ----                               -------
  normal  scheduled              57m   default-scheduler                  successfully assigned testproject-bdf5b54d-gkk92 to aks-nodepool1-39744669-0
  normal  successfulmountvolume  57m   kubelet, aks-nodepool1-39744669-0  mountvolume.setup succeeded for volume ""default-token-vkpjm""
  normal  pulled                 57m   kubelet, aks-nodepool1-39744669-0  container image ""acontainerregistry.azurecr.io/hellospring:v1"" already present on machine
  normal  created                57m   kubelet, aks-nodepool1-39744669-0  created container
  normal  started                57m   kubelet, aks-nodepool1-39744669-0  started container

",<docker><spring-boot><kubernetes><kubectl>,49943201,1,"let's start from the beginning: it is always better to use yaml config files to do anything with kubernetes. it will help you with debugging if something goes wrong and repeat your action in future.

first, you use the command to create the pod:


  kubectl run testproject --image acontainerregistry.azurecr.io/hellospring:v1 


where yaml looks like:

apiversion: v1
kind: pod
metadata:
  name: test-app
spec:
  containers:
  - name: java-app
    image: acontainerregistry.azurecr.io/hellospring:v1
    ports:
    - containerport: 8090


and you can apply it as a command:

kubectl apply -f ./pod.yaml


you get the same result as while running your command, but additionally you have the config file which can be used in future.

you`re trying to expose your pod using command:


  kubectl expose deployments testproject --port=5000 --type=loadbalancer


yaml for your service looks like:

apiversion: v1
kind: service
metadata:
  name: java-service
  labels:
    name: test-app
spec:
  type: loadbalancer
  ports:
  - port: 5000
    targetport: 8090
    name: http
  selector:
    name: test-app


doing the same but with using yaml allows to describe more and be sure you don't miss anything.

you tried to curl the localhost but i`m not sure what did you expect from this command:


  amhg$ curl http://127.0.0.1:8001/api/v1/proxy/namespaces/default/pods/testproject-bdf5b54d-gkk92/
      internal server error


after you create the service, you call kubectl describe service $service_name, which you can find here: 

loadbalancer ingress:     xx.xx.xx.xx
port:                     http  5000/tcp


you can curl this address and receive the answer from your application.

curl -v xx.xx.xx.xx:5000


don't forget to open the port on azure firewall. 
"
60959714,gke autoscaling with a custom metric from deployment,"i am trying to auto-scale my redis workers based on queue size, i am collecting the metrics using redis_exporter and promethues-to-sd sidecars in my redis deployment as so:

spec:
  containers:
    - name: master
      image: redis
      env:
        - name: master
          value: ""true""
      ports:
        - containerport: 6379
      resources:
        limits:
          cpu: ""100m""
        requests:
          cpu: ""100m""
    - name: redis-exporter
      image: oliver006/redis_exporter:v0.21.1
      env:
      ports:
        - containerport: 9121
      args: [""--check-keys=rq*""]
      resources:
        requests:
          cpu: 100m
          memory: 100mi
    - name: prometheus-to-sd
      image: gcr.io/google-containers/prometheus-to-sd:v0.9.2
      command:
        - /monitor
        - --source=:http://localhost:9121
        - --stackdriver-prefix=custom.googleapis.com
        - --pod-id=$(pod_id)
        - --namespace-id=$(pod_namespace)
        - --scrape-interval=15s
        - --export-interval=15s
      env:
        - name: pod_id
          valuefrom:
            fieldref:
              apiversion: v1
              fieldpath: metadata.uid
        - name: pod_namespace
          valuefrom:
            fieldref:
              fieldpath: metadata.namespace
      resources:
        requests:
          cpu: 100m
          memory: 100mi


i can then view the metric (redis_key_size) in metrics explorer as:

metric.type=""custom.googleapis.com/redis_key_size"" 
resource.type=""gke_container""


(i can't view the metric if i change resource.type=k8_pod)

however i can't seem to get the hpa to read in these metrics getting a failed to get metrics error, and can't seem to figure out the correct object definition. 

i've tried both .object.target.kind=pod and deployment, with deployment i get the additional error ""get namespaced metric by name for resource \""deployments\"""" is not implemented.

i don't know if this issue is related to the resource.type=""gke_container"" and how to change that?

apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  name: {{ template ""webapp.backend.fullname"" . }}-workers
  namespace: default
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: {{ template ""webapp.backend.fullname"" . }}-workers
  minreplicas: 1
  maxreplicas: 4
  metrics:
    - type: object
      object:
        target:
          kind: &lt;not sure&gt;
          name: &lt;not sure&gt;
        metricname: redis_key_size
        targetvalue: 4


--- update ---

this works if i use kind: pod and manually set name to the pod name created by the deployment, however this is far from perfect.

i also tried this setup using type pods, however the hpa says it can't read the metrics horizontal-pod-autoscaler  failed to get object metric value: unable to get metric redis_key_size: no metrics returned from custom metrics api

apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  name: {{ template ""webapp.backend.fullname"" . }}-workers
  namespace: default
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: {{ template ""webapp.backend.fullname"" . }}-workers
  minreplicas: 1
  maxreplicas: 4
  metrics:
  - type: pods
    pods:
      metricname: redis_key_size
      targetaveragevalue: 4

",<kubernetes><google-kubernetes-engine><prometheus><autoscaling><stackdriver>,60960888,1,"as a workaround for deployments it appears that the metrics have to be exported from pods in the target deployment. 

to get this working i had to move the prometheus-to-sd container to the deployment i wanted to scale and then scrape the exposed metrics from redis-exporter in the redis deployment via the redis service, exposing 9121 on the redis service, and changing the cla for the the prometheus-to-sd container such that:

- --source=:http://localhost:9121 -> - --source=:http://my-redis-service:9121

and then using the hpa

apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  name: {{ template ""webapp.backend.fullname"" . }}-workers
  namespace: default
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: {{ template ""webapp.backend.fullname"" . }}-workers
  minreplicas: 1
  maxreplicas: 4
  metrics:
  - type: pods
    pods:
      metricname: redis_key_size
      targetaveragevalue: 4

"
77027475,how to find out the api group a resource belongs to?,"in my k3s cluster, i am editing a role to add permission to a resource e.g. deployment.
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  creationtimestamp: &quot;2023-09-02t08:00:23z&quot;
  name: foo
  namespace: default
  resourceversion: &quot;432595&quot;
  uid: 603d0e0b-62f8-4e46-a42c-66436203bdc9
rules:
- apigroups:
  - apps. # i know api group is 'apps' but i wonder how to figure it out in general
  resources:
  - deployments
  verbs:
  - get
  - list
  - watch

in the apigroups field above, i know the api group for deployment resource is app based on my googling.
but i wonder in general how can i figure out which api group certain resource belongs to using kubectl command?
for example, i know also there is kubectl api-resources command, but if i run it for deployment resource as an example:
k api-resources | grep 'deployment'
name            shortnames   apiversion   namespaced   kind
deployments     deploy       apps/v1      true         deployment

the output tells me the apiversion is apps/v1.  but what exactly is the api group when i see value of apps/v1? should i consider the value of apiversion is the api group of this resource or is it always the left side of / be the api group?
could someone please clarify for me? thank you.
",<kubernetes><kubectl>,77030980,1,"yes, you can consider that the left side of the / is always the apigroup. app/v1 indicates the rest api path, in general the api path looks like /apis/$group\_name/$version where app is the apigroup and v1 is the apiversion.
there is one exception though, if you are using components from core group or legacy group, the definition looks like apiversion: v1 instead of apiversion: core/v1.
when we are trying to use a particular object, let's say role in your example, we can go through the official documentation of that particular component or you can also go through the api reference documentation for finding the apigroup of that object.
"
76979310,query kubectl resources in config file,"i have a yaml file that looks like this
---
apiversion: batch/v1
kind: cronjob
metadata:
  name: hello-cron-job
  namespace: hello-world
spec:
  schedule: &quot;0 * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            imagepullpolicy: ifnotpresent
            env:
            - name: services
              value: $(kubectl get service -a)
            command: [&quot;echo&quot;, &quot;$services&quot;]
            volumemounts:
            - name: scripts
              mountpath: /tmp/python
          restartpolicy: onfailure
          volumes:
          - name: scripts
            configmap:
              name: test-scripts`

i'd like to get all currently running services and set services env variable to those running services. is it possible to achieve this?
i've tried using
- name: services
  value: $(kubectl get service -a)

but it just treats value as string.
",<kubernetes><kubectl>,76979786,1,"there are 2 cases:

you want to list services dynamically (at every cron execution)
you want to pass the list of services statically (list of services when applying the manifest)

listing services dynamically
if you want to list the services at every cron execution, you'll need to run the kubectl command from inside the pod. you'll also need to create a dedicated serviceaccount for your cronjob. this serviceaccount must have the right permissions to list all services in the cluster. for that, you can define a clusterrole (to list all services in the cluster) and link it to your serviceaccount using a clusterrolebinding. this is similar to what david maze explained in his comment.
apiversion: v1
kind: serviceaccount
metadata:
  name: hello-cron-job
  namespace: hello-world
---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  name: list-all-services
rules:
- apigroups:
  - &quot;&quot;
  resources:
  - services
  verbs:
  - list
---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: hello-cron-job
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: list-all-services
subjects:
- kind: serviceaccount
  name: hello-cron-job
  namespace: hello-world

from there, you can define your cronjob using that service account:
apiversion: batch/v1
kind: cronjob
metadata:
  name: hello-cron-job
  namespace: hello-world
spec:
  schedule: &quot;0 * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: alpine/k8s:1.28.0
            command:
            - sh
            - -c
            - |
              services=$(kubectl get services -a -o name)
              echo $services
          restartpolicy: onfailure
          serviceaccountname: hello-cron-job

notice how spec.jobtemplate.spec.template.spec.serviceaccountname is set to the previously created serviceaccount.
i'm using alpine/k8s image here, as it has already kubectl installed.
listing services statically
in case you only want to pass the list of services when applying the manifest, you can define your cronjob in a file (let's say cronjob.yaml):
apiversion: batch/v1
kind: cronjob
metadata:
  name: hello-cron-job
  namespace: hello-world
spec:
  schedule: &quot;0 * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            env:
            - name: services
              value: &quot;$services&quot;
            command: [&quot;echo&quot;, &quot;$(services)&quot;]
          restartpolicy: onfailure

then, substitute $services value (with envsubst) before kubectl applying:
services=$(kubectl get svc -a -o name) \
    envsubst '$services' &lt; cronjob.yaml | \
    kubectl apply -f -

this will create the cronjob object with the right services env var (names of the services when applying the manifest).
i've used -o name to get only the names of the services, but you can do whatever you want here.
also note that the command must be [&quot;echo&quot;, &quot;$(services)&quot;] and not [&quot;echo&quot;, &quot;$services&quot;] (see &quot;use environment variables to define arguments&quot; in kubernetes docs).
"
76968325,making daemonset node initialisation pod run only once per node,"i want to run an initialisation script on each node, and i want to run them only once.
here i have the yaml to do some basic initialisation on each node, but once the initialisation script finish execution, the pod exits with exit code: 0 and the daemonset restarts the pod, running the initialisation script again and again.
---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  name: test-init-node-cr
rules:
- apigroups:
  - &quot;&quot;
  resources:
  - nodes
  verbs:
  - get
  - patch
---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: test-init-node-sa
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: test-init-node-cr
subjects:
- kind: serviceaccount
  name: test-init-node-sa
  namespace: default
---
apiversion: v1
kind: serviceaccount
metadata:
  name: test-init-node-sa
  namespace: default
---
apiversion: apps/v1
kind: daemonset
metadata:
  name: test-init-node
  namespace: default
spec:
  selector:
    matchlabels:
      app.kubernetes.io/name: test-init-node
      app.kubernetes.io/component: configurator
  # replicas: 3
  template:
    metadata:
      name: test-init-node
      labels:
        app.kubernetes.io/name: test-init-node
        app.kubernetes.io/component: configurator
    spec:
      affinity:
        nodeaffinity:
          requiredduringschedulingignoredduringexecution:
            nodeselectorterms:
            - matchexpressions:
              - key: k8s.amazee.io/node-configured
                operator: doesnotexist
      hostpid: true
      hostnetwork: true
      tolerations:
      - effect: noschedule
        key: node-role.kubernetes.io/master
      serviceaccount: test-init-node-sa
      containers:
      - name: init
        env:
        - name: my_node_name
          valuefrom:
            fieldref:
              fieldpath: spec.nodename
        command: 
        - nsenter
        - --mount=/proc/1/ns/mnt
        - --
        - bash
        - -xc
        - |
          echo &quot;starting the magic&quot;
          echo &quot;*   hard  core    unlimited&quot; &gt;&gt;  /etc/security/limits.d/game.conf 
          echo &quot;*   soft  core    unlimited&quot; &gt;&gt;  /etc/security/limits.d/game.conf 

        image: alpine/k8s:1.28.0
        resources:
          requests:
            cpu: 50m
            memory: 50m
        securitycontext:
          runasuser: 0
          privileged: true

is there any way for me to prevent the daemonset from restarting the pod if the pod exits?
i.e. ensuring that the initialisation only happen once per node.
i tried adding a prestop but it does not seem to have any effect.
the idea is that if k8s.amazee.io/node-configured is set then the daemonset will not schedule onto that node.
          prestop:
            exec:
              command:
              - /bin/sh&quot;
              - -c
              - kubectl label node &quot;$my_node_name&quot; k8s.amazee.io/node-configured=$(date +%s)

neither does adding a semicolon (well this is expected but i thought why not give it a try)
        command: 
        - nsenter
        - --mount=/proc/1/ns/mnt
        - --
        - bash
        - -xc
        - |
          echo &quot;starting the magic&quot;
          echo &quot;*   hard  core    unlimited&quot; &gt;&gt;  /etc/security/limits.d/game.conf 
          echo &quot;*   soft  core    unlimited&quot; &gt;&gt;  /etc/security/limits.d/game.conf 
        - ; 
        - /bin/sh&quot;
        - -c
        - kubectl label node &quot;$my_node_name&quot; k8s.amazee.io/node-configured=$(date +%s)

is there any way for me to prevent the daemonset from restarting the pod if the pod exits properly? i.e. ensuring that the initialisation only happen once per node.
",<kubernetes><google-kubernetes-engine>,76997239,1,"running the daemonset pod works, but it will still take up some resource and does not feel elegant.
from @norbjd's answer, i saw this in the gcp tutorial.
initcontainers:
    - image: ubuntu:18.04
      name: node-initializer
      command: [&quot;/scripts/entrypoint.sh&quot;]
      env:
        - name: root_mount_dir
          value: /root
      securitycontext:
        privileged: true
  containers:
    - image: &quot;gcr.io/google-containers/pause:2.0&quot;
      name: pause

the tutorial is talking about using the pause contain from google-containers to avoid a restart of the pod. however, what caught my eye was the initcontainers.
source:
init containers are exactly like regular containers, except:

init containers always run to completion.
each init container must complete successfully before the next one starts.

this gave me an idea. what if i ran 2 containers, the first being an initcontainers that does all the initialisations, and the second containers will run a command to add a label to prevent scheduling, hence stopping any further pod creation/restart of the daemonset for that particular node.
of course, by the same logic, both can be initcontainers, but in my case i used 1 initcontainers and 1 containers, since containers will wait for all initcontainers to be completed so they have the same result as both initcontainers.
working example
---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  name: test-init-node-cr
rules:
- apigroups:
  - &quot;&quot;
  resources:
  - nodes
  verbs:
  - get
  - patch
---
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: test-init-node-sa
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: test-init-node-cr
subjects:
- kind: serviceaccount
  name: test-init-node-sa
  namespace: default
---
apiversion: v1
kind: serviceaccount
metadata:
  name: test-init-node-sa
  namespace: default
---
apiversion: apps/v1
kind: daemonset
metadata:
  name: test-init-node
  namespace: default
spec:
  selector:
    matchlabels:
      app.kubernetes.io/name: test-init-node
      app.kubernetes.io/component: configurator
  # replicas: 3
  template:
    metadata:
      name: test-init-node
      labels:
        app.kubernetes.io/name: test-init-node
        app.kubernetes.io/component: configurator
    spec:
      affinity:
        nodeaffinity:
          requiredduringschedulingignoredduringexecution:
            nodeselectorterms:
            - matchexpressions:
              - key: test-init-node-date
                operator: doesnotexist
      hostpid: true
      hostnetwork: true
      tolerations:
      - effect: noschedule
        key: node-role.kubernetes.io/master
      serviceaccount: test-init-node-sa
      initcontainers:
      - name: init
        command:         
        - nsenter
        - --mount=/proc/1/ns/mnt
        - --
        - bash
        - -xc
        - |
          echo &quot;starting the magic&quot;
          echo &quot;*   hard  core    unlimited&quot; &gt;&gt;  /etc/security/limits.d/game.conf 
          echo &quot;*   soft  core    unlimited&quot; &gt;&gt;  /etc/security/limits.d/game.conf 
          echo &quot;user00   soft  core    unlimited&quot; &gt;&gt;  /etc/security/limits.d/game.conf 
        image: alpine/k8s:1.28.0
        resources:
          requests:
            cpu: 50m
            memory: 50m
        securitycontext:
          runasuser: 0
          privileged: true

      containers: 
      - name: add-label-to-remove-scheduling
        env:
        - name: my_node_name
          valuefrom:
            fieldref:
              fieldpath: spec.nodename
        command:
        - sh
        - -c
        - |
          kubectl label node &quot;$my_node_name&quot; test-init-node-date=$(date +%s) 
        image: alpine/k8s:1.28.0
        resources:
          requests:
            cpu: 50m
            memory: 50m
        securitycontext:
          runasuser: 0
          privileged: true

rough explanation:

create the relevant service account and permissions
check if the test-init-node-date label is set. if it is, skip and
do nothing
create the initcontainers and run the init
script as needed
create the containers and add test-init-node-date label

sample labels:
kubernetes.io/os=linux
node.kubernetes.io/instance-type=n2d-standard-8
test-init-node-date=1693280374 

this will then create a daemonset that runs the init pod once, start another pod to add test-init-node-date label. since the test-init-node-date label is set, no new pods will be scheduled by the daemonset.
and finally, to quote norbjd, to prevent accidental re-run of the init script(e.g. someone deleted the label), you can add a safeguard check before you run the script.
if [ ! -f /etc/game-conf-limits-updated ]
then
    echo &quot;starting the magic&quot;
    echo &quot;*   hard  core    unlimited&quot; &gt;&gt; /etc/security/limits.d/game.conf
    echo &quot;*   soft  core    unlimited&quot; &gt;&gt; /etc/security/limits.d/game.conf

    touch /etc/game-conf-limits-updated
fi

"
76895591,eks loadbalancer created successfully with external ip but web app is unreachable on assigned port,"i am learning k8s with eksctl and used this to create a loadbalancer:
apiversion: v1
kind: service
metadata:
  name: lb
spec:
  type: loadbalancer
  selector:
    app: lb
  ports:
    - protocol: tcp
      port: 3000
      targetport: 3000

it was created ok and kubectl get service/lb lists it as well a long aws domain name representing the external ip (let's call this &lt;awsdomain&gt;).
i then deployed my app:
apiversion: apps/v1
kind: deployment
metadata:
  name: myapp-deployment
  namespace: default
  labels:
    app: myapp
spec:
  replicas: 2
  selector:
    matchlabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp
          image: &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/myapp:latest
          ports:
            - containerport: 3000

i did kubectl apply -f deployment.yml and that also seems to have worked. however, when i go to my browser, http://&lt;awsdomain&gt;:3000 doesn't return anything :(
is there another resource i'm supposed to create? thanks.
",<kubernetes><amazon-eks><eksctl>,76896717,1,"your service selector will not select any pod. try:
apiversion: v1
kind: service
metadata:
  name: lb
spec:
  type: loadbalancer
  selector:
    app: myapp    # &lt;-- change to match the pod template
...

"
60095686,understanding istio authn and authz for kubernetes pods,"i am bit confused about using istio with eks. we have 2 spring boot microservices, one is a rest service provider and the other the consumer. we want to implement authn and authz using istio.

for that:
1. on provider service side : i have the a virtualservice, a destination rule (stating that the tls mode should be istio_mutual for incoming traffic) , an authorizationpolicy which basically whitelists the client serviceaccounts. i also have a authenticationpolicy as below:

apiversion: ""authentication.istio.io/v1alpha1""
kind: ""policy""
metadata:
  name: $app_name-$feature_name-authenticationpolicy
  namespace: $name_space
spec:
  targets:
  - name: ""$app_name-$feature_name""
  peers:
  - mtls:
      mode: strict


my understanding here is that this policy wont allow any incoming traffic which is non mtls.

now i have a doubt that how do i configure my client pod to send all mtls outgoing traffic.i understand i have to create a serviceaccount which is whitelisted at the provider side using authz policy.i am more concerned about my client pod here since i am not sure how to enable mtls at the pod level. fyi, i dont want to enable mtls at the namespace level.i want to do it at the pod level using a yaml file.

is my understanding about the usage of the destination rule, authn and authz policies correct? is it correct that destination rule, authn and authz policies have to be at the service provider level? and the client just has to enable mtls for the communication to work successfully? i have been go thru istio documentation but this is where i have a doubt
",<kubernetes><istio><amazon-eks>,60110088,1,"
  my understanding here is that this policy wont allow any incoming traffic which is non mtls.


that's true, if you set the tls mode to strict then client cert must be presented, connection is in tls.




  i am more concerned about my client pod here since i am not sure how to enable mtls at the pod level.


there is good article about how to make that work, specially the part 

setting up mtls for a single connection between two services


  as bookinfo is the hello world of istio, i am going to use this to explain how to set up mtls from productpage to details service as shown in the above graph snippet.
  
  there are two parts to this:
  
  install a policy to tell details that it wants to receive tls traffic (only):


apiversion: authentication.istio.io/v1alpha1
kind: policy
metadata:
  name: details-receive-tls
spec:
  targets:
  - name: details
  peers:
  - mtls: {}



  
  install a destinationrule to tell clients (productpage) to talk tls with details:
  


apiversion: networking.istio.io/v1alpha3
kind: destinationrule
metadata:
  name: details-istio-mtls
spec:
  host: details.bookinfo.svc.cluster.local
  trafficpolicy:
    tls:
      mode: istio_mutual



  the following is a graphical representation of the involved services and where the previous two configuration documents apply.





  now when you look closely at the policy above you will see and entry for the peer authentication


peers:
- mtls: {}



  this means that tls verification is strict and istio (or rather the envoy proxy in the pod) requires tls traffic and a valid certificate. we can pass a flag to get permissive mode:


peers:
- mtls: 
    mode: permissive





  is it correct that destination rule, authn and authz policies have to be at the service provider level?


as far as i know yes.


  and the client just has to enable mtls for the communication to work successfully?


i'm not sure about that, since mtls works inside mesh, it depends on your application requirements.




  i want to do it at the pod level using a yaml file.


there is a link to istio documentation about authentication which include


create an https service with istio sidecar with mutual tls enabled 
mutual tls deep-dive


and another one from github


enable mtls for frontend service


or you can extend your gateways definition to support mutual tls. change the credentials of the ingress gateway by deleting its secret and creating a new one. the server uses the ca certificate to verify its clients, and we must use the name cacert to hold the ca certificate. you can use cert-manager to generate a client certificate.


https://istio.io/docs/tasks/traffic-management/ingress/secure-ingress-sds/#configure-a-mutual-tls-ingress-gateway
https://istio.io/docs/tasks/traffic-management/ingress/ingress-certmgr/




i have found some tutorials which might be helpful, check it out.


managing mutual tls between services with istio
authorization
jwt
istio mtls,jwt,authn




let me know if you have any more questions.
"
77664196,helm template entire yaml section as json configmap,"i'm trying to pass an entire yaml stanza as a json config mapped file into my service. a simple example is
values.yaml:
servicename: &quot;mysupercoolservice&quot;

configmaps:
  - filename: &quot;file1.json&quot;
    content: &quot;{{ .values.content1 }}&quot;
  - filename: &quot;file2.json&quot;
    content: &quot;{{ .values.content2 }}&quot;

content1:
  field1: &quot;value1&quot;
  field2:
    field3: &quot;value3&quot;
    field4: &quot;value4&quot;

content2:
  field5: &quot;value5&quot;
  field6:
    field7: &quot;value7&quot;
    field8: &quot;value8&quot;

templates/configmap.yaml:
apiversion: v1
kind: configmap
metadata:
  name: {{ .values.servicename }}-jsonconfigmap
data:
  {{- range .values.configmaps }}
  {{ .filename }}: | 
  {{ tpl .content $ | toprettyjson | indent 4}}
  {{- end }}

my desired output for the configmap.yaml template would be:
apiversion: v1
kind: configmap
metadata:
  name: mysupercoolservice-jsonconfigmap
data:
  file1.json: | 
    {
      &quot;field1&quot;: &quot;value1&quot;,
      &quot;field2&quot;:
      {
        &quot;field3&quot;: &quot;value3&quot;,
        &quot;field4&quot;: &quot;value4&quot;
      }
    }
  file2.json: | 
    {
      &quot;field5&quot;: &quot;value5&quot;,
      &quot;field6&quot;:
      {
        &quot;field7&quot;: &quot;value7&quot;,
        &quot;field8&quot;: &quot;value8&quot;
      }
    }

however no matter what various things i try it always seems to come out:
apiversion: v1
kind: configmap
metadata:
  name: mysupercoolservice-jsonconfigmap
data:
  file1.json: | 
      &quot;map[field1:value1 field2:map[field3:value3 field4:value4]]&quot;
  file2.json: | 
      &quot;map[field5:value5 field6:map[field7:value7 field8:value8]]&quot;


how to i convert the map type object into actual json?
",<kubernetes><kubernetes-helm>,77666106,1,"a couple of aspects of the go text/template language are very oriented around strings.  a {{ ... }} double-brace expression always evaluates to a string; in helm more specifically, include and tpl always return strings.  if maps or lists are returned here, they get converted back to a string using a default go serialization, which is the map[key:value] syntax you see.
the most direct answer to this is to make sure, when you include part of the values inside a template expression, that you serialize it to json there:
configmaps:
  - filename: &quot;file1.json&quot;
    content: &quot;{{ .values.content1 | toprettyjson }}&quot;
  - filename: &quot;file2.json&quot;
    content: &quot;{{ .values.content2 | toprettyjson }}&quot;

{{ tpl .content $ | indent 4}}

i wonder if you're trying to make the configuration too flexible, though.  what you have shown as configuration closely mirrors the structure of a kubernetes configmap, to the point where just writing out a configmap yaml wouldn't actually be more difficult.  putting keys in a configmap on its own isn't useful unless a pod knows to access them.  it might make more sense to enumerate the specific known configmap keys in your template file, which in this particular case would remove tpl entirely
apiversion: v1
kind: configmap
metadata:
  name: {{ .release.name }}-jsonconfigmap
data:
  file1.json: |
{{ .values.content1 | toprettyjson | indent 4 }}
  file2.json: |
{{ .values.content2 | toprettyjson | indent 4 }}

or even to put the core of the json structure in your template code, and take more specific configuration values where needed:
data:
  file1.json: |
    {
      &quot;field1&quot;: {
        &quot;field2&quot;: {{ .values.frobnicationlevel | tojson }}
      }
    }

"
71826829,is it mandatory to upgrade crds deprecated apiversions?,"i have a few external crds with old apiversion applied in the cluster, and operators based on those crds deployed.
as said in official docs about kubernetes api and feature removals in 1.22.

you can use the v1 api to retrieve or update existing objects, even if they were created using an older api version. if you defined any custom resources in your cluster, those are still served after you upgrade.

based on the quote, does it mean i could leave those apiextensions.k8s.io/v1beta1 crds in the cluster? will controllers/operators continue to work normally?
",<kubernetes><kubernetes-custom-resources>,71929285,1,"the custom resources will still be served after you upgrade
suppose we define a resource called mykind
apiversion: apiextensions.k8s.io/v1beta1
kind: customresourcedefinition
metadata:
  name: mykinds.grp.example.com
spec:
  group: grp.example.com
  versions:
    - name: v1beta1
      served: true
      storage: true

then, on any cluster where this has been applied i can always define a mykind resource:
apiversion: grp.example.com/v1beta1
kind: mykind
metadata:
  name: mykind-instance

and this resource will still be served normally after upgrade even if the crd for mykind was created under v1beta1.
however, anything in the controller / operator code referencing v1beta1 crd won't work. this could be applying the crd itself (if your controller has permissions to do that) for example. that's something to watch out for if your operator is managed by the operator lifecycle manager. but watching for changes in the crs would be unaffected by the upgrade.
so if your controller / operator isn't watching customresourcedefinitions then technically you can leave these crds on the cluster and your operator will work as normal. but you won't be able to uninstall + reinstall should you need to.
another thing to explore is if / how that might affect your ability to bump api versions later though.
"
71804383,ingress.yaml for multiple ingresses in values.yaml,"i have the following values.yaml
ingresses:
  - name: public
    class: &quot;nginx&quot;
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: 122m
      nginx.ingress.kubernetes.io/proxy-connect-timeout: &quot;7&quot;
      nginx.ingress.kubernetes.io/proxy-read-timeout: &quot;60&quot;
      nginx.ingress.kubernetes.io/proxy-send-timeout: &quot;30&quot;
    labels: {}

    rules:
      - host: example.com
        http:
        paths:
        - path: /asd/as
          pathtype: implementationspecific
          backend:
            service:
              name: one
              port: 
                number: 8080
        - backend:
            service:
              name: log
              port:
                number: 8081
          path: /path/log
          pathtype: implementationspecific
        - backend:
            service:
              name: got
              port:
                number: 8082
          path: /api/got
          pathtype: implementationspecific
    tls:
    - hosts:
      - example.com
      secretname: cert
  
 - name: public
   annotations:
   labels: {}
   rules:
    - host: example1.com
      http:
      paths:
        - backend:
            service:
              name: web
              port:
                number: 8090
          pathtype: implementationspecific
      
    tls:
    - hosts:
      - example1.com
      secretname: qwe

and i have the following ingress file:
{{- $top := . -}}
{{- range $ingress := .values.ingresses }}
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: {{ $ingress.name }}
  namespace: {{ $ingress.namespace }}
  {{- with $ingress.annotations }}
  annotations:
    {{- toyaml . | nindent 8 }}
  {{- end }}
spec:
  {{- if and $ingress.class (semvercompare &quot;&gt;=1.18-0&quot; $.capabilities.kubeversion.gitversion) }}
  ingressclassname: {{ $ingress.class }}
  {{- end }}
  {{- if $ingress.tls }}
  tls:
    {{- range $ingress.tls }}
    - hosts:
        {{- range .hosts }}
        - {{ . | quote }}
        {{- end }}
      secretname: {{ .secretname }}
    {{- end }}
  {{- end }}
  rules:
    {{- range $ingress.rules }}
    - host: {{ .host | quote }}
      http:
        paths:
          {{- range .paths }}
          {{- if and .path (semvercompare &quot;&gt;=1.18-0&quot; $.capabilities.kubeversion.gitversion) }}
          - path: {{ .path }}
          {{ end }}
            {{- if and .pathtype (semvercompare &quot;&gt;=1.18-0&quot; $.capabilities.kubeversion.gitversion) }}
            pathtype: {{ .pathtype }}
            {{- end }}
            backend:
              service:
                name: {{ .backend.service.name }}
                port:
                  number: {{ .backend.service.port.number}}
          {{- end }}
    {{- end }}
    {{- end }}

this only generates one ingress (whichever is the last one in values files). i tried using range $ingress := .values.ingress but it keeps giving me an error whenever i try $ingress.name . what changes do i make to the ingress.yaml to be able to deploy both these ingresses.
edit: made edits based on david's answer.
",<kubernetes><kubernetes-helm>,71807432,1,"you need to break the two separate ingress configurations up in the helm values somehow.  right now they're in a single map object under ingress:, so .values.ingress.name for example only has one value rather than being something you can iterate over.
a yaml list here makes sense:
# values.yaml
ingresses:
  - name: example-com
    class: nginx
    rules: [...]
  - name: example1-com
    class: nginx
    rules: [...]

then you can iterate over this list with a range loop.  the important thing to know about a range loop is that it rebinds the . special variable, which is the base of constructs like .values; that means that you need to save the original value of . outside the loop (the $ special variable may work as well).  you can generate multiple kubernetes objects in a single helm template file so long as each begins with the yaml --- start-of-document marker (and it's valid to generate no output at all).
{{-/* save the original value of . */-}}
{{- $top := . -}}

{{-/* iterate over the ingress configurations */-}}
{{- range $ingress := .values.ingresses }}
---
{{-/* your existing conditionals can go here, simplifying */}}
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  {{-/* this comes from the per-ingress config */}}
  {{- with $ingress.annotations }}
  annotations: {{- toyaml . | nindent 4 }}
  {{- end }}

  {{-/* if you need to use &quot;standard&quot; helper functions, make sure
        to pass the saved $top value as their parameter */}}
  name: {{ include &quot;mychart.fullname $top }}-{{ $ingress.name }}
spec: { ... }
{{- end }}

you also may want to reconsider how much of this is appropriate to include in arbitrarily-configurable values.  rather than essentially write out the entire ingress object in helm values, you may find it easier to write out things like the path mappings in the template files themselves, and have a few high-level controls (&quot;enabled&quot;, &quot;host name&quot;, &quot;tls secret name&quot;) exposed.  things like the backend service name and port will correspond to other things in your chart and you may need to compute the service name; someone just installing your chart shouldn't need to configure this.
"
71864161,ingress host url not providing response,"ingress ip is providing expected result but host returns 404 http not found
ingress.yaml
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: helloworld-ing
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot; 
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot; 
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  defaultbackend:
    service:
      name: helloworld-svc
      port:
        number: 8080
  ingressclassname: nginx
  tls:
    - hosts:
        - helloworld.dev.com
      secretname: ingress-tls-csi
  rules:
    - host: helloworld.dev.com
      http:
        paths:
          - path: /helloworld
            pathtype: prefix
            backend:
              service:
                name: helloworld-svc
                port:
                  number: 8080

ingress ip was not working earlier but adding default backend resolved that issue.
i believe this can be the issue that its not going past backend and not even reaching rules.
i do not see warning/errors in ingress logs but if i remove default backend i can not even access app using ingress ip.
i am not sure what i am missing in my ingress configuration.
i am trying same path for url and ip -
curl http://10.110.45.61/helloworld/service/result

curl http://helloworld.dev.com/helloworld/service/result

i am happy to provide more information if required.
",<kubernetes><kubernetes-ingress><nginx-ingress>,71867285,1,"hello, hope you'are enjoying your kubernetes journey !
so this is what i have tested for a first try (i havent tested tls now):
first i have setup a kind cluster locally with this configuration (info here: https://kind.sigs.k8s.io/docs/user/quick-start/):
kind: cluster
apiversion: kind.x-k8s.io/v1alpha4
name: so-cluster-1
nodes:
- role: control-plane
  image: kindest/node:v1.23.5
- role: control-plane
  image: kindest/node:v1.23.5
- role: control-plane
  image: kindest/node:v1.23.5
- role: worker
  image: kindest/node:v1.23.5
- role: worker
  image: kindest/node:v1.23.5
- role: worker
  image: kindest/node:v1.23.5

after this i create my cluster with this command:
kind create cluster --config=config.yaml

next, i have created a test namespace (manifest obtained with: kubectl create ns so-tests -o yaml --dry-run):
apiversion: v1
kind: namespace
metadata:
  name: so-tests

then i created this vanilla nginx deployment and exposed it with a service, here is the config (manifest from here https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment):
apiversion: apps/v1
kind: deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerport: 80

same with the pod (manifest from obtained with: k expose deployment nginx-deployment --dry-run -o yaml ):
apiversion: v1
kind: service
metadata:
  creationtimestamp: null
  labels:
    app: nginx
  name: nginx-deployment
spec:
  ports:
  - port: 80
    protocol: tcp
    targetport: 80
  selector:
    app: nginx

after applying every manifests to my cluster and checked if my pods were running, i made sure that i could access my nginx pod web homepage by running:
kubectl port-forward pod/nginx-deployment-74d589986c-c85r9 8080:80 #checked on localhost:8080 and it was succesful

i've done the same against the service to make sure that it was correctly redirecting the traffic to the pod:
k port-forward service/nginx-deployment 8080:80 #checked on localhost:8080 and it was succesful

when i was sure that my workload was correctly running and accessible, i installed nginx ingress controller (from here: https://kubernetes.github.io/ingress-nginx/deploy/) with this command:
helm upgrade --install ingress-nginx ingress-nginx \
  --repo https://kubernetes.github.io/ingress-nginx \
  --namespace ingress-nginx --create-namespace

then i created the ingress k8s resource, here is the config (obtained by running: k create ingress demo-localhost --class=nginx --rule=&quot;demo.localdev.me/*=demo:80&quot; and by replacing the service.name by the name of my service.):
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  creationtimestamp: null
  name: demo-localhost
spec:
  ingressclassname: nginx
  rules:
  - host: demo.localdev.me
    http:
      paths:
      - backend:
          service:
            name: nginx-deployment
            port:
              number: 80
        path: /
        pathtype: prefix

then to check if my ingress was corretly redirecting the traffic i ran:
kubectl port-forward --namespace=ingress-nginx service/ingress-nginx-controller 8080:80 #i also tested this on my browser localhost:8080

and guess what? -&gt; 404 not found.
so, i decided to replace the name &quot;demo.localdev.me&quot; by &quot;localhost&quot; and it worked, here is the conf:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: demo-localhost
spec:
  ingressclassname: nginx
  rules:
  - host: localhost #demo.localdev.me
    http:
      paths:
      - backend:
          service:
            name: nginx-deployment
            port:
              number: 80
        path: /
        pathtype: prefix

i went to my  c:\windows\system32\drivers\etc\hosts file on windows (equivalent to /etc/hosts on linux, or equivalent to your dns serveur in enterprise) to check if it was a dns issue, i added this line:
127.0.0.1       demo.localdev.me

and it worked as expected. (make sure to clear you browser cache when playing with /etc/hosts file or to use private navigation to get accurate resulsts)
so, you can do some testing: make sure that:

your path is correctly reachable (you can do the testing with
kubectl port-forward, as i have done before)
you do have the correct entry in dns that redirects the traffic to your nginx service (not your application pod service)
you do not have a firewall that is blocking your ports
the tls configuration is working properly (try with and without)

if you have more information to share with us, feel free, i'll be happy to help you !
have a nice day man !
"
71856217,pass file to docker container in a kubernetes pod,"i'm beginner in kubernetes, what i would like to achieve is :

pass user's ssh private/public key to the pod and then to the docker container (there's a shell script that will be using this key)

so i would like to know if it's possible to do that in the kubectl apply ?
my pod.yaml looks like :
apiversion: v1
kind: pod
metadata:
  generatename: testing
  labels:
    type: testing
  namespace: ns-test
  name: testing-config
spec:
  restartpolicy: onfailure
  hostnetwork: true
  containers:
    - name: mycontainer
      image: &quot;.../mycontainer:latest&quot;

",<docker><kubernetes><kubectl>,71856431,1,"first, you create a secret with your keys: kubectl create secret generic mysecret-keys --from-file=privatekey=&lt;/path/to/the/key/file/on/your/host&gt; --from-file=publickey=&lt;/path/to/the/key/file/on/your/host&gt;
then you refer to the key files using the secret in your pod:
apiversion: v1
kind: pod
metadata:
  ...
spec:
  ...
  containers:
  - name: mycontainer
    image: &quot;.../mycontainer:latest&quot;
    volumemounts:
    - name: mysecret-keys
      mountpath: /path/in/the/container  # &lt;-- privatekey &amp; publickey will be mounted as file in this directory where your shell script can access
  volumes:
  - name: mysecret-keys
    secret:
      secretname: mysecret-keys  # &lt;-- mount the secret resource you created above

you can check the secret with kubectl get secret mysecret-keys --output yaml. you can check the pod and its mounting with kubectl describe pod testing-config.
"
59875886,kubernetes ingress configuration are not updated on gke cluster?,"i want to update the ingress configuration and which will apply on ingress instance running on kuberntes cluter on gcloud.

for this i have performed two steps:


firstly, people ask that set both annotation in ingress.yml and then re-create ingress will solve the issue mentioned on this. 



kubernetes.io/ingress.class: ""gce""
nginx.ingress.kubernetes.io/proxy-body-size: 20m



after deleting the ingress from cluster and create the ingress again
    also declared me unlucky.

ingress.yml

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-ingress
  namespace: default
  annotations:
    kubernetes.io/ingress.class: ""gce""
    nginx.ingress.kubernetes.io/proxy-body-size: 20m
    nginx.org/client-max-body-size: ""20m""



secondly, configure the configmap file on the gcloul cluster, so that our ingress configuration will update, but come up with the negative result mentioned on this.


nginx-config.yml

apiversion: v1
kind: configmap
metadata:
  name: nginx-config
  namespace: default
data:
  proxy-body-size: ""20m""


so how can i update my ingress properties such as annotation nginx.ingress.kubernetes.io/proxy-body-size, so that i can upload data more than 1 mb (where my cluster deployed on gke)? 

any help would be appreciated. thanks
",<nginx><kubernetes><google-kubernetes-engine><kubernetes-ingress>,59935901,1,"you are misinterpreting the annotations part in your ingress resource. let me elaborate on that. 

the problem is that you trying to use gce controller and apply annotations specifically for nginx ingress controller.  you cannot use nginx ingress controller annotations with gce controller. 

for your configuration to work you would need to deploy nginx ingress controller. 

you can deploy it by following official documentation. 

after deploying nginx ingress controller the part of the ingress definition should look like that: 

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-ingress
  namespace: default
  annotations:
    kubernetes.io/ingress.class: ""nginx""
    nginx.ingress.kubernetes.io/proxy-body-size: ""20m""


take a specific look at part below: 

    kubernetes.io/ingress.class: ""nginx""
    nginx.ingress.kubernetes.io/proxy-body-size: ""20m""


please refer to official documentation when applying annotations for nginx ingress controller. 
"
72080098,how to set up rtmp protocol in ingress nginx?,"i have a kubernetes cluster set up on google kubernetes engine. i have a pod that hosts an rtmp application. this application requires two ports, one for http and one for rtmp. i have set up an ingress nginx load balancer. it routes the http traffic correctly. i want to be able to route the rtmp traffic as well. but when i write rtmp in my yaml file. it shows it as an unknown field. i am using skaffold and google cloud build to deploy.
here is the ingress-srv file
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-service
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/use-regex: 'true'
spec:
  rules:
    - http:
        paths:
          - path: /?(.*)
            backend:
              servicename: stream-server-srv
              serviceport: 8000

what i want to be able to do is have an entry for rtmp on port 1935. that i can then use to stream love videos using rtmp protocol.
",<nginx><kubernetes><google-kubernetes-engine><load-balancing><rtmp>,72097260,1,"rtmp is a tcp-based protocol and standard ingress does not support tcp services.
the nginx ingress controller (which it looks like you are using) can be configured to expose tcp services.
first, you'll need to make sure that you expose both the http and rtmp ports in your stream-server-srv service:
apiversion: v1
kind: service
metadata:
  name: stream-server-srv
  namespace: default
spec:
  selector:
    app: stream-server
  type: clusterip
  ports:
    - name: http-port
      port: 8000
      targetport: 8000
      protocol: tcp
    - name: rtmp-port
      port: 1935
      targetport: 1935
      protocol: tcp

(replace default with your namespace)
you will also need to make sure that the service used to expose the nginx ingress exposes port 1935 as well.  for example:
apiversion: v1
kind: service
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.2.0
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  externaltrafficpolicy: local
  ipfamilies:
  - ipv4
  ipfamilypolicy: singlestack
  ports:
  - appprotocol: http
    name: http
    port: 80
    protocol: tcp
    targetport: http
  - appprotocol: https
    name: https
    port: 443
    protocol: tcp
    targetport: https
  - name: rtmp
    port: 1935
    protocol: tcp
    targetport: 1935
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: loadbalancer

finally, you'll need to update / patch the nginx tcp services configmap:
kubectl patch configmap tcp-services -n ingress-nginx --patch '{&quot;data&quot;:{&quot;1935&quot;:&quot;default/stream-server-srv:1935&quot;}}'

(replace &quot;default/stream-server-srv&quot; with your namespace/servicename)
"
51249127,how to config kubernetes ingress nginx anontations whitelist only apply to http,"i have config my ingress support ssl:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ""service""
  annotations:
    nginx.ingress.kubernetes.io/whitelist-source-range: ""x.x.x.x/xx""
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
spec:
  tls:
  - hosts:
    - ""example.com""
    secretname: example.name
  rules:
  - host: ""example.com""
    http:
      paths:
      - path: /
        backend:
          servicename: service
          serviceport: 80


in my config above, only ip in whitelist can access the domain for both http &amp; https. but i would like to config all ip addresses can access https://example.com (https) and some ip addresses in whitelist can access without ssl - http://example.com.
",<ssl><nginx><kubernetes><kubernetes-ingress><kubernetes-security>,51277083,1,"i have resolved my issue by add more config to nginx location (listen both http and https) by use nginx.ingress.kubernetes.io/configuration-snippet annotation.

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ""service""
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
    # the configs to allow all ips access via https and allow some ips in
    # security whitelist access via http
    nginx.ingress.kubernetes.io/configuration-snippet: |

      if ($https) {
        set $allow_ip true;
      }

      if ($remote_addr ~ (x.x.x.x|y.y.y.y) {
        set $allow_ip true;
      }

      if ($allow_ip != true) {
        return 403;
      }
spec:
  tls:
  - hosts:
    - ""example.com""
    secretname: example.name
  rules:
  - host: ""example.com""
    http:
      paths:
      - path: /
        backend:
          servicename: service
          serviceport: 80

"
59341169,setup two ingress for two services with same names in different namespaces,"i have 2 namespaces called dev and stage
in both namespaces i have similar setups. in both namespaces i have service called frontend.

i wanted to set up an ingress for this. i set up ingress in both namespaces with the following config: 

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: dev.myapp.io
    http:
      paths:
      - backend:
          servicename: frontend
          serviceport: 80


in the stage just changed the host to stage.myapp.io. it is not working for one of the namespaces. 
does my approach is correct? or i need to set up ingress in another namepace (kube-system maybe) and point paths in the same ingress? 

ps: if i change service names and keep it different, 2 ingress works just fine but i want to set up services with same namespace, as it simplifies my other deployments.
",<kubernetes><kubernetes-ingress>,59355592,1,"your are supposed to include the namespace annotation to your ingress. considering it, your yaml files should look like this: 

dev:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-ingress-dev
  namespace: dev
spec:
  rules:
  - host: dev.myapp.io
    http:
      paths:
      - backend:
          servicename: frontend
          serviceport: 80    


stage:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-ingress-stage
  namespace: stage
spec:
  rules:
  - host: stage.myapp.io
    http:
      paths:
      - backend:
          servicename: frontend
          serviceport: 80   

"
50622853,traefik as ingress controller: 404 when using letsencrypt for https,"some days ago i created a kubernetes cluster using traefik as it's ingress controller. afterwards i enabled the traefik web ui for the subdomain traefik.mydomain.de. now i'm trying to use letsencrypt to 


redirect any requests to mydomain.de &amp; traefik.mydomain.de on port 80 to port 443
serve the traefik web ui over https


this is my full configuration traefik.yml:

---
kind: clusterrole
apiversion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: traefik-ingress-controller
rules:
  - apigroups:
      - """"
    resources:
      - services
      - endpoints
      - secrets
    verbs:
      - get
      - list
      - watch
  - apigroups:
      - extensions
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
---
kind: clusterrolebinding
apiversion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: traefik-ingress-controller
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: traefik-ingress-controller
subjects:
- kind: serviceaccount
  name: traefik-ingress-controller
  namespace: kube-system
---
apiversion: v1
kind: serviceaccount
metadata:
  name: traefik-ingress-controller
  namespace: kube-system
---
apiversion: v1
kind: configmap
metadata:
  name: traefik-config
  namespace: kube-system
data:
  traefik.toml: |
    [entrypoints]
      [entrypoints.http]
      address = "":80""
        [entrypoints.http.redirect]
        entrypoint = ""https""
      [entrypoints.https]
      address = "":443""
        [entrypoints.https.tls]

    [acme]
    email = ""admin@mydomain.de""
    storage = ""/acme/acme.json""
    onhostrule = true
    caserver = ""https://acme-staging-v02.api.letsencrypt.org/directory""
    entrypoint = ""https""
      [acme.httpchallenge]
      entrypoint = ""http""

    [[acme.domains]]
    main = ""mydomain.de""
    sans = [""traefik.mydomain.de""]
---
kind: daemonset
apiversion: extensions/v1beta1
metadata:
  name: traefik-ingress-controller
  namespace: kube-system
  labels:
    k8s-app: traefik-ingress-lb
spec:
  template:
    metadata:
      labels:
        k8s-app: traefik-ingress-lb
        name: traefik-ingress-lb
    spec:
      serviceaccountname: traefik-ingress-controller
      terminationgraceperiodseconds: 60
      hostnetwork: true
      volumes:
        - name: config
          configmap:
            name: traefik-config
        - name: acme
          hostpath:
            path: /srv/configs/acme.json
            type: file
      containers:
      - image: traefik
        name: traefik-ingress-lb
        volumemounts:
            - mountpath: ""/config""
              name: ""config""
            - mountpath: ""/acme/acme.json""
              name: ""acme""
        ports:
        - name: http
          containerport: 80
          hostport: 80
        - name: https
          containerport: 443
          hostport: 443
        securitycontext:
          capabilities:
            drop:
            - all
            add:
            - net_bind_service
        args:
        - --configfile=/config/traefik.toml
        - --api
        - --kubernetes
        - --loglevel=debug
---
kind: service
apiversion: v1
metadata:
  name: traefik-ingress-service
  namespace: kube-system
spec:
  selector:
    k8s-app: traefik-ingress-lb
  ports:
  - protocol: tcp
    port: 80
    name: http
  - protocol: tcp
    port: 443
    name: https
  type: nodeport
---
apiversion: v1
kind: service
metadata:
  name: traefik-web-ui
  namespace: kube-system
spec:
  selector:
    k8s-app: traefik-ingress-lb
  ports:
  - protocol: tcp
    port: 8080
    name: webui
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: traefik-web-ui
  namespace: kube-system
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
  rules:
  - host: traefik.mydomain.de
    http:
      paths:
      - path: /
        backend:
          servicename: traefik-web-ui
          serviceport: 8080


the result:


requests to http://traefik.mydomain.de gets redirected to https://traefik.mydomain.de - this seems to work correctly
requests to https://traefik.mydomain.de return with 404 page not found


debug output:

time=""2018-05-31t10:54:58z"" level=info msg=""using toml configuration file /config/traefik.toml""
time=""2018-05-31t10:54:58z"" level=info msg=""traefik version v1.6.2 built on 2018-05-22_03:19:06pm""
time=""2018-05-31t10:54:58z"" level=info msg=""\nstats collection is disabled.\nhelp us improve traefik by turning this feature on :)\nmore details on: https://docs.traefik.io/basics/#collected-data\n""
time=""2018-05-31t10:54:58z"" level=debug msg=""global configuration loaded {\""lifecycle\"":{\""requestacceptgracetimeout\"":0,\""gracetimeout\"":10000000000},\""gracetimeout\"":0,\""debug\"":false,\""checknewversion\"":true,\""sendanonymoususage\"":false,\""accesslogsfile\"":\""\"",\""accesslog\"":null,\""traefiklogsfile\"":\""\"",\""traefiklog\"":null,\""tracing\"":null,\""loglevel\"":\""debug\"",\""entrypoints\"":{\""http\"":{\""address\"":\"":80\"",\""tls\"":null,\""redirect\"":{\""entrypoint\"":\""https\""},\""auth\"":null,\""whitelistsourcerange\"":null,\""whitelist\"":null,\""compress\"":false,\""proxyprotocol\"":null,\""forwardedheaders\"":{\""insecure\"":true,\""trustedips\"":null}},\""https\"":{\""address\"":\"":443\"",\""tls\"":{\""minversion\"":\""\"",\""ciphersuites\"":null,\""certificates\"":null,\""clientcafiles\"":null,\""clientca\"":{\""files\"":null,\""optional\"":false}},\""redirect\"":null,\""auth\"":null,\""whitelistsourcerange\"":null,\""whitelist\"":null,\""compress\"":false,\""proxyprotocol\"":null,\""forwardedheaders\"":{\""insecure\"":true,\""trustedips\"":null}},\""traefik\"":{\""address\"":\"":8080\"",\""tls\"":null,\""redirect\"":null,\""auth\"":null,\""whitelistsourcerange\"":null,\""whitelist\"":null,\""compress\"":false,\""proxyprotocol\"":null,\""forwardedheaders\"":{\""insecure\"":true,\""trustedips\"":null}}},\""cluster\"":null,\""constraints\"":[],\""acme\"":null,\""defaultentrypoints\"":[\""http\""],\""providersthrottleduration\"":2000000000,\""maxidleconnsperhost\"":200,\""idletimeout\"":0,\""insecureskipverify\"":false,\""rootcas\"":null,\""retry\"":null,\""healthcheck\"":{\""interval\"":30000000000},\""respondingtimeouts\"":null,\""forwardingtimeouts\"":null,\""allowminweightzero\"":false,\""web\"":null,\""docker\"":null,\""file\"":null,\""marathon\"":null,\""consul\"":null,\""consulcatalog\"":null,\""etcd\"":null,\""zookeeper\"":null,\""boltdb\"":null,\""kubernetes\"":{\""watch\"":true,\""filename\"":\""\"",\""constraints\"":[],\""trace\"":false,\""templateversion\"":0,\""debugloggeneratedtemplate\"":false,\""endpoint\"":\""\"",\""token\"":\""\"",\""certauthfilepath\"":\""\"",\""disablepasshostheaders\"":false,\""enablepasstlscert\"":false,\""namespaces\"":null,\""labelselector\"":\""\"",\""ingressclass\"":\""\""},\""mesos\"":null,\""eureka\"":null,\""ecs\"":null,\""rancher\"":null,\""dynamodb\"":null,\""servicefabric\"":null,\""rest\"":null,\""api\"":{\""entrypoint\"":\""traefik\"",\""dashboard\"":true,\""debug\"":false,\""currentconfigurations\"":null,\""statistics\"":null},\""metrics\"":null,\""ping\"":null}""
time=""2018-05-31t10:54:58z"" level=info msg=""preparing server https &amp;{address::443 tls:0xc42057e900 redirect:&lt;nil&gt; auth:&lt;nil&gt; whitelistsourcerange:[] whitelist:&lt;nil&gt; compress:false proxyprotocol:&lt;nil&gt; forwardedheaders:0xc420020480} with readtimeout=0s writetimeout=0s idletimeout=3m0s""
time=""2018-05-31t10:54:59z"" level=info msg=""preparing server http &amp;{address::80 tls:&lt;nil&gt; redirect:0xc420092a80 auth:&lt;nil&gt; whitelistsourcerange:[] whitelist:&lt;nil&gt; compress:false proxyprotocol:&lt;nil&gt; forwardedheaders:0xc4200204a0} with readtimeout=0s writetimeout=0s idletimeout=3m0s""
time=""2018-05-31t10:54:59z"" level=info msg=""preparing server traefik &amp;{address::8080 tls:&lt;nil&gt; redirect:&lt;nil&gt; auth:&lt;nil&gt; whitelistsourcerange:[] whitelist:&lt;nil&gt; compress:false proxyprotocol:&lt;nil&gt; forwardedheaders:0xc4200204c0} with readtimeout=0s writetimeout=0s idletimeout=3m0s""
time=""2018-05-31t10:54:59z"" level=info msg=""starting provider configuration.provideraggregator {}""
time=""2018-05-31t10:54:59z"" level=info msg=""starting server on :443""
time=""2018-05-31t10:54:59z"" level=info msg=""starting server on :80""
time=""2018-05-31t10:54:59z"" level=info msg=""starting server on :8080""
time=""2018-05-31t10:54:59z"" level=info msg=""starting provider *kubernetes.provider {\""watch\"":true,\""filename\"":\""\"",\""constraints\"":[],\""trace\"":false,\""templateversion\"":0,\""debugloggeneratedtemplate\"":false,\""endpoint\"":\""\"",\""token\"":\""\"",\""certauthfilepath\"":\""\"",\""disablepasshostheaders\"":false,\""enablepasstlscert\"":false,\""namespaces\"":null,\""labelselector\"":\""\"",\""ingressclass\"":\""\""}""
time=""2018-05-31t10:54:59z"" level=info msg=""starting provider *acme.provider {\""email\"":\""admin@mydomain.de\"",\""acmelogging\"":false,\""caserver\"":\""https://acme-staging-v02.api.letsencrypt.org/directory\"",\""storage\"":\""/acme/acme.json\"",\""entrypoint\"":\""https\"",\""onhostrule\"":true,\""ondemand\"":false,\""dnschallenge\"":null,\""httpchallenge\"":{\""entrypoint\"":\""http\""},\""domains\"":[{\""main\"":\""mydomain.de\"",\""sans\"":[\""traefik.mydomain.de\""]}],\""store\"":{}}""
time=""2018-05-31t10:54:59z"" level=debug msg=""using ingress label selector: \""\""""
time=""2018-05-31t10:54:59z"" level=info msg=""ingress label selector is: \""\""""
time=""2018-05-31t10:54:59z"" level=info msg=""creating in-cluster provider client""
time=""2018-05-31t10:54:59z"" level=info msg=""testing certificate renew...""
time=""2018-05-31t10:54:59z"" level=debug msg=""configuration received from provider acme: {\""tls\"":[{\""entrypoints\"":[\""https\""],\""certificate\"":{\""certfile\"":\""-----begin certificate-----&lt;&lt;&lt; cert here &gt;&gt;&gt;-----end certificate-----\\n\\n-----begin certificate-----&lt;&lt;&lt; another cert here &gt;&gt;&gt;-----end certificate-----\\n\"",\""keyfile\"":\""-----begin rsa private&lt;&lt;&lt; rsa data here &gt;&gt;&gt;-----end rsa private key-----\\n\""}}]}""
time=""2018-05-31t10:54:59z"" level=debug msg=""looking for provided certificate(s) to validate [\""mydomain.de\"" \""traefik.mydomain.de\""]...""
time=""2018-05-31t10:54:59z"" level=debug msg=""no acme certificate to generate for domains [\""mydomain.de\"" \""traefik.mydomain.de\""].""
time=""2018-05-31t10:54:59z"" level=debug msg=""add certificate for domains mydomain.de,traefik.mydomain.de""
time=""2018-05-31t10:54:59z"" level=info msg=""server configuration reloaded on :8080""
time=""2018-05-31t10:54:59z"" level=info msg=""server configuration reloaded on :443""
time=""2018-05-31t10:54:59z"" level=info msg=""server configuration reloaded on :80""
time=""2018-05-31t10:54:59z"" level=debug msg=""received kubernetes event kind *v1.service""
time=""2018-05-31t10:54:59z"" level=debug msg=""configuration received from provider kubernetes: {\""backends\"":{\""traefik.mydomain.de/\"":{\""servers\"":{\""traefik-ingress-controller-lqkjn\"":{\""url\"":\""https://11.22.33.44:8080\"",\""weight\"":1}},\""loadbalancer\"":{\""method\"":\""wrr\""}}},\""frontends\"":{\""traefik.mydomain.de/\"":{\""entrypoints\"":[\""http\""],\""backend\"":\""traefik.mydomain.de/\"",\""routes\"":{\""/\"":{\""rule\"":\""pathprefix:/\""},\""traefik.mydomain.de\"":{\""rule\"":\""host:traefik.mydomain.de\""}},\""passhostheader\"":true,\""priority\"":0,\""basicauth\"":[]}}}""
time=""2018-05-31t10:54:59z"" level=debug msg=""creating frontend traefik.mydomain.de/""
time=""2018-05-31t10:54:59z"" level=debug msg=""wiring frontend traefik.mydomain.de/ to entrypoint http""
time=""2018-05-31t10:54:59z"" level=debug msg=""creating route traefik.mydomain.de host:traefik.mydomain.de""
time=""2018-05-31t10:54:59z"" level=debug msg=""creating route / pathprefix:/""
time=""2018-05-31t10:54:59z"" level=debug msg=""creating entry point redirect http -&gt; https""
time=""2018-05-31t10:54:59z"" level=debug msg=""creating backend traefik.mydomain.de/""
time=""2018-05-31t10:54:59z"" level=debug msg=""creating load-balancer wrr""
time=""2018-05-31t10:54:59z"" level=debug msg=""creating server traefik-ingress-controller-lqkjn at https://11.22.33.44:8080 with weight 1""
time=""2018-05-31t10:54:59z"" level=debug msg=""add certificate for domains mydomain.de,traefik.mydomain.de""
time=""2018-05-31t10:54:59z"" level=info msg=""server configuration reloaded on :443""
time=""2018-05-31t10:54:59z"" level=info msg=""server configuration reloaded on :80""
time=""2018-05-31t10:54:59z"" level=info msg=""server configuration reloaded on :8080""
time=""2018-05-31t10:54:59z"" level=debug msg=""try to challenge certificate for domain [traefik.mydomain.de] founded in host rule""
time=""2018-05-31t10:54:59z"" level=debug msg=""no domain parsed in rule \""pathprefix:/\""""
time=""2018-05-31t10:54:59z"" level=debug msg=""looking for provided certificate(s) to validate [\""traefik.mydomain.de\""]...""
time=""2018-05-31t10:54:59z"" level=debug msg=""no acme certificate to generate for domains [\""traefik.mydomain.de\""].""
time=""2018-05-31t10:54:59z"" level=debug msg=""received kubernetes event kind *v1.secret""
time=""2018-05-31t10:54:59z"" level=debug msg=""skipping kubernetes event kind *v1.secret""
time=""2018-05-31t10:54:59z"" level=debug msg=""received kubernetes event kind *v1.secret""
&lt;&lt;&lt; many more skipped events &gt;&gt;&gt;
time=""2018-05-31t10:55:16z"" level=debug msg=""skipping kubernetes event kind *v1.endpoints""
time=""2018-05-31t10:55:16z"" level=debug msg=""received kubernetes event kind *v1.endpoints""
time=""2018-05-31t10:55:16z"" level=debug msg=""skipping kubernetes event kind *v1.endpoints""
&lt;&lt;&lt; many more skipped events &gt;&gt;&gt;


unfortunately i'm lacking required debugging skills to analyse much further. i checked that my configured config files are available and readable. i checked that the acme.json is being used - it contains informations about the issued certificates.

note: i tried my best to keep this readable and as short as possible (without leaving out important information) but was most likely not able to keep it as minimal as it should be. pardon me for that - asking questions is way harder when you are not yet firm in a topic.
",<kubernetes><traefik><kubernetes-ingress>,50669572,1,"after a lot of research: there were (at least) 2 errors in my configuration.


missing defaultentrypoints. it seems that the traefik web ui does not configure an endpoint for the frontend (at least not by itself). as a result the web ui was not reachable in my configuration. adding the default entrypoints (which are used when there is no specific frontend entrypoint configuration) solves that issue. 


solution: add this line in the traefik.toml definition for the
    configmap named traefik-config (see the full configuration
    below):

defaultentrypoints = [""http"", ""https""]



erroneous port configuration. the port configuration was not linking correctly from port the exposed service port 80 to the internal service port 8080 that the traefik web ui uses. this can be changed by updating the configuration of the service named traefik-web-ui and adding targetport: 8080 (see the complete configuration below).


after these changes my setup works as intended.

the complete config just for reference:

---
kind: clusterrole
apiversion: rbac.authorization.k8s.io/v1beta1
metadata:
name: traefik-ingress-controller
rules:
- apigroups:
    - """"
    resources:
    - services
    - endpoints
    - secrets
    verbs:
    - get
    - list
    - watch
- apigroups:
    - extensions
    resources:
    - ingresses
    verbs:
    - get
    - list
    - watch
---
kind: clusterrolebinding
apiversion: rbac.authorization.k8s.io/v1beta1
metadata:
name: traefik-ingress-controller
roleref:
apigroup: rbac.authorization.k8s.io
kind: clusterrole
name: traefik-ingress-controller
subjects:
- kind: serviceaccount
name: traefik-ingress-controller
namespace: kube-system
---
apiversion: v1
kind: serviceaccount
metadata:
name: traefik-ingress-controller
namespace: kube-system
---
apiversion: v1
kind: configmap
metadata:
name: traefik-config
namespace: kube-system
data:
traefik.toml: |
    defaultentrypoints = [""http"", ""https""]

    [entrypoints]
    [entrypoints.http]
    address = "":80""
        [entrypoints.http.redirect]
        entrypoint = ""https""
    [entrypoints.https]
    address = "":443""
        [entrypoints.https.tls]

    [acme]
    email = ""admin@mydomain.de""
    storage = ""/acme/acme.json""
    onhostrule = true
    caserver = ""https://acme-staging-v02.api.letsencrypt.org/directory""
    entrypoint = ""https""
    [acme.httpchallenge]
    entrypoint = ""http""

    [[acme.domains]]
    main = ""mydomain.de""
    sans = [""traefik.mydomain.de""]
---
kind: daemonset
apiversion: extensions/v1beta1
metadata:
name: traefik-ingress-controller
namespace: kube-system
labels:
    k8s-app: traefik-ingress-lb
spec:
template:
    metadata:
    labels:
        k8s-app: traefik-ingress-lb
        name: traefik-ingress-lb
    spec:
    serviceaccountname: traefik-ingress-controller
    terminationgraceperiodseconds: 60
    hostnetwork: true
    volumes:
        - name: config
        configmap:
            name: traefik-config
        - name: acme
        hostpath:
            path: /srv/configs/acme.json
            type: file
    containers:
    - image: traefik
        name: traefik-ingress-lb
        volumemounts:
            - mountpath: ""/config""
            name: ""config""
            - mountpath: ""/acme/acme.json""
            name: ""acme""
        ports:
        - name: http
        containerport: 80
        hostport: 80
        - name: https
        containerport: 443
        hostport: 443
        securitycontext:
        capabilities:
            drop:
            - all
            add:
            - net_bind_service
        args:
        - --configfile=/config/traefik.toml
        - --api
        - --kubernetes
        - --loglevel=debug
---
kind: service
apiversion: v1
metadata:
name: traefik-ingress-service
namespace: kube-system
spec:
selector:
    k8s-app: traefik-ingress-lb
ports:
- protocol: tcp
    port: 80
    name: http
- protocol: tcp
    port: 443
    name: https
type: nodeport
---
apiversion: v1
kind: service
metadata:
name: traefik-web-ui
namespace: kube-system
spec:
selector:
    k8s-app: traefik-ingress-lb
ports:
- protocol: tcp
    port: 80
    targetport: 8080
    name: webui
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
name: traefik-web-ui
namespace: kube-system
annotations:
    kubernetes.io/ingress.class: traefik
spec:
rules:
- host: traefik.mydomain.de
    http:
    paths:
    - path: /
        backend:
        servicename: traefik-web-ui
        serviceport: 80

"
50779578,kubernetes - expose content from nginx docker image,"i have a docker nginx container image nha/my-nginx-img.
and i have a one-node kubernetes cluster, installed on bare metal. i am trying to deploy and expose this nginx container (which works fine locally otherwise).

i ran kubectl apply -f nginx.yaml on this file:

apiversion: apps/v1
kind: deployment
metadata:
  name: my-nginx
  labels:
    app: nginx
spec:
  selector:
    matchlabels:
      run: my-nginx
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nha/my-nginx-img
        ports:
        - containerport: 80
        - containerport: 443
      imagepullsecrets:
        - name: regcred
#
# expose service
#
apiversion: v1
kind: service
metadata:
  name: my-nginx
  labels:
    run: my-nginx
spec:
  # i thought using nodeport here 
  # would expose the port on the node?
  type: nodeport
  ports:
  - name: http
    protocol: tcp
    port: 80
    targetport: 80
  - name: https
    protocol: tcp
    port: 443
    targetport: 443
  selector:
    run: my-nginx


i can see them running:

kubectl get pods -l run=my-nginx -o wide
name                        ready     status    restarts   age       ip               node
my-nginx-5ccbf78584-4lxsn   1/1       running   0          1d        10.233.102.181   node1
my-nginx-5ccbf78584-6qkml   1/1       running   0          1d        10.233.102.182   node1


however:
- the ips show in the resulting command above are not the ip of my machine
- when i curl the ip of my machine, i do not get a reply on either the port 80 or 443

how do i get this static content to be served by nginx?

additional information

kubectl get services gives:

name         type        cluster-ip     external-ip   port(s)                      age
kubernetes   clusterip   10.233.0.1     &lt;none&gt;        443/tcp                      2d
my-nginx     nodeport    10.233.16.91   &lt;none&gt;        80:31081/tcp,443:31856/tcp   5h


(10.233.16.91 is not my ip)

and kubectl describe service my-nginx:

name:                     my-nginx
namespace:                default
labels:                   run=my-nginx
annotations:              kubectl.kubernetes.io/last-applied-configuration={""apiversion"":""v1"",""kind"":""service"",""metadata"":{""annotations"":{},""labels"":{""run"":""my-nginx""},""name"":""my-nginx"",""namespace"":""default""},""spec"":{""ports"":[...
selector:                 run=my-nginx
type:                     nodeport
ip:                       10.233.16.91
port:                     http  80/tcp
targetport:               80/tcp
nodeport:                 http  31081/tcp
endpoints:                10.233.102.181:80,10.233.102.182:80
port:                     https  443/tcp
targetport:               443/tcp
nodeport:                 https  31856/tcp
endpoints:                10.233.102.181:443,10.233.102.182:443
session affinity:         none
external traffic policy:  cluster
events:                   &lt;none&gt;


again, i do not see my ip anywhere in there. 

also, i created the cluster using kubespray.
",<docker><nginx><kubernetes><kubernetes-ingress>,50779654,1,"you'll want the service, not the pod. the pod is not exposed to anything except the local kubernetes network, this is why you create the service.

apiversion: v1
kind: service
metadata:
  name: my-nginx
  labels:
    run: my-nginx
spec:
  # i thought using nodeport here 
  # would expose the port on the node?
  type: nodeport
  ports:
  - name: http
    protocol: tcp
    port: 80
    targetport: 80
  - name: https
    protocol: tcp
    port: 443
    targetport: 443
  selector:
    run: my-nginx


this here defines the service, and it does expose the port on the node, but the ip you see is the internal ""cluster ip"" which can only be accessed by other pods.

so you might try kubectl get services 

 

this will show you the external ip of the port that is exposed.

also check out kubectl describe service yourservicename
"
51024198,load balancing k8s pods with operator-framework,"i built a simple operator, by tweaking the memcached example. the only major difference is that i need two docker images in my pods. got the deployment running. my test.yaml used to deploy with kubectl.

apiversion: ""cache.example.com/v1alpha1""
kind: ""memcached""
metadata:
  name: ""solar-demo""
spec:
  size: 3
  group: cache.example.com
  names:
    kind: memcached
    listkind: memcachedlist
    plural: solar-demos
    singular: solar-demo
  scope: namespaced
  version: v1alpha1


i am still missing one piece though - load-balancing part. currently, under docker we are using the nginx image working as a reverse-proxy configured as:

upstream api_microservice {
  server api:3000;
}
upstream solar-svc_microservice {
  server solar-svc:3001;
}
server {
  listen $nginx_port default;

  location /city {
    proxy_pass http://api_microservice;
  }

  location /solar {
    proxy_pass http://solar-svc_microservice;
  }

  root /html;
  location / {
    try_files /$uri /$uri/index.html /$uri.html /index.html=404;
  }
}


i want my cluster to expose the port 8080 and forward to ports 3000 and 3001 to my images running inside pods.

my deployment:

dep := &amp;appsv1.deployment{
    typemeta: metav1.typemeta{
        apiversion: ""apps/v1"",
        kind:       ""deployment"",
    },
    objectmeta: metav1.objectmeta{
        name:      m.name,
        namespace: m.namespace,
    },
    spec: appsv1.deploymentspec{
        replicas: &amp;replicas,
        selector: &amp;metav1.labelselector{
            matchlabels: ls,
        },
        template: v1.podtemplatespec{
            objectmeta: metav1.objectmeta{
                labels: ls,
            },
            spec: v1.podspec{
                containers: []v1.container{
                    {
                        image:   ""shmukler/docker_solar-svc"",
                        name:    ""solar-svc"",
                        command: []string{""npm"", ""run"", ""start-solar-svc""},
                        ports: []v1.containerport{{
                            containerport: 3001,
                            name:          ""solar-svc"",
                        }},
                    },
                    {
                        image:   ""shmukler/docker_solar-api"",
                        name:    ""api"",
                        command: []string{""npm"", ""run"", ""start-api""},
                        ports: []v1.containerport{{
                            containerport: 3000,
                            name:          ""solar-api"",
                        }},
                    },
                },
            },
        },
    }


what do i need to add have ingress or something running in front of my pods?

thank you
",<go><kubernetes><kubernetes-helm><kubernetes-ingress>,51046006,1,"
  what do i need to add have ingress or something running in front of my pods?


yes, ingress is designed for that kind of tasks. 

ingress has a path-based routing, which will be able  to set up the same configuration as you mentioned in your example with nginx. moreover, one of the most popular implementations of ingress is nginx as a proxy.

ingress is basically a set of rules that allows traffic, otherwise dropped or forwarded elsewhere, to reach the cluster services.
here is an example of an ingress configuration:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-app
spec:
  rules:
  - host: '' # empty value means any host
    http:
      paths:
      - path: /city
        backend:
          servicename: myapp
          serviceport: 3000
      - path: /solar
        backend:
          servicename: myapp
          serviceport: 3001


also, because a pod is not a static thing, you should create a service object which will be a static entry point of your application for ingress.

here is an example of the service:

kind: service
apiversion: v1
metadata:
  name: myapp
spec:
  selector:
    app: ""name_of_your_deployment""
  ports:
  - name: city
    protocol: tcp
    port: 3000
    targetport: 3000
  - name: solar
    protocol: tcp
    port: 3001
    targetport: 3001

"
50216488,why does this setup with contour on kubernetes (gke) result in 2 functioning external ips?,"i've been experimenting with contour as an alternative ingress controller on a test gke kubernetes cluster.

following the contour deployment docs with a few modifications, i've got a working setup serving test http responses.

first, i created a ""helloworld"" pod that serves http responses, exposed via a nodeport service and an ingress:

apiversion: extensions/v1beta1
kind: deployment
metadata:
    name: helloworld
spec:
  replicas: 4
  template:
    metadata:
      labels:
        app: helloworld
    spec:
      containers:
        - name: ""helloworld-http""
          image: ""nginxdemos/hello:plain-text""
          imagepullpolicy: always
          resources:
            requests:
              cpu: 250m
              memory: 256mi
      affinity:
        podantiaffinity:
          preferredduringschedulingignoredduringexecution:
          - weight: 100
            podaffinityterm:
              labelselector:
                matchexpressions:
                - key: app
                  operator: in
                  values:
                  - helloworld
              topologykey: ""kubernetes.io/hostname""
---
apiversion: v1
kind: service
metadata:
  name: helloworld-svc
spec:
  ports:
  - port: 80
    protocol: tcp
    targetport: 80
  selector:
    app: helloworld
  sessionaffinity: none
  type: nodeport
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: helloworld-ingress
spec:
  backend:
    servicename: helloworld-svc
    serviceport: 80


then, i created a deployment for contour that's directly copied from their docs:

apiversion: v1
kind: namespace
metadata:
  name: heptio-contour
---
apiversion: v1
kind: serviceaccount
metadata:
  name: contour
  namespace: heptio-contour
---
apiversion: extensions/v1beta1
kind: deployment
metadata:
  labels:
    app: contour
  name: contour
  namespace: heptio-contour
spec:
  selector:
    matchlabels:
      app: contour
  replicas: 2
  template:
    metadata:
      labels:
        app: contour
      annotations:
        prometheus.io/scrape: ""true""
        prometheus.io/port: ""9001""
        prometheus.io/path: ""/stats""
        prometheus.io/format: ""prometheus""
    spec:
      containers:
      - image: docker.io/envoyproxy/envoy-alpine:v1.6.0
        name: envoy
        ports:
        - containerport: 8080
          name: http
        - containerport: 8443
          name: https
        command: [""envoy""]
        args: [""-c"", ""/config/contour.yaml"", ""--service-cluster"", ""cluster0"", ""--service-node"", ""node0"", ""-l"", ""info"", ""--v2-config-only""]
        volumemounts:
        - name: contour-config
          mountpath: /config
      - image: gcr.io/heptio-images/contour:master
        imagepullpolicy: always
        name: contour
        command: [""contour""]
        args: [""serve"", ""--incluster""]
      initcontainers:
      - image: gcr.io/heptio-images/contour:master
        imagepullpolicy: always
        name: envoy-initconfig
        command: [""contour""]
        args: [""bootstrap"", ""/config/contour.yaml""]
        volumemounts:
        - name: contour-config
          mountpath: /config
      volumes:
      - name: contour-config
        emptydir: {}
      dnspolicy: clusterfirst
      serviceaccountname: contour
      terminationgraceperiodseconds: 30
      affinity:
        podantiaffinity:
          preferredduringschedulingignoredduringexecution:
          - weight: 100
            podaffinityterm:
              labelselector:
                matchlabels:
                  app: contour
              topologykey: kubernetes.io/hostname
---
apiversion: v1
kind: service
metadata:
  name: contour
  namespace: heptio-contour
spec:
 ports:
 - port: 80
   name: http
   protocol: tcp
   targetport: 8080
 - port: 443
   name: https
   protocol: tcp
   targetport: 8443
 selector:
   app: contour
 type: loadbalancer
---


the default and heptio-contour namespaces now look like this:

$ kubectl get pods,svc,ingress -n default
name                              ready     status    restarts   age
pod/helloworld-7ddc8c6655-6vgdw   1/1       running   0          6h
pod/helloworld-7ddc8c6655-92j7x   1/1       running   0          6h
pod/helloworld-7ddc8c6655-mlvmc   1/1       running   0          6h
pod/helloworld-7ddc8c6655-w5g7f   1/1       running   0          6h

name                     type        cluster-ip      external-ip   port(s)        age
service/helloworld-svc   nodeport    10.59.240.105   &lt;none&gt;        80:31481/tcp   34m
service/kubernetes       clusterip   10.59.240.1     &lt;none&gt;        443/tcp        7h

name                                    hosts     address         ports     age
ingress.extensions/helloworld-ingress   *         y.y.y.y   80        34m

$ kubectl get pods,svc,ingress -n heptio-contour
name                          ready     status    restarts   age
pod/contour-9d758b697-kwk85   2/2       running   0          34m
pod/contour-9d758b697-mbh47   2/2       running   0          34m

name              type           cluster-ip     external-ip     port(s)                      age
service/contour   loadbalancer   10.59.250.54   x.x.x.x   80:30882/tcp,443:32746/tcp   34m


there's 2 publicly routable ip addresses:


x.x.x.x - a gce tcp load balancer that forwards to the contour pods
y.y.y.y - a gce http load balancer that forwards to the helloworld pods via the helloworld-ingress


a curl on both public ips returns a valid http response from the helloworld pods.

# the tcp load balancer
$ curl -v x.x.x.x
* rebuilt url to: x.x.x.x/  
*   trying x.x.x.x...
* tcp_nodelay set
* connected to x.x.x.x (x.x.x.x) port 80 (#0)
&gt; get / http/1.1
&gt; host: x.x.x.x
&gt; user-agent: curl/7.58.0
&gt; accept: */*
&gt;
&lt; http/1.1 200 ok
&lt; server: envoy
&lt; date: mon, 07 may 2018 14:14:39 gmt
&lt; content-type: text/plain
&lt; content-length: 155
&lt; expires: mon, 07 may 2018 14:14:38 gmt
&lt; cache-control: no-cache
&lt; x-envoy-upstream-service-time: 1
&lt;
server address: 10.56.4.6:80
server name: helloworld-7ddc8c6655-w5g7f
date: 07/may/2018:14:14:39 +0000
uri: /
request id: ec3aa70e4155c396e7051dc972081c6a

# the http load balancer
$ curl http://y.y.y.y 
* rebuilt url to: y.y.y.y/
*   trying y.y.y.y...
* tcp_nodelay set
* connected to y.y.y.y (y.y.y.y) port 80 (#0)
&gt; get / http/1.1
&gt; host: y.y.y.y
&gt; user-agent: curl/7.58.0
&gt; accept: */*
&gt; 
&lt; http/1.1 200 ok
&lt; server: nginx/1.13.8
&lt; date: mon, 07 may 2018 14:14:24 gmt
&lt; content-type: text/plain
&lt; content-length: 155
&lt; expires: mon, 07 may 2018 14:14:23 gmt
&lt; cache-control: no-cache
&lt; via: 1.1 google
&lt; 
server address: 10.56.2.8:80
server name: helloworld-7ddc8c6655-mlvmc
date: 07/may/2018:14:14:24 +0000
uri: /
request id: 41b1151f083eaf30368cf340cfbb92fc


is it by design that i have two public ips? which one should i use for customers? can i choose based on my preference between a tcp and http load balancer? 
",<kubernetes><google-cloud-platform><google-kubernetes-engine><kubernetes-ingress>,50223038,1,"probably you have glbc ingress configured (https://github.com/kubernetes/ingress-gce/blob/master/docs/faq/gce.md#how-do-i-disable-the-gce-ingress-controller)

could you try using following ingress definition?

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: ""contour""
  name: helloworld-ingress
spec:
  backend:
    servicename: helloworld-svc
    serviceport: 80


if you would like to be sure that your traffic goes via contour you should use x.x.x.x ip.
"
50390602,connection refused error while deploying nginx service in google cloud,"i am deploying nginx image using following deployment files in google cloud.

for replicationcontroller :

apiversion: v1
kind: replicationcontroller
metadata:
  name: nginx-web
  labels:
    name: nginx-web
    app: demo
spec:
  replicas: 2
  template:
    metadata:
      labels:
        name: nginx-web
    spec:
      containers:
        - name: nginx-web
          image: nginx
          ports:
            - containerport: 5000
              name: http
              protocol: tcp


for service deployment

apiversion: v1
kind: service
metadata:
  name: nginx-web
  labels:
    name: nginx-web
    app: demo
spec:
  selector:
    name: nginx-web
  type: loadbalancer
  ports:
   - port: 84
     targetport: 5000
     protocol: tcp


but when i do curl on external_ip (i got from loadbalancer) on port 84, i get connection refused error. what might be the issue?
",<kubernetes><google-cloud-platform><google-kubernetes-engine>,50391938,1,"the nginx image you are using in your replication controller is listening on port 80 (that's how the image is build).

you need to fix your replication controller spec like this:

apiversion: v1
kind: replicationcontroller
metadata:
  name: nginx-web
  labels:
    name: nginx-web
    app: demo
spec:
  replicas: 2
  template:
    metadata:
      labels:
        name: nginx-web
    spec:
      containers:
        - name: nginx-web
          image: nginx
          ports:
            - containerport: 80
              name: http
              protocol: tcp


and also adjust your service like this:

apiversion: v1
kind: service
metadata:
  name: nginx-web
  labels:
    name: nginx-web
    app: demo
spec:
  selector:
    name: nginx-web
  type: loadbalancer
  ports:
   - port: 84
     targetport: 80
     protocol: tcp

"
50582016,"k8s ingress, initiate ingress controller nginx error?","i have two spring boot container, i want to setup ingress service. as document here says, ingress has two parts, one is controller, the other is resources. 

my two resources are two containers: gearbox-rack-eureka-server and gearbox-rack-config-server. the difference is port so that ingress could route traffic by different ports. my yaml files are listed below:

eureka_pod.yaml

apiversion: v1
kind: pod
metadata:
  name: gearbox-rack-eureka-server
  labels:
    app: gearbox-rack-eureka-server
    purpose: platform_eureka_demo
spec:
  containers:
  - name:  gearbox-rack-eureka-server
    image: 192.168.1.229:5000/gearboxrack/gearbox-rack-eureka-server
    ports:
        - containerport: 8761


eureka_svc.yaml

apiversion: v1
kind: service
metadata:
  name: gearbox-rack-eureka-server
  labels:
    name: gearbox_rack_eureka_server
spec:
  selector:
    app: gearbox-rack-eureka-server
  type: nodeport
  ports:
    - port: 8761
      nodeport: 31501
      name: tcp


config_pod.yaml

apiversion: v1
kind: pod
metadata:
  name: gearbox-rack-config-server
  labels:
    app: gearbox-rack-config-server
    purpose: platform-demo
spec:
  containers:
  - name:  gearbox-rack-config-server
    image: 192.168.1.229:5000/gearboxrack/gearbox-rack-config-server
    ports:
    - containerport: 8888
    env:
      - name: eureka_server
        value: http://172.16.100.83:8761


config_svc.yaml

apiversion: v1
kind: service
metadata:
  name: gearbox-rack-config-server
  labels:
    name: gearbox-rack-config-server
spec:
  selector:
    app: gearbox-rack-config-server
  type: nodeport
  ports:
    - port: 8888
      nodeport: 31502
      name: tcp


my ingress-nginx controller is mostly copied from the link above, 

ingress_nginx_ctl.yaml:

kind: service
apiversion: v1
metadata:
  name: ingress-nginx
spec:
  type: loadbalancer
  selector:
    app: ingress-nginx
  ports:
  - name: http
    port: 80
    targetport: http
  - name: https
    port: 443
    targetport: https
---
kind: deployment
apiversion: extensions/v1beta1
metadata:
  name: ingress-nginx
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: ingress-nginx
    spec:
      terminationgraceperiodseconds: 60
      containers:
      - image: nginx:1.13.12
        name: ingress-nginx
        imagepullpolicy: always
        ports:
          - name: http
            containerport: 80
            protocol: tcp
          - name: https
            containerport: 443
            protocol: tcp
        livenessprobe:
          httpget:
            path: /healthz
            port: 10254
            scheme: http
          initialdelayseconds: 30
          timeoutseconds: 5
        env:
          - name: pod_name
            valuefrom:
              fieldref:
                fieldpath: metadata.name
          - name: pod_namespace
            valuefrom:
              fieldref:
                fieldpath: metadata.namespace
        args:
        - /nginx-ingress-controller
        - --default-backend-service=$(pod_namespace)/nginx-default-backend


i did following commands, they are successful.

kubectl apply -f eureka_pod.yaml
kubectl apply -f eureka_svc.yaml
kubectl apply -f config_pod.yaml
kubectl apply -f config_svc.yaml


then i got error from execute kubectl apply -f ingress_nginx_ctl.yaml, the pod does not start, logs are listed below:

[root@master3 nginx-ingress-controller]# kubectl get pods
name                             ready     status             restarts   age
gearbox-rack-config-server       1/1       running            0          39m
gearbox-rack-eureka-server       1/1       running            0          40m
ingress-nginx-686c9975d5-7d464   0/1       crashloopbackoff   6          7m
[root@master3 nginx-ingress-controller]# kubectl logs -f ingress-nginx-686c9975d5-7d464
container_linux.go:247: starting container process caused ""exec: \""/nginx-ingress-controller\"": stat /nginx-ingress-controller: no such file or directory""


i created a directory /nginx-ingress-controller under root, and repeat the steps again, it still said same error. does someone could point me the problem?

i put my ingress_nginx_res.yaml as follows for reference, it may have errors also.

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: 172.16.100.83
    http:
      paths:
      - backend:
          servicename: gearbox-rack-eureka-server
          serviceport: 8761
  - host: 172.16.100.83
    http:
      paths:
      - path:
        backend:
          servicename: gearbox-rack-config-server
          serviceport: 8888


==========================================

second   edition

after change image link, the previous errors disappear, but still it has following permission problem:

[root@master3 ingress]# kubectl get pods
name                             ready     status             restarts   age
gearbox-rack-config-server       1/1       running            0          15m
gearbox-rack-eureka-server       1/1       running            0          15m
ingress-nginx-8679f9c8ff-5sxw7   0/1       crashloopbackoff   5          12m


the log message is as follows:

[root@master3 kube]# kubectl logs ingress-nginx-8679f9c8ff-5sxw7
w0530 07:54:22.290114       5 client_config.go:533] neither --kubeconfig nor --master was specified.  using the inclusterconfig.  this might not work.
i0530 07:54:22.290374       5 main.go:158] creating api client for https://10.96.0.1:443
-------------------------------------------------------------------------------
nginx ingress controller
  release:    0.15.0
  build:      git-df61bd7
  repository: https://github.com/kubernetes/ingress-nginx
-------------------------------------------------------------------------------
i0530 07:54:22.298248       5 main.go:202] running in kubernetes cluster version v1.9 (v1.9.2) - git (clean) commit 5fa2db2bd46ac79e5e00a4e6ed24191080aa463b - platform linux/amd64
f0530 07:54:22.298610       5 main.go:80]  it seems the cluster it is running with authorization enabled (like rbac) and there is no permissions for the ingress controller. please check the configuration


it is rbac problem. i check the install script which is downloaded from forum:

heapster-rbac.yaml:

kind: clusterrolebinding
apiversion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: heapster
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: system:heapster
subjects:
- kind: serviceaccount
  name: heapster
  namespace: kube-system


one of related kubelet start argument is as follows: (i do not know whether it is relevant). 

environment=""kubelet_authz_args=--authorization-mode=webhook --client-ca-file=/etc/kubernetes/pki/ca.crt""


by which way, i could grant permission to ingress controller? just put namespace kube-system to ingress_nginx_ctl.yaml?

================================================================

             third edition

i put kun li's codes into ingress_nginx_role_rb.yaml, and run the following commands:

kubectl apply -f eureka_pod.yaml
kubectl apply -f eureka_svc.yaml
kubectl apply -f config_pod.yaml
kubectl apply -f config_svc.yaml
kubectl apply -f ingress_nginx_role_rb.yaml (just copy paste from kun li's answer)
kubectl apply -f nginx_default_backend.yaml
kubectl apply -f ingress_nginx_ctl.yaml


nginx_default_backend.yaml file is listed below:

kind: service
apiversion: v1
metadata:
  name: nginx-default-backend
  namespace: kube-system
spec:
  ports:
  - port: 80
    targetport: http
  selector:
    app: nginx-default-backend
---
kind: deployment
apiversion: extensions/v1beta1
metadata:
  name: nginx-default-backend
  namespace: kube-system
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-default-backend
    spec:
      terminationgraceperiodseconds: 60
      containers:
      - name: default-http-backend
        image: chenliujin/defaultbackend
        livenessprobe:
          httpget:
            path: /healthz
            port: 8080
            scheme: http
          initialdelayseconds: 30
          timeoutseconds: 5
        resources:
          limits:
            cpu: 10m
            memory: 20mi
          requests:
            cpu: 10m
            memory: 20mi
        ports:
        - name: http
          containerport: 8080
          protocol: tcp


ingress_nginx_ctl.yaml is listed below:

kind: service
apiversion: v1
metadata:
  name: ingress-nginx
spec:
  type: loadbalancer
  selector:
    app: ingress-nginx
  ports:
  - name: http
    port: 80
    targetport: http
  - name: https
    port: 443
    targetport: https
---
kind: deployment
apiversion: extensions/v1beta1
metadata:
  name: ingress-nginx
  namespace: kube-system
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: ingress-nginx
    spec:
      terminationgraceperiodseconds: 60
      serviceaccount: lb
      containers:
      - image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.15.0
        name: ingress-nginx
        imagepullpolicy: always
        ports:
          - name: http
            containerport: 80
            protocol: tcp
          - name: https
            containerport: 443
            protocol: tcp
        livenessprobe:
          httpget:
            path: /healthz
            port: 10254
            scheme: http
          initialdelayseconds: 30
          timeoutseconds: 5
        env:
          - name: pod_name
            valuefrom:
              fieldref:
                fieldpath: metadata.name
          - name: pod_namespace
            valuefrom:
              fieldref:
                fieldpath: metadata.namespace
        args:
        - /nginx-ingress-controller
        - --default-backend-service=$(pod_namespace)/nginx-default-backend


from here, we could see service ingress-nginx namespace is default, not kube-system. but anyway, controller is up. 

[root@master3 ingress]# kubectl get pods -n kube-system
name                                      ready     status    restarts   age
calico-etcd-cdn8z                         1/1       running   0          11m
calico-kube-controllers-d554689d5-tzdq5   1/1       running   0          11m
calico-node-dz4d6                         2/2       running   1          11m
coredns-65dcdb4cf-h62bh                   1/1       running   0          11m
etcd-master3                              1/1       running   0          10m
heapster-5c448886d-swp58                  1/1       running   0          11m
ingress-nginx-6ccc799fbc-hq2rm            1/1       running   0          9m
kube-apiserver-master3                    1/1       running   0          10m


ingress-nginx pod's namespace is kube-system (shown above), but its service's namespace is default.(shown below).

[root@master3 ingress]# kubectl get service
name                         type           cluster-ip      external-ip   port(s)                      age
gearbox-rack-config-server   nodeport       10.97.211.136   &lt;none&gt;        8888:31502/tcp               43m
gearbox-rack-eureka-server   nodeport       10.106.69.13    &lt;none&gt;        8761:31501/tcp               43m
ingress-nginx                loadbalancer   10.105.114.64   &lt;pending&gt;     80:30646/tcp,443:31332/tcp   42m
kubernetes                   clusterip      10.96.0.1       &lt;none&gt;        443/tcp                      44m


as mentioned in the comments, expert's response help me to move forward.
",<kubernetes><kubernetes-ingress>,50583036,1,"for ingress-controller, image quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.15.0 should be used. and you need setup nginx-default-backend pod and service.

about rbac, i think you need a seviceaccount to deploy your nginx-ingress-controller, with the following roles and bindings:

apiversion: v1
kind: serviceaccount
metadata:
  name: lb
  namespace: kube-system

---

apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrole
metadata:
  name: nginx-ingress-normal
rules:
  - apigroups:
      - """"
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apigroups:
      - """"
    resources:
      - nodes
    verbs:
      - get
  - apigroups:
      - """"
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apigroups:
      - ""extensions""
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apigroups:
      - """"
    resources:
        - events
    verbs:
        - create
        - patch
  - apigroups:
      - ""extensions""
    resources:
      - ingresses/status
    verbs:
      - update

---

apiversion: rbac.authorization.k8s.io/v1beta1
kind: role
metadata:
  name: nginx-ingress-minimal
  namespace: kube-system
rules:
  - apigroups:
      - """"
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apigroups:
      - """"
    resources:
      - configmaps
    resourcenames:
      - ""ingress-controller-leader-dev""
      - ""ingress-controller-leader-prod""
    verbs:
      - get
      - update
  - apigroups:
      - """"
    resources:
      - configmaps
    verbs:
      - create
  - apigroups:
      - """"
    resources:
      - endpoints
    verbs:
      - get

---

apiversion: rbac.authorization.k8s.io/v1beta1
kind: rolebinding
metadata:
  name: nginx-ingress-minimal
  namespace: kube-system
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: nginx-ingress-minimal
subjects:
  - kind: serviceaccount
    name: lb
    namespace: kube-system

---

apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrolebinding
metadata:
  name: nginx-ingress-normal
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: clusterrole
  name: nginx-ingress-normal
subjects:
  - kind: serviceaccount
    name: lb
    namespace: kube-system

"
72279424,nested templating helm,"i am working on a problem to implement a helm chart with customize configmap and trying to populate the configmap based on the environment mode.
values.yaml
externalipservice:
 ip: 1.1.1.1
 port:  80 

emsconfig: &quot;receivers:
 otlp:
   protocols:
     http:
processors:
 batch:
exporters:
 otlp/ems:
   endpoint: {{ .values.externalipservice.ip }}:{{ .values.externalipservice.port }}
service:
 pipelines:
   traces:
     receivers: [otlp]
     processors: [batch]
     exporters: [otlp/ems]
   metrics:
     receivers: [otlp]
     processors: [batch]
     exporters: [otlp/ems]
   logs:
     receivers: [otlp]
     processors: [batch]
     exporters: [otlp/ems]
&quot;

configmap.yaml
apiversion: v1
kind: configmap
metadata:
  name: simple-demo
data:
  message: &quot;{{ tpl .values.emsconfig .}}&quot;

the helm template output is a plain string and not a yaml content. i have tried toyaml as well but it did not help either. could someone please help to find a way to do nested rendering and to be able to use the final output in the confimap
apiversion: v1
kind: configmap
metadata:
  name: simple-demo
data:
  message: &quot;receivers: otlp: protocols: http: processors: batch: exporters: otlp/ems: endpoint: {{ .values.externalipservice.ip }}:{{ .values.externalipservice.port }} service: pipelines: traces: receivers: [otlp] processors: [batch] exporters: [otlp/ems] metrics: receivers: [otlp] processors: [batch] exporters: [otlp/ems] logs: receivers: [otlp] processors: [batch] exporters: [otlp/ems] &quot;

",<kubernetes><kubernetes-helm>,72280009,1,"in this situation, you need to use _halpers.tpl file.
first, add this to the _halpers.tpl file:
{{- define &quot;appname.emsconfig&quot; -}}
receivers:
 otlp:
   protocols:
     http:
processors:
 batch:
exporters:
 otlp/ems:
   endpoint: {{ .values.externalipservice.ip }}:{{ .values.externalipservice.port }}
service:
 pipelines:
   traces:
     receivers: [otlp]
     processors: [batch]
     exporters: [otlp/ems]
   metrics:
     receivers: [otlp]
     processors: [batch]
     exporters: [otlp/ems]
   logs:
     receivers: [otlp]
     processors: [batch]
     exporters: [otlp/ems]
{{- end }}

the values.yaml file will look like this:
externalipservice:
 ip: 1.1.1.1
 port:  80 


and the configmap.yaml file, will need to look like this:
apiversion: v1
kind: configmap
metadata:
  name: simple-demo
data:
  message: |-
  {{ include &quot;appname.emsconfig&quot; . | nindent 4}}

"
72318287,call endpoint of pod a from pod b when running on the same node,"i have created 2 services and for each of the service there is a corresponding deployment. all these are in the same pc. after applying the yaml, the services and deployments are running properly.
sudo npm kubectl apply -f deployment.yaml

now, if i try to curl to the ingress ip address, then i get 404 page not found as the response.
sudo kubectl get ingress -o wide
sudo curl &lt;ip address of ingress&gt;

if i try to curl to the ip address of the employee service or employee deployment, it gives that the connection was refused after waiting for sometime. which ever port i mentioned for the employee service and deployment i try, i always get error instead of the proper response.
can someone from the community please assist. i need to call the /emplist endpoint of myshop pod from employee pod.
myshop.js
const express = require('express')
const app = express()
const port = 3000

app.get('/emplist', (req, res) =&gt; {
    res.status(200).send( emplistobj )
});

app.listen(port, '0.0.0.0', () =&gt; {
  console.log(`listening on port ${port}`)
})

employee.js
const axios = require('axios').default;

const express = require('express')
const app = express()
const port = 3001

app.get('/', (req, res) =&gt; {
  
  axios({
      method: 'get',
      url: 'http://myshopservice/emplist'
    })
    .then(function (response) {
        res.status(200).send(response.data.emplist)
    })
    .catch(function (error) {
        res.status(200).send(`error because ${error}`);
    })
})

app.listen(port, '0.0.0.0', () =&gt; {
  console.log(`listening on port ${port}`)
})

now the corresponding deployment.yaml is:
---
apiversion: networking.k8s.io/v1 
kind: ingress 
metadata: 
  name: myshop-ingress 
  annotations: 
    nginx.ingress.kubernetes.io/rewrite-target: / 
spec: 
  rules: 
  - host: myshop.com 
    http: 
      paths: 
      - path: / 
        pathtype: prefix 
        backend: 
          service: 
            name: myshopservice 
            port: 
              number: 4000 

---

apiversion: v1
kind: service
metadata:
  name: myshopservice
spec:
  selector:
    app: myshop
  ports:
    - protocol: &quot;tcp&quot;
      port: 4000
      targetport: 3000
      nodeport: 30000
  type: loadbalancer

---
apiversion: apps/v1
kind: deployment
metadata:
  name: myshop
spec:
  replicas: 1
  selector:
    matchlabels:
      app: myshop
  template:
    metadata:
      labels:
        app: myshop
    spec:
      containers:
        - name: myshop
          image: mydockerhubid/myshop:1
          ports:
            - containerport: 3000

---
apiversion: v1
kind: service
metadata:
  name: employeeservice
spec:
  selector:
    app: employee
  ports:
    - protocol: &quot;tcp&quot;
      port: 4001
      targetport: 3001
      nodeport: 30001
  type: loadbalancer

---
apiversion: apps/v1
kind: deployment
metadata:
  name: employee
spec:
  replicas: 1
  selector:
    matchlabels:
      app: employee
  template:
    metadata:
      labels:
        app: employee
    spec:
      containers:
        - name: hud
          image: mydockerhubid/employee:1
          ports:
            - containerport: 3001

---

",<kubernetes><kubernetes-ingress><k3s>,72319034,1,"looks like there is issue with ingress mainly. i would suggest first to check the services status if it's running or not at &lt;loadbalancer ip:port/path&gt; (both services)
your app is listening on one specific path but you are not requesting that path in curl. if you want to do it that way you can also do it however i would suggest try simple way first and thn play with route/path based routing.
plus your ingress is sending requests at myshop service but it only listening or have route : /emplist
so try once ingress by removing annotation and host as you are on local.
if you want to keep host you have to add entry locally into /etc/hosts for host and ingress ip mapping and you should be using the or try curl on domain instead ingress ip then.
are you running any ingress controller? nginx or so?  : https://kubernetes.github.io/ingress-nginx/
---
apiversion: networking.k8s.io/v1 
kind: ingress 
metadata: 
  name: myshop-ingress 
spec: 
  rules: 
  - http: 
      paths: 
      - path: / 
        pathtype: prefix 
        backend: 
          service: 
            name: myshopservice 
            port: 
              number: 4000 

check this very simple example for ingress routing and controller setup : https://dev.to/xaviergeerinck/creating-a-kubernetes-nginx-ingress-controller-and-create-a-rule-to-a-sample-application-4and
"
54091572,kubernetes ingress not working: default backend 404,"i'm new to kubernertes and we have one app that can be customized to several costumers. 

the deployments are fine: they are running pods correctly. the problem is to access the api outside the cluster.

the aws routes are being created as expected by kubernetes ingress.

the existing ones are working fine, but when i try reach the new one (lets say client09), always return default-backend-404.

plus, when curl the url, it shows a kubernetes ingress controller fake certificate message.

kubectl version
client 1.6
server 1.9

also my user does not have full access, so i can't provide any information about nginx controller. we just have the same paste and copy for new costumers, but don't know what might be wrong.

any thoughts what is wrong?

service

apiversion: v1
kind: service
metadata:
 name: client09-svc
 labels:
   run: client09-deploy
 spec:
   type: clusterip
   ports:
     - port: 8080
       targetport: 8080
       protocol: tcp
       name: api
   selector:
     run: client09-deploy


deploy

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: client09-deploy
  namespace: default
spec:
  replicas: 1
  strategy:
    rollingupdate:
      maxsurge: 1
      maxunavailable: 0
    type: rollingupdate
  template:
    metadata:
      labels:
        run: client09-deploy
    spec:
      terminationgraceperiodseconds: 60
      containers:
      - name: client09
        image: mycontainer
        ports:
        - containerport: 8080
          name: api
        readinessprobe:
          httpget:
            path: /health
            port: 8080
          initialdelayseconds: 30
          periodseconds: 10
        livenessprobe:
          httpget:
            path: /health
            port: 8080
          initialdelayseconds: 30
          periodseconds: 10
        imagepullpolicy: always
        resources:
          limits:
            cpu: 1800m
            memory: 2000mi
          requests:
            cpu: 400m
            memory: 1000mi
        volumemounts:
          - mountpath: /secret-volume
            name: secretvolume
      imagepullsecrets:
        - name: dockerhubkey
      volumes:
        - name: secretvolume
          secret:
            secretname: client09-secret


ingress

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: my-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: ""true""
    ingress.kubernetes.io/ssl-redirect: ""true""
    ingress.kubernetes.io/use-port-in-redirects: ""true""
  namespace: default
spec:
  tls:
  - hosts:
    - client01.domain.com
    - client02.domain.com
    - client09.domain.com
  secretname: my-ingress-tls
rules:
- host: client01.domain.com
  http:
    paths:
    - backend:
        servicename: client01-svc
        serviceport: 8080
      path: /
- host: client02.domain.com
  http:
    paths:
    - backend:
        servicename: client02-svc
        serviceport: 8080
      path: /
- host: client09.domain.com
  http:
    paths:
    - backend:
        servicename: client09-svc
        serviceport: 8080
      path: /

",<kubernetes><kubernetes-ingress>,54106561,1,"looks like problem with selector. 
could you update service yaml to this:

apiversion: v1
kind: service
metadata:
 name: client09-svc
 labels:
   run: client09-deploy
 spec:
   type: clusterip
   ports:
     - port: 8080
       targetport: 8080
       protocol: tcp
       name: api
   selector:
     name: client09-deploy

"
63509912,how to add configmap created from a .txt to a pod?,"i am trying to make a simple config map from a config.txt file:
config.txt:
----------
key1=val1
key2=val2

this is the pod yaml:
apiversion: v1
kind: pod
metadata:
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ]
    env:
      - name: key_values
        valuefrom:
          configmapkeyref:
            name: keyvalcfgmap
            key1: key1
            key2: key2

by running  kubectl create configmap keyvalcfgmap --from-file=&lt;filepath&gt;  -o yaml &gt; configmap.yaml and applying the created configmap, i supposedly can use it in a pod. the question is how? i tried adding it as a volume or calling it using --from-file= and even envfrom but the best i could get was that the volume just mounted the file itself and not the configmap.
",<kubernetes><kubernetes-pod><configmap>,63510022,1,"you can use envfrom like this
apiversion: v1
kind: pod
metadata:
  name: dapi-test-pod
spec:
  containers:
  - name: test-container
    image: k8s.gcr.io/busybox
    command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ]
    envfrom:
    - configmapref:
        name: keyvalcfgmap        #&lt;--------------here
  restartpolicy: never

or you can use configmap as env variables
env:
  - name: name
    valuefrom:
      configmapkeyref:
        name: keyvalcfgmap        #&lt;--------------here
        key: key1
  - name: name
    valuefrom:
      configmapkeyref:
        name: keyvalcfgmap       #&lt;--------------here
        key: key2

"
53550367,gitlab-runner on a kubernetes cluster error while creating mount source path '/usr/share/ca-certificates/mozilla',"i'm trying to get gitlab-runner ""run"" on a kubernetes cluster, after following the official doc -> https://docs.gitlab.com/runner/install/kubernetes.html (using kubernetes executor) i'm getting an error once i deploy:


  error: failed to start container ""gitlab-runner"": error response from
  daemon: error while creating mount source path
  '/usr/share/ca-certificates/mozilla': mkdir
  /usr/share/ca-certificates/mozilla: read-only file system


i'm using the examples in that web and can't figure out why isn't allowing to create that dir (as i understand the default user is root)

here my config-map.yaml:

apiversion: v1
kind: configmap
metadata:
  name: gitlab-runner
  namespace: gitlab
data:
  config.toml: |
    concurrent = 1
    [[runners]]
      name = ""kubernetes runner""
      url = ""url""
      token = ""token""
      executor = ""kubernetes""
      [runners.kubernetes]
        namespace = ""gitlab""


and this is the deployment.yaml

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: gitlab-runner
  namespace: gitlab
spec:
  replicas: 1
  selector:
    matchlabels:
      name: gitlab-runner
  template:
    metadata:
      labels:
        name: gitlab-runner
    spec:
      containers:
        - args:
            - run
          image: gitlab/gitlab-runner:alpine-v11.5.0
          imagepullpolicy: always
          name: gitlab-runner
          volumemounts:
            - mountpath: /etc/gitlab-runner
              name: config
            - mountpath: /etc/ssl/certs
              name: cacerts
              readonly: true
      restartpolicy: always
      volumes:
        - configmap:
            name: gitlab-runner
          name: config
        - hostpath:
            path: /usr/share/ca-certificates/mozilla
          name: cacerts


here is the complete list of events initializing the pod:

events:
  type     reason                 age                from                                                          message
  ----     ------                 ----               ----                                                          -------
  normal   scheduled              29s                default-scheduler                                             successfully assigned gitlab-runner-5b689c7cbc-hw6r5 to gke-my-project-dev-default-pool-0d32b263-6skk
  normal   successfulmountvolume  29s                kubelet, gke-my-project-dev-default-pool-0d32b263-6skk  mountvolume.setup succeeded for volume ""cacerts""
  normal   successfulmountvolume  29s                kubelet, gke-my-project-dev-default-pool-0d32b263-6skk  mountvolume.setup succeeded for volume ""config""
  normal   successfulmountvolume  29s                kubelet, gke-my-project-dev-default-pool-0d32b263-6skk  mountvolume.setup succeeded for volume ""default-token-6hr2h""
  normal   pulling                23s (x2 over 28s)  kubelet, gke-my-project-dev-default-pool-0d32b263-6skk  pulling image ""gitlab/gitlab-runner:alpine-v11.5.0""
  normal   pulled                 19s (x2 over 24s)  kubelet, gke-my-project-dev-default-pool-0d32b263-6skk  successfully pulled image ""gitlab/gitlab-runner:alpine-v11.5.0""
  normal   created                19s (x2 over 24s)  kubelet, gke-my-project-dev-default-pool-0d32b263-6skk  created container
  warning  failed                 19s (x2 over 24s)  kubelet, gke-my-project-dev-default-pool-0d32b263-6skk  error: failed to start container ""gitlab-runner"": error response from daemon: error while creating mount source path '/usr/share/ca-certificates/mozilla': mkdir /usr/share/ca-certificates/mozilla: read-only file system
  warning  backoff                14s                kubelet, gke-my-project-dev-default-pool-0d32b263-6skk  back-off restarting failed container


any clue will be appreciated

thanks
",<kubernetes><gitlab-ci-runner><google-kubernetes-engine>,53575784,1,"finally, i got it working here what i use to register and run the gitlab-runner on gke

configmap:

apiversion: v1
kind: configmap
metadata:
  name: gitlab-runner-cm
  namespace: gitlab
data:
  config.toml: |
    concurrent = 4
    check_interval = 30
  entrypoint: |
    #!/bin/bash

    set -xe
    cp /scripts/config.toml /etc/gitlab-runner/

    # register the runner
    /entrypoint register --non-interactive \
      --url $gitlab_url \
      --tag-list ""kubernetes, my_project"" \
      --kubernetes-image ""alpine:latest"" \
      --kubernetes-namespace ""gitlab"" \
      --executor kubernetes \
      --config ""/etc/gitlab-runner/config.toml"" \
      --locked=false \
      --run-untagged=true \
      --description ""my project - kubernetes runner"" \
      --kubernetes-privileged

    # start the runner
    /entrypoint run --user=gitlab-runner \
      --working-directory=/home/gitlab-runner \
      --config ""/etc/gitlab-runner/config.toml""


deployment:

apiversion: extensions/v1beta1
kind: deployment
metadata:
  name: gitlab-runner
  namespace: gitlab
spec:
  replicas: 1
  selector:
    matchlabels:
      app: gitlab-runner
  template:
    metadata:
      labels:
        app: gitlab-runner
    spec:
      containers:
        - name: gitlab-runner
          image: gitlab/gitlab-runner:latest
          command: [""/bin/bash"", ""/scripts/entrypoint""]
          env:
            - name: gitlab_url
              value: ""url""
            - name: registration_token
              value: ""token""
            - name: kubernetes_namespace
              value: gitlab
            - name: google_application_credentials
              value: /var/secrets/google/key.json
          imagepullpolicy: always
          volumemounts:
            - name: config
              mountpath: /scripts
            - name: google-cloud-key
              mountpath: /var/secrets/google
      restartpolicy: always
      volumes:
        - name: config
          configmap:
            name: gitlab-runner-cm
        - name: google-cloud-key
          secret:
            secretname: gitlab-runner-sa


and autoscaling:

apiversion: autoscaling/v2beta1
kind: horizontalpodautoscaler
metadata:
  name: gitlab-runner-hpa
  namespace: gitlab
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: gitlab-runner
  minreplicas: 1
  maxreplicas: 3
  metrics:
    - type: resource
      resource:
        name: cpu
        targetaverageutilization: 50


i hope this helps someone trying to run a gitlab runner in a kubernetes cluster on google kubernetes engine
"
63230929,share data between local and docker container in kubernetes,"i was successfully able to share data between a docker container and host using
docker run -it -v /path/to/host/folder:/container/path image-name

now i am trying to run this docker image through a kubernetes cronjob every minute for which my yaml file is as follows :
apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: automation
spec:
  schedule: &quot;*/1 * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          containers:
          - name: automation
            image: localhost:32000/image-name:registry  
          restartpolicy: onfailure

but here how do i share data between my local and k8s so as to basically replicate the -v /path/to/host/folder:/container/path functionality from the docker run command  ?
what should i add to my yaml file ?
please help.
",<docker><kubernetes><kubernetes-cronjob><microk8s>,63231077,1,"if you're just playing with one node and need to map a volume from this node to a pod running in the same node, then you need to use a hostpath volume.
in summary, your code will look like this :
apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: automation
spec:
  schedule: &quot;*/1 * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          containers:
          - name: automation
            image: localhost:32000/image-name:registry  
            volumemounts:
            - mountpath: /container/path
              name: test-volume
          restartpolicy: onfailure
          volumes:
          - name: test-volume
            hostpath:
              # directory location on host
              path: /path/to/host/folder
              # this field is optional
              type: directory


warning, this will work only if you just have a one node cluster.
if you have a multi-node cluster, then you need to havec a look at distributed storage solution and how to use them with kubernetes.
here is the doc about volumes in k8s
"
53378787,rbac not working as expected when trying to lock namespace,"i'm trying to lock down a namespace in kubernetes using rbac so i followed this tutorial.
i'm working on a baremetal cluster (no minikube, no cloud provider) and installed kubernetes using ansible.

i created the folowing namespace :

apiversion: v1
kind: namespace
metadata:
  name: lockdown


service account :

apiversion: v1
kind: serviceaccount
metadata:
  name: sa-lockdown
  namespace: lockdown


role :

kind: role
apiversion: rbac.authorization.k8s.io/v1
metadata:
  name: lockdown
rules:
- apigroups: [""""] # """" indicates the core api group
  resources: [""""]
  verbs: [""""]


rolebinding :

apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: rb-lockdown
subjects:
- kind: serviceaccount
  name: sa-lockdown
roleref:
  kind: role
  name: lockdown
  apigroup: rbac.authorization.k8s.io


and finally i tested the authorization using the next command

kubectl auth can-i get pods --namespace lockdown --as system:serviceaccount:lockdown:sa-lockdown


this should be returning ""no"" but i got ""yes"" :-(  

what am i doing wrong ?
thx
",<kubernetes><roles><kubectl><rbac>,53432669,1,"i finally found what was the problem.  

the role and rolebinding must be created inside the targeted namespace.  

i changed the following role and rolebinding types by specifying the namespace inside the yaml directly.

kind: role
apiversion: rbac.authorization.k8s.io/v1
metadata:
  name: lockdown
  namespace: lockdown
rules:
- apigroups:
  - """"
  resources:
  - pods
  verbs:
  - get
  - watch
  - list
---
apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: rb-lockdown
  namespace: lockdown
subjects:
- kind: serviceaccount
  name: sa-lockdown
roleref:
  kind: role
  name: lockdown
  apigroup: rbac.authorization.k8s.io


in this example i gave permission to the user sa-lockdown to get, watch and list the pods in the namespace lockdown.



now if i ask to get the pods : kubectl auth can-i get pods --namespace lockdown --as system:serviceaccount:lockdown:sa-lockdown it will return yes.  

on the contrary if ask to get the deployments : kubectl auth can-i get deployments --namespace lockdown --as system:serviceaccount:lockdown:sa-lockdown it will return no.



you can also leave the files like they were in the question and simply create them using kubectl create -f &lt;file&gt; -n lockdown.
"
53289455,how do i add a service and traefik ingress to an eks cluster?,"notes

i am trying to deploy a service and ingress for a demo service (from 'kubernetes in action') to an aws eks cluster in which the traefik ingress controller has been helm installed.

i am able to access the traefik dashboard from the traefik.example.com hostname after manually adding the ip address of the aws elb provisioned by traefik to that hostname in my local /etc/hosts file.

if i describe the service and ingress of the traefik-dashboard:

$ kubectl describe svc -n kube-system traefik-dashboard
name:              traefik-dashboard
namespace:         kube-system
labels:            app=traefik
                   chart=traefik-1.52.6
                   heritage=tiller
                   release=traefik
annotations:       &lt;none&gt;
selector:          app=traefik,release=traefik
type:              clusterip
ip:                10.100.164.81
port:              &lt;unset&gt;  80/tcp
targetport:        8080/tcp
endpoints:         172.31.27.70:8080
session affinity:  none
events:            &lt;none&gt;

$ kubectl describe ing -n kube-system traefik-dashboard
name:             traefik-dashboard
namespace:        kube-system
address:
default backend:  default-http-backend:80 (&lt;none&gt;)
rules:
host                 path  backends
----                 ----  --------
traefik.example.com
                        traefik-dashboard:80 (172.31.27.70:8080)
annotations:
events:  &lt;none&gt;


the service and ingress controller seem to be using the running traefik-575cc584fb-v4mfn pod in the kube-system namespace.

given this info and looking at the traefik docs, i try to expose a demo service through its ingress with the following yaml:

apiversion: apps/v1beta2
kind: replicaset
metadata:
name: kubia
spec:
replicas: 3
selector:
    matchlabels:
    app: kubia
template:
    metadata:
    labels:
        app: kubia
    spec:
    containers:
    - name: kubia
        image: luksa/kubia

---

apiversion: v1
kind: service
metadata:
name: kubia
namespace: default
spec:
selector:
    app: traefik
    release: traefik
ports:
- name: web
    port: 80
    targetport: 8080

---

apiversion: extensions/v1beta1
kind: ingress
metadata:
name: kubia
namespace: default
spec:
rules:
- host: kubia.int
    http:
    paths:
    - path: /
        backend:
        servicename: kubia
        serviceport: web


after applying this, i am unable to access the kubia service from the kubia.int hostname after manually adding the ip address of the aws elb provisioned by traefik to that hostname in my local /etc/hosts file.  instead, i get a service unavailable in the response.  describing the created resources shows some differing info.

$ kubectl describe svc kubia
name:              kubia
namespace:         default
labels:            &lt;none&gt;
annotations:       kubectl.kubernetes.io/last-applied-configuration:
                    {""apiversion"":""v1"",""kind"":""service"",""metadata"":{""annotations"":{},""name"":""kubia"",""namespace"":""default""},""spec"":{""ports"":[{""name"":""web"",""por...
selector:          app=traefik,release=traefik
type:              clusterip
ip:                10.100.142.243
port:              web  80/tcp
targetport:        8080/tcp
endpoints:         &lt;none&gt;
session affinity:  none
events:            &lt;none&gt;

$ kubectl describe ing kubia
name:             kubia
namespace:        default
address:
default backend:  default-http-backend:80 (&lt;none&gt;)
rules:
host       path  backends
----       ----  --------
kubia.int
            /   kubia:web (&lt;none&gt;)
annotations:
kubectl.kubernetes.io/last-applied-configuration:  {""apiversion"":""extensions/v1beta1"",""kind"":""ingress"",""metadata"":{""annotations"":{},""name"":""kubia"",""namespace"":""default""},""spec"":{""rules"":[{""host"":""kubia.int"",""http"":{""paths"":[{""backend"":{""servicename"":""kubia"",""serviceport"":""web""},""path"":""/""}]}}]}}

events:  &lt;none&gt;


i also notice that the demo kubia service has no endpoints, and the corresponding ingress shows no available backends.

another thing i notice is that the demo kubia service and ingress is in the default namespace, while the traefik-dashboard service and ingress are in the kube-system namespace.

does anything jump out to anyone?  any suggestions on the best way to diagnose it?

many thanks in advance!
",<kubernetes><traefik-ingress><amazon-eks>,53290428,1,"it would seem that you are missing the kubernetes.io/ingress.class: traefik that tells your traefik ingress controller to serve for that ingress definition.

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: kubia
  namespace: default
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
rules:
- host: kubia.int
    http:
    paths:
    - path: /
        backend:
        servicename: kubia
        serviceport: web


if you look at the examples in the docs you can see that the only ingress that doesn't have annotation is traefik-web-ui that points to the traefik web ui.
"
52507029,kubernetes nginx ingress configuration not working for grafana,"i am new to configuring ingress rules for my kubernetes cluster.

my kubernetes cluster is deployed on bare metal. no cloud.

i followed this link to set up my nginx-controller with rbac in my cluster.

this is what i have deployed :

# kubectl get all -n ingress-nginx
name                                           ready     status    restarts   age
pod/default-http-backend-7c5bc89cc9-ks6kd      1/1       running   0          2h
pod/nginx-ingress-controller-5b6864749-8xbhf   1/1       running   0          2h

name                           type        cluster-ip     external-ip   port(s)                      age
service/default-http-backend   clusterip   10.233.15.56   &lt;none&gt;        80/tcp                       2h
service/ingress-nginx          nodeport    10.233.38.84   &lt;none&gt;        80:31118/tcp,443:32003/tcp   2h

name                                       desired   current   up-to-date   available   age
deployment.apps/default-http-backend       1         1         1            1           2h
deployment.apps/nginx-ingress-controller   1         1         1            1           2h

name                                                 desired   current   ready     age
replicaset.apps/default-http-backend-7c5bc89cc9      1         1         1         2h
replicaset.apps/nginx-ingress-controller-5b6864749   1         1         1         2h


given that i have my setup, i want to access my grafana dashboard using a url.

my grafana setup is working perfectly fine.

# kubectl get all -n default
name                           ready     status    restarts   age
pod/grafana-67c6585fbd-4jl7p   1/1       running   0          2h

name                 type        cluster-ip     external-ip   port(s)          age
service/grafana      nodeport    10.233.5.111   &lt;none&gt;        3000:32093/tcp   2h


name                      desired   current   up-to-date   available   age
deployment.apps/grafana   1         1         1            1           2h

name                                 desired   current   ready     age
replicaset.apps/grafana-67c6585fbd   1         1         1         2h


i can access the dashboard using http://10.27.239.145:32093 which is the ip of one of my k8s worker nodes.

now rather than accessing via ip:nodeport, i want to access via url e.g. grafana.test.mydomain.com

so the ingress rule that i configured in my default namespace is :

apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
  creationtimestamp: 2018-09-25t20:32:24z
  generation: 5
  name: grafana
  namespace: default
  resourceversion: ""28485""
  selflink: /apis/extensions/v1beta1/namespaces/default/ingresses/jenkins-tls
  uid: 1c51cece-c102-11e8-bf0f-02000a1bef39
spec:
  rules:
  - host: grafana.test.mydomain.com
    http:
      paths:
      - backend:
          servicename: grafana
          serviceport: 3000
        path: /


on my local laptop from where i am testing, ive added to my /etc/hosts the following entry :

10.27.239.145 grafana.test.mydomain.com


and in my browser, i am trying to access http://grafana.test.mydomain.com but i only get this site cant be reached
grafana.test.mydomain.com refused to connect.

i have a strong feeling that i am missing out on something but can't figure it out.

i changed the nodeport to clusterip but no luck.

i know that my ingress controller is working since everytime i make a change to my ingress rules, i get logs from my ingress controller.

i0925 21:00:19.041440       9 event.go:221] event(v1.objectreference{kind:""ingress"", namespace:""default"", name:""grafana"", uid:""1c51cece-c102-11e8-bf0f-02000a1bef39"", apiversion:""extensions/v1beta1"", resourceversion:""28485"", fieldpath:""""}): type: 'normal' reason: 'update' ingress default/grafana
i0925 21:00:19.041732       9 controller.go:171] configuration changes detected, backend reload required.
i0925 21:00:19.216044       9 controller.go:187] backend successfully reloaded.
i0925 21:00:19.217645       9 controller.go:204] dynamic reconfiguration succeeded.


any help will strongly be appreciated regarding what might i have missed.
",<kubernetes><kubernetes-ingress><nginx-ingress>,52508588,1,"from what i see, you need to set grafana.test.mydomain.com to point to 10.233.38.84. 

basically, your nginx controller service is directing the traffic to your ingress and then your ingress forwards it to the backend on the nodeport (this is implicit in the ingress). it works for me, but i'm using an aws elb, i basically set grafana.test.mydomain.com to point to aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-0000000000.us-west-2.elb.amazonaws.com

$ kubectl get all -n ingress-nginx
name                                            ready     status    restarts   age
pod/default-http-backend-6586bc58b6-snxbv       1/1       running   0          1h
pod/grafana-5b969bb7f9-tsv5k                    1/1       running   0          52m
pod/nginx-ingress-controller-6bd7c597cb-lfwcf   1/1       running   0          1h
pod/prometheus-server-5dbf9f4fc9-mnwn4          1/1       running   0          53m

name                           type           cluster-ip       external-ip                                                               port(s)                      age
service/default-http-backend   clusterip      10.x.x.x         &lt;none&gt;                                                                    80/tcp                       1h
service/grafana                nodeport       10.x.x.x         &lt;none&gt;                                                                    3000:30073/tcp               52m
service/ingress-nginx          loadbalancer   10.x.x.x         aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-0000000000.us-west-2.elb.amazonaws.com   80:30276/tcp,443:32011/tcp   1h
service/prometheus-server      nodeport       10.x.x.x         &lt;none&gt;                                                                    9090:32419/tcp               53m

name                                       desired   current   up-to-date   available   age
deployment.apps/default-http-backend       1         1         1            1           1h
deployment.apps/grafana                    1         1         1            1           52m
deployment.apps/nginx-ingress-controller   1         1         1            1           1h
deployment.apps/prometheus-server          1         1         1            1           53m

name                                                  desired   current   ready     age
replicaset.apps/default-http-backend-6586bc58b6       1         1         1         1h
replicaset.apps/grafana-5b969bb7f9                    1         1         1         52m
replicaset.apps/nginx-ingress-controller-6bd7c597cb   1         1         1         1h
replicaset.apps/prometheus-server-5dbf9f4fc9          1         1         1         53m

$ kubectl describe ingress grafana-ingress -n ingress-nginx
name:             grafana-ingress
namespace:        ingress-nginx
address:          aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-0000000000.us-west-2.elb.amazonaws.com
default backend:  default-http-backend:80 (&lt;none&gt;)
rules:
  host                       path  backends
  ----                       ----  --------
  grafana.test.mydomain.com
                             /   grafana:3000 (&lt;none&gt;)
annotations:
  kubectl.kubernetes.io/last-applied-configuration:  {""apiversion"":""extensions/v1beta1"",""kind"":""ingress"",""metadata"":{""annotations"":{""nginx.ingress.kubernetes.io/rewrite-target"":""/""},""name"":""grafana-ingress"",""namespace"":""ingress-nginx""},""spec"":{""rules"":[{""http"":{""paths"":[{""backend"":{""servicename"":""grafana"",""serviceport"":3000},""path"":""/""}]}}]}}

  nginx.ingress.kubernetes.io/rewrite-target:  /
events:
  type    reason  age                from                      message
  ----    ------  ----               ----                      -------
  normal  create  40m                nginx-ingress-controller  ingress ingress-nginx/grafana-ingress
  normal  update  22m (x2 over 40m)  nginx-ingress-controller  ingress ingress-nginx/grafana-ingress

"
52679250,i can't access https urls served on google cloud kubernetes,"all, 

i followed this tutorial: https://github.com/ahmetb/gke-letsencrypt.
i have an ingress setup for kubernetes in google cloud, i have a static ip address and the secrets are created.
this is my ingress config:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: helloweb
  annotations:
    kubernetes.io/ingress.global-static-ip-name: helloweb-ip
    certmanager.k8s.io/acme-http01-edit-in-place: ""true""
  labels:
    app: hello
spec:
  backend:
    servicename: helloweb-backend
    serviceport: 8080
  tls:
  - secretname: dogs-com-tls
    hosts:
    - app-solidair-vlaanderen.com


i can access http://app-solidair-vlaanderen.com, but not the https url.

if i call describe ingress i get this output:

name:             helloweb
namespace:        default
address:          35.190.68.173
default backend:  helloweb-backend:8080 (10.32.0.17:8080)
tls:
  dogs-com-tls terminates app-solidair-vlaanderen.com
rules:
  host                         path  backends
  ----                         ----  --------
  app-solidair-vlaanderen.com
                               /.well-known/acme-challenge/q8kcfsz0zujo58xzyvbk6s-cjiwu-egwpcdd8nfyoxq   cm-acme-http-solver-mhqnf:8089 (&lt;none&gt;)
annotations:
  url-map:          k8s-um-default-helloweb--17a833239f9491d9
  backends:         {""k8s-be-30819--17a833239f9491d9"":""unknown"",""k8s-be-32482--17a833239f9491d9"":""healthy""}
  forwarding-rule:  k8s-fw-default-helloweb--17a833239f9491d9
  target-proxy:     k8s-tp-default-helloweb--17a833239f9491d9
events:
  type     reason  age                from                     message
  ----     ------  ----               ----                     -------
  normal   add     45m                loadbalancer-controller  default/helloweb
  normal   create  44m                loadbalancer-controller  ip: 35.190.68.173
  warning  sync    7m (x22 over 28m)  loadbalancer-controller  error during sync: error while evaluating the ingress spec: could not find service ""default/cm-acme-http-solver-mhqnf""


does someone know what i'm missing?
",<kubernetes><google-kubernetes-engine><kubernetes-ingress><cert-manager>,52686221,1,"you have some mess up in your ingress definition, why the hosts is under the tls?
here is an example that is working for me:

apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: {{ .values.ingressname }}-ingress
annotations:
  kubernetes.io/ingress.global-static-ip-name: {{ .values.staticipname }}-static-ip
  kubernetes.io/ingress.allow-http: ""false""
labels:
  ...
spec:
  tls:
  - secretname: sslcerts
  rules:
  - host: {{ .values.restapihost }}
  http:
    paths:
    - backend:
        servicename: rest-api-internal-service
        serviceport: 80

"
69297768,"amazon-efs failed, reason given by server: no such file or directory","apiversion: v1
kind: persistentvolume
metadata:
    name: ****-pv-public
    namespace: ****
spec:
    storageclassname: efs-sc
    capacity:
        storage: 3gi
    accessmodes:
        - readwritemany
    persistentvolumereclaimpolicy: retain
    csi:
        driver: efs.csi.aws.com
        volumehandle: fs-***
        volumeattributes:
            path: /***/public

mounting arguments: -t efs -o tls fs-2f974c54:/****/public /var/lib/kubelet/pods/9784d80e-4678-4b0b-96ae-a5cccf7db7a0/volumes/kubernetes.io~csi/******/mount
output: could not start amazon-efs-mount-watchdog, unrecognized init system &quot;aws-efs-csi-dri&quot;
b'mount.nfs4: mounting 127.0.0.1:/****/public failed, reason given by server: no such file or directory'

",<kubernetes><amazon-eks><amazon-efs>,69316111,1,"here, how i fixed it
first, create an access point
apiversion: v1
    kind: persistentvolume
    metadata:
        name: **-pv-public
        namespace: laravel-test
    spec:
        storageclassname: efs-sc
        capacity:
            storage: 3gi
        accessmodes:
            - readwritemany
        persistentvolumereclaimpolicy: retain
        csi:
            driver: efs.csi.aws.com
            volumehandle: fs-**::fsap-***

and fs-::fsap-* (::) not (:)
"
52944989,kubernetes routing to specific pod in function of a param in url,"the need i have just looks like this stuff :

apiversion: apps/v1beta1
kind: statefulset
metadata:
  name: http
spec:
  servicename: ""nginx-set""
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: gcr.io/google_containers/nginx-slim:0.8
        ports:
        - containerport: 80
          name: http
----
apiversion: v1
kind: service
metadata:
  name: nginx-set
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: http
  clusterip: none
  selector:
    app: nginx


here is the interesting part :

apiversion: voyager.appscode.com/v1beta1
kind: ingress
metadata:
  name: test-ingress
  namespace: default
spec:
  rules:
  - host: appscode.example.com
    http:
      paths:
      - path: '/testpath'
        backend:
          hostnames:
          - web-0
          servicename: nginx-set #! there is no extra service. this
          serviceport: '80'      # is the statefulset's headless service


i'm able to target a specific pod because of setting the hostname in function of the url.

now i'd like to know if it's possible in kubernetes to create a rule like that 

apiversion: voyager.appscode.com/v1beta1
kind: ingress
metadata:
  name: test-ingress
  namespace: default
spec:
  rules:
  - host: appscode.example.com
    http:
      paths:
      - path: '/connect/(\d+)'
        backend:
          hostnames:
          - web-(result of regex match with \d+)
          servicename: nginx-set #! there is no extra service. this
          serviceport: '80'      # is the statefulset's headless service


or if i have to wrote a rule for each pod ?
",<kubernetes><kubernetes-ingress>,52946733,1,"sorry that isn't possible, the best solution is to create multiple paths, each one referencing one pod:

apiversion: voyager.appscode.com/v1beta1
kind: ingress
metadata:
  name: test-ingress
  namespace: default
spec:
  rules:
  - host: appscode.example.com
    http:
      paths:
      - path: '/connect/0'
        backend:
          hostnames:
          - web-0
          servicename: nginx-set
          serviceport: '80'
      - path: '/connect/1'
        backend:
          hostnames:
          - web-1
          servicename: nginx-set
          serviceport: '80'

"
52822610,which apiversion to use with k8s jobs and imagepullsecret,"i have a job that runs migration for a python service. here is the job spec:

apiversion: batch/v1
kind: job
metadata:
  name: migration
  annotations:
    buildid: ""__buildid__""
    branchname: ""__branchname__""
    commitid: ""__commitid__""
spec:
  template:
    spec:
      containers:
      - name: service
        image: &lt;repo&gt;/service:__buildid__
        imagepullpolicy: always
        imagepullsecrets:
        - name: acr-key
        command: [""/bin/sh"",""-c""]
        args: [""python manage.py migrate --noinput --database=default &amp;&amp; python manage.py migrate --noinput --database=data_001 &amp;&amp; python manage.py migrate --noinput --database=data_002""]
        envfrom:
        - configmapref:
            name: configuration
        - secretref:
            name: secrets
        resources:
          requests:
            memory: ""200mi""
            cpu: ""250m""
          limits:
            memory: ""4000mi""
            cpu: ""2000m""
      restartpolicy: never


it doesn't look like there is an apiversion that supports both, imagepullsecrets and kubernetes job. any ideas on how can i get this to work?

here's my k8s configuration:

client version: version.info{major:""1"", minor:""9"", gitversion:""v1.9.6"", gitcommit:""9f8ebd171479bec0ada837d7ee641dec2f8c6dd1"", gittreestate:""clean"", builddate:""2018-03-21t15:21:50z"", goversion:""go1.9.3"", compiler:""gc"", platform:""darwin/amd64""}
server version: version.info{major:""1"", minor:""9"", gitversion:""v1.9.6"", gitcommit:""9f8ebd171479bec0ada837d7ee641dec2f8c6dd1"", gittreestate:""clean"", builddate:""2018-03-21t15:13:31z"", goversion:""go1.9.3"", compiler:""gc"", platform:""linux/amd64""}

",<kubernetes><kubectl><azure-aks>,52823118,1,"imagepullsecrets should be outside of the containers scope. this works for me:

apiversion: batch/v1
kind: job
metadata:
  name: migration
  annotations:
    buildid: ""__buildid__""
    branchname: ""__branchname__""
    commitid: ""__commitid__""
spec:
  template:
    spec:
      imagepullsecrets:
      - name: acr-key
      containers:
      - name: service
        image: &lt;repo&gt;/service:__buildid__
        imagepullpolicy: always
        command: [""/bin/sh"",""-c""]
        args: [""python manage.py migrate --noinput --database=default &amp;&amp; python manage.py migrate --noinput --database=data_001 &amp;&amp; python manage.py migrate --noinput --database=data_002""]
        envfrom:
        - configmapref:
            name: configuration
        - secretref:
            name: secrets
        resources:
          requests:
            memory: ""200mi""
            cpu: ""250m""
          limits:
            memory: ""4000mi""
            cpu: ""2000m""
      restartpolicy: never

"
52854985,helm appears to parse my chart differently depending on if i use --dry-run --debug?,"so i was deploying a new cronjob today and got the following error:

error: release acs-export-cronjob failed: cronjob.batch ""acs-export-cronjob"" is invalid: [spec.jobtemplate.spec.template.spec.containers: required value, spec.jobtemplate.spec.template.spec.restartpolicy: unsupported value: ""always"": supported values: ""onfailure"", ""never""]


here's some output from running helm on the same chart, no changes made, but with the --debug --dry-run flags:

 name:   acs-export-cronjob
revision: 1
released: wed oct 17 14:12:02 2018
chart: generic-job-0.1.0
user-supplied values:
applicationname: users
command: publishallforrealm
image: &lt;censored&gt;.amazonaws.com/sonic/acs-export:latest
jobappargs: """"
jobvmargs: """"
jobgroup: acs-export-jobs
name: acs-export-cronjob
schedule: 0 * * * *

computed values:
applicationname: users
command: publishallforrealm
image: &lt;censored&gt;.amazonaws.com/sonic/acs-export:latest
jobappargs: """"
jobvmargs: """"
jobgroup: acs-export-jobs
name: acs-export-cronjob
resources:
cpu: 100m
memory: 1gi
schedule: 0 * * * *
sonicnodegroup: api
springprofiles: export-job

hooks:
manifest:

---
# source: generic-job/templates/rbac.yaml
apiversion: v1
kind: serviceaccount
metadata:
name: acs-export-cronjob-sa
---
# source: generic-job/templates/rbac.yaml
kind: role
apiversion: rbac.authorization.k8s.io/v1beta1
metadata:
name: acs-export-cronjob-manager
rules:
- apigroups: [""extensions""]
resources: [""deployments""]
verbs: [""get""]
---
# source: generic-job/templates/rbac.yaml
kind: rolebinding
apiversion: rbac.authorization.k8s.io/v1beta1
metadata:
name: acs-export-cronjob-binding
subjects:
- kind: serviceaccount
name: acs-export-cronjob-sa
roleref:
kind: role
name: acs-export-cronjob-manager
apigroup: rbac.authorization.k8s.io
---
# source: generic-job/templates/generic-job.yaml
apiversion: batch/v1beta1
kind: cronjob
metadata:
name: acs-export-cronjob
labels:
    app: generic-job
    chart: ""generic-job-0.1.0""
    release: ""acs-export-cronjob""
    heritage: ""tiller""
spec:
schedule: 0 * * * *
successfuljobshistorylimit: 5
failedjobshistorylimit: 5
concurrencypolicy: forbid
startingdeadlineseconds: 120
jobtemplate:
    spec:
    metadata:
        name: acs-export-cronjob
        labels:
        jobgroup: acs-export-jobs
        app: generic-job
        chart: ""generic-job-0.1.0""
        release: ""acs-export-cronjob""
        heritage: ""tiller""
    spec:
        template:
        metadata:
            labels:
            jobgroup: acs-export-jobs
            app: generic-job
            chart: ""generic-job-0.1.0""
            release: ""acs-export-cronjob""
            heritage: ""tiller""
            annotations:
            iam.amazonaws.com/role: arn:aws:iam::&lt;censored&gt;:role/k8s-service-role
        spec:
            restartpolicy: never   #&lt;----------this is not 'always'!!
            serviceaccountname: acs-export-cronjob-sa
            tolerations:
            - key: sonic-node-group
            operator: equal
            value: api
            effect: noschedule
            nodeselector:
            sonic-node-group: api
            volumes:
            - name: config
            emptydir: {}
            initcontainers:
            - name: ""get-users-vmargs-from-deployment""
            image: &lt;censored&gt;.amazonaws.com/utils/kubectl-helm:latest
            command: [""sh"", ""-c"", ""kubectl -n eu1-test get deployment users-vertxapp -o jsonpath=\""{..spec.containers[0].env[?(@.name=='app_specific_vm_args')].value}\"" &gt; /config/users-vmargs &amp;&amp; cat /config/users-vmargs""]
            volumemounts:
            - mountpath: /config
                name: config
            - name: ""get-users-yaml-appconfig-from-deployment""
            image: &lt;censored&gt;.amazonaws.com/utils/kubectl-helm:latest
            command: [""sh"", ""-c"", ""kubectl -n eu1-test get deployment users-vertxapp -o jsonpath=\""{..spec.containers[0].env[?(@.name=='app_yaml_config')].value}\"" &gt; /config/users-appconfig &amp;&amp; cat /config/users-appconfig""]
            volumemounts:
            - mountpath: /config
                name: config
            containers:     #&lt;--------this field is not missing!
            - image: &lt;censored&gt;.amazonaws.com/sonic/acs-export:latest
            imagepullpolicy: always
            name: ""users-batch""
            command:
            - ""bash""
            - ""-c""
            - 'app_specific_vm_args=""$(cat /config/users-vmargs) "" app_yaml_config=""$(cat /config/users-appconfig)"" /vertx-app/startvertx.sh'
            env:
            - name: frenv
                value: ""batch""
            - name: stackname
                value: eu1-test
            - name: spring_profiles
                value: ""export-job""
            - name: namespace
                valuefrom:
                fieldref:
                    fieldpath: metadata.namespace
            volumemounts:
            - mountpath: /config
                name: config
            resources:
                limit:
                cpu: 100m
                memory: 1gi


if you paid attention, you may have noticed line 101 (i added the comment afterwards) in the debug-output, which sets restartpolicy to never, quite the opposite of always as the error message claims it to be.

you may also have noticed line 126 (again, i added the comment after the fact) of the debug output, where the mandatory field containers is specified, again, much in contradiction to the error-message.

whats going on here?
",<kubernetes><kubernetes-helm>,52870655,1,"hah! found it! it was a simple mistake actually. i had an extra spec:metadata section under jobtemplate which was duplicated. removing one of the dupes fixed my issues.

i really wish the error-messages of helm would be more helpful. 

the corrected chart looks like:

 name:   acs-export-cronjob
revision: 1
released: wed oct 17 14:12:02 2018
chart: generic-job-0.1.0
user-supplied values:
applicationname: users
command: publishallforrealm
image: &lt;censored&gt;.amazonaws.com/sonic/acs-export:latest
jobappargs: """"
jobvmargs: """"
jobgroup: acs-export-jobs
name: acs-export-cronjob
schedule: 0 * * * *

computed values:
applicationname: users
command: publishallforrealm
image: &lt;censored&gt;.amazonaws.com/sonic/acs-export:latest
jobappargs: """"
jobvmargs: """"
jobgroup: acs-export-jobs
name: acs-export-cronjob
resources:
cpu: 100m
memory: 1gi
schedule: 0 * * * *
sonicnodegroup: api
springprofiles: export-job

hooks:
manifest:

---
# source: generic-job/templates/rbac.yaml
apiversion: v1
kind: serviceaccount
metadata:
name: acs-export-cronjob-sa
---
# source: generic-job/templates/rbac.yaml
kind: role
apiversion: rbac.authorization.k8s.io/v1beta1
metadata:
name: acs-export-cronjob-manager
rules:
- apigroups: [""extensions""]
resources: [""deployments""]
verbs: [""get""]
---
# source: generic-job/templates/rbac.yaml
kind: rolebinding
apiversion: rbac.authorization.k8s.io/v1beta1
metadata:
name: acs-export-cronjob-binding
subjects:
- kind: serviceaccount
name: acs-export-cronjob-sa
roleref:
kind: role
name: acs-export-cronjob-manager
apigroup: rbac.authorization.k8s.io
---
# source: generic-job/templates/generic-job.yaml
apiversion: batch/v1beta1
kind: cronjob
metadata:
name: acs-export-cronjob
labels:
    app: generic-job
    chart: ""generic-job-0.1.0""
    release: ""acs-export-cronjob""
    heritage: ""tiller""
spec:
schedule: 0 * * * *
successfuljobshistorylimit: 5
failedjobshistorylimit: 5
concurrencypolicy: forbid
startingdeadlineseconds: 120
jobtemplate:
   spec:
      template:
         metadata:
            labels:
            jobgroup: acs-export-jobs
            app: generic-job
            chart: ""generic-job-0.1.0""
            release: ""acs-export-cronjob""
            heritage: ""tiller""
            annotations:
            iam.amazonaws.com/role: arn:aws:iam::&lt;censored&gt;:role/k8s-service-role
        spec:
            restartpolicy: never   
            serviceaccountname: acs-export-cronjob-sa
            tolerations:
            - key: sonic-node-group
            operator: equal
            value: api
            effect: noschedule
            nodeselector:
            sonic-node-group: api
            volumes:
            - name: config
            emptydir: {}
            initcontainers:
            - name: ""get-users-vmargs-from-deployment""
            image: &lt;censored&gt;.amazonaws.com/utils/kubectl-helm:latest
            command: [""sh"", ""-c"", ""kubectl -n eu1-test get deployment users-vertxapp -o jsonpath=\""{..spec.containers[0].env[?(@.name=='app_specific_vm_args')].value}\"" &gt; /config/users-vmargs &amp;&amp; cat /config/users-vmargs""]
            volumemounts:
            - mountpath: /config
                name: config
            - name: ""get-users-yaml-appconfig-from-deployment""
            image: &lt;censored&gt;.amazonaws.com/utils/kubectl-helm:latest
            command: [""sh"", ""-c"", ""kubectl -n eu1-test get deployment users-vertxapp -o jsonpath=\""{..spec.containers[0].env[?(@.name=='app_yaml_config')].value}\"" &gt; /config/users-appconfig &amp;&amp; cat /config/users-appconfig""]
            volumemounts:
            - mountpath: /config
                name: config
            containers:     
            - image: &lt;censored&gt;.amazonaws.com/sonic/acs-export:latest
            imagepullpolicy: always
            name: ""users-batch""
            command:
            - ""bash""
            - ""-c""
            - 'app_specific_vm_args=""$(cat /config/users-vmargs) "" app_yaml_config=""$(cat /config/users-appconfig)"" /vertx-app/startvertx.sh'
            env:
            - name: frenv
                value: ""batch""
            - name: stackname
                value: eu1-test
            - name: spring_profiles
                value: ""export-job""
            - name: namespace
                valuefrom:
                fieldref:
                    fieldpath: metadata.namespace
            volumemounts:
            - mountpath: /config
                name: config
            resources:
                limit:
                cpu: 100m
                memory: 1gi

"
69126927,nginx ingress returning 404 when accessing the services,"i have setup k8s cluster on aws. i have followed the nginx ingress setup using the link - ingress-setup. i then tried to deploy a coffee application using the link - demo-application and accessing the coffee application, i am getting a 404 error. i am getting a 200 ok response when accessing the curl http://localhost:8080/coffee from within the pod. i am not sure how to troubleshoot this issue.
[ec2-user@ip-172-31-37-241 service]$ curl -vv --resolve cafe.example.com:$ic_https_po rt:$ic_ip https://cafe.example.com:$ic_https_port/coffee --insecure
* added cafe.example.com:443:65.1.245.71 to dns cache
* hostname cafe.example.com was found in dns cache
*   trying 65.1.245.71:443...
* connected to cafe.example.com (65.1.245.71) port 443 (#0)
* alpn, offering h2
* alpn, offering http/1.1
* cipher selection: all:!export:!export40:!export56:!anull:!low:!rc4:@strength
* successfully set certificate verify locations:
*  cafile: /etc/pki/tls/certs/ca-bundle.crt
*  capath: none
* tlsv1.2 (out), tls header, certificate status (22):
* tlsv1.2 (out), tls handshake, client hello (1):
* tlsv1.2 (in), tls handshake, server hello (2):
* tlsv1.2 (in), tls handshake, certificate (11):
* tlsv1.2 (in), tls handshake, server key exchange (12):
* tlsv1.2 (in), tls handshake, server finished (14):
* tlsv1.2 (out), tls handshake, client key exchange (16):
* tlsv1.2 (out), tls change cipher, change cipher spec (1):
* tlsv1.2 (out), tls handshake, finished (20):
* tlsv1.2 (in), tls change cipher, change cipher spec (1):
* tlsv1.2 (in), tls handshake, finished (20):
* ssl connection using tlsv1.2 / ecdhe-rsa-aes256-gcm-sha384
* alpn, server accepted to use http/1.1
* server certificate:
*  subject: cn=nginxingresscontroller
*  start date: sep 12 18:03:35 2018 gmt
*  expire date: sep 11 18:03:35 2023 gmt
*  issuer: cn=nginxingresscontroller
*  ssl certificate verify result: self signed certificate (18), continuing anyway.
&gt; get /coffee http/1.1
&gt; host: cafe.example.com
&gt; user-agent: curl/7.76.1
&gt; accept: */*
&gt;
* mark bundle as not supporting multiuse
&lt; http/1.1 404 not found
&lt; server: nginx/1.21.0
&lt; date: fri, 10 sep 2021 03:24:23 gmt
&lt; content-type: text/html
&lt; content-length: 153
&lt; connection: keep-alive
&lt;
&lt;html&gt;
&lt;head&gt;&lt;title&gt;404 not found&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;404 not found&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx/1.21.0&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
* connection #0 to host cafe.example.com left intact

ingress definition:
apiversion: v1
kind: service
metadata:
  name: nginx-ingress
  namespace: nginx-ingress
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: &quot;tcp&quot;
    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: &quot;*&quot;
spec:
  type: loadbalancer
  ports:
  - port: 80
    targetport: 80
    protocol: tcp
    name: http
  - port: 443
    targetport: 443
    protocol: tcp
    name: https
  selector:
    app: nginx-ingress

successful response when accessing the pods directly
[ec2-user@ip-172-31-37-241 service]$ kubectl get pods
name                      ready   status    restarts   age
coffee-6f4b79b975-b5ph5   1/1     running   0          29m
coffee-6f4b79b975-grzh5   1/1     running   0          29m
tea-6fb46d899f-5hskc      1/1     running   0          29m
tea-6fb46d899f-bzp88      1/1     running   0          29m
tea-6fb46d899f-plq6j      1/1     running   0          29m
[ec2-user@ip-172-31-37-241 service]$ kubectl exec -it coffee-6f4b79b975-b5ph5 /bin/sh kubectl exec [pod] [command] is deprecated and will be removed in a future version. use kubectl exec [pod] -- [command] instead.
/ $ curl -vv http://localhost:8080/coffee
*   trying 127.0.0.1:8080...
* connected to localhost (127.0.0.1) port 8080 (#0)
&gt; get /coffee http/1.1
&gt; host: localhost:8080
&gt; user-agent: curl/7.78.0
&gt; accept: */*
&gt;
* mark bundle as not supporting multiuse
&lt; http/1.1 200 ok
&lt; server: nginx/1.21.3
&lt; date: fri, 10 sep 2021 03:29:08 gmt
&lt; content-type: text/plain
&lt; content-length: 159
&lt; connection: keep-alive
&lt; expires: fri, 10 sep 2021 03:29:07 gmt
&lt; cache-control: no-cache
&lt;
server address: 127.0.0.1:8080
server name: coffee-6f4b79b975-b5ph5
date: 10/sep/2021:03:29:08 +0000
uri: /coffee
request id: e7fbd46fde0c34df3d1eac64a36e0192
* connection #0 to host localhost left intact

",<kubernetes><kubernetes-ingress><nginx-ingress><kubernetes-service>,69664499,1,"your application is listening on port 8080. in your service file you need to use the targetport as 8080.
apiversion: v1
kind: service
metadata:
  name: nginx-ingress
  namespace: nginx-ingress
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: &quot;tcp&quot;
    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: &quot;*&quot;  
spec:
  type: loadbalancer
  ports:
  - port: 80
    targetport: 8080
    protocol: tcp
    name: http
  - port: 443
    targetport: 8080
    protocol: tcp
    name: https
  selector:
    app: nginx-ingress

"
61643319,two ingress controller on same k8s cluster,"i have installed the following two different ingress controllers on my digitalocean managed k8s cluster: 


nginx   
istio  


and they have been assigned to two different ip addresses. my question is, if it is wrong to have two different ingress controllers on the same k8s cluster?  

the reason, why i have done it, because nginx is for tools like harbor, argocd, etc. and istio for microservices.   

i have also figured out, when both are installed alongside each other, sometimes during the deployment, the k8s suddenly goes down.

for example, i have deployed: 

apiversion: v1
kind: service
metadata:
  name: hello-kubernetes-first
  namespace: dev
spec:
  type: clusterip
  ports:
    - port: 80
      targetport: 8080
  selector:
    app: hello-kubernetes-first
---
apiversion: apps/v1
kind: deployment
metadata:
  name: hello-kubernetes-first
  namespace: dev
spec:
  replicas: 3
  selector:
    matchlabels:
      app: hello-kubernetes-first
  template:
    metadata:
      labels:
        app: hello-kubernetes-first
    spec:
      containers:
        - name: hello-kubernetes
          image: paulbouwer/hello-kubernetes:1.7
          ports:
            - containerport: 8080
          env:
            - name: message
              value: hello from the first deployment!
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: istio
  name: helloworld-ingress
  namespace: dev
spec:
  rules:
    - host: hello.service.databaker.io
      http:
        paths:
          - path: /*
            backend:
              servicename: hello-kubernetes-first
              serviceport: 80
---


then i've got: 

error from server (internalerror): error when creating ""istio-app.yml"": internal error occurred: failed calling webhook ""validate.nginx.ingress.kubernetes.io"": post https://ingress-nginx-controller-admission.nginx.svc:443/extensions/v1beta1/ingresses?timeout=30s: dial tcp 10.245.107.175:443: i/o timeout  

",<nginx><kubernetes><kubernetes-ingress><istio>,62814939,1,"you have raised several points - before answering your question, let's take a step back.

k8s ingress not recommended by istio
it is important to note how istio does not recommend using k8s ingress:

using the istio gateway, rather than ingress, is recommended to make use of the full feature set that istio offers, such as rich traffic management and security features.

ref: https://istio.io/latest/docs/tasks/traffic-management/ingress/kubernetes-ingress/
as noted, istio gateway (istio ingressgateway and egressgateway) acts as the edge, which you can find more in https://istio.io/latest/docs/tasks/traffic-management/ingress/ingress-control/.

multiple endpoints within istio
if you need to assign one public endpoint for business requirement, and another for monitoring (such as argo cd, harbor as you mentioned), you can achieve that by using istio only. there are roughly 2 approaches to this.

create separate istio ingressgateways - one for main traffic, and another for monitoring
create one istio ingressgateway, and use gateway definition to handle multiple access patterns

both are valid, and depending on requirements, you may need to choose one way or the other.
as to the approach #2., it is where istio's traffic management system shines. it is a great example of istio's power, but the setup is slightly complex if you are new to it. so here goes an example.
example of approach #2
when you create istio ingressgateway by following the default installation, it would create istio-ingressgateway like below (i overly simplified yaml definition):
apiversion: v1
kind: service
metadata:
  labels:
    app: istio-ingressgateway
    istio: ingressgateway
  name: istio-ingressgateway
  namespace: istio-system
  # ... other attributes ...
spec:
  type: loadbalancer
  # ... other attributes ...

this lb service would then be your endpoint. (i'm not familiar with digitalocean k8s env, but i suppose they would handle lb creation.)
then, you can create gateway definition like below:
apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: your-gateway
  namespace: istio-system
spec:
  selector:
    app: istio-ingressgateway
    istio: ingressgateway
  servers:
    - port:
        number: 3000
        name: https-your-system
        protocol: https
      hosts:
        - &quot;your-business-domain.com&quot;
        - &quot;*.monitoring-domain.com&quot;
      # ... other attributes ...

you can then create 2 or more virtualservice definitions.
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: business-virtsvc
spec:
  gateways:
    - istio-ingressgateway.istio-system.svc.cluster.local
  hosts:
    - &quot;your-business-domain.com&quot;
  http:
    - match:
        - port: 3000
      route:
        - destination:
            host: some-business-pod
            port:
              number: 3000
    # ... other attributes ...

apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: monitoring-virtsvc
spec:
  gateways:
    - istio-ingressgateway.istio-system.svc.cluster.local
  hosts:
    - &quot;harbor.monitoring-domain.com&quot;
  http:
    - match:
        - port: 3000
      route:
        - destination:
            host: harbor-pod
            port:
              number: 3000
    # ... other attributes ...

note: the above is assuming a lot of things, such as port mapping, traffic handling, etc.. please check out the official doc for details.

so, back to the question after long detour:
question: [is it] wrong to have two different ingress controllers on the same k8s cluster[?]
i believe it is ok, though this can cause an error like you are seeing, as two ingress controller fight for the k8s ingress resource.
as mentioned above, if you are using istio, it's better to stick with istio ingressgateway instead of k8s ingress. if you need k8s ingress for some specific reason, you could use other ingress controller for k8s ingress, like nginx.
as to the error you saw, it's coming from nginx deployed webhook, that ingress-nginx-controller-admission.nginx.svc is not available. this means you have created a k8s ingress helloworld-ingress with kubernetes.io/ingress.class: istio annotation, but nginx webhook is interfering with k8s ingress handling. the webhook is then failing to handle the resource, as the pod / svc responsible for webhook traffic is not found.
the error itself just says something is unhealthy in k8s - potentially not enough node allocated to the cluster, and thus pod allocation not happening. it's also good to note that istio does require some cpu and memory footprint, which may be putting more pressure to the cluster.
"
71203737,how to add api key confidentially inside config map?,"i am new to k8s. i am trying to deploy a datadog agent to my cluster to collect logs and this happens through a configmap.
apiversion: v1
kind: configmap
metadata:
  name: fluent-cm
data:
  fluent-bit.conf: |
    [service]
        parsers_file     parsers.conf
    
    [input]
        name              tail
        path              /tmp/app.log
        parser            nginx

    [output]
        name           datadog
        match          *
        host           http-intake.logs.datadoghq.com
        tls            off
        apikey         &lt;api key to be used confidentially&gt;
        dd_service     abcd
        dd_source      abcd
        dd_tags        env:dev

  parsers.conf: |
    [parser]
        name   nginx
        format regex
        regex ^(?&lt;remote&gt;[^ ]*) (?&lt;host&gt;[^ ]*) (?&lt;user&gt;[^ ]*) \[(?&lt;time&gt;[^\]]*)\] &quot;(?&lt;method&gt;\s+)(?: +(?&lt;path&gt;[^\&quot;]*?)(?: +\s*)?)?&quot; (?&lt;code&gt;[^ ]*) (?&lt;size&gt;[^ ]*)(?: &quot;(?&lt;referer&gt;[^\&quot;]*))&quot; &quot;(?&lt;agent&gt;[^\&quot;]*)&quot;(?: &quot;(?&lt;target&gt;[^\&quot;]*))&quot;$
        time_key time
        time_format %d/%b/%y:%h:%m:%s %z

i do not want to add the api key as plain text. can i do it using secrets? please suggest ways to achieve this.
p.s: i need to use the config map in order to add the datadog agent, and hence the config map cannot be replaced.
",<kubernetes><datadog><configmap><kubernetes-secrets>,71205006,1,"in theory you could secret as a environment variable, and pass it to a pod using configmap.
apiversion: v1
kind: secret
metadata:
  name: fluentbit-secret-test
type: opaque
data:
  apikey: &lt;base64 encoded api key&gt;

apiversion: v1
kind: configmap
metadata:
  name: fluent-cm
data:
  fluent-bit.conf: |
...
        apikey         $fluentbit_api_key
...

apiversion: v1
kind: pod
metadata:
  name: secret-env-test
spec:
  containers:
  - name: test
    image: nginx
    env:
      - name: fluentbit_api_key
        valuefrom:
          secretkeyref:
            name: fluentbit-secret-test
            key: apikey


however, you must remember, that kubernetes secrets are not encrypted by default, only base64 encoded. anyone with api access can retrieve or modify a secret, and so can anyone with access to etcd
"
62008203,how to run a command on persistentvolume creation?,"i have a statefulset which looks like this

apiversion: v1
kind: statefulset
metadata:
  name: web
spec:
  ...
  volumeclaimtemplates:
   metadata:
      name: www
    spec:
      resources:
        requests:
          storage: 1gi


it will create a persistentvolumeclaim (pvc) and a persistentvolume (pv) for each pod of a service it controls. 

i want to execute some commands on those pvs before the pod creation.

i was thinking to create a job which mounts those pvs and runs the commands but how do i know how many of pvs were created?

is there a kubernetes-native solution to trigger some pod execution on pv creation?
",<kubernetes><persistent-volumes><kubernetes-statefulset>,62008270,1,"the solution is initcontianer.

you can add it to a spec of your statufulset:

apiversion: apps/v1
kind: statefulset
metadata:
  name:  web
spec:
...
  spec:
    initcontainers:
    - name: init-myapp
      image: ubuntu:latest
      command:
      - bash
      - ""-c""
      - ""your command""
      volumemounts:
      - name: yourvolume
        mountpath: /mnt/myvolume

"
71122425,"error validating data: [validationerror(cronjob.spec.jobtemplate.spec.template.spec): unknown field ""container"" in io.k8s.api.core.v1.podspec,","this is my yaml file that i am trying to use for cronjob creation. i am getting error like unknown field &quot;container&quot; in io.k8s.api.core.v1.podspec,
apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: abc-service-cron-job
spec:
  schedule: &quot;* * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          container:
          - name: abc-service-cron-job
            image: docker.repo1.xyz.com/hui-services/abc-application/replace_me
            imagepullpolicy: always
            command:
            - /bin/sh
            - -c
            - date; echo hello from the kubernetes cluster
          restartpolicy: onfailure  

",<kubernetes><kubernetes-cronjob>,71122612,1,"apiversion: batch/v1beta1
kind: cronjob
metadata:
  ...
spec:
  ...
  jobtemplate:
    spec:
      template:
        spec:
          containers:  # &lt;-- you have spelling error here, should be &quot;containers&quot;
          ...

"
71095684,set environment variable in kubernetes secret,"when using kubernetes .yml files, i can do the following:
$ cat configmap.yml

apiversion: v1
kind: configmap
metadata:
  name: my-configmap
data:
  foo: ${foo}
  bar: ${bar}
  static: doesnotchange

$ export foo=myfooval
$ export bar=mybarval
$ cat configmap.yml | envsubst | kubectl apply -f -

this would replace ${foo} and ${bar} in the configmap.yml file before actually applying the file to the cluster.
how could i achieve the very same behavior with a kubernetes secret which has it's data values base64 encoded?
i would need to read all the keys in the data: field, decode the values, apply the environment variables and encode it again.
a tool to decode and encode the data: values inplace would be much appreciated.
",<kubernetes><kubernetes-secrets>,71095787,1,"it is actually possible, to store the secret.yml with stringdata instead of data which allows to keep the files in plain text (sops encryption is still possible and encouraged)
$ cat secret.yml

apiversion: v1
kind: secret
metadata:
  name: test-secret
  namespace: default
type: opaque
stringdata:
  dotenv: |
    database_url=&quot;postgresql://test:test@localhost:5432/test?schema=public&quot;
    api_port=${port}
    foo=${foo}
    bar=${bar}

$ export port=80
$ export foo=myfoovalue
$ export bar=mybarvalue
$ cat secret.yml | envsubst | kubectl apply -f -

a plus is for sure, that this not only allows for creation of the secret, but updating is also possible.
just for documentation, here would be the full call with sops:
$ sops --decrypt secret.enc.yml | envsubst | kubectl apply -f -

"
71446454,command line arguments for container in kubernetes,"i'm trying to deploy a docker container to my kubernetes cluster, but i'm running into an issue with passing the required command-line arguments to the container. i need to pass two arguments called --provider local and --basedir /tmp. here is what the docker run command looks like (i can run this without any issues on my docker host):
docker run -d -p 8080:8080 --name transfer-sh -v /tmp:/tmp dutchcoders/transfer.sh:latest --provider local --basedir /tmp

however, when i apply the deployment yaml to my cluster the container fails with this error (i'm running kubectl apply -f deploy.yaml to apply my changes to the cluster):

incorrect usage. flag provided but not defined: -provider local

so my yaml specifies that the flag should be --provider, but for some reason i haven't been able to find yet the container only sees -provider which is indeed not a valid option. this is the full help message:
name:
transfer.sh - transfer.sh

description:
easy file sharing from the command line

usage:
transfer.sh [flags] command [arguments...]

commands:
version
help, h  shows a list of commands or help for one command

flags:
--listener value                     127.0.0.1:8080 (default: &quot;127.0.0.1:8080&quot;) [$listener]
--profile-listener value             127.0.0.1:6060 [$profile_listener]
--force-https                         [$force_https]
--tls-listener value                 127.0.0.1:8443 [$tls_listener]
--tls-listener-only                   [$tls_listener_only]
--tls-cert-file value                 [$tls_cert_file]
--tls-private-key value               [$tls_private_key]
--temp-path value                    path to temp files (default: &quot;/tmp&quot;) [$temp_path]
--web-path value                     path to static web files [$web_path]
--proxy-path value                   path prefix when service is run behind a proxy [$proxy_path]
--proxy-port value                   port of the proxy when the service is run behind a proxy [$proxy_port]
--email-contact value                email address to link in contact us (front end) [$email_contact]
--ga-key value                       key for google analytics (front end) [$ga_key]
--uservoice-key value                key for user voice (front end) [$uservoice_key]
--provider value                     s3|gdrive|local [$provider]
--s3-endpoint value                   [$s3_endpoint]
--s3-region value                    (default: &quot;eu-west-1&quot;) [$s3_region]
--aws-access-key value                [$aws_access_key]
--aws-secret-key value                [$aws_secret_key]
--bucket value                        [$bucket]
--s3-no-multipart                    disables s3 multipart puts [$s3_no_multipart]
--s3-path-style                      forces path style urls, required for minio. [$s3_path_style]
--gdrive-client-json-filepath value   [$gdrive_client_json_filepath]
--gdrive-local-config-path value      [$gdrive_local_config_path]
--gdrive-chunk-size value            (default: 16) [$gdrive_chunk_size]
--storj-access value                 access for the project [$storj_access]
--storj-bucket value                 bucket to use within the project [$storj_bucket]
--rate-limit value                   requests per minute (default: 0) [$rate_limit]
--purge-days value                   number of days after uploads are purged automatically (default: 0) [$purge_days]
--purge-interval value               interval in hours to run the automatic purge for (default: 0) [$purge_interval]
--max-upload-size value              max limit for upload, in kilobytes (default: 0) [$max_upload_size]
--lets-encrypt-hosts value           host1, host2 [$hosts]
--log value                          /var/log/transfersh.log [$log]
--basedir value                      path to storage [$basedir]
--clamav-host value                  clamav-host [$clamav_host]
--perform-clamav-prescan             perform-clamav-prescan [$perform_clamav_prescan]
--virustotal-key value               virustotal-key [$virustotal_key]
--profiler                           enable profiling [$profiler]
--http-auth-user value               user for http basic auth [$http_auth_user]
--http-auth-pass value               pass for http basic auth [$http_auth_pass]
--ip-whitelist value                 comma separated list of ips allowed to connect to the service [$ip_whitelist]
--ip-blacklist value                 comma separated list of ips not allowed to connect to the service [$ip_blacklist]
--cors-domains value                 comma separated list of domains allowed for cors requests [$cors_domains]
--random-token-length value          (default: 6) [$random_token_length]
--help, -h                           show help

here is the docker hub for the image i'm trying to deploy: dutchcoders/transfer.sh
here is the github: https://github.com/dutchcoders/transfer.sh
my cluster's version is 1.23.4 and the full deployment yaml is here:
apiversion: apps/v1
kind: deployment
metadata:
  name: transfer-sh
  namespace: transfer-sh
  labels:
    app: &quot;transfer-sh&quot;
spec:
  replicas: 1
  selector:
    matchlabels:
      app: transfer-sh
  template:
    metadata:
      labels:
        app: transfer-sh
    spec:
      containers:
      - name: transfer-sh
        image: dutchcoders/transfer.sh:latest
        args:
        - &quot;--provider local&quot;
        - &quot;--basedir /tmp&quot;
        ports:
        - containerport: 8080

i intentionally have not included any persistent volume claims yet. at this point i'm just testing to make sure the container will run.
initially, i though maybe it was some sort of escape sequence issue. after trying all manner of ways to potentially escape the two dashes nothing really changed. i also tried setting environment variables that contained those arguments, but that just resulted in the same behavior where --profile turned into -profile.
if anyone has any thoughts i could use the help. i'm a bit stuck at the moment (although still troubleshooting). i am curious if maybe there is a different way to pass in command flags as opposed to arguments (or maybe there isn't any difference as far as k8s is concerned).
",<kubernetes><kubernetes-deployment>,71446568,1,"try:
apiversion: apps/v1
kind: deployment
metadata:
  name: transfer-sh
  namespace: transfer-sh
  labels:
    app: transfer-sh
spec:
  replicas: 1
  selector:
    matchlabels:
      app: transfer-sh
  template:
    metadata:
      labels:
        app: transfer-sh
    spec:
      containers:
      - name: transfer-sh
        image: dutchcoders/transfer.sh:latest
        args:  # &lt;-- in this case each arg is individual
        - --provider
        - local
        - --basedir
        - /tmp
        ports:
        - containerport: 8080


name          ready   up-to-date   available   age
transfer-sh   1/1     1            1           91s

"
62668639,kubernetes: error converting yaml to json: yaml: line 12: did not find expected key,"i'm trying to add ciao to my kubernetes single node cluster and every time i run the kubectl apply -f command, i keep running into the error &quot; error converting yaml to json: yaml: line 12: did not find expected key &quot;. i looked at the other solutions but they were no help. any help will be appreciated.
kind: namespace
metadata:
  name: monitoring
---
apiversion: v1
kind: secret
metadata:
  name: ciao
  namespace: monitoring
data:
  basic_auth_username: ywrtaw4=
  basic_auth_password: cgfzc3dvcmq=
---
apiversion: apps/v1
kind: deployment
metadata:
  name: ciao
  namespace: monitoring
spec:
  replicas: 1
  template:
  metadata:
  selector:
      labels:
        app: ciao
    spec:
      containers:
      - image: brotandgames/ciao:latest
        imagepullpolicy: ifnotpresent
        name: ciao
        volumemounts: # emit if you do not have persistent volumes
        - mountpath: /app/db/sqlite/
          name: persistent-volume
          subpath: ciao
        ports:
        - containerport: 3000
        resources:
          requests:
            memory: 256mi
            cpu: 200m
          limits:
            memory: 512mi
            cpu: 400m
        envfrom:
        - secretref:
            name: ciao
---
apiversion: v1
kind: service
metadata:
  name: ciao
  namespace: monitoring
spec:
  ports:
    - port: 80
      targetport: 3000
      protocol: tcp
  type: nodeport
  selector:
    app: ciao

",<kubernetes><compiler-errors><kubectl>,62668780,1,"looks there's an indentation in your deployment definition. this should work:
apiversion: apps/v1
kind: deployment
metadata:
  name: ciao
  namespace: monitoring
  labels:
    app: ciao
spec:
  replicas: 1
  selector:
    matchlabels:
      app: ciao
  template:
    metadata:
      labels:
        app: ciao
    spec:
      containers:
      - image: brotandgames/ciao:latest
        imagepullpolicy: ifnotpresent
        name: ciao
        volumemounts: # emit if you do not have persistent volumes
        - mountpath: /app/db/sqlite/
          name: persistent-volume
          subpath: ciao
        ports:
        - containerport: 3000
        resources:
          requests:
            memory: 256mi
            cpu: 200m
          limits:
            memory: 512mi
            cpu: 400m
        envfrom:
        - secretref:
            name: ciao

keep in mind that in this definition the pv persistent-volume needs to exist in your cluster/namespace.
"
71579764,add zip / binary file to configmap,"i am trying to add a zip file to our configmap due to the amount of files exceeding the 1mb limit. i deploy our charts with helm and was looking into binarydata but cannot get it to work properly. i wanted to see if anyone had any suggestions on how i could integrate this with helm so when the job is finished it deletes the configmap with it
here is my configmap:
apiversion: v1
kind: configmap
metadata:
  name: {{ template &quot;db-migration.fullname&quot; . }}
  labels:
    app: {{ template &quot;db-migration.name&quot; . }}
    chart: {{ template &quot;db-migration.chart&quot; . }}
    draft: {{ .values.draft | default &quot;draft-app&quot; }}
    release: {{ .release.name }}
    heritage: {{ .release.service }}
binarydata:
  {{ .files.get &quot;migrations.zip&quot; | b64enc }}
immutable: true

---

apiversion: v1
kind: configmap
metadata:
  name: {{ template &quot;db-migration.fullname&quot; . }}-test
  labels:
    app: {{ template &quot;db-migration.name&quot; . }}
    chart: {{ template &quot;db-migration.chart&quot; . }}
    draft: {{ .values.draft | default &quot;draft-app&quot; }}
    release: {{ .release.name }}
    heritage: {{ .release.service }}
binarydata:
  {{ .files.get &quot;test.zip&quot; | b64enc }}
immutable: true


the two zip files live inside the charts and i have a command to unzip them and then run the migration afterwards
",<kubernetes><kubernetes-helm>,71582943,1,"binarydata exepcts a map but you are passing a string to it.
when debugging the template we can see
error: installation failed: unable to build kubernetes objects from release manifest: error validating &quot;&quot;: error validating data: validationerror(configmap.binarydata): invalid type for io.k8s.api.core.v1.configmap.binarydata: got &quot;string&quot;, expected &quot;map&quot;

the way to fix this is to add a key before {{ .files.get &quot;test.zip&quot; | b64enc }}.
apiversion: v1
kind: configmap
metadata:
  name: {{ template &quot;db-migration.fullname&quot; . }}
  labels:
    app: {{ template &quot;db-migration.name&quot; . }}
    chart: {{ template &quot;db-migration.chart&quot; . }}
    draft: {{ .values.draft | default &quot;draft-app&quot; }}
    release: {{ .release.name }}
    heritage: {{ .release.service }}
binarydata:
  migrations: {{ .files.get &quot;migrations.zip&quot; | b64enc }}
immutable: true

---

apiversion: v1
kind: configmap
metadata:
  name: {{ template &quot;db-migration.fullname&quot; . }}-test
  labels:
    app: {{ template &quot;db-migration.name&quot; . }}
    chart: {{ template &quot;db-migration.chart&quot; . }}
    draft: {{ .values.draft | default &quot;draft-app&quot; }}
    release: {{ .release.name }}
    heritage: {{ .release.service }}
binarydata:
  test: {{ .files.get &quot;test.zip&quot; | b64enc }}
immutable: true

"
62773671,routing requests to services based on uri prefix in istio vs,"this is what i want to achieve:
route traffic to a service based on the uri prefix
problem i'm facing:
unable to segregate the prefix from the context path
explanation:
i want to route traffic to a services based on the prefix.
say, /dev/service/context/path/  and /test/service/context/path/ .
but i'm not able to do so without changing the context path of the application itself.
is there a way i can segregate the prefix part of uri from the context path of application?
this is what my vs looks like:
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: servicea
  namespace: dev
spec:
  hosts:
  - &quot;*&quot;
  gateways:
  - dev-gateway
  http:
  - match:
    - uri:
        prefix: /dev
    route:
    - destination:
        port:
          number: 8080
        host: servicea

thanks
",<kubernetes><kubernetes-ingress><istio><contextpath>,62775623,1,"not sure if i understand your question correctly. i guess you can just add a rewrite rule like this:
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: servicea
  namespace: dev
spec:
  hosts:
  - &quot;*&quot;
  gateways:
  - dev-gateway
  http:
  - match:
    - uri:
        prefix: /dev
    rewrite:
       uri: /
    route:
    - destination:
        port:
          number: 8080
        host: servicea

by this your traffic for /dev/service/context/path/ becomes /service/context/path/.
"
62743356,how can i isolate pods in namespace using networkpolicy without disabling external traffic to kubernetes pods,"i am trying to isolate my pods in namespace from other namespaces. i have tried to create a networkpolicy:
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: deny-from-other-namespaces
spec:
  podselector:
    matchlabels:
  ingress:
  - from:
    - podselector: {}

this networkpolicy successfully isolating pods in my namespace from another namespace. but this policy, once applied, disables all external traffic to these pods. is there any method for only block traffic from other namespaces and allow all external traffic to the pods.
",<kubernetes><kubernetes-ingress><kubernetes-networkpolicy><kubernetes-networking>,62746293,1,"using a kubernetes networkpolicy i don't believe its possible to deny communication between pods while allowing all external traffic.  this is because the kubernetes networkpolicy resource doesn't have a concept of explicit deny rules.  i would either adjust your approach or consider another network policy that has deny rules (such as calico).
solution:
apiversion: projectcalico.org/v3
kind: networkpolicy
metadata:
  name: deny-other-namespaces
  namespace: prod
spec:
  selector: all()
  types:
  - ingress
  - egress
  ingress:
  - action: deny
    protocol: tcp
    source:
      namespaceselector: name == 'dev'
  - action: allow
  egress:
  - action: allow

"
62498518,how to read env property file in the config map,"i have a property file in chart/properties folder. for example chart/properties/dev is the file
and the contents of it looks like the below
var1=somevalue1
var2=somevalue2


var3=somepwd=

var4=http://someurl.company.com


some of the value strings in property file have an =. there are also some empty lines in the property file.
and chart/configmap.yaml looks like below
apiversion: v1
kind: configmap
metadata:
  name: env-configmap
  namespace: {{ .release.namespace }}
data:
{{ range .files.lines &quot;properties&quot;/.values.env.required.environment }}
  {{ . | replace &quot;=&quot; &quot;: &quot; }}
{{ end }}

generated yaml file:
---
# source: app/templates/configmap.yaml
apiversion: v1
kind: configmap
metadata:
  name: env-configmap
  namespace: default
data:

  var1: somevalue1

  var2: somevalue2
  
  var3: somepwd:

  var4: http://someurl.company.com

the generated output property entries are missing double quote in the value, as a result deployment complains of it when the value strings contain special characters.
i'm expecting the configmap.yaml data block to be a proper yaml (indent 2) like file with the above changes. with above changes, there are extra lines after each property entry in yaml file. i got this to work partially when there are no blank lines and no value strings with =. need help to get this working correctly.
expected yaml file:
---
# source: app/templates/configmap.yaml
apiversion: v1
kind: configmap
metadata:
  name: env-configmap
  namespace: default
data:
  var1: &quot;somevalue1&quot;
  var2: &quot;somevalue2&quot;
  var3: &quot;somepwd=&quot;
  var4: &quot;http://someurl.company.com&quot;

",<kubernetes><kubernetes-helm><kubernetes-pod>,62501265,1,"you can follow go template syntax to do that. i update config.yaml like following works
apiversion: v1
kind: configmap
metadata:
  name: env-configmap
  namespace: {{ .release.namespace }}
data:
  {{ range .files.lines &quot;properties&quot;/.values.env.required.environment }}
    {{- if ne . &quot;&quot; -}}
    {{- $parts := splitn &quot;=&quot; 2 . -}} # details about split function http://masterminds.github.io/sprig/string_slice.html
    {{ $parts._0 }}: {{ $parts._1 | quote }}
    {{end}}
  {{ end }}

"
63000327,istio ingress gateway logs,"we have set up istio, and we are using istio ingress gateway for inbound traffic. we have set up tls for tcp port. sample code can be found here.
we also enabled logs by following this istio guide
we tested the tls connection using openssl and it works fine.
however, when we try to connect from an application, the tls negotiation fails. i have provided more details with wireshark here
we would like to get logs from istio on the tls negotiation ... and find why it fails.
istio gateway yaml
apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: dremio-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: http
    tls:
      httpsredirect: true
    hosts:
    - testdomain.net
  - port:
      number: 443
      name: https
      protocol: https
    tls:
      mode: simple
      credentialname: testdomain-credentials
    hosts:
    - testdomain.net
  - port:
      number: 31020
      name: odbc-dremio-tls
      protocol: tls
    tls:
      mode: simple
      minprotocolversion: tlsv1_0
      maxprotocolversion: tlsv1_3
      credentialname: testdomain-credentials
    hosts:
    - testdomain.net

virtual service
apiversion: networking.istio.io/v1alpha3
kind: virtualservice
metadata:
  name: dremio
spec:
  hosts:
  - testdomain.net
  gateways:
  - dremio-gateway
  http:
  - match:
    - port: 443
    - port: 80
    route:
    - destination:
        host: dremio-client
        port:
          number: 9047
  tcp:
  - match:
    - port: 31020
    route:
    - destination:
        host: dremio-client
        port:
          number: 31010

partial config dump
{
     &quot;name&quot;: &quot;0.0.0.0_31020&quot;,
     &quot;active_state&quot;: {
      &quot;version_info&quot;: &quot;2020-07-21t12:11:49z/9&quot;,
      &quot;listener&quot;: {
       &quot;@type&quot;: &quot;type.googleapis.com/envoy.api.v2.listener&quot;,
       &quot;name&quot;: &quot;0.0.0.0_31020&quot;,
       &quot;address&quot;: {
        &quot;socket_address&quot;: {
         &quot;address&quot;: &quot;0.0.0.0&quot;,
         &quot;port_value&quot;: 31020
        }
       },
       &quot;filter_chains&quot;: [
        {
         &quot;filter_chain_match&quot;: {
          &quot;server_names&quot;: [
           &quot;testdomain.net&quot;
          ]
         },
         &quot;filters&quot;: [
          {
           &quot;name&quot;: &quot;istio.stats&quot;,
           &quot;typed_config&quot;: {
            &quot;@type&quot;: &quot;type.googleapis.com/udpa.type.v1.typedstruct&quot;,
            &quot;type_url&quot;: &quot;type.googleapis.com/envoy.extensions.filters.network.wasm.v3.wasm&quot;,
            &quot;value&quot;: {
             &quot;config&quot;: {
              &quot;root_id&quot;: &quot;stats_outbound&quot;,
              &quot;vm_config&quot;: {
               &quot;vm_id&quot;: &quot;tcp_stats_outbound&quot;,
               &quot;runtime&quot;: &quot;envoy.wasm.runtime.null&quot;,
               &quot;code&quot;: {
                &quot;local&quot;: {
                 &quot;inline_string&quot;: &quot;envoy.wasm.stats&quot;
                }
               }
              },
              &quot;configuration&quot;: &quot;{\n  \&quot;debug\&quot;: \&quot;false\&quot;,\n  \&quot;stat_prefix\&quot;: \&quot;istio\&quot;\n}\n&quot;
             }
            }
           }
          },
          {
           &quot;name&quot;: &quot;envoy.tcp_proxy&quot;,
           &quot;typed_config&quot;: {
            &quot;@type&quot;: &quot;type.googleapis.com/envoy.config.filter.network.tcp_proxy.v2.tcpproxy&quot;,
            &quot;stat_prefix&quot;: &quot;outbound|31010||dremio-client.dremio.svc.cluster.local&quot;,
            &quot;cluster&quot;: &quot;outbound|31010||dremio-client.dremio.svc.cluster.local&quot;,
            &quot;access_log&quot;: [
             {
              &quot;name&quot;: &quot;envoy.file_access_log&quot;,
              &quot;typed_config&quot;: {
               &quot;@type&quot;: &quot;type.googleapis.com/envoy.config.accesslog.v2.fileaccesslog&quot;,
               &quot;path&quot;: &quot;/dev/stdout&quot;,
               &quot;format&quot;: &quot;[%start_time%] \&quot;%req(:method)% %req(x-envoy-original-path?:path)% %protocol%\&quot; %response_code% %response_flags% \&quot;%dynamic_metadata(istio.mixer:status)%\&quot; \&quot;%upstream_transport_failure_reason%\&quot; %bytes_received% %bytes_sent% %duration% %resp(x-envoy-upstream-service-time)% \&quot;%req(x-forwarded-for)%\&quot; \&quot;%req(user-agent)%\&quot; \&quot;%req(x-request-id)%\&quot; \&quot;%req(:authority)%\&quot; \&quot;%upstream_host%\&quot; %upstream_cluster% %upstream_local_address% %downstream_local_address% %downstream_remote_address% %requested_server_name% %route_name%\n&quot;
              }
             }
            ]
           }
          }
         ],
         &quot;transport_socket&quot;: {
          &quot;name&quot;: &quot;envoy.transport_sockets.tls&quot;,
          &quot;typed_config&quot;: {
           &quot;@type&quot;: &quot;type.googleapis.com/envoy.api.v2.auth.downstreamtlscontext&quot;,
           &quot;common_tls_context&quot;: {
            &quot;tls_params&quot;: {
             &quot;tls_minimum_protocol_version&quot;: &quot;tlsv1_0&quot;,
             &quot;tls_maximum_protocol_version&quot;: &quot;tlsv1_3&quot;
            },
            &quot;alpn_protocols&quot;: [
             &quot;h2&quot;,
             &quot;http/1.1&quot;
            ],
            &quot;tls_certificate_sds_secret_configs&quot;: [
             {
              &quot;name&quot;: &quot;testdomain-credentials&quot;,
              &quot;sds_config&quot;: {
               &quot;api_config_source&quot;: {
                &quot;api_type&quot;: &quot;grpc&quot;,
                &quot;grpc_services&quot;: [
                 {
                  &quot;google_grpc&quot;: {
                   &quot;target_uri&quot;: &quot;unix:/var/run/ingress_gateway/sds&quot;,
                   &quot;stat_prefix&quot;: &quot;sdsstat&quot;
                  }
                 }
                ]
               }
              }
             }
            ]
           },
           &quot;require_client_certificate&quot;: false
          }
         }
        }
       ],
       &quot;listener_filters&quot;: [
        {
         &quot;name&quot;: &quot;envoy.listener.tls_inspector&quot;,
         &quot;typed_config&quot;: {
          &quot;@type&quot;: &quot;type.googleapis.com/envoy.config.filter.listener.tls_inspector.v2.tlsinspector&quot;
         }
        }
       ],
       &quot;traffic_direction&quot;: &quot;outbound&quot;
      },
      &quot;last_updated&quot;: &quot;2020-07-21t12:11:50.303z&quot;
     }
    }

by enabling tracing on envoy conn_handler, we can see the following message:

closing connection: no matching filter chain found

",<ssl><kubernetes><kubernetes-ingress><istio><dremio>,63245835,1,"after getting the message of no matching filter chain, i found the filter chain for the port 31020 with the domain that i have provided in my gateway config. it looks like while connecting my application(odbc), the host was not being provided.
the solution is simply to replace the host domain by '*'
apiversion: networking.istio.io/v1alpha3
kind: gateway
metadata:
  name: dremio-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: http
    tls:
      httpsredirect: true
    hosts:
    - testdomain.net
  - port:
      number: 443
      name: https
      protocol: https
    tls:
      mode: simple
      credentialname: testdomain-credentials
    hosts:
    - testdomain.net
  - port:
      number: 31020
      name: odbc-dremio-tls
      protocol: tls
    tls:
      mode: simple
      minprotocolversion: tlsv1_0
      maxprotocolversion: tlsv1_3
      credentialname: testdomain-credentials
    hosts:
    - '*'

"
69842518,multiple ingress-nginx in kubernetes not validating webhook not working,"as stated in the title, i currently have a configuration with 2 ingress-nginx v1.0.0 on gke v1.20.10.
when i deploy one alone the configuration is working and i have no issue, but when i deploy the second one the validatingwebhook and then try to deploy an ingress the 2 validatingwebhook try to evaluate the newly created ingress.
this result in this error:
**error from server (internalerror): error when creating &quot;ingress-example.yaml&quot;: internal error occurred: failed calling webhook &quot;validate.nginx-public.ingress.kubernetes.io&quot;: post &quot;https://ingress-nginx-controller-admission-public.ingress-nginx.svc:443/networking/v1/ingresses?timeout=10s&quot;: x509: certificate is valid for ingress-nginx-controller-admission-private, ingress-nginx-controller-admission-private.ingress-nginx.svc, not ingress-nginx-controller-admission-public.ingress-nginx.svc**

i checked and everything seems to be correctly separated, my validatingwebhook is deployed like that, the {{ ingress_type }} is a placeholder for -public or -private:
---
apiversion: admissionregistration.k8s.io/v1
kind: validatingwebhookconfiguration
metadata:
  labels:
    app.kubernetes.io/name: ingress-nginx{{ ingress_type }}
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/component: admission-webhook
  name: ingress-nginx-admission{{ ingress_type }}
webhooks:
  - name: validate.nginx{{ ingress_type }}.ingress.kubernetes.io
    matchpolicy: equivalent
    objectselector:
      matchlabels:
        ingress-nginx : nginx{{ ingress_type }}
    rules:
      - apigroups:
          - networking.k8s.io
        apiversions:
          - v1
        operations:
          - create
          - update
        resources:
          - ingresses
    failurepolicy: fail
    sideeffects: none
    admissionreviewversions:
      - v1
    clientconfig:
      service:
        namespace: ingress-nginx
        name: ingress-nginx-controller-admission{{ ingress_type }}
        path: /networking/v1/ingresses
---
apiversion: v1
kind: service
metadata:
  labels:
    app.kubernetes.io/name: ingress-nginx{{ ingress_type }}
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/component: controller
  name: ingress-nginx-controller-admission{{ ingress_type }}
spec:
  type: clusterip
  ports:
    - name: https-webhook
      port: 443
      targetport: webhook
      appprotocol: https
  selector:
    app.kubernetes.io/name: ingress-nginx{{ ingress_type }}

i can't seem to find a solution, there is an old github issue on that with no answer, maybe i'm doing something wrong but i just can't see it.
as asked in comment, here is the ingress-example i'm trying to deploy, this works perfectly fine with only one ingress, not with two:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: example-ingress
  annotations:
    kubernetes.io/ingress.class: nginx-private
#    external-dns.alpha.kubernetes.io/target: &quot;ip&quot;
  labels:
    ingress-nginx : nginx-public
spec:
  rules:
    - host: hello.mydomainhere
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: web
                port:
                  number: 8080

",<kubernetes><google-kubernetes-engine><ingress-nginx>,69855781,1,"so for those that may encounter this error.
i tried different things before finding what was wrong. you have to rename all the labels but the version of the ingress-nginx, i did not think that it would break for so little, but it does. in the end i'm using something like this:
---
apiversion: admissionregistration.k8s.io/v1
kind: validatingwebhookconfiguration
metadata:
  labels:
    app.kubernetes.io/name: ingress-nginx{{ ingress_type }}
    app.kubernetes.io/instance: ingress-nginx{{ ingress_type }}
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/component: admission-webhook{{ ingress_type }}
  name: ingress-nginx-admission{{ ingress_type }}
webhooks:
  - name: validate.nginx{{ ingress_type }}.ingress.kubernetes.io
    matchpolicy: equivalent
    objectselector:
      matchlabels:
        ingress-nginx : nginx{{ ingress_type }}
    rules:
      - apigroups:
          - networking.k8s.io
        apiversions:
          - v1
        operations:
          - create
          - update
        resources:
          - ingresses
    failurepolicy: fail
    sideeffects: none
    admissionreviewversions:
      - v1
    clientconfig:
      service:
        namespace: ingress-nginx
        name: ingress-nginx-controller-admission{{ ingress_type }}
        path: /networking/v1/ingresses
---
apiversion: v1
kind: service
metadata:
  labels:
    app.kubernetes.io/name: ingress-nginx{{ ingress_type }}
    app.kubernetes.io/instance: ingress-nginx{{ ingress_type }}
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/component: controller{{ ingress_type }}
  name: ingress-nginx-controller-admission{{ ingress_type }}
spec:
  type: clusterip
  ports:
    - name: https-webhook
      port: 443
      targetport: webhook
      appprotocol: https
  selector:
    app.kubernetes.io/name: ingress-nginx{{ ingress_type }}

i think in this case it's really important to do the same on all the resources.
"
69775529,how do i get kubernetes service to open my django application on a web browser using local host?,"i have been trying to get my kubernetes to launch my web application on a browser through my local host. when i try to open local host it times out and i have tried using minikube service --url  and that also does not work. all of my deployment, and service pods are running. i have also tried port forward and changing the type to nodeport.  i have provided my yaml, docker, and svc code.
apiversion: v1
kind: service
metadata:
  name: mywebsite
spec:
  type: loadbalancer
  selector:
    app: mywebsite
  ports:
  - protocol: tcp
    name: http
    port: 8743
    targetport: 5000


---
apiversion: apps/v1
kind: deployment
metadata:
  name: mywebsite
spec:
  selector:
    matchlabels:
      app: mywebsite
  template:
    metadata:
      labels:
        app: mywebsite
    spec:
      containers:
      - name: mywebsite
        image: mywebsite
        imagepullpolicy: never
        ports:
        - containerport: 5000
        resources:
          requests:
            cpu: 100m
            memory: 250mi
          limits:
            memory: &quot;2gi&quot;
            cpu: &quot;500m&quot;   

# for more information, please refer to https://aka.ms/vscode-dockerpython
from python:3.8-slim-buster

expose 8000
# keeps python from generating .pyc files in the container
env pythondontwritebytecode=1

# turns off buffering for easier container logging
env pythonunbuffered=1

# install pip requirements
copy requirements.txt .
run python -m pip install -r requirements.txt

workdir lapp
copy . .

# creates a non-root user with an explicit uid and adds permission to access the /app folder
# for more info, please refer to https://aka.ms/vscodedocker-pythonconfigure-containers
run adduser -u 5678 --disab1ed-password --gecos &quot;&quot; appuser &amp;&amp; chown -r appuser /app
user appuser

cmd [&quot;python&quot;, &quot;manage.py&quot;, &quot;runserver&quot;, &quot;0.0.0.0:8000&quot;]

name:                     mywebsite
namespace:                default
labels:                   &lt;none&gt;
annotations:              &lt;none&gt;
selector:                 app=mywebsite
type:                     loadbalancer
ip family policy:         singlestack
ip families:              ipv4
ip:                       10.99.161.241
ips:                      10.99.161.241
port:                     http 8743/tcp
targetport:               5000/tcp
nodeport:                 http 32697/tcp
endpoints:                172.17.0.3:5000
session affinity:         none
external traffic policy:  cluster
events:                   &lt;none&gt;

",<docker><kubernetes><yaml><minikube><kubernetes-service>,69777315,1,"it's due to your container running on port 8000
but your service is forwarding the traffic to 5000
apiversion: v1
kind: service
metadata:
  name: mywebsite
spec:
  type: loadbalancer
  selector:
    app: mywebsite
  ports:
  - protocol: tcp
    name: http
    port: 8743
    targetport: **8000**

deployment should be
apiversion: apps/v1
kind: deployment
metadata:
  name: mywebsite
spec:
  selector:
    matchlabels:
      app: mywebsite
  template:
    metadata:
      labels:
        app: mywebsite
    spec:
      containers:
      - name: mywebsite
        image: mywebsite
        imagepullpolicy: never
        ports:
        - containerport: **8000**
        resources:
          requests:
            cpu: 100m
            memory: 250mi
          limits:
            memory: &quot;2gi&quot;
            cpu: &quot;500m&quot;

you need to change the targetport in svc
and containerport in deployment
or else
change the
expose 8000 to 5000 and command to run the application on 5000
cmd [&quot;python&quot;, &quot;manage.py&quot;, &quot;runserver&quot;, &quot;0.0.0.0:5000&quot;]

dont forget to docker build one more time after the above changes.
"
63062336,how can i assign the same rbac role to two different iam roles to access a cluster in eks?,"i would like to give a certain team access to the system:masters group in rbac. my team (awsreservedsso_admin_xxxxxxxxxx in example below) already has it and it works when i only add that one rolearn, but when i apply the configmap below with the additional rolearn, users under the awsreservedsso_dev_xxxxxxxxxxrole still get this error when trying to access the cluster: error: you must be logged in to the server (unauthorized)
(note: we are using aws sso, so the iam roles are assumed):
---
apiversion: v1
kind: configmap
data:
  maproles: |
    - rolearn: arn:aws:iam::xxxxxxxxxxx:role/eks-node-group
      groups:
      - system:bootstrappers
      - system:nodes
      username: system:node:{{ec2privatednsname}}
    - rolearn: arn:aws:iam::xxxxxxxxxxx:role/aws-reserved/sso.amazonaws.com/awsreservedsso_admin_xxxxxxxxxx
      groups:
      - system:masters
      username: admin
    - rolearn: arn:aws:iam::xxxxxxxxxxx:role/aws-reserved/sso.amazonaws.com/awsreservedsso_dev_xxxxxxxxxx
      groups:
        - system:masters
      username: admin
metadata:
  name: aws-auth
  namespace: kube-system

",<kubernetes><yaml><amazon-iam><rbac><amazon-eks>,63848556,1,"thanks rico. when you sign in with sso, you are assuming a role in sts. you can verify this by running aws sts get-caller-identity.
you werew right that that the username wrong but it didn't solve the whole issue.
took a long time but my teammate finally found the solution for this in this guide
the problem was the arn for the iam role:
rolearn: arn:aws:iam::xxxxxxxxxxx:role/aws-reserved/sso.amazonaws.com/awsreservedsso_dev_xxxxxxxxxx

this part aws-reserved/sso.amazonaws.com/ needs to be removed from the name. so in the end combined with rico's suggested username fix:
---
apiversion: v1
kind: configmap
data:
  maproles: |
    - rolearn: arn:aws:iam::xxxxxxxxxxx:role/eks-node-group
      groups:
      - system:bootstrappers
      - system:nodes
      username: system:node:{{ec2privatednsname}}
    - rolearn: arn:aws:iam::xxxxxxxxxxx:role/awsreservedsso_admin_xxxxxxxxxx
      groups:
      - system:masters
      username: admin
    - rolearn: arn:aws:iam::xxxxxxxxxxx:role/awsreservedsso_dev_xxxxxxxxxx
      groups:
        - system:masters
      username: admin2
metadata:
  name: aws-auth
  namespace: kube-system

the issue is finally fixed, and sso users assuming the role can run kubectl commands!
"
63070335,missing write permissions to the following paths: /var/www/html/pub/media,"kubectl -n magento logs magento-install-jssk6

i am getting database found in configmodel.php line 166:missing write permissions to the following paths: /var/www/html/pub/media in install job:
apiversion: batch/v1
kind: job
metadata:
  name: magento-install
  namespace: magento
spec:
  template:
    metadata:
      name: install
      labels:
        app: magento-install
        k8s-app: magento
    spec:
      containers:
      - name: magento-setup
        image: kiweeteam/magento2:vanilla-2.3.4-php7.3-fpm
        command: [&quot;/bin/sh&quot;]
        args:
        - -c
        - |
          /bin/bash &lt;&lt;'eof'
          bin/install.sh
          php bin/magento setup:perf:generate-fixtures setup/performance-toolkit/profiles/ce/small.xml
          magerun index:list | awk '{print $2}' | tail -n+4 | xargs -i{} magerun index:set-mode schedule {}
          magerun cache:flush
          eof
        envfrom:
        - configmapref:
            name: config
        volumemounts:
        - mountpath: /var/www/html/pub/media
          name: media
      volumes:
      - name: media
        persistentvolumeclaim:
          claimname: media
      restartpolicy: onfailure

",<docker><magento><kubernetes><kubernetes-pod>,63122695,1,"
and when i try to change permissions i am getting chown: changing
ownership of '/var/www/html/pub/media': operation not permitted

it happens because you run chown as www-data user and the current owner of this directory is root.
you can resolve your issue by using the init container run as root (user with id 0). below you can see a modified version of your magento-install job with the init cotntainer already added:
apiversion: batch/v1
kind: job
metadata:
  name: magento-install
  namespace: magento
spec:
  template:
    metadata:
      name: install
      labels:
        app: magento-install
        k8s-app: magento
    spec:
      initcontainers:
      - name: magento-chown
        securitycontext:
          runasuser: 0
        image: kiweeteam/magento2:vanilla-2.3.4-php7.3-fpm
        command: ['sh', '-c', 'chown -r www-data:www-data /var/www/html/pub/media']
        volumemounts:
        - name: media
          mountpath: &quot;/var/www/html/pub/media&quot;
      containers:
      - name: magento-setup
        image: kiweeteam/magento2:vanilla-2.3.4-php7.3-fpm
        command: [&quot;/bin/sh&quot;]
        args:
        - -c
        - |
          /bin/bash &lt;&lt;'eof'
          bin/install.sh
          php bin/magento setup:perf:generate-fixtures setup/performance-toolkit/profiles/ce/small.xml
          magerun index:list | awk '{print $2}' | tail -n+4 | xargs -i{} magerun index:set-mode schedule {}
          magerun cache:flush
          eof
        envfrom:
        - configmapref:
            name: config
        volumemounts:
        - mountpath: /var/www/html/pub/media
          name: media
      volumes:
      - name: media
        persistentvolumeclaim:
          claimname: media
      restartpolicy: onfailure

once you attach to your newly created pod by using:
kubectl exec -ti -n magento magento-install-z66qg -- /bin/bash

you'll see that the current owner of the /var/www/html/pub/media directory isn't any more root but www-data user:
www-data@magento-install-z66qg:~/html$ ls -ld /var/www/html/pub/media
drwxr-xr-x 3 www-data www-data 4096 jul 27 18:45 /var/www/html/pub/media

we can simplify it even more. the init container doesn't even need to use the kiweeteam/magento2:vanilla-2.3.4-php7.3-fpm image. it might as well be a simple container based on busybox, which runs as root by default so you can omit the security context from the previous example and your initcontainers section will look as follows:
initcontainers:
- name: magento-chown
  image: busybox
  command: ['sh', '-c', 'chown -r www-data:www-data /var/www/html/pub/media']
  volumemounts:
  - name: media

the final effect will be exactly the same.
"
62865642,"aws eks cluster , dockerhub and traefik, not accessible from internet","kubectl get nodes
name                              status   roles    age   version
ip-192-168-119-55.ec2.internal    ready    &lt;none&gt;   29h   v1.16.12-eks-904af05
ip-192-168-156-180.ec2.internal   ready    &lt;none&gt;   29h   v1.16.12-eks-904af05
ip-192-168-193-177.ec2.internal   ready    &lt;none&gt;   29h   v1.16.12-eks-904af05

kubectl get svc

name          type        cluster-ip       external-ip   port(s)   age
hostname-v2   clusterip   10.100.163.163   &lt;none&gt;        80/tcp    29h
kubernetes    clusterip   10.100.0.1       &lt;none&gt;        443/tcp   36h
my-app        clusterip   10.100.147.193   &lt;none&gt;        80/tcp    9m48s

kubectl get svc -n kube-system

name                      type           cluster-ip       external-ip                                                              port(s)                       age
kube-dns                  clusterip      10.100.0.10      &lt;none&gt;                                                                   53/udp,53/tcp                 25h
traefik-ingress-service   loadbalancer   10.100.113.186   a262f21c7a0c740949c3321ab77a0259-639235071.us-east-1.elb.amazonaws.com   80:30015/tcp,8080:31515/tcp   88m


kubectl describe pod my-app-898f57d6f-dsfg6

name:         my-app-898f57d6f-dsfg6
namespace:    default
priority:     0
node:         ip-192-168-119-55.ec2.internal/192.168.119.55
start time:   sun, 12 jul 2020 16:53:53 -0400
labels:       app=my-app
              pod-template-hash=898f57d6f
annotations:  kubernetes.io/psp: eks.privileged
status:       running
ip:           192.168.79.54
ips:
  ip:           192.168.79.54
controlled by:  replicaset/my-app-898f57d6f
containers:
  simple-node:
    container id:   docker://cd6c686fe8f5460d5985a81a8d75da9c76371e26572b5144d5d43b55a0415ddd
    image:          pythonss/ex1-node-app
    image id:       docker-pullable://pythonss/ex1-node-app@sha256:1ad843251ce45c21df4be52a34565217ea7cc441f2961d90c8e466af14473003
    port:           80/tcp
    host port:      0/tcp
    state:          running
      started:      sun, 12 jul 2020 16:53:54 -0400
    ready:          true
    restart count:  0
    environment:    &lt;none&gt;
    mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jm5s2 (ro)
conditions:
  type              status
  initialized       true
  ready             true
  containersready   true
  podscheduled      true
volumes:
  default-token-jm5s2:
    type:        secret (a volume populated by a secret)
    secretname:  default-token-jm5s2
    optional:    false
qos class:       besteffort
node-selectors:  &lt;none&gt;
tolerations:     node.kubernetes.io/not-ready:noexecute for 300s
                 node.kubernetes.io/unreachable:noexecute for 300s
events:          &lt;none&gt;

kubectl --namespace=kube-system get pods
name                                         ready   status    restarts   age
aws-node-j76qj                               1/1     running   0          19h
aws-node-sqrqq                               1/1     running   0          19h
aws-node-ws9kv                               1/1     running   0          19h
coredns-55c5fcd78f-2jvzg                     1/1     running   0          25h
coredns-55c5fcd78f-dlftl                     1/1     running   0          25h
kube-proxy-g9pbv                             1/1     running   0          19h
kube-proxy-wzfpc                             1/1     running   0          19h
kube-proxy-znptc                             1/1     running   0          19h
traefik-ingress-controller-5bdbcfc59-87rd8   1/1     running   0          88m

i also applied the traefik manifest and at the same time  changed the lb type from nodeport to loadbalancer
kubectl apply -f &lt;(curl -so - https://raw.githubusercontent.com/containous/traefik/v1.7/examples/k8s/traefik-deployment.yaml | sed -e 's/nodeport/loadbalancer/')

kubectl get svc -n kube-system

name                      type           cluster-ip       external-ip                                                              port(s)                       age
kube-dns                  clusterip      10.100.0.10      &lt;none&gt;                                                                   53/udp,53/tcp                 2d20h

traefik-ingress-service   loadbalancer   10.100.113.186   a262f21c7a0c740949c3321ab77a0259-639235071.us-east-1.elb.amazonaws.com   80:30015/tcp,8080:31515/tcp   44h

host a262f21c7a0c740949c3321ab77a0259-639235071.us-east-1.elb.amazonaws.com
a262f21c7a0c740949c3321ab77a0259-639235071.us-east-1.elb.amazonaws.com has address 107.22.153.204
a262f21c7a0c740949c3321ab77a0259-639235071.us-east-1.elb.amazonaws.com has address 52.44.97.64
a262f21c7a0c740949c3321ab77a0259-639235071.us-east-1.elb.amazonaws.com has address 34.195.130.205

i applied a hostname manifest so i could later do
curl 34.195.130.205 hostname-v1.local
as under
hostname-ingress.yaml
---
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: hostname-ingress
  namespace: default
spec:
  rules:
  - host: hostname-v1.local
    http:
      paths:
      - path: /
        backend:
          servicename: my-app
          serviceport: web

kubectl create -f hostname-ingress.yaml
# output
ingress.extensions/hostname-ingress created

take ingress_addr and associate it with the domain name in /etc/hosts
echo &quot;$ingress_addr hostname-v1.local&quot; | sudo tee -a /etc/hosts
# output
34.195.130.205 hostname-v1.local

kubectl create -f hostname-ingress.yaml
# output
ingress.extensions/hostname-ingress created

kubectl get ep
name          endpoints                               age
hostname-v2   &lt;none&gt;                                  20h
kubernetes    192.168.219.41:443,192.168.94.137:443   27h
my-app        &lt;none&gt;                                  19h

now let's take our ingress_addr and associate it with the hosts in etc/hosts
echo &quot;$ingress_addr hostname-v1.local&quot; | sudo tee -a /etc/hosts
# output
34.195.130.205 hostname-v1.local

here is also service.yaml and deployment.yaml
service.yaml
apiversion: v1
kind: service
metadata:
  name: my-app
  labels:
    run: my-app
spec:
  ports:
  - port: 80
    protocol: tcp
  selector:
    run: my-app


deployment.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: my-app
  labels:
    app: my-app
spec:
  replicas: 3
  selector:
    matchlabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: simple-node
        image: pythonss/ex1-node-app
        ports:
        - containerport: 80

 kubectl get pods --show-labels |egrep 'app=my-app'                                                                 
my-app-898f57d6f-dsfg6   1/1     running   0          128m   app=my-app,pod-template-hash=898f57d6f
my-app-898f57d6f-hchhb   1/1     running   0          128m   app=my-app,pod-template-hash=898f57d6f
my-app-898f57d6f-hh4cw   1/1     running   0          128m   app=my-app,pod-template-hash=898f57d6f

and
kubectl describe svc my-app   
                                                                                  
name:              my-app
namespace:         default
labels:            app=my-app
annotations:       &lt;none&gt;
selector:          app=my-app
type:              clusterip
ip:                10.100.147.193
port:              &lt;unset&gt;  80/tcp
targetport:        80/tcp
endpoints:         192.168.158.248:80,192.168.218.93:80,192.168.79.54:80
session affinity:  none
events:            &lt;none&gt;

i can see http://hostname-v1.local:8080

i can even log in to one of the pod and see that the container is correctly running my app
kubectl exec -it my-app-898f57d6f-dsfg6 /bin/bash

root@my-app-898f57d6f-dsfg6:/usr/src/app# curl localhost

hello world !!


but
http://hostname-v1.local
shows
service unavailable
question:
why is that i can not see my simple app running on:
http://hostname-v1.local (34.195.130.205 )
even though it is running inside the container?
br
",<amazon-web-services><kubernetes><traefik><amazon-eks>,62865933,1,"your service selector is incorrect and hence, the pod isn't added as its endpoint. selector has to be a set of labels that are present on the pods that you want to select for this service.
apiversion: v1
kind: service
metadata:
  name: my-app
spec:
  selector:
    app: my-app
  ports:
    - protocol: tcp
      port: 80
      targetport: 80

in your ingress definition, you're using hostname-v1 as the backend service which doesn't seem to exist anywhere. use my-app service instead.
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: hostname-ingress
  namespace: default
spec:
  rules:
  - host: hostname-v1.local
    http:
      paths:
      - path: /
        backend:
          servicename: my-app
          serviceport: 80

"
69650319,why can't i curl endpoint on gcp?,"i am working my way through a kubernetes tutorial using gke, but it was written with azure in mind - tho it has been working ok so far.
the first part where it has not worked has been with exercises regarding coredns - which i understand does not exist on gke - it's kubedns only?
is this why i can't get a pod endpoint with:
export podip=$(kubectl get endpoints hello-world-clusterip -o jsonpath='{ .subsets[].addresses[].ip}')

and then curl:
curl http://$podip:8080

my deployment is definitely on the right port:
ports:
        - containerport: 8080

and, in fact, the deployment for the tut is from a google sample.
is this to do with coredns or authorisation/needing a service account? what can i do to make the curl request work?
deployment yaml is:
apiversion: apps/v1
kind: deployment
metadata:
  name: hello-world-customdns
spec:
  replicas: 3
  selector:
    matchlabels:
      app: hello-world-customdns
  template:
    metadata:
      labels:
        app: hello-world-customdns
    spec:
      containers:
      - name: hello-world
        image: gcr.io/google-samples/hello-app:1.0
        ports:
        - containerport: 8080
      dnspolicy: &quot;none&quot;
      dnsconfig:
        nameservers:
          - 9.9.9.9
---
apiversion: v1
kind: service
metadata:
  name: hello-world-customdns
spec:
  selector:
    app: hello-world-customdns
  ports:
  - port: 80
    protocol: tcp
    targetport: 8080

",<kubernetes><google-kubernetes-engine>,69828639,1,"having a deeper insight on what gari comments, when exposing a service outside your cluster, this services must be configured as nodeport or loadbalancer, since clusterip only exposes the service on a cluster-internal ip making the service only reachable from within the cluster, and since cloud shell is a a shell environment for managing resources hosted on google cloud, and not part of the cluster, that's why you're not getting any response. to change this, you can change your yaml file with the following:
apiversion: apps/v1
kind: deployment
metadata:
  name: hello-world-customdns
spec:
  replicas: 3
  selector:
    matchlabels:
      app: hello-world-customdns
  template:
    metadata:
      labels:
        app: hello-world-customdns
    spec:
      containers:
      - name: hello-world
        image: gcr.io/google-samples/hello-app:1.0
        ports:
        - containerport: 8080
      dnspolicy: &quot;none&quot;
      dnsconfig:
        nameservers:
          - 9.9.9.9
---
apiversion: v1
kind: service
metadata:
  name: hello-world-customdns
spec:
  selector:
    app: hello-world-customdns
  type: nodeport
  ports:
  - port: 80
    protocol: tcp
    targetport: 8080

after redeploying your service, you can run command kubectl get all -o wide on cloud shell to validate that nodeport type service has been created with a node and target port.
to test your deployment just throw a curl test to he external ip from one of your nodes incluiding the node port that was assigned, the command should look like something like:
curl &lt;node_ip_address&gt;:&lt;node_port&gt;
"
70145472,getting connection refused while trying to access service from kubernetes pod,"i am new to kubernetes and i am trying to learn it by deploying a simple node server using aws eks. (kubernetes is alreay setup to talk to the created aws eks cluster)
here is code for my simple node file (server.js)
const express = require('express')
const app = express()
const port = 8080

app.get('/', (req, res) =&gt; {
 res.send('hello world!')
})

app.listen(port, () =&gt; {
console.log(`example app listening at http://localhost:${port}`)
})

here is how the dockerfile looks like:
from node:12
# create app directory
workdir /usr/src/app
# install app dependencies
copy package*.json ./
run npm ci
# bundle app source
copy . .
expose 8080
cmd [ &quot;node&quot;, &quot;server.js&quot; ]

i am able to run the above server in my local by creating a docker image.
now, in order to deploy this server here are the steps that i followed: 
first, i pushed the above image to doceker hub (aroraankit7/simple-server)
second, i created a deployment.yaml file which looks like this:
apiversion: apps/v1
kind: deployment
metadata:
  name: simple-server-app
  labels:
    app: simple-server-app
spec:
  replicas: 2
  selector:
    matchlabels:
      app: simple-server-app
   template:
     metadata:
       labels:
         app: simple-server-app
     spec:
       containers:
       - name: simple-server
         image: aroraankit7/simple-server:v1
         ports:
         - containerport: 8080

third, i deployed this using kubectl apply command. here is the output for kubectl get pods

then, i created the service.yaml file. here is how it looks:
apiversion: v1
kind: service
metadata:
name: simple-server-svc
labels:
  run: simple-server
spec:
  ports:
  - port: 8080
    targetport: 8080
    protocol: tcp
  selector:
    run: simple-server

i then deployed this using the kubectl apply command. output for kubectl describe services:

next, i logged into on of my pods by using the command: kubectl -it exec simple-server-app-758dfb88f4-4cfmp bash
while inside this pod, i ran the following the command: curl http://simple-server-svc:8080 and this was the output that i got: curl: (7) failed to connect to simple-server-svc port 8080: connection refused
why is the connection getting refused?
when i am running curl http:localhost://8080, i am getting the right output (hello world! in this case)
",<node.js><docker><kubernetes><amazon-eks><kubernetes-pod>,70145832,1,"your service is not bound to the deployment. you need to modify the selector in your service.yaml to the following:
apiversion: v1
kind: service
metadata:
 name: simple-server-svc
 labels:
  run: simple-server
spec:
  ports:
  - port: 8080
    targetport: 8080
    protocol: tcp
  selector:
    app: simple-server-app

you can use kubectl expose command to avoid mistakes like this.
"
70204722,split string and extract variables with shell script,"question
given this single-line string:
pg_user=postgres pg_port=1234 pg_pass=icontain=and*symbols

what would be the right way to assign each value to its designated variable so that i can use it afterward?

context
i'm parsing the context of a k8s secret within a cronjob so that i can periodically call a stored procedure in our postgres database.
to do so, i plan on using:
pg_output_value=$(pgpassword=$pg_passwd psql -qtax -h $pg_host -p $pg_port -u $pg_user -d $pg_database -c $pg_tr_cleanup_query)

echo $pg_output_value

the actual entire helm chart i'm currently trying to fix looks like this:
apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: {{ template &quot;fullname&quot; $ }}-tr-cleanup-cronjob
spec:
  concurrencypolicy: forbid
  schedule: &quot;* * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          restartpolicy: onfailure
          volumes:
          - name: postgres
            secret:
              secretname: {{ template &quot;fullname&quot; $ }}-postgres
          containers:
          - name: {{ template &quot;fullname&quot; $ }}-tr-cleanup-pod
            image: postgres:12-alpine
            imagepullpolicy: always
            env:
              - name: pg_props
                valuefrom:
                  secretkeyref:
                    name: {{ template &quot;fullname&quot; $ }}-postgres
                    key: postgres.properties
            command:
              - /bin/sh
              - -c
              - echo &quot;props:&quot; &amp;&amp; echo $pg_props &amp;&amp; pg_user=$(grep &quot;^pg_user=&quot; | cut -d&quot;=&quot; -f2-) &amp;&amp; echo $pg_user &amp;&amp; pg_tr_cleanup_query=&quot;select something from public.somewhere;&quot; &amp;&amp; echo $pg_tr_cleanup_query &amp;&amp; pg_output_value=$(pgpassword=$pg_passwd psql -qtax -h $pg_host -p $pg_port -u $pg_user -d $pg_database -c $pg_tr_cleanup_query) &amp;&amp; echo pg_output_value
            volumemounts:
              - name: postgres
                mountpath: /etc/secrets/postgres

current approach
as you can see, i'm currently using:
pg_user=$(grep &quot;^pg_user=&quot; | cut -d&quot;=&quot; -f2-)

that is because i initially thought the secret would be output on multiple lines, but it turns out that i was wrong. the echo $pg_user displays an empty string.
",<shell><kubernetes><kubernetes-cronjob>,70205752,1,"option 1
this function can be reused to assign each variable individually:
extract() {
  echo &quot;$input&quot; | grep -o &quot;$1=.*&quot; | cut -d&quot; &quot; -f1 | cut -d&quot;=&quot; -f2- ;
}

and to use it:
pg_user=$(extract pg_user)
pg_port=$(extract pg_port)
pg_pass=$(extract pg_pass)


option 2
another potential solution, with a security concern, is to simply use:
eval &quot;$input&quot;

it should only be used if you have validated the input.

contextual complete answer
and because i've presented the k8s context in the question, here is the answer as plugged into that solution.
apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: {{ template &quot;fullname&quot; $ }}-cronjob
spec:
  concurrencypolicy: forbid
  schedule: &quot;* * * * *&quot;
  jobtemplate:
    spec:
      template:
        spec:
          restartpolicy: onfailure
          volumes:
          - name: postgres
            secret:
              secretname: {{ template &quot;fullname&quot; $ }}-postgres
          containers:
          - name: {{ template &quot;fullname&quot; $ }}-cronjob-pod
            image: postgres:12-alpine
            imagepullpolicy: always
            env:
              - name: pg_props
                valuefrom:
                  secretkeyref:
                    name: {{ template &quot;fullname&quot; $ }}-postgres
                    key: postgres.properties
            command:
              - /bin/sh
              - -c
              - &gt;-
                extract() { echo &quot;$pg_props&quot; | grep -o &quot;$1=.*&quot; | cut -d&quot; &quot; -f1 | cut -d&quot;=&quot; -f2- ; } &amp;&amp;

                export pghost=$(extract pg_host) &amp;&amp;
                export pgport=$(extract pg_port) &amp;&amp;
                export pgdatabase=$(extract pg_database) &amp;&amp;
                export pguser=$(extract pg_user) &amp;&amp;

                pg_schema=$(extract pg_schema) &amp;&amp;
                pg_query=&quot;select tenant_schema from $pg_schema.tenant_schema_mappings;&quot; &amp;&amp;

                pgpassword=$(extract pg_passwd) psql --echo-all -c &quot;$pg_query&quot;
            volumemounts:
              - name: postgres
                mountpath: /etc/secrets/postgres

"
73178417,google kubernetes engine deploy error with github actions,"i'm trying to deploy my code to gke using github actions but getting an error during the deploy step:

here is my deployment.yaml:
apiversion: apps/v1
kind: deployment
metadata:
  name: nginx-3
  namespace: default
  labels:
    type: nginx
spec:
  replicas: 1
  selector:
    matchlabels:
    - type: nginx 
  template:
    metadata:
      labels:
      - type: nginx 
    spec:
      containers:
      - image: nginx:1.14
        name: renderer
        ports:
        - containerport: 80

service.yaml:
apiversion: v1
kind: service
metadata:
  name: nginx-3-service
spec:
  ports:
    port: 80
    protocol: tcp
    targetport: 80

and my dockerfile:
from ubuntu/redis:5.0-20.04_beta

# install.
run apt-get update &amp;&amp;  debian_frontend=noninteractive apt-get install -y tzdata
run \
  sed -i 's/# \(.*multiverse$\)/\1/g' /etc/apt/sources.list &amp;&amp; \
  apt-get update &amp;&amp; \
  apt-get -y upgrade &amp;&amp; \
  apt-get install -y build-essential &amp;&amp; \
  apt-get install -y software-properties-common &amp;&amp; \
  apt-get install -y byobu curl git htop man unzip vim wget &amp;&amp; \
  rm -rf /var/lib/apt/lists/*

# set environment variables.
env home /root

# define working directory.
workdir /root

# define default command.
cmd [&quot;bash&quot;]

this is what the cloud deployments(workloads) looks like:


i'm trying to push a c++ code using an ubuntu image. i just want to simply push my code to google cloud kubernetes engine.
update:
i've deleted the deployment and re-run the action and got this:
it said that deployment is successfully created but gives off another error:
deployment.apps/nginx-3 created
error from server (notfound): deployments.apps &quot;gke-deployment&quot; not found

",<kubernetes><google-cloud-platform><google-kubernetes-engine><github-actions>,73180332,1,"try:
apiversion: apps/v1
kind: deployment
metadata:
  ...
  labels:
    type: nginx  # &lt;-- correct
spec:
  ...
  selector:
    matchlabels:
      type: nginx  # incorrect, remove the '-'
  template:
    metadata:
      labels:
        type: nginx  # incorrect, remove the '-' 
    spec:
      ...
---
apiversion: v1
kind: service
...
spec:
  ...
  ports:
  - port: 80  # &lt;-- add '-'
    protocol: tcp
    targetport: 80

"
63968881,unable to connect internet from pod after applying egress network policy in gke,"i have a pod (kubectl run app1 --image tomcat:7.0-slim) in gke after applying the egress network policy apt-get update command unable to connect internet.
before applying policy:

after applying policy:

this is the policy applied:
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: app2-np
  namespace: default
spec:
  podselector:
    matchlabels:
      name: app2
  policytypes:
  - egress
  - ingress
  ingress:
  - {}
  egress:
  - to:
    - podselector:
        matchlabels:
          name: app3
    ports:
    - port: 8080

  - ports:
    - port: 80
    - port: 53
    - port: 443

the here am able to connect 8080 port of app3 pod in same namespace. please help in correcting my netpol.
",<kubernetes><google-cloud-platform><google-kubernetes-engine><kubernetes-ingress><gke-networking>,63990014,1,"it happens because you are defining the egress rule only for app3 on port 8080, and it will block all internet connect attempts.
if you need to use access internet from some of your pods, you can tag them and create a networkpolicy to permit the internet access.
in the example below, the pods with the tag networking/allow-internet-egress: &quot;true&quot; will be able to reach the internet:
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: internet-egress
spec:
  podselector:
    matchlabels:
      networking/allow-internet-egress: &quot;true&quot;
  egress:
  - {}
  policytypes:
  - egress

another option is allow by ip blocks, in the example below, a rule will allow the internet access (0.0.0.0) except for the ipblocks 10.0.0.0/8
kind: networkpolicy
apiversion: networking.k8s.io/v1
metadata:
  name: allow-internet-only
spec:
  podselector: {}
  policytypes:
    - egress
  egress:
    - to:
      - ipblock:
        cidr: 0.0.0.0/0
          except:
            - 10.0.0.0/8

finally, in this site you can visualize your networkpolices in a good way to understand what is the exact behaviour.
references:
https://www.stackrox.com/post/2020/01/kubernetes-egress-network-policies/
kubernets networkpolicy allow external traffic to internet only
"
73148964,"kubernetes helm ""ingress.spec.rules[0].http.paths"" got ""map"", expected ""array""","so i have been trying to fix some charts we inherited and all the others went fine except this 1 which is giving me a headache.
i understand what thew error is telling me
error: unable to build kubernetes objects from release manifest: error validating &quot;&quot;: error validating data: validationerror(ingress.spec.rules[0].http.paths): invalid type for io.k8s.api.networking.v1.httpingressrulevalue.paths: got &quot;map&quot;, expected &quot;array&quot;  but i can't find where this map appears in the spec below. i see the paths being in list format.
does anyone have any idea where exactly the problem is?
azure aks 1.24.0
{{- if .values.ingress.enabled -}}
{{- $fullname := include &quot;something.fullname&quot; . -}}
{{- $serviceport := .values.service.port -}}
{{- $ingresspath := .values.ingress.path -}}
{{- if .capabilities.apiversions.has &quot;networking.k8s.io/v1&quot; }}
apiversion: networking.k8s.io/v1
{{- else if .capabilities.apiversions.has &quot;extensions/v1beta1&quot; }}
apiversion: extensions/v1beta1
{{- else }}
{{ fail &quot;unsupported api version&quot;}}
{{- end }}
kind: ingress
metadata:
  name: {{ $fullname }}
  labels:
    app.kubernetes.io/name: {{ .chart.name }}
    app.kubernetes.io/instance: {{ .release.name }}
    app.kubernetes.io/version: {{ .chart.appversion }}
    app.kubernetes.io/component: {{ .values.component }}
    app.kubernetes.io/part-of: {{ .values.partof }}
    app.kubernetes.io/managed-by: {{ .release.service }}
    helm.sh/chart: {{ .chart.name }}-{{ .chart.version | replace &quot;+&quot; &quot;_&quot; }}

    {{- with .values.labels.something}}
    {{ toyaml . }}
    {{- end }}
{{- with .values.ingress.annotations }}
  annotations:
{{ toyaml . | indent 4 }}
{{- end }}
spec:
{{- if .values.ingress.tls }}
  tls:
  {{- range .values.ingress.tls }}
    - hosts:
      {{- range .hosts }}
        - {{ . }}
      {{- end }}
      secretname: {{ .secretname }}
  {{- end }}
{{- end }}
  rules:
  {{- range .values.ingress.hosts }}
    - host: {{ . }}
  {{- end }}
      http:
        paths:
          {{- if .capabilities.apiversions.has &quot;networking.k8s.io/v1&quot; }}
          pathtype: prefix
          backend:
            service:
              name: {{ default $fullname .values.service.name }}
              port:
                number: {{ .values.service.port }}
          {{- else if .capabilities.apiversions.has &quot;extensions/v1beta1&quot; }}
            backend:
              servicename: {{ default $fullname .values.service.name }}
              serviceport: {{ .values.service.port }}
          {{- end }}
{{- end }}

edit 1
doing a helm lint . with the same flags that the helm upgrade --install would do, throws no errors
",<kubernetes><kubernetes-helm><azure-aks><nginx-ingress>,73149408,1,"you have:
    paths:
      {{- if .capabilities.apiversions.has &quot;networking.k8s.io/v1&quot; }}
      pathtype: prefix
      ... 

you're missing the actual path:
    paths:
      - path: /
        {{- if .capabilities.apiversions.has &quot;networking.k8s.io/v1&quot; }}
        pathtype: prefix
        ... 

"
73143622,re-using uuid in helm configmap,"there's a similar question that alludes to the possibility of auto-generating a uuid in helm charts when used as a secret or configmap. i'm trying precisely to do that, but i'm getting a new uuid each time.
my test case:
---
{{- $config := (lookup &quot;v1&quot; &quot;configmap&quot; .release.namespace &quot;{{ .release.name }}-testcase&quot;) -}}
apiversion: v1
kind: configmap
metadata:
  name: &quot;{{ .release.name }}-testcase&quot;
  namespace: &quot;{{ .release.namespace }}&quot;
  labels:
    app.kubernetes.io/managed-by: &quot;{{ .release.service }}&quot;
    app.kubernetes.io/instance: &quot;{{ .release.name }}&quot;
    app.kubernetes.io/version: &quot;{{ .chart.appversion }}&quot;
    helm.sh/chart: &quot;{{ .chart.name }}-{{ .chart.version }}&quot;
data:
{{- if $config }}
  test_value: {{ $config.data.test_value | quote }}
{{- else }}
  test_value: {{ uuidv4 | quote }}
{{ end }}

i initially deploy this with:
helm upgrade --install --namespace test mytest .

if i run it again, or run with helm diff upgrade --namespace test mytest . i get a new value for test_value. when i dump the contents of $config it's an empty map {}.
i'm using helm v3.9.0, kubectl 1.24, and kube server is 1.22.
note: i couldn't ask in a comment thread on the other post because i don't have enough reputation.
",<kubernetes><kubernetes-helm><configmap>,73191021,1,"referring to my issue where you enclosed your stack overflow post : https://github.com/helm/helm/issues/11187
a way to make your configmap work is to save as a variable before conditionally set your value. this means every time you upgrade, you'll generate a uuid which will normally will not be used, but this is not dramatic.
when assigning an existing value, := should become =. also don't forget to b64enc your value in your manifest
{{- $config := uuidv4 | b64enc | quote -}}
{{- $config_lookup := (lookup &quot;v1&quot; &quot;configmap&quot; .release.namespace &quot;{{ .release.name }}-testcase&quot;) -}}
{{- if $config_lookup -}}
{{- $config = $config_lookup.data.test_value -}}
---
apiversion: v1
kind: configmap
metadata:
  name: &quot;{{ .release.name }}-testcase&quot;
  namespace: &quot;{{ .release.namespace }}&quot;
  labels:
    app.kubernetes.io/managed-by: &quot;{{ .release.service }}&quot;
    app.kubernetes.io/instance: &quot;{{ .release.name }}&quot;
    app.kubernetes.io/version: &quot;{{ .chart.appversion }}&quot;
    helm.sh/chart: &quot;{{ .chart.name }}-{{ .chart.version }}&quot;
data:
  test_value: {{ $config | quote }}

"
73037808,ingress routing to different k8s service for sub domains,"i am trying to create an ingress that routes to service1 when service1.domain.com is entered &amp; service2 when service2.domain.com is entered.
my ingress file looks like this below.
i am on azure aks and using azure app gateway as my ingress controller. below is how my ingress yaml looks like.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: app-ingress
  annotations: 
    kubernetes.io/ingress.class: azure/application-gateway
spec:
  rules:
  - host: service1.domain.com
  - http:
      paths:
      - path: /
        backend:
          service:
            name: service1
            port:
              number: 80
        pathtype: prefix      
  - host: service2.domain.com
  - http:
      paths:
      - path: /
        backend:
          service:
            name: service2
            port:
              number: 80
        pathtype: prefix

however, both the domains are getting routed to service2. can someone help what i am missing here.
",<kubernetes><kubernetes-ingress><azure-aks><azure-application-gateway>,73038009,1,"the spec look wrong. it should be:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: app-ingress
  annotations: 
    kubernetes.io/ingress.class: azure/application-gateway
spec:
  rules:
  - host: service1.domain.com
    http:
      paths:
      - path: /
        backend:
          service:
            name: service1
            port:
              number: 80
        pathtype: prefix      
  - host: service2.domain.com
    http:
      paths:
      - path: /
        backend:
          service:
            name: service2
            port:
              number: 80
        pathtype: prefix

there should be two items in your rules, one for each subdomain. http and host are two fields of the same item.
"
73041680,how to update ingress controller with a new deployment,"i am very new to azure kubernetes! right now i am just playing around with it, trying to work out how to create things using kubectl.
i already have my cluster set up, and i have created a couple of pods using an existing container in our registry. i also created an ingress controller using azure application gateway.
what i want to do next is use a deployment yaml file to add a new replicated set of pods to the cluster, with a different image in there, and add the new service endpoint to the ingress controller. remember, i am very new to this, so i am not sure if this is what you should do, or if you should create the pods/service first and then change the ingress controller?
or if you should have a separate deployment yaml for the ingress and recreate it on its own?
anyway, here is the deployment yaml file that i have. when i try and run it with :
kubectl apply -f deployment.yml

i just get the error :
error: error validating &quot;deployment.yml&quot;: error validating data: invalid object to validate; if you choose to ignore these errors, turn validation off with --validate=false

so i guess my question is, am i doing this right? and if this is the way i should be adding to my cluster, any idea what is wrong with this yaml?
---
  apiversion: apps/v1
  kind: deployment
  metadata:
    name: new-kuber-endpoint
    namespace: kuber-ut
  spec:
    replicas: 2
    selector:
      matchlabels:
        app: new-kuber-endpoint
    template:
      metadata:
        labels:
          app: new-kuber-endpoint
      spec:
        nodeselector:
          kubernetes.io/os: linux
        containers:
          - name: amica-endpoint
            image: mycontainerreg.azurecr.io/new-endpoint:20220707.1
            ports:
              - containerport: 80
            resources:
              requests:
                cpu: '0'
                memory: '0'
              limits:
                cpu: '256'
                memory: 11400g
---
  apiversion: v1
  kind: service
  metadata:
    name: new-kuber-endpoint-service
    namespace: kuber-ut
  spec:
    type: loadbalancer
    ports:
      - targetport: 80
        name: port80
        port: 80
        protocol: tcp
    selector:
      app: new-kuber-endpoint
---
  apiversion: networking.k8s.io/v1
  kind: ingress
  metadata:
    name: kuberdevingress
    namespace: kuber-ut
    uid: d9d490f0-1d3a-4433-bb03-7f1ba0dc611f
    resourceversion: '1490171'
    generation: 3
  spec:
    rules:
      - http:
          paths:
            - path: /old
              pathtype: prefix
              backend:
                service:
                  name: old-endpoint-service
                  port:
                    number: 80
            - path: /new
              pathtype: prefix
              backend:
                service:
                  name: new-kuber-endpoint-service
                  port:
                    number: 80
  status:
    loadbalancer:
      ingress:
        - ip: &lt;ip goes here&gt;

i copied the kuberdevingress section from the azure portal, so i am not confident that is how i would recreate that ingress?
",<kubernetes><kubectl><azure-aks>,73042060,1,"you can use the single yaml file and manage it as per the requirement of creating multiple files for different resources that are on you.
if you want to update the deployment you can simply update the yaml and run
kubectl apply -f &lt;filename&gt;.yaml 

also, there is an issue with your yaml, you can separate the resource using the --- in yaml if you don't manage the three different files for deployment, service, and ingress
  apiversion: apps/v1
  kind: deployment
  metadata:
    name: new-kuber-endpoint
    namespace: kuber-test
  spec:
    replicas: 2
    selector:
      matchlabels:
        app: new-kuber-endpoint
    template:
      metadata:
        labels:
          app: new-kuber-endpoint
      spec:
        nodeselector:
          kubernetes.io/os: linux
        containers:
          - name: new-endpoint
            image: mycontainerregistry.azurecr.io/new-endpoint:20220707.1
            ports:
              - containerport: 80
            resources:
              requests:
                cpu: '0'
                memory: '0'
              limits:
                cpu: '256'
                memory: 11400g
---
  apiversion: v1
  kind: service
  metadata:
    name: new-kuber-endpoint-service
    namespace: kuber-test
  spec:
    type: loadbalancer
    ports:
      - targetport: 80
        name: port80
        port: 80
        protocol: tcp
    selector:
      app: new-kuber-endpoint
---
  apiversion: v1
  kind: ingress
  metadata:
    name: kuberdevingress
    namespace: kuber-test
    annotations:
      kubernetes.io/ingress.class: azure/application-gateway
  spec:
    rules:
      - http:
          paths:
            - path: /
              pathtype: prefix
              backend:
                service:
                  name: old-endpoint-service
                  port:
                    number: 80
            - path: /
              pathtype: prefix
              backend:
                service:
                  name: new-endpoint-service
                  port:
                    number: 80

you can now update any field in the above yaml and re-apply the changes again to the kubernetes cluster, it will update the changes.
if it's in service it will update that, if it's in ingress it will update those changes accordingly.
if you want to change the docker image you can update the deployment image with a different docker tag it will fetch and deploy that version.
"
64440761,k8s api cloud.google.com not available in gke v1.16.13-gke.401,"i am trying to create a backendconfig resource on a gke cluster v1.16.13-gke.401 but it gives me the following error:
unable to recognize &quot;backendconfig.yaml&quot;: no matches for kind &quot;backendconfig&quot; in version &quot;cloud.google.com/v1&quot;

i have checked the available apis with the kubectl api-versions command and cloud.google.com is not available. how can i enable it?
i want to create a backendconfig whit a custom health check like this:
apiversion: cloud.google.com/v1
kind: backendconfig
metadata:
  name: my-backendconfig
spec:
  healthcheck:
    checkintervalsec: 8
    timeoutsec: 1
    healthythreshold: 1
    unhealthythreshold: 3
    type: http
    requestpath: /health
    port: 10257

and attach this backendconfig to a service like this:
apiversion: v1
kind: service
metadata:
  annotations:
    cloud.google.com/backend-config: '{&quot;default&quot;: &quot;my-backendconfig&quot;}'

",<kubernetes><google-kubernetes-engine>,64500771,1,"as mentioned in the comments, issue was caused due to the lack of http load balancing add-on in your cluster.
when you are creating gke cluster with all default setting, feature like http load balancing is enabled.

the http load balancing add-on is required to use the google cloud load balancer with kubernetes ingress. if enabled, a controller will be installed to coordinate applying load balancing configuration changes to your gcp project

more details can be found in gke documentation.
for test i have created cluster-1 without http load balancing add-on. there was no backendconfig crd - custom resource definition.

the customresourcedefinition api resource allows you to define custom resources. defining a crd object creates a new custom resource with a name and schema that you specify. the kubernetes api serves and handles the storage of your custom resource. the name of a crd object must be a valid dns subdomain name.

without backendconfig and without cloud apiversion like below
user@cloudshell:~ (k8s-tests-xxx)$ kubectl get crd | grep backend
user@cloudshell:~ (k8s-tests-xxx)$ kubectl api-versions | grep cloud

i was not able to create any backendconfig.
user@cloudshell:~ (k8s-tests-xxx) $ kubectl apply -f bck.yaml
error: unable to recognize &quot;bck.yaml&quot;: no matches for kind &quot;backendconfig&quot; in version &quot;cloud.google.com/v1&quot;

to make it work, you have to enable http load balancing you can do it via ui or command.
using ui:

navigation menu &gt; clusters &gt; [cluster-name] &gt; details &gt; clikc on
edit &gt; scroll down to add-ons and expand &gt; find http load balancing and change from disabled to enabled.

or command:
gcloud beta container clusters update &lt;clustername&gt; --update-addons=httploadbalancing=enabled --zone=&lt;your-zone&gt;

$ gcloud beta container clusters update cluster-1 --update-addons=httploadbalancing=enabled --zone=us-central1-c
warning: warning: basic authentication is deprecated, and will be removed in gke control plane versions 1.19 and newer. for a list of recommended authentication methods, see: https://cloud.google.com/kubernetes-engine/docs/how-to/api-server-authentication

after a while, when add-on was enabled:
$ kubectl get crd | grep backend
backendconfigs.cloud.google.com             2020-10-23t13:09:29z
$ kubectl api-versions | grep cloud
cloud.google.com/v1
cloud.google.com/v1beta1
$ kubectl apply -f bck.yaml 
backendconfig.cloud.google.com/my-backendconfig created

"
64533410,"kubernetes dashboard an error on the server (""unknown"") has prevented the request from succeeding","after getting my k8s cluster up and going i faithfully deployed the following webui dashboard
using the command:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.4/aio/deploy/recommended.yaml

when i try to access it i get the following error:
metric client health check failed: an error on the server (&quot;unknown&quot;) has prevented the request from succeeding (get services dashboard-metrics-scraper)

if i get all the services i get:
k get services --all-namespaces
namespace              name                        type        cluster-ip    external-ip   port(s)         age
default                kubernetes                  clusterip   10.96.0.1     &lt;none&gt;        443/tcp         8d
kube-system            kube-dns                    clusterip   10.96.0.10    &lt;none&gt;        53/udp,53/tcp   8d
kubernetes-dashboard   dashboard-metrics-scraper   clusterip   10.96.0.65    &lt;none&gt;        8000/tcp        6m10s
kubernetes-dashboard   kubernetes-dashboard        clusterip   10.96.0.173   &lt;none&gt;        443/tcp         6m10s

can someone shed some light? what am i missing?
more info:
in the dashboard yaml i found these roles:
kind: role
apiversion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
rules:
  - apigroups: [&quot;&quot;]
    resources: [&quot;secrets&quot;]
    resourcenames: [&quot;kubernetes-dashboard-key-holder&quot;, &quot;kubernetes-dashboard-certs&quot;, &quot;kubernetes-dashboard-csrf&quot;]
    verbs: [&quot;get&quot;, &quot;update&quot;, &quot;delete&quot;]
   
 map.
  - apigroups: [&quot;&quot;]
    resources: [&quot;configmaps&quot;]
    resourcenames: [&quot;kubernetes-dashboard-settings&quot;]
    verbs: [&quot;get&quot;, &quot;update&quot;]
    
  - apigroups: [&quot;&quot;]
    resources: [&quot;services&quot;]
    resourcenames: [&quot;heapster&quot;, &quot;dashboard-metrics-scraper&quot;]
    verbs: [&quot;proxy&quot;]
  - apigroups: [&quot;&quot;]
    resources: [&quot;services/proxy&quot;]
    resourcenames: [&quot;heapster&quot;, &quot;http:heapster:&quot;, &quot;https:heapster:&quot;, &quot;dashboard-metrics-scraper&quot;, &quot;http:dashboard-metrics-scraper&quot;]
    verbs: [&quot;get&quot;]

    ---
    
    kind: clusterrole
    apiversion: rbac.authorization.k8s.io/v1
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
      name: kubernetes-dashboard
    rules:
      
      - apigroups: [&quot;metrics.k8s.io&quot;]
        resources: [&quot;pods&quot;, &quot;nodes&quot;]
        verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
    
    ---
    
    apiversion: rbac.authorization.k8s.io/v1
    kind: rolebinding
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
      name: kubernetes-dashboard
      namespace: kubernetes-dashboard
    roleref:
      apigroup: rbac.authorization.k8s.io
      kind: role
      name: kubernetes-dashboard
    subjects:
      - kind: serviceaccount
        name: kubernetes-dashboard
        namespace: kubernetes-dashboard
    
    ---
    
    apiversion: rbac.authorization.k8s.io/v1
    kind: clusterrolebinding
    metadata:
      name: kubernetes-dashboard
    roleref:
      apigroup: rbac.authorization.k8s.io
      kind: clusterrole
      name: kubernetes-dashboard
    subjects:
      - kind: serviceaccount
        name: kubernetes-dashboard
        namespace: kubernetes-dashboard

looks like the kubernetes-dashboard user has access to the metrics service i might be wrong
",<kubernetes><kubernetes-ingress><kubernetes-pod><kubernetes-dashboard>,64533593,1,"it looks like the kubernetes-dashboard's serviceaccount doesn't have access to all kubernetes resources (in particular, it can't access the metric server service).
to fix this you should create a new serviceaccount for the dashboard and give it more permissions.
here's one that i found on another similar post (be careful since it will give admin privileges to the dashboard, and whoever uses it will be able to destroy/create new or existing resources on your kubernetes cluster):
   apiversion: rbac.authorization.k8s.io/v1beta1
kind: clusterrolebinding
metadata:
   name: kubernetes-dashboard
   labels:
       k8s-app: kubernetes-dashboard
roleref:
   apigroup: rbac.authorization.k8s.io
   kind: clusterrole
   name: cluster-admin
subjects:
- kind: serviceaccount
  name: kubernetes-dashboard
  namespace: kube-system

if you don't have a cluster-admin serviceaccount, create one following this template:
apiversion: v1
kind: serviceaccount
metadata:
  name: admin
  namespace: kube-system
  labels:
    kubernetes.io/cluster-service: &quot;true&quot;
    addonmanager.kubernetes.io/mode: reconcile

admin clusterrole:
kind: clusterrole
apiversion: rbac.authorization.k8s.io/v1alpha1
metadata:
  name: admin
rules:
  - apigroups: [&quot;*&quot;]
    resources: [&quot;*&quot;]
    verbs: [&quot;*&quot;]
    nonresourceurls: [&quot;*&quot;]

"
64517650,microk8s deploy webserver,"i am playing with microk8s and i am trying to deploy nextcloud to get more familiar with it. however the deployment of nextcloud went fine, i am facing some issues with setting ingress for that. maybe you could take a look at my manifests and ingress resource and help find me the problem.
this is the deployment file:
apiversion: v1
kind: service
metadata:
  namespace: nextcloud
  name: nextcloud-service
  labels:
    run: nextcloud-app
spec:
  ports:
    - port: 80
      targetport: 8080
  selector:
    run: nextcloud-app
---
apiversion: apps/v1
kind: deployment
metadata:
  namespace: nextcloud
  name: nextcloud-deployment
  labels:
    app: nextcloud-app
spec:
  replicas: 1
  selector:
    matchlabels:
      app: nextcloud-app
  template:
    metadata:
      labels:
        app: nextcloud-app
    spec:
      containers:
      - image: nextcloud:latest
        name: nextcloud
        env:
        - name: nextcloud_admin_user
          valuefrom:
            configmapkeyref:
              name: nextcloud-configuration
              key: nextcloud_admin_user
        - name: nextcloud_admin_password
          valuefrom:
            secretkeyref:
              name: nextcloud-secret
              key: admin_password
        ports:
        - containerport: 8080
          name: http
        volumemounts:
        - name: nextcloud-pv
          mountpath: /var/www/html/data
      volumes:
      - name: nextcloud-pv
        persistentvolumeclaim:
          claimname: nextcloud-pv-claim

and this is the ingress resource file:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: nextcloud-ingress
  namespace: nextcloud
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /nextcloud
        pathtype: prefix
        backend:
          service:
            name: nextcloud-service
            port:
              number: 80

following addons are enabled on my microk8s:

dns
ingress

now i would like to show you some k8s output.
kubectl -n nextcloud describe svc nextcloud-service
name:              nextcloud-service
namespace:         nextcloud
labels:            run=nextcloud-app
annotations:       &lt;none&gt;
selector:          run=nextcloud-app
type:              clusterip
ip:                10.152.183.189
port:              &lt;unset&gt;  80/tcp
targetport:        8080/tcp
endpoints:         &lt;none&gt;
session affinity:  none
events:            &lt;none&gt;

kubectl -n nextcloud describe ingress nextcloud-ingress
name:             nextcloud-ingress
namespace:        nextcloud
address:          192.168.60.2
default backend:  default-http-backend:80 (&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;)
rules:
  host        path  backends
  ----        ----  --------
  *           
              /nextcloud   nextcloud-service:80   &lt;none&gt;)
annotations:  nginx.ingress.kubernetes.io/rewrite-target: /
events:
  type    reason  age                 from                      message
  ----    ------  ----                ----                      -------
  normal  create  11m                 nginx-ingress-controller  ingress nextcloud/nextcloud-ingress
  normal  create  11m                 nginx-ingress-controller  ingress nextcloud/nextcloud-ingress
  normal  update  63s (x22 over 11m)  nginx-ingress-controller  ingress nextcloud/nextcloud-ingress
  normal  update  63s (x22 over 11m)  nginx-ingress-controller  ingress nextcloud/nextcloud-ingress

kubectl -n ingress logs pod/nginx-ingress-microk8s-controller-k2q6c
i1024 19:56:37.955953       6 status.go:275] updating ingress nextcloud/nextcloud-ingress status from [{192.168.60.2 }] to [{127.0.0.1 }]
w1024 19:56:37.963861       6 controller.go:909] service &quot;nextcloud/nextcloud-service&quot; does not have any active endpoint.
i1024 19:56:37.964276       6 event.go:278] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;nextcloud&quot;, name:&quot;nextcloud-ingress&quot;, uid:&quot;913dcf73-e5df-4ad9-a23b-22d6ad8b83a7&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;192287&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'update' ingress nextcloud/nextcloud-ingress
i1024 19:56:39.491960       6 event.go:278] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;nextcloud&quot;, name:&quot;nextcloud-ingress&quot;, uid:&quot;913dcf73-e5df-4ad9-a23b-22d6ad8b83a7&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;192295&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'update' ingress nextcloud/nextcloud-ingress
w1024 19:56:41.297313       6 controller.go:909] service &quot;nextcloud/nextcloud-service&quot; does not have any active endpoint.
i1024 19:57:37.955734       6 status.go:275] updating ingress nextcloud/nextcloud-ingress status from [{192.168.60.2 }] to [{127.0.0.1 }]
w1024 19:57:37.969214       6 controller.go:909] service &quot;nextcloud/nextcloud-service&quot; does not have any active endpoint.
i1024 19:57:37.969711       6 event.go:278] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;nextcloud&quot;, name:&quot;nextcloud-ingress&quot;, uid:&quot;913dcf73-e5df-4ad9-a23b-22d6ad8b83a7&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;192441&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'update' ingress nextcloud/nextcloud-ingress
i1024 19:57:39.492467       6 event.go:278] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;nextcloud&quot;, name:&quot;nextcloud-ingress&quot;, uid:&quot;913dcf73-e5df-4ad9-a23b-22d6ad8b83a7&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;192446&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'update' ingress nextcloud/nextcloud-ingress
w1024 19:57:41.302640       6 controller.go:909] service &quot;nextcloud/nextcloud-service&quot; does not have any active endpoint.
i1024 19:58:37.956198       6 status.go:275] updating ingress nextcloud/nextcloud-ingress status from [{192.168.60.2 }] to [{127.0.0.1 }]
w1024 19:58:37.964655       6 controller.go:909] service &quot;nextcloud/nextcloud-service&quot; does not have any active endpoint.
i1024 19:58:37.965017       6 event.go:278] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;nextcloud&quot;, name:&quot;nextcloud-ingress&quot;, uid:&quot;913dcf73-e5df-4ad9-a23b-22d6ad8b83a7&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;192592&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'update' ingress nextcloud/nextcloud-ingress
i1024 19:58:39.493436       6 event.go:278] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;nextcloud&quot;, name:&quot;nextcloud-ingress&quot;, uid:&quot;913dcf73-e5df-4ad9-a23b-22d6ad8b83a7&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;192600&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'update' ingress nextcloud/nextcloud-ingress
w1024 19:58:41.298097       6 controller.go:909] service &quot;nextcloud/nextcloud-service&quot; does not have any active endpoint.
i1024 19:59:37.955569       6 status.go:275] updating ingress nextcloud/nextcloud-ingress status from [{192.168.60.2 }] to [{127.0.0.1 }]
w1024 19:59:37.964975       6 controller.go:909] service &quot;nextcloud/nextcloud-service&quot; does not have any active endpoint.
i1024 19:59:37.965045       6 event.go:278] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;nextcloud&quot;, name:&quot;nextcloud-ingress&quot;, uid:&quot;913dcf73-e5df-4ad9-a23b-22d6ad8b83a7&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;192746&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'update' ingress nextcloud/nextcloud-ingress
i1024 19:59:39.491840       6 event.go:278] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;nextcloud&quot;, name:&quot;nextcloud-ingress&quot;, uid:&quot;913dcf73-e5df-4ad9-a23b-22d6ad8b83a7&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;192750&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'update' ingress nextcloud/nextcloud-ingress
w1024 19:59:41.298496       6 controller.go:909] service &quot;nextcloud/nextcloud-service&quot; does not have any active endpoint.
i1024 20:00:37.956061       6 status.go:275] updating ingress nextcloud/nextcloud-ingress status from [{192.168.60.2 }] to [{127.0.0.1 }]
w1024 20:00:37.965139       6 controller.go:909] service &quot;nextcloud/nextcloud-service&quot; does not have any active endpoint.
i1024 20:00:37.965212       6 event.go:278] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;nextcloud&quot;, name:&quot;nextcloud-ingress&quot;, uid:&quot;913dcf73-e5df-4ad9-a23b-22d6ad8b83a7&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;192896&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'update' ingress nextcloud/nextcloud-ingress
i1024 20:00:39.489924       6 event.go:278] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;nextcloud&quot;, name:&quot;nextcloud-ingress&quot;, uid:&quot;913dcf73-e5df-4ad9-a23b-22d6ad8b83a7&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;192904&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'update' ingress nextcloud/nextcloud-ingress
w1024 20:00:41.298762       6 controller.go:909] service &quot;nextcloud/nextcloud-service&quot; does not have any active endpoint.
i1024 20:01:37.955481       6 status.go:275] updating ingress nextcloud/nextcloud-ingress status from [{192.168.60.2 }] to [{127.0.0.1 }]
w1024 20:01:37.963612       6 controller.go:909] service &quot;nextcloud/nextcloud-service&quot; does not have any active endpoint.
i1024 20:01:37.963681       6 event.go:278] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;nextcloud&quot;, name:&quot;nextcloud-ingress&quot;, uid:&quot;913dcf73-e5df-4ad9-a23b-22d6ad8b83a7&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;193049&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'update' ingress nextcloud/nextcloud-ingress
i1024 20:01:39.490523       6 event.go:278] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;nextcloud&quot;, name:&quot;nextcloud-ingress&quot;, uid:&quot;913dcf73-e5df-4ad9-a23b-22d6ad8b83a7&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;193058&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'update' ingress nextcloud/nextcloud-ingress
w1024 20:01:41.297141       6 controller.go:909] service &quot;nextcloud/nextcloud-service&quot; does not have any active endpoint.

calling http://k8s.ip/nextcloud results with 503. any ideas what i am missing?
",<kubernetes><kubernetes-ingress>,64594087,1,"as i posted in the comments:

you are receiving 503 code because you have a missmatch in your service -&gt; .spec.selector (run: nextcloud-app) and your deployment -&gt; .spec.selector.matchlabels (app: nextcloud-app). you will need to have them both the same. you can see it also when describing the service (no endpoint).

the issue in this particular setup is that there is a missmatch between a matchlabel in a deployment and selector in the service:
deployment
spec:
  replicas: 1
  selector:
    matchlabels:
      app: nextcloud-app # &lt;-- here!

service:
spec:
  ports:
    - port: 80
      targetport: 8080
  selector:
    run: nextcloud-app # &lt;-- here!

to fix that you will need to have both of them matching (for example):

app: nextcloud-app in a deployment and a service


some of the ways to identify mismatched selector (by using examples from post):

manual inspection of yaml definitions as shown above
$ kubectl -n nextcloud describe svc nextcloud-service

name:              nextcloud-service
namespace:         nextcloud
labels:            run=nextcloud-app
annotations:       &lt;none&gt;
selector:          run=nextcloud-app
type:              clusterip
ip:                10.152.183.189
port:              &lt;unset&gt;  80/tcp
targetport:        8080/tcp
endpoints:         &lt;none&gt; # &lt;-- here
session affinity:  none
events:            &lt;none&gt;

above describe shows that the service is created but there are no endpoints (pods) to send the traffic to.

no endpoint could be also related to the fact that pod is not ready or is not in healthy state


$ kubectl get endpoint -n nextcloud

name                 endpoints            age
nextcloud-service    &lt;none&gt;               1m     


logs from ingress controller (posted in the question):

i1024 19:56:37.955953       6 status.go:275] updating ingress nextcloud/nextcloud-ingress status from [{192.168.60.2 }] to [{127.0.0.1 }]
w1024 19:56:37.963861       6 controller.go:909] service &quot;nextcloud/nextcloud-service&quot; does not have any active endpoint.
i1024 19:56:37.964276       6 event.go:278] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;nextcloud&quot;, name:&quot;nextcloud-ingress&quot;, uid:&quot;913dcf73-e5df-4ad9-a23b-22d6ad8b83a7&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;192287&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'update' ingress nextcloud/nextcloud-ingress

&lt;--redacted--&gt; service &quot;nextcloud/nextcloud-service&quot; does not have any active endpoint.

i encourage you to check the helm chart of nextcloud:

github.com: nextcloud: helm: charts: nextcloud
microk8s.io: docs: commands


additional resources:

kubernetes.io: services networking: service
kubernetes.io: services networking: ingress
microk8s.io: docs: addon ingress

"
72576608,about helm yaml syntax,"the configmap.yaml
apiversion: v1
kind: configmap
metadata:
  name: {{ include &quot;test.fullname&quot; . }}-config-map
data:
  brokerconf: |
    {{ .values.configmap }}



the below values.yaml is right.
configmap: |
  key=values

but the values.yaml is wrong
configmap: |
  key=values
  key2=values2


the helm template core content is
apiversion: v1
kind: configmap
metadata:
  name: test-config-map
data:
  brokerconf: |
    key=values
key2=values2


the error
error: yaml parse error on test/templates/config-map.yaml: error converting yaml to json: yaml: line 9: could not find expected ':'
helm.go:84: [debug] error converting yaml to json: yaml: line 9: could not find expected ':'
yaml parse error on v5-proxy/templates/config-map.yaml
helm.sh/helm/v3/pkg/releaseutil.(*manifestfile).sort
    helm.sh/helm/v3/pkg/releaseutil/manifest_sorter.go:146
helm.sh/helm/v3/pkg/releaseutil.sortmanifests
    helm.sh/helm/v3/pkg/releaseutil/manifest_sorter.go:106
helm.sh/helm/v3/pkg/action.(*configuration).renderresources
    helm.sh/helm/v3/pkg/action/action.go:165

how to fix it?
",<kubernetes><yaml><kubernetes-helm>,72577094,1,"you may update the configmap as below:
apiversion: v1
kind: configmap
metadata:
  name: config-map
data:
  brokerconf:
    {{ .values.configmap| indent 4|trim }}

the error is caused because the 2nd line in data.brokerconf is not indented properly. something like below, where key2=values2 is an invalid statement in yaml world, the correct is key2: values2.
configmap: 
  key=values
key2=values2

to fix it we have to use indent, but it will do indent the first line additionally. to address that, trim is used.
"
66098797,kubernetes vpa for cronjob,"i need to run vpa for cronjob. i refer to this doc.
i think i followed it properly but it doesn't work for me.

using gke, 1.17
vpa version is vpa-release-0.8
i created cronjob and vpa with this file.

apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: hello
spec:
  schedule: &quot;*/1 * * * *&quot;
  jobtemplate:
    spec:
      template:
        metadata:
          labels:
            app: hello
        spec:          
          containers:
          - name: hello
            image: busybox
            imagepullpolicy: ifnotpresent
            args:
            - /bin/sh
            - -c
            - date; echo hello from the kubernetes cluster
          restartpolicy: onfailure
---
apiversion: autoscaling.k8s.io/v1
kind: verticalpodautoscaler
metadata:
  name: my-vpa
spec:
  targetref:
    apiversion: &quot;batch/v1beta1&quot;
    kind: cronjob
    name: hello
  updatepolicy:
    updatemode: &quot;auto&quot;

when i type this command:
kubectl describe vpa

i got this result:
name:         my-vpa
namespace:    default
labels:       &lt;none&gt;
annotations:  &lt;none&gt;
api version:  autoscaling.k8s.io/v1
kind:         verticalpodautoscaler
metadata:
  creation timestamp:  2021-02-08t07:38:23z
  generation:          2
  resource version:    3762
  self link:           /apis/autoscaling.k8s.io/v1/namespaces/default/verticalpodautoscalers/my-vpa
  uid:                 07803254-c549-4568-a062-144c570a8d41
spec:
  target ref:
    api version:  batch/v1beta1
    kind:         cronjob
    name:         hello
  update policy:
    update mode:  auto
status:
  conditions:
    last transition time:  2021-02-08t07:39:14z
    status:                false
    type:                  recommendationprovided
  recommendation:
events:  &lt;none&gt;

",<kubernetes><autoscaling><kubernetes-cronjob>,67650394,1,"
@mario oh!! so there was not enough time to get metrics to recommend
resource....   feb 10 at 2:36

yes, exactly. if the only task of your cronjob is to echo hello from the kubernetes cluster and exit you won't get any recommendations from vpa as this is not a resource-intensive task.
however if you modify your command so that it generates an artificial load in your cronjob-managed pods:
apiversion: batch/v1beta1
kind: cronjob
metadata:
  name: hello
spec:
  schedule: &quot;*/1 * * * *&quot;
  jobtemplate:
    spec:
      template:
        metadata:
          labels:
            app: hello
        spec:
          containers:
          - name: hello
            image: busybox
            imagepullpolicy: ifnotpresent
            args:
            - /bin/sh
            - -c
            - date; dd if=/dev/urandom | gzip -9 &gt;&gt; /dev/null
          restartpolicy: onfailure

after a few minutes you'll get the expected result:
$ kubectl describe vpa my-vpa
name:         my-vpa
namespace:    default
labels:       &lt;none&gt;
annotations:  &lt;none&gt;
api version:  autoscaling.k8s.io/v1
kind:         verticalpodautoscaler
metadata:
  creation timestamp:  2021-05-22t13:02:27z
  generation:          8

...

    manager:         vpa-recommender
    operation:       update
    time:            2021-05-22t13:29:40z
  resource version:  5534471
  self link:         /apis/autoscaling.k8s.io/v1/namespaces/default/verticalpodautoscalers/my-vpa
  uid:               e37abd79-296d-4f72-8bd5-f2409457e9ff
spec:
  target ref:
    api version:  batch/v1beta1
    kind:         cronjob
    name:         hello
  update policy:
    update mode:  auto
status:
  conditions:
    last transition time:  2021-05-22t13:39:40z
    status:                false
    type:                  lowconfidence
    last transition time:  2021-05-22t13:29:40z
    status:                true
    type:                  recommendationprovided
  recommendation:
    container recommendations:
      container name:  hello
      lower bound:
        cpu:     1185m
        memory:  2097152
      target:
        cpu:     1375m
        memory:  2097152
      uncapped target:
        cpu:     1375m
        memory:  2097152
      upper bound:
        cpu:     96655m
        memory:  115343360
events:          &lt;none&gt;

important: just don't leave it running for too long as you might be quite surprised with your bill 
"
72635667,authenticate to gke cluster without setting google_application_credentials environment variable,"what is the best way to authenticate to a gke cluster without actually setting env variables for google_application_credentials or kubeconfig. i have an application running on a container which has to communicate with multiple gke clusters at once. hence i am trying to generate the kubeconfig yaml file on the fly like so. but when i try to run any kubectl commands i get a non authorized error as it is expecting the service account creds to be set in the environment variable. is there any way to actually avoid this and auth to multiple clusters on the fly by generating the kubeconfig file ?
apiversion: v1
clusters:
- cluster:
    certificate-authority-data: {{.certdata}}
    server: {{.masterurl}}
  name: {{.clustername}}
contexts:
- context:
    cluster: {{.clustername}}
    user: {{.clustername}}
  name: {{.clustername}}
current-context: {{.clustername}}
kind: config
preferences: {}
users:
- name: {{.clustername}}
  user:
    auth-provider:
      config:
        cmd-args: config config-helper --format=json
        cmd-path: /usr/local/bin/gcloud/google-cloud-sdk/bin/gcloud
        expiry-key: '{.credential.token_expiry}'
        token-key: '{.credential.access_token}'
      name: {{.clustername}}

",<kubernetes><google-cloud-platform><google-kubernetes-engine><kubectl><kubeconfig>,72637214,1,"i ended up using oauth2 to generate a token and inpute that value in the kubeconfig token.
sample code to extract the token

func getgketoken(creds []byte, ctx context.context) (string, error){
    var token *oauth2.token
    scopes := []string{&quot;https://www.googleapis.com/auth/cloud-platform&quot;,}
    cred, err := auth.credentialsfromjson(ctx, creds, scopes...)
    if err != nil {
        fmt.printf(&quot;failed to authenticate using credentials to extract token. error %s \n&quot;, err.error())
        return &quot;&quot;, err
    }
    
    token, err = cred.tokensource.token()
    if err != nil {
        return &quot;&quot;, err
    }

    return token.accesstoken, nil
}

gke kubeconfig template
apiversion: v1
clusters:
- cluster:
    certificate-authority-data: {{.certdata}}
    server: {{.masterurl}}
  name: {{.clustername}}
contexts:
- context:
    cluster: {{.clustername}}
    user: {{.clustername}}
  name: {{.clustername}}
current-context: {{.clustername}}
kind: config
preferences: {}
users:
- name: {{.clustername}}
  user:
    token: {{.accesstoken}}

"
65674473,deploy mongodb in eks/efs with mongo operator,"i'm trying to deploy mongodb with the kubernetes operator on aws eks with efs for the storage class. just following the documentation examples here:
https://github.com/mongodb/mongodb-kubernetes-operator
i don't understand how to define the pvc naming properly. i've looked through the github issues and stack overflow. just not finding the example to resolve what seems to be a simple issue.

apiversion: mongodb.com/v1
kind: mongodb
metadata:
  name: mongodb
spec:
  members: 1
  type: replicaset
  version: &quot;4.2.6&quot;
  security:
    authentication:
      modes: [&quot;scram&quot;]
  users:
    - name: my-user
      db: admin
      passwordsecretref: # a reference to the secret that will be used to generate the user's password
        name: my-user-password
      roles:
        - name: clusteradmin
          db: admin
        - name: useradminanydatabase
          db: admin
      scramcredentialssecretname: my-scram
  statefulset:
    spec:
     volumeclaimtemplates:
       - metadata:
         name: data-volume
         spec:
          accessmodes: [ &quot;readwriteonce&quot; ]
          resources:
            requests:
              storage: 1gi
            storageclassname: &quot;efs-sc&quot;

events:
create pod mongodb-0 in statefulset mongodb failed error: failed to create pvc -mongodb-0: persistentvolumeclaim &quot;-mongodb-0&quot; is invalid: metadata.name: invalid value: &quot;-mongodb-0&quot;: a dns-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*') 

",<mongodb><kubernetes><amazon-eks><amazon-efs>,65681904,1,"i replicated this error and i found an error in your yaml file. there is a missing indentation in the voluemclaimtemplates name.
this is your yaml file:
     volumeclaimtemplates:
       - metadata:
       name: data-volume
         spec:
          accessmodes: [ &quot;readwriteonce&quot; ]
          resources:
            requests:
              storage: 1gi
            storageclassname: &quot;efs-sc&quot;

and this is the correct part with fixed indentation:
     volumeclaimtemplates:
       - metadata:
         name: data-volume
         spec:
          accessmodes: [ &quot;readwriteonce&quot; ]
          resources:
            requests:
              storage: 1gi
            storageclassname: &quot;efs-sc&quot;

it appears that because of this error the operator could not get the name correctly and he was trying to create volumes with its default template. you can verify that yourself with kubectl get sts mongodb -o yaml:
  volumeclaimtemplates:
  - apiversion: v1
    kind: persistentvolumeclaim
    metadata:
    spec:
      accessmodes:
      - readwriteonce
      resources:
        requests:
          storage: 1gi
      volumemode: filesystem
  - apiversion: v1
    kind: persistentvolumeclaim
    metadata:
      name: data-volume
    spec:
      accessmodes:
      - readwriteonce
      resources:
        requests:
          storage: 10g
      volumemode: filesystem


"
65662315,istio exclusion matching not working for healthz api without jwt principal,"my requestauthentication is this
apiversion: security.istio.io/v1beta1
kind: requestauthentication
metadata:
  name: testing-dev-authenticator
  namespace: istio-system
spec:
  selector:
    matchlabels:
      istio: ingressgateway
  jwtrules:
  - issuer: &quot;https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com&quot;
    jwksuri: &quot;https://securetoken.google.com/&lt;project-name&gt;&quot;

my authorizationpolicy is this
apiversion: security.istio.io/v1beta1
kind: authorizationpolicy
metadata:
 name: test-dev-authorizer-all-svc
 namespace: dev
spec:
 action: allow
 rules:
 - from:
   - source:
       notrequestprincipals: [&quot;*&quot;]
   to:
   - operation:
       notpaths: [&quot;/message/ping&quot;]

my requirement is i dont want jwt auth to check in the healthz(my case is /message/ping), but am getting always
response of the above is &quot;rbac: access denied&quot;
",<kubernetes><kubernetes-ingress><istio><istio-sidecar>,65680932,1,"
i wanted all the pods deployed in &quot;dev&quot; namespace to be authenticated except a healthcheck, path of it is path : [&quot;/user/ping&quot;, &quot;/message/ping&quot;] but iam unable to give both at a time

i've reproduced your issue and i think it's working as you wanted it to work.

there are my requestauthentication and authorizationpolicy yamls.
apiversion: security.istio.io/v1beta1
kind: requestauthentication
metadata:
  name: testing-dev-authenticator
  namespace: istio-system
spec:
  selector:
    matchlabels:
      istio: ingressgateway
  jwtrules:
  - issuer: &quot;testing@secure.istio.io&quot;
    jwksuri: &quot;https://raw.githubusercontent.com/istio/istio/release-1.8/security/tools/jwt/samples/jwks.json&quot;

---

apiversion: security.istio.io/v1beta1
kind: authorizationpolicy
metadata:
  name: require-jwt
  namespace: istio-system
spec:
  selector:
    matchlabels:
      istio: ingressgateway
  action: allow
  rules:
  - from:
    - source:
        requestprincipals: [&quot;*&quot;]
    to:
    - operation:
        paths: [&quot;/productpage&quot;]
  - to:
    - operation:
        paths: [&quot;/api/v1/products&quot;]

you can use the following to exclude path (e.g. &quot;/api/v1/products&quot; ) from jwt, when &quot;/productpage&quot; require jwt and will reject all requests without the token.
if you want to exclude more than one path then this should work:
paths: [&quot;/api/v1/products&quot;,&quot;/login&quot;]

so in your case that would be
paths: [&quot;/user/ping&quot;, &quot;/message/ping&quot;] 


i have tested above configuration on bookinfo application.
there is the token i have used
token=$(curl https://raw.githubusercontent.com/istio/istio/release-1.8/security/tools/jwt/samples/demo.jwt -s)

tests:
api/v1/products
without token
200
with token
200
------------------------------------------------------------------
/productpage
without token
403
with token
200


you also mentioned that you want to do that in particular namespace, then you could try with these requestauthentication and authorizationpolicy yamls.
apiversion: security.istio.io/v1beta1
kind: requestauthentication
metadata:
  name: testing-dev-authenticator
  namespace: dev
spec:
  jwtrules:
  - issuer: &quot;testing@secure.istio.io&quot;
    jwksuri: &quot;https://raw.githubusercontent.com/istio/istio/release-1.8/security/tools/jwt/samples/jwks.json&quot;

---

apiversion: security.istio.io/v1beta1
kind: authorizationpolicy
metadata:
 name: test-dev-only-authorized-api
 namespace: dev
spec:
 action: deny
 rules:
 - from:
   - source:
        notrequestprincipals: [&quot;*&quot;]
   to:
   - operation:
       paths: [&quot;/productpage&quot;]

also based on the bookinfo application.
tests:
api/v1/products
without token
200
with token
200
------------------------------------------------------------------
/productpage
without token
403
with token
200


additional resources:

disable requestauthentication jwt rules for specific paths
authorization with jwt
authentication policy

"
65698905,kafka connection refused with kubernetes nodeport,"i am trying to expose kafka in my kubernetes setup for external usage using node port.
my helmcharts kafka-service.yaml is as follows:
apiversion: v1
kind: service
metadata:
  name: kafka
  namespace: test
  labels:
    app: kafka-test
    unit: kafka
spec:
  type: nodeport
  selector:
    app: test-app
    unit: kafka
    parentdeployment: test-kafka
  ports:
    - name: kafka
      port: 9092
      targetport: 9092
      nodeport: 30092
      protocol: tcp

kafka-deployment.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: kafka
  namespace: {{ .values.test.namespace }}
  labels:
    app: test-app
    unit: kafka
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: test-app
        unit: kafka
        parentdeployment: test-kafka
    spec:
      hostname: kafka
      subdomain: kafka
      securitycontext:
        fsgroup: {{ .values.test.groupid }}
      containers:
        - name: kafka
          image: test_kafka:{{ .values.test.kafkaimagetag }}
          imagepullpolicy: ifnotpresent
          ports:
            - containerport: 9092
          env:
            - name: is_kafka_cluster
              value: 'false'
            - name: kafka_zookeeper_connect
              value: zookeeper:2281
            - name: kafka_listeners
              value: ssl://:9092
            - name: kafka_keystore_path
              value: /opt/kafka/conf/kafka.keystore.jks
            - name: kafka_truststore_path
              value: /opt/kafka/conf/kafka.truststore.jks
            - name: kafka_keystore_password
              valuefrom:
                secretkeyref:
                  name: kafka-secret
                  key: jkskey
            - name: kafka_truststore_password
              valuefrom:
                secretkeyref:
                  name: kafka-secret
                  key: jkskey
            - name: kafka_log_dirs
              value: /opt/kafka/data
            - name: kafka_adv_listeners
              value: ssl://kafka:9092
            - name: kafka_client_auth
              value: none
          volumemounts:
            - mountpath: &quot;/opt/kafka/conf&quot;
              name: kafka-conf-pv
            - mountpath: &quot;/opt/kafka/data&quot;
              name: kafka-data-pv
      volumes:
        - name: kafka-conf-pv
          persistentvolumeclaim:
            claimname: kafka-conf-pvc
        - name: kafka-data-pv
          persistentvolumeclaim:
            claimname: kafka-data-pvc
  selector:
    matchlabels:
      app: test-app
      unit: kafka
      parentdeployment: test-kafka

zookeeper service yaml
apiversion: v1
kind: service
metadata:
  name: zookeeper
  namespace: {{ .values.test.namespace }}
  labels:
    app: test-ra
    unit: zookeeper
spec:
  type: clusterip
  selector:
    app: test-ra
    unit: zookeeper
    parentdeployment: test-zookeeper
  ports:
    - name: zookeeper
      port: 2281

zookeeper deployment yaml file
apiversion: apps/v1
kind: deployment
metadata:
  name: zookeeper
  namespace: {{ .values.test.namespace }}
  labels:
    app: test-app
    unit: zookeeper
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: test-app
        unit: zookeeper
        parentdeployment: test-zookeeper
    spec:
      hostname: zookeeper
      subdomain: zookeeper
      securitycontext:
        fsgroup: {{ .values.test.groupid }}
      containers:
        - name: zookeeper
          image: test_zookeeper:{{ .values.test.zookeeperimagetag }}
          imagepullpolicy: ifnotpresent
          ports:
            - containerport: 2281
          env:
            - name: is_zookeeper_cluster
              value: 'false'
            - name: zookeeper_ssl_client_port
              value: '2281'
            - name: zookeeper_data_dir
              value: /opt/zookeeper/data
            - name: zookeeper_data_log_dir
              value: /opt/zookeeper/data/log
            - name: zookeeper_keystore_path
              value: /opt/zookeeper/conf/zookeeper.keystore.jks
            - name: zookeeper_keystore_password
              valuefrom:
                secretkeyref:
                  name: zookeeper-secret
                  key: jkskey
            - name: zookeeper_truststore_path
              value: /opt/zookeeper/conf/zookeeper.truststore.jks
            - name: zookeeper_truststore_password
              valuefrom:
                secretkeyref:
                  name: zookeeper-secret
                  key: jkskey
          volumemounts:
            - mountpath: &quot;/opt/zookeeper/data&quot;
              name: zookeeper-data-pv
            - mountpath: &quot;/opt/zookeeper/conf&quot;
              name: zookeeper-conf-pv
      volumes:
        - name: zookeeper-data-pv
          persistentvolumeclaim:
            claimname: zookeeper-data-pvc
        - name: zookeeper-conf-pv
          persistentvolumeclaim:
            claimname: zookeeper-conf-pvc
  selector:
    matchlabels:
      app: test-ra
      unit: zookeeper
      parentdeployment: test-zookeeper

kubectl describe for kafka also shows exposed nodeport
type:                     nodeport
ip:                       10.233.1.106
port:                     kafka  9092/tcp
targetport:               9092/tcp
nodeport:                 kafka  30092/tcp
endpoints:                10.233.66.15:9092
session affinity:         none
external traffic policy:  cluster

i have a publisher binary that will send some messages into kafka. as i am having a 3 node cluster deployment, i am using my primary node ip and kafka node port (30092) to connect with the kafka.
but my binary is getting dial tcp &lt;primary_node_ip&gt;:9092: connect: connection refused error. i am unable to understand why is it getting rejected even after nodeport to targetport conversion is successful. with the further debugging i am seeing the following debug logs in the kafka logs:
[2021-01-13 08:17:51,692] debug accepted connection from /10.233.125.0:1564 on /10.233.66.15:9092 and assigned it to processor 0, sendbuffersize [actual|requested]: [102400|102400] recvbuffersize [actual|requested]: [102400|102400] (kafka.network.acceptor)
[2021-01-13 08:17:51,692] debug processor 0 listening to new connection from /10.233.125.0:1564 (kafka.network.processor)
[2021-01-13 08:17:51,702] debug [ssltransportlayer channelid=10.233.66.15:9092-10.233.125.0:1564-245 key=sun.nio.ch.selectionkeyimpl@43dc2246] ssl peer is not authenticated, returning anonymous instead (org.apache.kafka.common.network.ssltransportlayer)
[2021-01-13 08:17:51,702] debug [ssltransportlayer channelid=10.233.66.15:9092-10.233.125.0:1564-245 key=sun.nio.ch.selectionkeyimpl@43dc2246] ssl handshake completed successfully with peerhost '10.233.125.0' peerport 1564 peerprincipal 'user:anonymous' ciphersuite 'tls_ecdhe_rsa_with_aes_128_gcm_sha256' (org.apache.kafka.common.network.ssltransportlayer)
[2021-01-13 08:17:51,702] debug [socketserver brokerid=1001] successfully authenticated with /10.233.125.0 (org.apache.kafka.common.network.selector)
[2021-01-13 08:17:51,707] debug [socketserver brokerid=1001] connection with /10.233.125.0 disconnected (org.apache.kafka.common.network.selector)
java.io.eofexception
        at org.apache.kafka.common.network.ssltransportlayer.read(ssltransportlayer.java:614)
        at org.apache.kafka.common.network.networkreceive.readfrom(networkreceive.java:95)
        at org.apache.kafka.common.network.kafkachannel.receive(kafkachannel.java:448)
        at org.apache.kafka.common.network.kafkachannel.read(kafkachannel.java:398)
        at org.apache.kafka.common.network.selector.attemptread(selector.java:678)
        at org.apache.kafka.common.network.selector.pollselectionkeys(selector.java:580)
        at org.apache.kafka.common.network.selector.poll(selector.java:485)
        at kafka.network.processor.poll(socketserver.scala:861)
        at kafka.network.processor.run(socketserver.scala:760)
        at java.lang.thread.run(thread.java:748)

with the same configuration, i was able to expose other services. what am i missing here?
update: when i added kafka_listeners and kafka_adv_listeners for external and changed the targetport to 30092, the error message during external connections disappeared, but started getting connection errors for internal connections.
solution:
i exposed another service for external communication like mentioned in the answer and exposed 30092 as the port and the node port for it. so there was no requirement of targetport. i also had to add additional kafka_listeners and kafka_adv_listeners in the deployment file for external communication
",<kubernetes><apache-kafka><kubernetes-helm>,65758790,1,"we faced a similar issue in one of our kafka setups; we ended up creating two k8s services, one using clusterip for internal communication and second service with same labels using nodeport for external communication.
internal access
apiversion: v1
kind: service
metadata:
  name: kafka-internal
  namespace: test
  labels:
    app: kafka-test
    unit: kafka
spec:
  type: nodeport
  selector:
    app: test-app
    unit: kafka
    parentdeployment: test-kafka
  ports:
    - name: kafka
      port: 9092
      protocol: tcp
  type: clusterip

external access
apiversion: v1
kind: service
metadata:
  name: kafka-external
  namespace: test
  labels:
    app: kafka-test
    unit: kafka
spec:
  type: nodeport
  selector:
    app: test-app
    unit: kafka
    parentdeployment: test-kafka
  ports:
    - name: kafka
      port: 9092
      targetport: 9092
      protocol: tcp
  type: nodeport

"
72775892,"how do you deploy an application, in openshift, that listens on port 80 with a service that uses port 8080?","i am having a hard time understanding kubernetes networking and would like someone to explain it a little simpler than the documentation does.
my test service is below and contains all that i understand so far about ports and networking in openshift/kubernetes.
it is returning a 503 right now.
# a service can map any incoming port to a targetport. 
# by default and for convenience, the `targetport` 
# is set to the same value as the port field
apiversion: v1
kind: service
metadata:
  name: my-app
spec:
  ports:
    - name: http
      # port is the stable port the service exposes inside 
      # the cluster  other pods in the cluster send traffic
      # to this port
      port: 5000 
      protocol: tcp
      
      # if you dont specify a targetport value, 
      # it defaults to the value specified in port

      # targetport is the port that the container, 
      # within the pod, listens on for incoming traffic.
      targetport: 80 
      selector:
        app: my-app

route.yml - targetport: 80
deployment.yml - containerport: 80

",<docker><kubernetes><deployment><openshift><kubernetes-helm>,72779317,1,"you can read out the tiny code snippets under my github account to get familiar with k8s networking in nutshell: https://github.com/nirgeier/kuberneteslabs/tree/master/labs/05-services
the lab will describe in detail exactly what you are asking.

you can also print out the information (which you already know i assume)

kubectl explain pod.spec.containers.ports


to answer your question:

the easiest way to &quot;find&quot; out what is going on is to use cli to create the resources and examine the output

# create ns
kubectl create ns codewizard

# create the deployment
kubectl create deployment \
        nginx             \
        -n codewizard     \
        --image=nginx

# create the service
kubectl expose deployment \
        nginx             \
        -n codewizard     \
        --port 5000       \
        --target-port 80  \
        --type clusterip  # default

# check the service
kubectl get svc -n codewizard -o wide

# check to verify that the pod is connected
kubectl get      endpoints -n codewizard
kubectl describe endpoints -n codewizard



lets see the output
service
# get the service yaml 
kubectl get svc nginx -n codewizard -o yaml

### partial...
apiversion: v1
kind: service
...
spec:
  ports:
  - port: 5000
    protocol: tcp
    targetport: 80

lets try to test our network
# create the 2nd deployment for our test
kubectl create deployment \
        multitool         \
        -n codewizard     \
        --image=praqma/network-multitool

# lets get the pod name so we can test the connection to nginx
pod_name=$(kubectl get pods          \
                 --no-headers      \
                 -n codewizard     \
                 -o custom-columns=&quot;:metadata.name&quot; \
                 | grep multitool)

# test the connection 
kubectl exec -n codewizard $pod_name -- curl -svl nginx:5000


"
72726507,why local persistent volumes not visible in eks?,"in order to test if i can get self written software deployed in amazon using docker images,
i have a test eks cluster.
i have written a small test script that reads and writes a file to see if i understand how to deploy. i have successfully deployed it in minikube, using three replica's. the replica's all use a shared directory on my local file system, and in minikube that is mounted into the pods with a volume
the next step was to deploy that in the eks cluster. however, i cannot get it working in eks. the problem is that the pods don't see the contents of the mounted directory.
this does not completely surprise me, since in minikube i had to create a mount first to a local directory on the server. i have not done something similar on the eks server.
my question is what i should do to make this working (if possible at all).
i use this yaml file to create a pod in eks:
apiversion: v1
kind: persistentvolume
metadata:
  name: &quot;pv-volume&quot;
spec:
  storageclassname: local-storage
  capacity:
    storage: &quot;1gi&quot;
  accessmodes:
   - &quot;readwriteonce&quot;
  hostpath:
    path: /data/k8s
    type: directoryorcreate
---
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: &quot;pv-claim&quot;
spec:
  storageclassname: local-storage
  accessmodes:
  - &quot;readwriteonce&quot;
  resources:
    requests:
      storage: &quot;500m&quot;
---
apiversion: v1
kind: pod
metadata:
  name: ruudtest
spec:
  containers:
  - name: ruud
    image: myimage
    volumemounts:
    - name: cmount
      mountpath: &quot;/config&quot;
  volumes:
  - name: cmount
    persistentvolumeclaim:
      claimname: pv-claim

so what i expect is that i have a local directory, /data/k8s, that is visible in the pods as path /config.
when i apply this yaml, i get a pod that gives an error message that makes clear the data in the /data/k8s directory is not visible to the pod.
kubectl gives me this info after creation of the volume and claim
[rdgon@nl013-ppdapp015 probeer]$ kubectl get pv,pvc
name                                                        capacity   access modes   reclaim policy   status      claim              storageclass   reason   age
persistentvolume/pv-volume                                  1gi        rwo            retain           available                                              15s
persistentvolume/pvc-156edfef-d272-4df6-ae16-09b12e1c2f03   1gi        rwo            delete           bound       default/pv-claim   gp2                     9s

name                             status   volume                                     capacity   access modes   storageclass   age
persistentvolumeclaim/pv-claim   bound    pvc-156edfef-d272-4df6-ae16-09b12e1c2f03   1gi        rwo            gp2            15s

which seems to indicate everything is ok. but it seems that the filesystem of the master node, on which i run the yaml file to create the volume, is not the location where the pods look when they access the /config dir.
",<kubernetes><amazon-eks>,72730200,1,"on eks, there's no storage class named 'local-storage' by default.
there is only a 'gp2' storage class, which is also used when you don't specify a storageclassname.
the 'gp2' storage class creates a dedicated ebs volume and attaches it your kubernetes node when required, so it doesn't use a local folder. you also don't need to create the pv manually, just the pvc:
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: &quot;pv-claim&quot;
spec:
  storageclassname: gp2
  accessmodes:
  - &quot;readwriteonce&quot;
  resources:
    requests:
      storage: &quot;500m&quot;
---
apiversion: v1
kind: pod
metadata:
  name: ruudtest
spec:
  containers:
  - name: ruud
    image: myimage
    volumemounts:
    - name: cmount
      mountpath: &quot;/config&quot;
  volumes:
  - name: cmount
    persistentvolumeclaim:
      claimname: pv-claim

if you want a folder on the node itself, you can use a 'hostpath' volume, and you don't need a pv or pvc for that:
apiversion: v1
kind: pod
metadata:
  name: ruudtest
spec:
  containers:
  - name: ruud
    image: myimage
    volumemounts:
    - name: cmount
      mountpath: &quot;/config&quot;
  volumes:
  - name: cmount
    hostpath:
      path: /data/k8s

this is a bad idea, since the data will be lost if another node starts up, and your pod is moved to the new node.
if it's for configuration only, you can also use a configmap, and put the files directly in your kubernetes manifest files.
apiversion: v1
kind: configmap
metadata:
  name: ruud-config
data:
  ruud.properties: |
    my ruud.properties file content...
---
apiversion: v1
kind: pod
metadata:
  name: ruudtest
spec:
  containers:
  - name: ruud
    image: myimage
    volumemounts:
    - name: cmount
      mountpath: &quot;/config&quot;
  volumes:
  - name: cmount
    configmap:
      name: ruud-config

"
72716404,unable to deploy kubernetes secrets using helm,"i'm trying to create my first helm release on an aks cluster using a gitlab pipeline,
but when i run the following command
- helm upgrade server ./aks/server
      --install
      --namespace demo
      --kubeconfig ${ci_project_dir}/.kube/config
      --set image.name=${ci_project_name}/${ci_project_name}-server
      --set image.tag=${ci_commit_sha}
      --set database.user=${postgres_user}
      --set database.password=${postgres_password}

i receive the following error:
&quot;error: secret in version &quot;v1&quot; cannot be handled as a secret: v1.secret.data: 
decode base64: illegal base64 data at input byte 8, error found in #10 byte of ...&quot;

it looks like something is not working with the secrets file, but i don't understand what.
the secret.yaml template file is defined as follows:
apiversion: v1
kind: secret
metadata:
  name: server-secret
  namespace: demo
type: opaque
data:
  user: {{ .values.database.user }}
  host: {{ .values.database.host }}
  database: {{ .values.database.name }}
  password: {{ .values.database.password }}
  port: {{ .values.database.port }}

i will also add the deployment and the service .yaml files.
deployment.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: {{ .values.app.name }}
  labels:
    app: {{ .values.app.name }}
spec:
  replicas: {{ .values.replicacount }}
  selector:
    matchlabels:
      tier: backend
      stack: node
      app: {{ .values.app.name }}
  template:
    metadata:
      labels:
        tier: backend
        stack: node
        app: {{ .values.app.name }}
    spec:
      containers:
        - name: {{ .values.app.name }}
          image: &quot;{{ .values.image.name }}:{{ .values.image.tag }}&quot;
          imagepullpolicy: ifnotpresent
          env:
          - name: user
            valuefrom:
              secretkeyref:
                name: server-secret
                key: user
                optional: false
          - name: host
            valuefrom:
              secretkeyref:
                name: server-secret
                key: host
                optional: false
          - name: database
            valuefrom:
              secretkeyref:
                name: server-secret
                key: database
                optional: false
          - name: password
            valuefrom:
              secretkeyref:
                name: server-secret
                key: password
                optional: false
          - name: ports
            valuefrom:
              secretkeyref:
                name: server-secret
                key: ports
                optional: false
          resources:
            limits:
              cpu: &quot;1&quot;
              memory: &quot;128m&quot;
          ports:
            - containerport: 3000

service.yaml
apiversion: v1
kind: service
metadata:
  name: server-service
spec:
  type: clusterip
  selector:
    tier: backend
    stack: node
    app: {{ .values.app.name }}
  ports:
    - protocol: tcp
      port: 3000
      targetport: 3000

any hint?
",<kubernetes><kubernetes-helm>,72716933,1,"you have to encode secret values to base64
check the doc encoding-functions
try below code
apiversion: v1
kind: secret
metadata:
  name: server-secret
  namespace: demo
type: opaque
data:
  user: {{ .values.database.user | b64enc }}
  host: {{ .values.database.host | b64enc }}
  database: {{ .values.database.name | b64enc }}
  password: {{ .values.database.password | b64enc }}
  port: {{ .values.database.port | b64enc }}

else use stringdata instead of data
stringdata will allow you to create the secrets without encode to base64
check the example in the link
apiversion: v1
kind: secret
metadata:
  name: server-secret
  namespace: demo
type: opaque
stringdata:
  user: {{ .values.database.user | b64enc }}
  host: {{ .values.database.host | b64enc }}
  database: {{ .values.database.name | b64enc }}
  password: {{ .values.database.password | b64enc }}
  port: {{ .values.database.port | b64enc }}

"
65516708,resource not found error performing ssa create using dynamic client,"i was following @ymmt2005 excellent dynamic client guide. all is good until the final step when i make the actual patch call, and i get a the server could not find the requested resource error. just about everything seems right, except i'm unsure about the 'fieldmanager' field in the pathoptions struct. i'm not sure what &quot;the actor or entity that is making these changes&quot; refers to. does this need to match something in my code or system? any other ideas?
package main

import (
...
)

const resourceyaml = `
apiversion: apps/v1
kind: deployment
metadata:
  name: mike-nginx
spec:
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: 'nginx:latest'
          ports:
            - containerport: 80
`

func main() {
    ctx := context.background()

    // create dynamic discovery client from local kubeconfig file
    kubepath := filepath.join(homedir.homedir(), &quot;.kube&quot;, &quot;config&quot;)
    cfg, err := clientcmd.buildconfigfromflags(&quot;&quot;, kubepath)
    if err != nil {
        log.fatalf(&quot;error building config, %v\n&quot;, err)
    }
    dynclient, err := dynamic.newforconfig(cfg)
    if err != nil {
        log.fatalf(&quot;error creating client, %v\n&quot;, err)
    }
    disclient, err := discovery.newdiscoveryclientforconfig(cfg)
    if err != nil {
        log.fatalf(&quot;error creating discovery client, %v\n&quot;, err)
    }

    // decode yaml manifest &amp; get gvk
    decodeunstr := yaml.newdecodingserializer(unstructured.unstructuredjsonscheme)
    obj := &amp;unstructured.unstructured{}
    _, gvk, err := decodeunstr.decode([]byte(resourceyaml), nil, obj)
    if err != nil {
        log.fatalf(&quot;error decoding manifest, %v\n&quot;, err)
    }
    jsonobj, err := json.marshal(obj)
    if err != nil {
        log.fatalf(&quot;error marshaling object, %v\n&quot;, err)
    }

    // find gvr using gvk
    mapper := restmapper.newdeferreddiscoveryrestmapper(memory.newmemcacheclient(disclient))
    mapping, err := mapper.restmapping(gvk.groupkind(), gvk.version)
    if err != nil {
        log.fatalf(&quot;error finding gvr, %v\n&quot;, err)
    }

    // get rest interface for the gvr, checking for namespace or cluster-wide
    var dr dynamic.resourceinterface
    if mapping.scope.name() == meta.restscopenamenamespace {
        // namespaced resource
        dr = dynclient.resource(mapping.resource).namespace(obj.getnamespace())
    } else {
        // cluster-wide resource
        dr = dynclient.resource(mapping.resource)
    }

    // create or update the object with ssa
    options := metav1.patchoptions{fieldmanager: &quot;sample-controller&quot;}
    _, err = dr.patch(ctx, obj.getname(), types.applypatchtype, jsonobj, options)
    if err != nil {
        log.fatalf(&quot;error patching, %v\n&quot;, err)
    }
}

[edit] i confirmed that i was only able to use 'patch' on a resource that already existed. i tweaked the code to use 'create' to create the resource, then i was able to successfully do a 'patch' against it to change. to overcome the fieldmanager inconsistencies i added force=true to the patchoptions which is recommended in the docs anyway. i'd still like to know how i can create if resource doesn't exist and update if it does--maybe just test for exist?
",<kubernetes><kubernetes-go-client>,65531925,1,"the answer is really trivial. the original code assumes that namespace is provided in the manifest. the deployment endpoint does not automatically set namespace to default if the provided namespace is &quot;&quot;, and errors out because &quot;&quot; is not a valid namespace. therefore, i added logic to set namespace to default if not provided and presto, the server side apply will create the resource if it doesn't exist and update if it does exist. thanks again @ymmt2005 .
package main

import (
...
)

const resourceyaml = `
apiversion: apps/v1
kind: deployment
metadata:
  name: mike-nginx
spec:
  selector:
    matchlabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: 'nginx:latest'
          ports:
            - containerport: 80
`

func main() {
    ctx := context.background()

    // create dynamic discovery client from local kubeconfig file
    kubepath := filepath.join(homedir.homedir(), &quot;.kube&quot;, &quot;config&quot;)
    cfg, err := clientcmd.buildconfigfromflags(&quot;&quot;, kubepath)
    if err != nil {
        log.fatalf(&quot;error building config, %v\n&quot;, err)
    }
    dynclient, err := dynamic.newforconfig(cfg)
    if err != nil {
        log.fatalf(&quot;error creating client, %v\n&quot;, err)
    }
    disclient, err := discovery.newdiscoveryclientforconfig(cfg)
    if err != nil {
        log.fatalf(&quot;error creating discovery client, %v\n&quot;, err)
    }

    // decode yaml manifest &amp; get gvk
    decodeunstr := yaml.newdecodingserializer(unstructured.unstructuredjsonscheme)
    obj := &amp;unstructured.unstructured{}
    _, gvk, err := decodeunstr.decode([]byte(resourceyaml), nil, obj)
    if err != nil {
        log.fatalf(&quot;error decoding manifest, %v\n&quot;, err)
    }
    jsonobj, err := json.marshal(obj)
    if err != nil {
        log.fatalf(&quot;error marshaling object, %v\n&quot;, err)
    }

    // find gvr using gvk
    mapper := restmapper.newdeferreddiscoveryrestmapper(memory.newmemcacheclient(disclient))
    mapping, err := mapper.restmapping(gvk.groupkind(), gvk.version)
    if err != nil {
        log.fatalf(&quot;error finding gvr, %v\n&quot;, err)
    }

    // set namespace to default if not provided in manifest
    var ns string
    if ns = obj.getnamespace(); ns == &quot;&quot; {
        ns = &quot;default&quot;
    }

    // get rest interface for the gvr, checking for namespace or cluster-wide
    var dr dynamic.resourceinterface
    if mapping.scope.name() == meta.restscopenamenamespace {
        // namespaced resource
        dr = dynclient.resource(mapping.resource).namespace(ns)
    } else {
        // cluster-wide resource
        dr = dynclient.resource(mapping.resource)
    }

    // create or update the object with ssa
    options := metav1.patchoptions{fieldmanager: &quot;sample-controller&quot;}
    _, err = dr.patch(ctx, obj.getname(), types.applypatchtype, jsonobj, options)
    if err != nil {
        log.fatalf(&quot;error patching, %v\n&quot;, err)
    }
}

"
73352409,kubernetes service per pod metacontroller,"i was trying to setup service per pod in stateful set with metacontroller: https://github.com/metacontroller/metacontroller/tree/master/examples/service-per-pod
i was following linked instruction, i was playing with various combination, but i still didn't achieve what is described. services are not created together with pod.
in this example, i was trying to put service-per-pod-label: &quot;web&quot; against the name of the pod.
apiversion: apps/v1
kind: statefulset
metadata:
  name: nginx
  annotations:
    service-per-pod-label: &quot;statefulset.kubernetes.io/pod-name&quot;
    service-per-pod-ports: &quot;80:80&quot;
spec:
  selector:
    matchlabels:
      app: nginx
  servicename: nginx
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      terminationgraceperiodseconds: 1
      containers:
      - name: nginx
        image: gcr.io/google_containers/nginx-slim:0.8
        ports:
        - containerport: 80
          name: web

my metacontroller is up and running
name                                   ready   status    restarts   age
metacontroller-metacontroller-helm-0   1/1     running   0          23m

logs from metacontroller:
kubectl logs metacontroller-metacontroller-helm-0
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1660492483.489682,&quot;msg&quot;:&quot;configuration information&quot;,&quot;discovery-interval&quot;:20,&quot;cache-flush-interval&quot;:1800,&quot;metrics-address&quot;:&quot;:9999&quot;,&quot;client-go-qps&quot;:5,&quot;client-go-burst&quot;:10,&quot;workers&quot;:5,&quot;events-qps&quot;:0.0033333333333333335,&quot;events-burst&quot;:25,&quot;pprofaddr&quot;:&quot;0&quot;,&quot;leader-election&quot;:false,&quot;leader-election-resource-lock&quot;:&quot;leases&quot;,&quot;leader-election-namespace&quot;:&quot;&quot;,&quot;leader-election-id&quot;:&quot;metacontroller&quot;,&quot;version&quot;:&quot;4.3.4&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1660492483.5292923,&quot;msg&quot;:&quot;communication with k8s api server successful&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1660492484.7299764,&quot;msg&quot;:&quot;waited for 1.19273971s due to client-side throttling, not priority and fairness, request: get:https://10.3.0.1:443/apis/authorization.k8s.io/v1?timeout=32s\n&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1660492489.1358352,&quot;logger&quot;:&quot;controller-runtime.metrics&quot;,&quot;msg&quot;:&quot;metrics server is starting to listen&quot;,&quot;addr&quot;:&quot;:9999&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1660492489.1373081,&quot;msg&quot;:&quot;starting eventsource&quot;,&quot;controller&quot;:&quot;composite-metacontroller&quot;,&quot;source&quot;:&quot;kind source: *v1alpha1.compositecontroller&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1660492489.1373482,&quot;msg&quot;:&quot;starting controller&quot;,&quot;controller&quot;:&quot;composite-metacontroller&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1660492489.1376293,&quot;msg&quot;:&quot;starting eventsource&quot;,&quot;controller&quot;:&quot;decorator-metacontroller&quot;,&quot;source&quot;:&quot;kind source: *v1alpha1.decoratorcontroller&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1660492489.1376536,&quot;msg&quot;:&quot;starting controller&quot;,&quot;controller&quot;:&quot;decorator-metacontroller&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1660492489.1371636,&quot;msg&quot;:&quot;starting server&quot;,&quot;path&quot;:&quot;/metrics&quot;,&quot;kind&quot;:&quot;metrics&quot;,&quot;addr&quot;:&quot;[::]:9999&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1660492489.2376945,&quot;msg&quot;:&quot;starting workers&quot;,&quot;controller&quot;:&quot;composite-metacontroller&quot;,&quot;worker count&quot;:1}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1660492489.2379737,&quot;msg&quot;:&quot;starting workers&quot;,&quot;controller&quot;:&quot;decorator-metacontroller&quot;,&quot;worker count&quot;:1}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1660492494.7370083,&quot;msg&quot;:&quot;waited for 5.587625994s due to client-side throttling, not priority and fairness, request: get:https://10.3.0.1:443/apis/networking.k8s.io/v1?timeout=32s\n&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1660492510.3382494,&quot;msg&quot;:&quot;waited for 1.165526209s due to client-side throttling, not priority and fairness, request: get:https://10.3.0.1:443/apis/flowcontrol.apiserver.k8s.io/v1beta2?timeout=32s\n&quot;}

",<kubernetes><kubernetes-helm><kubernetes-pod>,73352831,1,"as explained in the documentation, for kubernetes 1.9+, your service-per-pod-label should have &quot;statefulset.kubernetes.io/pod-name&quot; as value:
apiversion: apps/v1
kind: statefulset
metadata:
  name: nginx
  annotations:
    service-per-pod-label: &quot;statefulset.kubernetes.io/pod-name&quot;
    service-per-pod-ports: &quot;80:80&quot;
spec:
  selector:
    matchlabels:
      app: nginx
  servicename: nginx
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      terminationgraceperiodseconds: 1
      containers:
      - name: nginx
        image: gcr.io/google_containers/nginx-slim:0.8
        ports:
        - containerport: 80
          name: web

and for earlier versions, you should add the annotation pod-name-label: &quot;pod-name&quot; for your pod`:
apiversion: apps/v1
kind: statefulset
metadata:
  name: nginx
  annotations:
    service-per-pod-label: &quot;pod-name&quot;
    service-per-pod-ports: &quot;80:80&quot;
spec:
  selector:
    matchlabels:
      app: nginx
  servicename: nginx
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
      annotations:
        pod-name-label: &quot;pod-name&quot;
    spec:
      terminationgraceperiodseconds: 1
      containers:
      - name: nginx
        image: gcr.io/google_containers/nginx-slim:0.8
        ports:
        - containerport: 80
          name: web

in the two cases, you should use the term pod-name, and not another value. they use it to know the name they should use for the created services.
"
65575064,network policy to restrict communication of pods within namespace and port,"namespace 1: arango
namespace 2: apache - 8080
criteria to acheive:
the policy should not allow pods which are not listening in port 8080
the policy should not allow pods from any other namespace except &quot;arango&quot;
is the following ingress help acheive this? or is it manadtory to add egress as there are rules to deny other namespace pods and ports except 8080?
apiversion: networking.k8s.io/v1
metadata:
  name: access-nginx
spec:
  podselector:
    matchlabels:
      app: arango
  ingress:
  - from:
    - podselector:
        matchlabels:
          app: apache
    ports:
    - protocol: tcp
      port: 8080

",<kubernetes><kubernetes-networkpolicy>,65581919,1,"your current config
your current configuration, is allowing traffic to pods with label app: arango in default namespace on port: 8080 from pods which have label app: apache in default namespace
it will apply to default namespace as you didn't specify it. if namespace is not defined, kubernetes always is using default namespace.
questions

or is it manadtory to add egress as there are rules to deny other namespace pods and ports except 8080?

it depends on your requirements, if you want filter traffic from your pod to outside, from outside to your pod or both. it's well describe in network policy resource documentation.
networkpolicy is namespaced resource so it will run in the namespace it was created in. if you want to allow another namespaces you should use namespaceselector

the policytypes field indicates whether or not the given policy applies to ingress traffic to selected pod, egress traffic from selected pods, or both. if no policytypes are specified on a networkpolicy then by default ingress will always be set and egress will be set if the networkpolicy has any egress rules.

to sum up, ingress traffic is from outside to your pods and egress is from your pods to outside.
you want to apply two main rules:

the policy should not allow pods which are not listening in port 8080

if you would like to use this only for ingress traffic, it would looks like:
  ingress:
  - from:
    ports:
    - protocol: &lt;protocol&gt;
      port: 8080


the policy should not allow pods from any other namespace except &quot;arango&quot;

please keep in mind that networkpolicy is namespaced resource thus it will work in the namespace which was created. it should be specify in metadata.namespace
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: network-policy
  namespace: arango
spec:
...

requested network policy
i have tested this on my gke cluster with network policy enabled.
in example below, incoming traffic to pods with label app: arango in arango namespace are allowed only if they come from pod with label app: apache, are listening on port: 8080 and were deployed in arango namespace.
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: access-nginx
  namespace: arango
spec:
  podselector:
    matchlabels:
      app: arango
  ingress:
  - from:
    - podselector:
        matchlabels:
          app: apache
    ports:
    - protocol: tcp
      port: 8080

useful links:
guide to kubernetes ingress network policies
get started with kubernetes network policy
if this answer didn't solve your issue, please clarify/provide more details how it should work and i will edit answer.
"
65438260,kubernetes expanding pvc for cluster,"i have a pvc on my cluster.
i can expand it on my provider (digital ocean)
but then on the cluster do i need to somehow let it know that it expanded?
here is my file that i deployed to enable the creation of the pvc
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: test-pvc
  namespace: test
spec:
  accessmodes:
  - readwriteonce
  resources:
    requests:
      storage: 4gi
  storageclassname: do-block-storage

",<kubernetes><kubernetes-pvc>,65438629,1,"resizing volumes other than through the pvc object (e.g., the digitalocean cloud control panel) is not recommended as this can potentially cause conflicts. additionally, size updates will not be reflected in the pvc object status section immediately, and the section will eventually show the actual volume capacity
the recommendation is to update the capacity through the pvc object using kubectl edit and the csi driver will update the volume by calling digital ocean api. from the docs
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: csi-pvc
  namespace: default
spec:
  [...]
  resources:
    requests:
      # the field below can be increased.
      storage: 10gi
      [...]

after successful expansion, the status section of the pvc object will reflect the actual volume capacity.
"
65413720,how can i reference value from one kubernetes resource when defining another resource,"i'm using gke and helm v3 and i'm trying to create/reserve a static ip address using computeaddress and then to create dns a record with the previously reserved ip address.
reserve ip address
apiversion: compute.cnrm.cloud.google.com/v1beta1
kind: computeaddress
metadata:
  name: ip-address
  annotations:
    cnrm.cloud.google.com/project-id: project-id
spec:
  location: global

get reserved ip address
kubectl get computeaddress ip-address -o jsonpath='{.spec.address}'

create dns a record
apiversion: dns.cnrm.cloud.google.com/v1beta1
kind: dnsrecordset
metadata:
  name: dns-record-a
  annotations:
    cnrm.cloud.google.com/project-id: project-id
spec:
  name: &quot;{{ .release.name }}.example.com&quot;
  type: &quot;a&quot;
  ttl: 300
  managedzoneref:
    external: example-com
  rrdatas:
    - **ip-address-value** &lt;----

is there a way to reference the ip address value, created by computeaddress, in the dnsrecordset resource?
basically, i need something similar to the output values in terraform.
thanks!
",<kubernetes><google-cloud-platform><google-kubernetes-engine><kubernetes-helm>,65513053,1,"it's interesting that something similar exists for gke ingress where we can reference reserved ip address and managed ssl certificate using annotations:
annotations:
  kubernetes.io/ingress.global-static-ip-name: my-static-address

i have no idea why there is not something like this for dnsrecordset resource. hopefully, gke will introduce it in the future.
instead of running two commands, i've found a workaround by using helm's hooks.
first, we need to define job as post-install and post-upgrade hook which will pick up the reserved ip address when it becomes ready and then create appropriate dnsrecordset resource with it. the script which retrieves the ip address, and manifest for dnsrecordset are passed through configmap and mounted to pod.
apiversion: batch/v1
kind: job
metadata:
  name: &quot;{{ .release.name }}-dns-record-set-hook&quot;
  annotations:
    # this is what defines this resource as a hook. without this line, the
    # job is considered part of the release.
    &quot;helm.sh/hook&quot;: post-install,post-upgrade
    &quot;helm.sh/hook-delete-policy&quot;: before-hook-creation,hook-succeeded
spec:
  template:
    metadata:
      name: &quot;{{ .release.name }}-dns-record-set-hook&quot;
    spec:
      restartpolicy: onfailure
      containers:
        - name: post-install-job
          image: alpine:latest
          command:  ['sh', '-c', '/opt/run-kubectl-command-to-set-dns.sh']
          volumemounts:
            - name: volume-dns-record-scripts
              mountpath: /opt
            - name: volume-writable
              mountpath: /mnt
      volumes:
        - name: volume-dns-record-scripts
          configmap:
            name: dns-record-scripts
            defaultmode: 0777
        - name: volume-writable
          emptydir: {}

configmap definition with the script and manifest file:
apiversion: v1
kind: configmap
metadata:
  creationtimestamp: null
  name: dns-record-scripts
data:
  run-kubectl-command-to-set-dns.sh: |-
    # install kubectl command
    apk add curl &amp;&amp; \
    curl -lo https://storage.googleapis.com/kubernetes-release/release/v1.15.1/bin/linux/amd64/kubectl &amp;&amp; \
    chmod u+x kubectl &amp;&amp; \
    mv kubectl /bin/kubectl

    # wait for reserved ip address to be ready
    kubectl wait --for=condition=ready computeaddress/ip-address

    # get reserved ip address
    ip_address=$(kubectl get computeaddress ip-address -o jsonpath='{.spec.address}')

    echo &quot;reserved address: $ip_address&quot;
  
    # update ip_address in manifest
    sed &quot;s/##ip_address##/$ip_address/g&quot; /opt/dns-record.yml &gt; /mnt/dns-record.yml
  
    # create dns record
    kubectl apply -f /mnt/dns-record.yml

  dns-record.yml: |-
    apiversion: dns.cnrm.cloud.google.com/v1beta1
    kind: dnsrecordset
    metadata:
      name: dns-record-a
      annotations:
        cnrm.cloud.google.com/project-id: project-id
    spec:
      name: &quot;{{ .release.name }}.example.com&quot;
      type: a
      ttl: 300
      managedzoneref:
        external: example-com
      rrdatas:
        - &quot;##ip_address##&quot;

and, finally, for (default) service account to be able to retrieve the ip address and create/update dnsrecordset, we need to assign some roles to it:
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  name: dnsrecord-setter
rules:
  - apigroups: [&quot;compute.cnrm.cloud.google.com&quot;]
    resources: [&quot;computeaddresses&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;]
  - apigroups: [&quot;dns.cnrm.cloud.google.com&quot;]
    resources: [&quot;dnsrecordsets&quot;]
    verbs: [&quot;get&quot;, &quot;create&quot;, &quot;patch&quot;]
---
apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: dnsrecord-setter
subjects:
  - kind: serviceaccount
    name: default
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: dnsrecord-setter


"
65432613,kubernetes ingress subdomain - cloudfare setup,"so i have my own website that i am running and i want to migrate one of my services to my current cluster under a subdomain of my actual website and i'm having some trouble.
i have a website that i purchased off of namecheap and i'm using cloudfare for all the dns stuff. so everything is setup correctly. what i can't seem to figure out what to do is getting my subdomain website to actually work.
i have tried to add a &quot;a&quot; and &quot;cname&quot; record and still can't get it to work.
i also tried to follow this site and got no luck. i have tried other stackoverflow links and links posted by cloudfare. but i couldn't get anything to work still:
https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nginx-ingress-with-cert-manager-on-digitalocean-kubernetes
my services are also running with no issues. my pods and deployments are also fine showing no errors and my website is already running on another link which i'm removing to save money. www.ecoders.ca. all i did to migrate over my service was add stuff to my ingress and re-deploy everything to my current cluster. on my current cluster i'm using nginx.
lmk if more information is required.
ingress.yaml
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/from-to-www-redirect: &quot;true&quot;
#    nginx.ingress.kubernetes.io/rewrite-target: /
  name: ingress
spec:
  rules:
   - host: www.foo.com
     http:
       paths:
         - backend:
             servicename: nk-webapp-service
             serviceport: 80
           path: /
         - backend:
             servicename: stockapp-service
             serviceport: 80
           path: /stock
   - host: www.bar.foo.com &lt;----------- this does not work
     http:
       paths:
         - backend:
             servicename: ecoders-webapi-service
             serviceport: 80
           path: /



cloudfare setup

cname -&gt; www -&gt; foo.com  
cname -&gt; bar -&gt; foo.com  
a -&gt; foo.com -&gt; ip address  

service setup
apiversion: apps/v1
kind: deployment
metadata:
  name: ecoders-webapi
spec:
  replicas: 1
  selector:
    matchlabels:
      name: ecoders-webapi
  template:
    metadata:
      labels:
        name: ecoders-webapi
    spec:
      containers:
        - name: webapi
          image: astronik/webservice:latest
          imagepullpolicy: always
          ports:
            - containerport: 8080


apiversion: v1
kind: service
metadata:
  name: ecoders-webapi-service
spec:
  type: clusterip
  ports:
    - name: http
      port: 80
      targetport: 8080
  selector:
    name: ecoders-webapi


updated
ingress.yaml
   - host: www.bar.foo.com
     http:
       paths:
         - backend:
             servicename: ecoders-webapi-service
             serviceport: 80
           path: /
   - host: bar.foo.com
     http:
       paths:
         - backend:
             servicename: ecoders-webapi-service
             serviceport: 80
           path: /

i added in a &quot;www&quot; link version as well, and now i'm getting this (before i was getting noting):

does this have something to do with tls/ssl? meaning my subdomain doesn't have a certificate?
new update
so under &quot;ssl/tls&quot; on the cloudfare dashboard. once i turned it off i was able to access my subdomain with no problem. but now how do i get it to run on full? does my kubernetes cluster require a certificate?

solved
so it's all fixed up and it had to do with 2 little problems.
problem 1:

essentially i need to change my dns settings adding www. was adding another subdomain. i removed the 2 cnames i created before and did this.
a -&gt; bar -&gt; 10.0.0.0 
a -&gt; foo.com -&gt; 10.0.0.0 

problem 2:
needed to update my ingress to remove this line
nginx.ingress.kubernetes.io/from-to-www-redirect: &quot;true&quot;

updated ingress
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
  name: ingress
spec:
  rules:
   - host: foo.com
     http:
       paths:
         - backend:
             servicename: nk-webapp-service
             serviceport: 80
           path: /
         - backend:
             servicename: stockapp-service
             serviceport: 80
           path: /stock
   - host: bar.foo.com
     http:
       paths:
         - backend:
             servicename: ecoders-web-service
             serviceport: 80
           path: /

",<nginx><kubernetes><kubernetes-ingress>,65444012,1,"solved
so it's all fixed up and it had to do with 2 little problems. link cloudfare page
problem 1:

essentially i need to change my dns settings adding www. was adding another subdomain. i removed the 2 cnames i created before and did this.
a -&gt; bar -&gt; 10.0.0.0 
a -&gt; foo.com -&gt; 10.0.0.0 

problem 2:
needed to update my ingress to remove this line
nginx.ingress.kubernetes.io/from-to-www-redirect: &quot;true&quot;

updated ingress
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
  name: ingress
spec:
  rules:
   - host: foo.com
     http:
       paths:
         - backend:
             servicename: nk-webapp-service
             serviceport: 80
           path: /
         - backend:
             servicename: stockapp-service
             serviceport: 80
           path: /stock
   - host: bar.foo.com
     http:
       paths:
         - backend:
             servicename: ecoders-web-service
             serviceport: 80
           path: /

"
65415295,is it possible to have a dynamic nameprefix/namesuffix in kustomize?,"in helm, it is possible to specify a release name using
helm install my-release-name chart-path
this means, i can specify the release name and its components (using fullname) using the cli.
in kustomize (i am new to kustomize), there is a similar concept, nameprefix and namesuffix which can be defined in a kustomization.yaml
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization

nameprefix: overlook-

resources:
- deployment.yaml

however, this approach needs a custom file, and using a &quot;dynamic&quot; nameprefix would mean that a kustomization.yaml has to be generated using a template and kustomize is, well,  about avoiding templating.
is there any way to specify that value dynamically?
",<kubernetes><kubernetes-helm><kustomize>,65425449,1,"you can use kustomize edit to edit the nameprefix and namesuffix values.
for example:
deployment.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: the-deployment
spec:
  replicas: 5
  template:
    containers:
      - name: the-container
        image: registry/conatiner:latest

kustomization.yaml
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization

resources:
- deployment.yaml

then you can run kustomize edit set nameprefix dev- and kustomize build . will return following:
apiversion: apps/v1
kind: deployment
metadata:
  name: dev-the-deployment
spec:
  replicas: 5
  template:
    containers:
    - image: registry/conatiner:latest
      name: the-container

"
73292839,reference tls certificate used by aks's ingress from azure key vault,"i'm setting up an ingress for the application resides in the aks. but ran into a problem on binding the certificate to the ingress.
as you can see below, i am trying to reference ingress-cert from the kv and use it in the ingress through secretproviderclass
apiversion: secrets-store.csi.x-k8s.io/v1
kind: secretproviderclass
metadata:
  name: {{ include &quot;secretprovider.name&quot; . }}
spec:
  provider: azure
  secretobjects:
    - secretname: ingress-tls-csi
      type: kubernetes.io/tls
      data:
        - objectname: ingress-cert
          key: tls.key
        - objectname: ingress-cert
          key: tls.crt
  parameters:
    usepodidentity: &quot;false&quot;
    usevmmanagedidentity: &quot;true&quot;
    userassignedidentityid: {{ .values.keyvault.identity }}
    keyvaultname: {{ .values.keyvault.name }}
    objects: |
      array:
        - |
          objectname: ingress-cert
          objecttype: secret
    tenantid: {{ .values.keyvault.tenant }}

apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: {{ include &quot;ingress.name&quot; . }}
  annotations:
    kubernetes.io/ingress.class: azure/application-gateway
    kubernetes.io/ingress.allow-http: &quot;false&quot;
    appgw.ingress.kubernetes.io/override-frontend-port: &quot;443&quot;
spec:
  tls:
    - hosts:
        - {{ .values.ingress.host }}
      secretname: ingress-tls-csi
    # property `rules` is omitted

it's working fine when accessing other secrets from pods through env but for the ingress this is the output on describing it:
name:             my-ingress
namespace:        my-namespace
address:          x.x.x.x
default backend:  default-http-backend:80
tls:
  ingress-tls-csi terminates my.example.com
rules:
  host                   path  backends
  ----                   ----  --------
annotations:             appgw.ingress.kubernetes.io/override-frontend-port: 443
                         kubernetes.io/ingress.allow-http: false
                         kubernetes.io/ingress.class: azure/application-gateway
events:
  type     reason          age                   from                       message
  ----     ------          ----                  ----                       -------
  warning  secretnotfound  3m3s (x2 over 3m10s)  azure/application-gateway  unable to find the secret associated to secretid: [my-namespace/ingress-tls-csi]


i've set up the kv integration by following the use the azure key vault provider for secrets store csi driver in an aks cluster documentation.
but upon following the set up secrets store csi driver to enable nginx ingress controller with tls as a hint on how to implement the same with agic. i noticed that the certificate is added as a secret inside the aks which is then referenced inside the ingress with secretname: ingress-tls-csi.
kubectl get secret -n $namespace

name                                             type                                  data   age
ingress-tls-csi                                  kubernetes.io/tls                     2      1m34s

i assume that ingress can't reference the secret directly from secretproviderclass as the example in the documentation need to use the ingress-tls-csi as a secret object which i assumed (again) created by ingress-nginx chart.
my question is how can i implement the same as the ingress-nginx example with agic?

additional information:

i used agic with azure cni networking.
ingress is currently working with manually added certificate with kubectl command. the reason i need to use the one from kv is the aks will also be used by other people deploying under the same domain but different namespace and i think it's a bad idea to give direct access to certificate's private key.

",<azure><kubernetes><kubernetes-ingress><azure-aks>,73307984,1,"as i couldn't find a way to integratethe ingress withthe azure key vault, i've implementeda workaround withgithub actions to retrieve the certificate and add it to the aks. because most of them are bash commands, the workaround isn't exclusive to github actions.
name: assign cerfiticate from kv to aks

on:
  workflow_dispatch:

jobs:
  build-push:
    name: assign cerfiticate from kv to aks
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v2

      # azure authentication
      - uses: azure/login@v1
        with:
          creds: ${{ secrets.azure_credentials }}

      - name: authenticate and set context to aks cluster
        run: az aks get-credentials --name &quot;my-aks-cluster&quot; --resource-group &quot;my-rg&quot; --admin

      - uses: azure/setup-helm@v3
        with:
          token: ${{ secrets.pat }}

      # retrieve certificate and private key from azure key vault
      - name: download certificate and private key in pkcs#12 format
        run: |
          az keyvault secret download --name my-certificate \
          --vault-name my-kv \
          --encoding base64 \
          -f certificate.pfx

      - name: extract private key in rsa format
        run: |
          openssl pkcs12 -in certificate.pfx -nocerts -nodes -passin pass: | openssl rsa -out private.key

      - name: extract certificate
        run: |
          openssl pkcs12 -in certificate.pfx -clcerts -nokeys -passin pass: -out certificate.crt

      - name: deploy kubernetes configuration
        run: |
          helm upgrade --install release-name chart-name \
          -n my-namespace \
          --set-file mychart.ingress.key=private.key \
          --set-file mychart.ingress.certificate=certificate.crt

the brief explanation for the above:

download the certificate and private key in .pfx format
use openssl to extract into .crt and .key
pass the extracted files into helm install/upgrade using --set-file

here is the secret and ingress configuration:
apiversion: v1
kind: secret
metadata:
  name: ingress-tls
type: kubernetes.io/tls
data:
  tls.crt: {{ .values.ingress.certificate | b64enc }}
  tls.key: {{ .values.ingress.key | b64enc }}

apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: {{ include &quot;ingress.name&quot; . }}
  annotations:
    kubernetes.io/ingress.class: azure/application-gateway
    kubernetes.io/ingress.allow-http: &quot;false&quot;
    appgw.ingress.kubernetes.io/override-frontend-port: &quot;443&quot;
spec:
  tls:
    - hosts:
        - {{ .values.ingress.host }}
      secretname: ingress-tls
  # property `rules` is omitted

no additional modification to the secretproviderclass is required.
i hope this is just a workaround because it'd be nicer if the ingress can directly integrate with azure key vault.
"
65936076,python pod can't connect to mongodb when using ingress,"it can connect fine whenever i try to access it via the worker node's address, but not when i try access via the ingress gateway. i get the following error:
pymongo.errors.serverselectiontimeouterror
pymongo.errors.serverselectiontimeouterror: mongo:27017: timed out, timeout: 30s, topology description: &lt;topologydescription id: 60119598e7c0e0d52f58c52c, topology_type: single, servers: [&lt;serverdescription ('mongo', 27017) server_type: unknown, rtt: none, error=networktimeout('mongo:27017: timed out',)&gt;]&gt;

this is how i connect to mongodb via python which works fine when not accessing over the ingress url.
mongo = mongoclient(&quot;mongodb://mongo:27017/user_data&quot;)

this is my ingress.yaml file
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: weasel-ingress
spec:
  rules:
  - host: {host-address}
    http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          servicename: weasel
          serviceport: 5000
      - path: /
        pathtype: prefix
        backend:
          servicename: mongo
          serviceport: 27017

any idea's on how to get it to connect via ingress? i guess i need to add mongo to the ingress?
both services are already exposed via external ip's.
kubectl get svc
name         type           cluster-ip       external-ip      port(s)           age
kubernetes   clusterip      172.21.0.1       &lt;none&gt;           443/tcp           19h
mongo        loadbalancer   172.21.218.91    {exposed-ip}   27017:31308/tcp   17h
weasel       loadbalancer   172.21.152.134   {exposed-ip}   5000:32246/tcp    17h

ingress logs:
kubectl describe ingress weasel-ingress
name:             weasel-ingress
namespace:        default
address:          
default backend:  default-http-backend:80 (&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;)
rules:
  host                                                                                  path  backends
  ----                                                                                  ----  --------
  {host-address}  
                                                                                        /   weasel:5000 (172.30.27.69:5000)
                                                                                        /   mongo:27017 (&lt;none&gt;)
annotations:                                                                            &lt;none&gt;
events:
  type    reason  age   from                      message
  ----    ------  ----  ----                      -------
  normal  create  27s   nginx-ingress-controller  ingress default/weasel-ingress
  normal  create  27s   nginx-ingress-controller  ingress default/weasel-ingress
  normal  create  27s   nginx-ingress-controller  ingress default/weasel-ingress

",<python><mongodb><kubernetes><kubernetes-ingress>,65936787,1,"it was a problem with my deployment.yaml. it needed to be changed to the following:
apiversion: v1
kind: service
metadata:
  name: mongo
  labels:
    app: mongo
spec:
  type: loadbalancer
  ports:
  - port: 27017
    name: http
  selector:
    app: mongo
---
apiversion: apps/v1
kind: deployment
metadata:
  name: mongo
spec:
  replicas: 1
  selector:
    matchlabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
        version: v1
    spec:
      containers:
        - name: mongo
          image: mongo:latest
          ports:
          - containerport: 27017

"
65846911,"kubectl trying to create certificatesigningrequest error :error from server (badrequest): error when creating ""tcsr.yaml"":certificatesigningrequest","i try to create certificatesigningrequest
apiversion: certificates.k8s.io/v1beta1
kind: certificatesigningrequest
metadata:
  name: vault-csr
spec:
  groups:
  - system:authenticated
  request: ls0tls1crudjtibdrvjusuzjq0fursbsrvfvrvnuls0tls0ktuljrkleq0nbd2ddqvfbd0lerwvnqdhqtfvruf3d1zkbuyxykhrdwrtrjfisff0y0dwewmyohvjm1pqtuljqwpjakfoqmdrcwhrauc5dzbcqvffrkfbt0nbzzhbtuljq0ns0nbz0vbdfjubkfqr2r4bg1xdjhmow1gc29yoxjuck9jctvgtjmzmrdelzcvevuev6tdgzswfst1cya2lrnwfrm282d2nstmx1s3nzeul1c0zustfqr2djwjn0exkksdfqmlrommnhmhp4mgvaytjqk3jmvkkwsmvtdxfhnkdmy01rrzruduhzsgjradzuymgyalc5s0rtutvreknzdwo0rlg4bdzxvevilzdsemgwnct0rkdfamxvvktkakjycvqmhbc0nqemj2sy9gaehlrjjwrvpza1psnwtcbc80cm1klxxxxjq0fursbsrvfvrvnuls0tls0k
  usages:
  - digital signature
  - key encipherment
  - server auth

but im getting :
error from server (badrequest): error when creating &quot;tmp/csr.yaml&quot;: certificatesigningrequest in version &quot;v1beta1&quot; cannot be handled as a certificatesigningrequest: v1beta1.certificatesigningrequest.spec: v1beta1.certificatesigningrequestspec.usages: []v1beta1.keyusage: request: decode base64: illegal base64 data at input byte 2432, error found in #10 byte of ...|uls0tls0k&quot;,&quot;usages&quot;:|..., bigger context ...|ppqotls0tluvorcbdrvjusuzjq0fursbsrvfvrvnuls0tls0k&quot;,&quot;usages&quot;:[&quot;digital signature&quot;,&quot;key encipherment&quot;,|...

what does it mean?
",<kubernetes><kubectl><amazon-eks>,65851203,1,"something is definitely wrong with your request line.
your error goes from(most probably, am not sure!!, seems) from the wrong encoded data copy pasted data.
you can find really a lot of similar examples, like kubernetes doesnt create certificates
reproduced your minor example, seems everything work.
to reproduce i used create certificatesigningrequest official documentation page
small remark: there is a v1 apiversion in official doc - i wasnt able to create certificatesigningrequest with it, so i had to back to apiversion: certificates.k8s.io/v1beta1 one.
the error i received using apiversion: certificates.k8s.io/v1  was
error: unable to recognize &quot;sr.yaml&quot;: no matches for kind &quot;certificatesigningrequest&quot; in version &quot;certificates.k8s.io/v1&quot;

so, basically,
$ openssl genrsa -out vit.key 2048
generating rsa private key, 2048 bit long modulus (2 primes)
............................................................................................................................+++++
........+++++
e is 65537 (0x010001)

$ openssl req -new -key vit.key -out vit.csr
...
$ cat vit.csr | base64 | tr -d &quot;\n&quot;                                                                                                              
ls0tls1crudjtibdrvjusuzjq0fursbsrvfvrvnuls0tls0ktuljq2lqq0nbweldqvfbd1juruxnqwthqtfvrujotunrvlv4rxpbukjntlzcqwdnq2xodmjxvxrvm1jozedveapjvefmqmdovkjbb01hrwx1zedwewjtvjbjrmrwwkdkcgritwdvsfi1suv4mfpeq0nbu0l3rffzsktvwklodmnockfrrujcuufez2dfuefeq0nbuw9dz2dfqkfnzmzfsitftjz3wjd5emv4wja4autqowhuywvzsjh1cwt3u1nsu1qkdxhvbdlyci85yna2otd3ky9lqxrvtlf6ajlwngqvunhlsg0rmkvhwdllagown0nbzljrrfevv284dw1tls0tls1crudjtibdrvjusuzjq0fursbsrvfvrvnuls0tls0ktuljq2lqq0nbweldqvfbd1juruxnqwthqtfvrujotunrvlv4rxpbukjntlzcqwdnq2xodmjxvxrvm1jozedveapjvefmqmdovkjbb01hrwx1zedwewjtvjbjrmrwwkdkcgritwdvsfi1suv4mfpeq0nbu0l3rffzsktvwklodmnockfrrujcuufez2dfuefeq0nbuw9dz2dfqkfnzmzfsitftjz3wjd5emv4wja4autqowhuywvzsjh1cwt3u1nsu1qkdxhvbdlyci85yna2otd3ky9lqxrvtlf6ajlwngqvunhlsg0rmkvhwdllagown0nbzljrrfevv284dw1tuzrmzao1uetonmvxmmdvmwjkndrzqmpwafk4encwk1uyqxdzmelpbitccm9wewdgmvlcwwfkchyzsnbxqvpqb2g2nfbucmy0wthfnmptd0lnylptcxhlctddauewsdnhzdg1l0s4em5hwlfuywz2q3e2umc4sitss2z0rnn3qwdpl1bjslgkwexyekrcdss4oeracenjt0rjek9mejziymhbmk1gk2txn0rftljiz29eenjznhdnngxgdvnpwglpsve2l01gvapusmu5b1dnbfpnmjernfpsqun5relzunhwqmzqnlbbkzhowejjagk4r09ok2ziy0nbd0vbqwfbqu1bmeddu3fhclnjyjneuuvcq3dvque0sujbuunlm1jyaedoswv4dwr5b2ljnja0c0dgotdncexqv0y0rvuwk0dowgy5wwizrhikb2nsrg91ofvzqjhvtlpatw1lc21xzuozdevkq3i2ce1mmwi4u09vohhzyxdir3nhzhlrdzj5rwjvemdtwdr1bwphky9avjkynukwyvkwngfgow52qmvysdblbnh0rg9fdg8rovvnvfoxlzv6zvzowgirnnl0k1r6bvowoctqbm4vckhmuvmvdmtrvtdtnnrxnjqxbtjkuglck0y4mnzyenm4niths2gvyuk0odj2vxdjuzfrunlltes0zuvkognuueqkwhdevk9selhqctvumfh5zuorcnlhy0dryvpkb291tytvdupxvnlcn0dyznd5rennujhgzm8wzutsqwzbq1diawplz3h6ugn2zehtttbjclm2vku0swndvytycu5kumxyqwhny2jkm3daci0tls0tru5eienfulrjrkldqvrfifjfuvvfu1qtls0tlqo=


i manually copypasted key and put into the yaml using vi...
apiversion: certificates.k8s.io/v1beta1
kind: certificatesigningrequest
metadata:
  name: vit
spec:
  groups:
  - system:authenticated
  request: ls0tls1crudjtibdrvjusuzjq0fursbsrvfvrvnuls0tls0ktuljq2lqq0nbweldqvfbd1juruxnqwthqtfvrujotunrvlv4rxpbukjntlzcqwdnq2xodmjxvxrvm1jozedveapjvefmqmdovkjbb01hrwx1zedwewjtvjbjrmrwwkdkcgritwdvsfi1suv4mfpeq0nbu0l3rffzsktvwklodmnockfrrujcuufez2dfuefeq0nbuw9dz2dfqkfnzmzfsitftjz3wjd5emv4wja4autqowhuywvzsjh1cwt3u1nsu1qkdxhvbdlyci85yna2otd3ky9lqxrvtlf6ajlwngqvunhlsg0rmkvhwdllagown0nbzljrrfevv284dw1tuzrmzao1uetonmvxmmdvmwjkndrzqmpwafk4encwk1uyqxdzmelpbitccm9wewdgmvlcwwfkchyzsnbxqvpqb2g2nfbucmy0wthfnmptd0lnylptcxhlctddauewsdnhzdg1l0s4em5hwlfuywz2q3e2umc4sitss2z0rnn3qwdpl1bjslgkwexyekrcdss4oeracenjt0rjek9mejziymhbmk1gk2txn0rftljiz29eenjznhdnngxgdvnpwglpsve2l01gvapusmu5b1dnbfpnmjernfpsqun5relzunhwqmzqnlbbkzhowejjagk4r09ok2ziy0nbd0vbqwfbqu1bmeddu3fhclnjyjneuuvcq3dvque0sujbuunlm1jyaedoswv4dwr5b2ljnja0c0dgotdncexqv0y0rvuwk0dowgy5wwizrhikb2nsrg91ofvzqjhvtlpatw1lc21xzuozdevkq3i2ce1mmwi4u09vohhzyxdir3nhzhlrdzj5rwjvemdtwdr1bwphky9avjkynukwyvkwngfgow52qmvysdblbnh0rg9fdg8rovvnvfoxlzv6zvzowgirnnl0k1r6bvowoctqbm4vckhmuvmvdmtrvtdtnnrxnjqxbtjkuglck0y4mnzyenm4niths2gvyuk0odj2vxdjuzfrunlltes0zuvkognuueqkwhdevk9selhqctvumfh5zuorcnlhy0dryvpkb291tytvdupxvnlcn0dyznd5rennujhgzm8wzutsqwzbq1diawplz3h6ugn2zehtttbjclm2vku0swndvytycu5kumxyqwhny2jkm3daci0tls0tru5eienfulrjrkldqvrfifjfuvvfu1qtls0tlqo=
  usages:
  - client auth

result is:
$ kubectl apply -f sr.yaml
certificatesigningrequest.certificates.k8s.io/vit created


request is the base64 encoded value of the csr file content. you can
get the content using this command: cat john.csr | base64 | tr -d &quot;\n&quot;

you can also use request: $(cat server.csr | base64 | tr -d '\n') instead of  copy-pasting plain text.. just read info below plz..its important
csr generation not working as per doc

similar problem had been vexing me as well. after some
troubleshooting, it was observed the base64 and tr solution doesn't
work well in an macos environment. using the gbase64 utilities from
gnu has a '-w ' option that will not line wrap. once i installed gnu
coreutils and used gbase64, the scripts worked as expected. the
problem is related to 'tr' and line-wrapping using the original
combination. hope it helps future users who stumble into similar
environment related issues.

"
73842705,kubernetes multiple deployments using single template for ci/cd,"i'm bit new to kubernetes, i know we can create multiple deployments using same template. i have already gone through this. but my requirement is slight different. i have 30 deployment files wherein only two parameters that's deployment name and python script1.py keeps on updating for all deployments. below are sample deployment files
deployment1.yaml
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: deploy1    &lt;-- will be updated every time for all deployments
  name: deploy1     &lt;-- will be updated every time for all deployments
spec:
  replicas: 3
  selector:
    matchlabels:
      app: deploy1
  strategy:
    rollingupdate:
      maxsurge: 2
      maxunavailable: 0
    type: rollingupdate
  template:
    metadata:
      labels:
        app: deploy1
    spec:
      containers:
      - name: web
        image: nginx
        command: [&quot;/bin/sh&quot;]
        args:
          - -c
          - &gt;-
              python script1.py     &lt;-- will be updated every time for all deployments

deployment2.yaml
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: deploy2    &lt;-- will be updated every time for all deployments
  name: deploy2     &lt;-- will be updated every time for all deployments
spec:
  replicas: 3
  selector:
    matchlabels:
      app: deploy2
  strategy:
    rollingupdate:
      maxsurge: 2
      maxunavailable: 0
    type: rollingupdate
  template:
    metadata:
      labels:
        app: deploy2
    spec:
      containers:
      - name: web
        image: nginx
        command: [&quot;/bin/sh&quot;]
        args:
          - -c
          - &gt;-
              python script2.py     &lt;-- will be updated every time for all deployments

i want to know how can i convert this into single template so that multiple deployments can be deployed into the cluster. eventually i want to integrate this into cloud build as a part of my ci/cd.
any help would be appreciated here.
update 1 :
@moritz schmitz v. hlst i have updated my code to include below files in my helm chart.
here is my values.yaml
deployments:
  - image: nginx
  - name: deploy1
    script: script1.py
  - name: deploy2
    script: script2.py

template/deployment.yaml
{{- range .values.deployments }}
---
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: {{ .name }}
  name: {{ .name }}
spec:
  replicas: 3
  selector:
    matchlabels:
      app: {{ .name }}
  strategy:
    rollingupdate:
      maxsurge: 2
      maxunavailable: 0
    type: rollingupdate
  template:
    metadata:
      labels:
        app: {{ .name }}
    spec:
      containers:
      - name: web
        image: {{ .image }}
        ports:
        - containerport: 80
{{- end }}

template/service.yaml
{{- range .values.deployments }}
apiversion: v1
kind: service
metadata:
  name: {{ .name }}
spec:
  type: clusterip
  ports:
  - port: 80
    targetport: 80
  selector:
    app: {{ .name }}
{{- end }}

i get below error while running helm install demo-nginx demo-hello/,

error: installation failed: unable to build kubernetes objects from
release manifest: error validating &quot;&quot;: error validating data: [unknown
object type &quot;nil&quot; in deployment.metadata.labels.app, unknown object
type &quot;nil&quot; in deployment.spec.selector.matchlabels.app, unknown object
type &quot;nil&quot; in deployment.spec.template.metadata.labels.app]

",<kubernetes><kubernetes-helm><continuous-deployment><google-cloud-build><google-cloud-deploy>,73905523,1,"deployment.yaml:
{{- range .values.deployments }}
---
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: {{ .name }}
  name: {{ .name }}
spec:
  replicas: 3
  selector:
    matchlabels:
      app: {{ .name }}
  strategy:
    rollingupdate:
      maxsurge: 2
      maxunavailable: 0
    type: rollingupdate
  template:
    metadata:
      labels:
        app: {{ .name }}
    spec:
      containers:
      - name: web
        image: nginx
        command: [&quot;/bin/sh&quot;]
        args:
          - -c
          - &gt;-
              python {{ .script }}
{{- end }}

values.yaml:
deployments:
  - name: deploy1
    script: script1.py
  - name: deploy2
    script: script2.py

"
73845567,"my requirement is, if the configmap in values.yaml contains the value domain_name then i want replace it with {.values.global.domainname}","my requirement is if the configmap in values.yaml contains the value domain_name then i want replace it with {.values.global.domainname} please help me
values.yaml
global:
  infraname: abc-qa-bcd
  infrasecretregion: us-east-1
  domainname: xyz.abcde.com

configmap:
  allowed_origins: &quot;https://app.domain_name,https://abc-cd.domain_name&quot;
  create_admin_user: &quot;true&quot;
  create_admin_cdp_api: &quot;true&quot;
  port: &quot;8000&quot;

config-map.yaml
apiversion: v1
kind: configmap
metadata:
  name:  {{ .values.metadata.name }}
  namespace: {{ .release.namespace }}
data:
 {{- if .values.configmap }}
{{- range $key,$value := .values.configmap }}
  {{$key}}: {{$value | quote}}
{{- end }}
{{- end }}

thanks in advance.
",<kubernetes><kubernetes-helm>,73850185,1,"values.yaml
global:
  infraname: abc-qa-bcd
  infrasecretregion: us-east-1
  domainname: xyz.abcde.com

configmap:
  allowed_origins: &quot;https://app.domain_name,https://abc-cd.domain_name&quot;
  create_admin_user: &quot;true&quot;
  create_admin_cdp_api: &quot;true&quot;
  port: &quot;8000&quot;

config-map.yaml
apiversion: v1
kind: configmap
metadata:
  name:  {{ .values.metadata.name }}
  namespace: {{ .release.namespace }}
data: 
  data: |-
  {{- if .values.configmap }}
  {{- range $key,$value := .values.configmap }}
  {{- if contains &quot;domain_name&quot; $value }}
  {{- $value = ($value | replace &quot;domain_name&quot; $.values.global.domainname) }}
  {{- end }}
    {{ $key }}: {{ $value | quote }}
  {{- end }}
  {{- end }}

output
apiversion: v1
kind: configmap
metadata:
  name: test
  namespace: test
data: 
  data: |-
    allowed_origins: &quot;https://app.xyz.abcde.com,https://abc-cd.xyz.abcde.com&quot;
    create_admin_cdp_api: &quot;true&quot;
    create_admin_user: &quot;true&quot;
    port: &quot;8000&quot;

"
73762173,"how to debug and fix google gke, spilo/patroni pod label update that returns error code 4, gateway timeout","i'm using zalando's postgres operator and been having postgres cluster downtime now. i'm using connection pooler to connection to master and replica, but replica connection pooler not able to connect to replica pod due to replica svc does not have endpoints, which i think the problem to be the svc select postgres pod that has label of spilo-role to be replica but pods does not have such labels, both master and replica which they supposed to be.
the cluster has been running fine for one months and this incident just happened few days ago until now, we are still in soft production stage and there are only few test clients with very low traffic, but soon in real production.
logs of operator and postgres does not seems to have any errors of what i would have know so i looked into log explorer in google cloud console and found trace from audit logs that patroni actually call to set the pod label but result in 504 error. the error seems to be from misconfiguration, but it is strange that it has been running fine until now and i am running out of idea of how to debug this, so any guide or help to debug/fix this would be very much appreciated.
below is the audit logs from google cloud console log explorer that shows that pod has permission to do pod label update but it fails,
{
  &quot;protopayload&quot;: {
    &quot;@type&quot;: &quot;type.googleapis.com/google.cloud.audit.auditlog&quot;,
    &quot;authenticationinfo&quot;: {
      &quot;principalemail&quot;: &quot;system:serviceaccount:default:postgres-pod&quot;
    },
    &quot;authorizationinfo&quot;: [
      {
        &quot;granted&quot;: true,
        &quot;permission&quot;: &quot;io.k8s.core.v1.pods.patch&quot;,
        &quot;resource&quot;: &quot;core/v1/namespaces/default/pods/acid-abc-db-1&quot;
      }
    ],
    &quot;methodname&quot;: &quot;io.k8s.core.v1.pods.patch&quot;,
    &quot;request&quot;: {
      &quot;@type&quot;: &quot;k8s.io/patch&quot;,
      &quot;metadata&quot;: {
        &quot;annotations&quot;: {
          &quot;status&quot;: &quot;{\&quot;conn_url\&quot;:\&quot;postgres://10.52.3.36:5432/postgres\&quot;,\&quot;api_url\&quot;:\&quot;http://10.52.3.36:8008/patroni\&quot;,\&quot;state\&quot;:\&quot;running\&quot;,\&quot;role\&quot;:\&quot;replica\&quot;,\&quot;version\&quot;:\&quot;2.1.3\&quot;,\&quot;xlog_location\&quot;:50331648,\&quot;timeline\&quot;:1}&quot;
        },
        &quot;labels&quot;: {
          &quot;spilo-role&quot;: &quot;replica&quot;
        },
        &quot;name&quot;: &quot;acid-abc-db-1&quot;,
        &quot;namespace&quot;: &quot;default&quot;
      }
    },
    &quot;requestmetadata&quot;: {
      &quot;callerip&quot;: &quot;10.52.3.36&quot;,
      &quot;callersupplieduseragent&quot;: &quot;patroni/2.1.3 python/3.6.9 linux&quot;
    },
    &quot;resourcename&quot;: &quot;core/v1/namespaces/default/pods/acid-ml-db-1&quot;,
    &quot;response&quot;: {
      &quot;@type&quot;: &quot;core.k8s.io/v1.status&quot;,
      &quot;apiversion&quot;: &quot;v1&quot;,
      &quot;code&quot;: 504,
      &quot;details&quot;: {},
      &quot;kind&quot;: &quot;status&quot;,
      &quot;message&quot;: &quot;timeout: request did not complete within requested timeout - context canceled&quot;,
      &quot;metadata&quot;: {},
      &quot;reason&quot;: &quot;timeout&quot;,
      &quot;status&quot;: &quot;failure&quot;
    },
    &quot;servicename&quot;: &quot;k8s.io&quot;,
    &quot;status&quot;: {
      &quot;code&quot;: 4,
      &quot;message&quot;: &quot;gateway timeout&quot;
    }
  },
  &quot;insertid&quot;: &quot;b6e3cfe7-0125-4652-a77a-f44232198f8c&quot;,
  &quot;resource&quot;: {
    &quot;type&quot;: &quot;k8s_cluster&quot;,
    &quot;labels&quot;: {
      &quot;project_id&quot;: &quot;abc123&quot;,
      &quot;cluster_name&quot;: &quot;abc&quot;,
      &quot;location&quot;: &quot;asia-southeast1&quot;
    }
  },
  &quot;timestamp&quot;: &quot;2022-09-18t09:21:05.017886z&quot;,
  &quot;labels&quot;: {
    &quot;authorization.k8s.io/decision&quot;: &quot;allow&quot;,
    &quot;authorization.k8s.io/reason&quot;: &quot;rbac: allowed by clusterrolebinding \&quot;postgres-pod\&quot; of clusterrole \&quot;postgres-pod\&quot; to serviceaccount \&quot;postgres-pod/default\&quot;&quot;
  },
  &quot;logname&quot;: &quot;projects/ekyc-web-services/logs/cloudaudit.googleapis.com%2factivity&quot;,
  &quot;operation&quot;: {
    &quot;id&quot;: &quot;b6e3cfe7-0125-4652-a77a-f44232198f8c&quot;,
    &quot;producer&quot;: &quot;k8s.io&quot;,
    &quot;first&quot;: true,
    &quot;last&quot;: true
  },
  &quot;receivetimestamp&quot;: &quot;2022-09-18t09:21:10.235550735z&quot;
}

usually patronictl list should show state in running and ip address in host column, but now they are empty
+ cluster: acid-abc-db (7144662354080374866) -+-----------+
| member        | host | role    | state | tl | lag in mb |
+---------------+------+---------+-------+----+-----------+
| acid-abc-db-0 |      | leader  |       |    |           |
| acid-abc-db-1 |      | replica |       |    |   unknown |
+---------------+------+---------+-------+----+-----------+

i also tried to create entirely new cluster with any name, it would also give me the same result.
logs from master podacid-abc-db-0
2022-09-18 10:18:45,881 - bootstrapping - info - figuring out my environment (google? aws? openstack? local?)
2022-09-18 10:18:45,970 - bootstrapping - info - looks like your running google
2022-09-18 10:18:47,087 - bootstrapping - info - configuring bootstrap
2022-09-18 10:18:47,087 - bootstrapping - info - configuring pgqd
2022-09-18 10:18:47,088 - bootstrapping - info - configuring wal-e
2022-09-18 10:18:47,089 - bootstrapping - info - writing to file /run/etc/wal-e.d/env/wale_s3_prefix
2022-09-18 10:18:47,090 - bootstrapping - info - writing to file /run/etc/wal-e.d/env/walg_s3_prefix
2022-09-18 10:18:47,090 - bootstrapping - info - writing to file /run/etc/wal-e.d/env/aws_access_key_id
2022-09-18 10:18:47,091 - bootstrapping - info - writing to file /run/etc/wal-e.d/env/aws_secret_access_key
2022-09-18 10:18:47,091 - bootstrapping - info - writing to file /run/etc/wal-e.d/env/aws_region
2022-09-18 10:18:47,091 - bootstrapping - info - writing to file /run/etc/wal-e.d/env/walg_s3_sse
2022-09-18 10:18:47,092 - bootstrapping - info - writing to file /run/etc/wal-e.d/env/walg_download_concurrency
2022-09-18 10:18:47,092 - bootstrapping - info - writing to file /run/etc/wal-e.d/env/walg_upload_concurrency
2022-09-18 10:18:47,093 - bootstrapping - info - writing to file /run/etc/wal-e.d/env/use_walg_backup
2022-09-18 10:18:47,093 - bootstrapping - info - writing to file /run/etc/wal-e.d/env/use_walg_restore
2022-09-18 10:18:47,093 - bootstrapping - info - writing to file /run/etc/wal-e.d/env/wale_log_destination
2022-09-18 10:18:47,094 - bootstrapping - info - writing to file /run/etc/wal-e.d/env/pgport
2022-09-18 10:18:47,094 - bootstrapping - info - writing to file /run/etc/wal-e.d/env/backup_num_to_retain
2022-09-18 10:18:47,095 - bootstrapping - info - writing to file /run/etc/wal-e.d/env/tmpdir
2022-09-18 10:18:47,095 - bootstrapping - info - configuring log
2022-09-18 10:18:47,095 - bootstrapping - info - configuring patroni
2022-09-18 10:18:47,104 - bootstrapping - info - writing to file /run/postgres.yabc
2022-09-18 10:18:47,105 - bootstrapping - info - configuring pam-oauth2
2022-09-18 10:18:47,106 - bootstrapping - info - writing to file /etc/pam.d/postgresql
2022-09-18 10:18:47,106 - bootstrapping - info - configuring certificate
2022-09-18 10:18:47,107 - bootstrapping - info - generating ssl self-signed certificate
2022-09-18 10:18:47,226 - bootstrapping - info - configuring standby-cluster
2022-09-18 10:18:47,226 - bootstrapping - info - configuring crontab
2022-09-18 10:18:47,227 - bootstrapping - info - skipping creation of renice cron job due to lack of sys_nice capability
2022-09-18 10:18:47,242 - bootstrapping - info - configuring pgbouncer
2022-09-18 10:18:47,242 - bootstrapping - info - no pgbouncer_configuration was specified, skipping
2022-09-18 10:18:48,994 info: selected new k8s api server endpoint https://172.16.0.2:443
2022-09-18 10:18:49,017 info: no postgresql configuration items changed, nothing to reload.
2022-09-18 10:18:49,020 info: lock owner: none; i am acid-abc-db-0
2022-09-18 10:18:54,082 info: trying to bootstrap a new cluster
the files belonging to this database system will be owned by user &quot;postgres&quot;.
this user must also own the server process.

the database cluster will be initialized with locale &quot;en_us.utf-8&quot;.
the default text search configuration will be set to &quot;english&quot;.

data page checksums are enabled.

fixing permissions on existing directory /home/postgres/pgdata/pgroot/data ... ok
creating subdirectories ... ok
selecting dynamic shared memory implementation ... posix
selecting default max_connections ... 100
selecting default shared_buffers ... 128mb
selecting default time zone ... etc/utc
creating configuration files ... ok
running bootstrap script ... ok
performing post-bootstrap initialization ... ok
syncing data to disk ... ok

success. you can now start the database server using:

/usr/lib/postgresql/14/bin/pg_ctl -d /home/postgres/pgdata/pgroot/data -l logfile start

2022-09-18 10:18:56,761 info: postmaster pid=92
/var/run/postgresql:5432 - no response
2022-09-18 10:18:56 utc [92]: [1-1] 6326f090.5c 0     log:  auto detecting pg_stat_kcache.linux_hz parameter...
2022-09-18 10:18:56 utc [92]: [2-1] 6326f090.5c 0     log:  pg_stat_kcache.linux_hz is set to 500000
2022-09-18 10:18:56 utc [92]: [3-1] 6326f090.5c 0     log:  redirecting log output to logging collector process
2022-09-18 10:18:56 utc [92]: [4-1] 6326f090.5c 0     hint:  future log output will appear in directory &quot;../pg_log&quot;.
/var/run/postgresql:5432 - accepting connections
/var/run/postgresql:5432 - accepting connections
2022-09-18 10:18:57,834 info: establishing a new patroni connection to the postgres cluster
2022-09-18 10:19:02,852 info: running post_bootstrap
do
grant role
do
do
create extension
notice:  version &quot;1.1&quot; of extension &quot;pg_auth_mon&quot; is already installed
alter extension
grant
create extension
do
notice:  version &quot;1.4&quot; of extension &quot;pg_cron&quot; is already installed
alter extension
alter policy
revoke
grant
revoke
grant
alter policy
revoke
grant
create function
revoke
grant
revoke
grant
revoke
grant
revoke
grant
revoke
grant
revoke
grant
revoke
grant
revoke
grant
create extension
do
create table
grant
alter table
alter table
alter table
create foreign table
grant
create view
alter view
grant
create foreign table
grant
create view
alter view
grant
create foreign table
grant
create view
alter view
grant
create foreign table
grant
create view
alter view
grant
create foreign table
grant
create view
alter view
grant
create foreign table
grant
create view
alter view
grant
create foreign table
grant
create view
alter view
grant
create foreign table
grant
create view
alter view
grant
reset
set
notice:  schema &quot;zmon_utils&quot; does not exist, skipping
drop schema
do
notice:  language &quot;plpythonu&quot; does not exist, skipping
drop language
notice:  function plpython_call_handler() does not exist, skipping
drop function
notice:  function plpython_inline_handler(internal) does not exist, skipping
drop function
notice:  function plpython_validator(oid) does not exist, skipping
drop function
create schema
grant
set
create type
create function
create function
grant
you are now connected to database &quot;postgres&quot; as user &quot;postgres&quot;.
create schema
grant
set
create function
create function
revoke
grant
comment
create function
revoke
grant
comment
create function
revoke
grant
comment
create function
revoke
grant
comment
create function
revoke
grant
comment
create function
revoke
grant
comment
create function
revoke
grant
comment
create function
revoke
grant
comment
grant
reset
create extension
create extension
create extension
notice:  version &quot;3.0&quot; of extension &quot;set_user&quot; is already installed
alter extension
grant
grant
grant
create schema
grant
grant
set
create function
revoke
grant
grant
create view
revoke
grant
grant
create function
revoke
grant
grant
create view
revoke
grant
grant
create function
revoke
grant
grant
create view
revoke
grant
grant
reset
you are now connected to database &quot;template1&quot; as user &quot;postgres&quot;.
create schema
grant
set
create function
create function
revoke
grant
comment
create function
revoke
grant
comment
create function
revoke
grant
comment
create function
revoke
grant
comment
create function
revoke
grant
comment
create function
revoke
grant
comment
create function
revoke
grant
comment
create function
revoke
grant
comment
grant
reset
create extension
create extension
create extension
notice:  version &quot;3.0&quot; of extension &quot;set_user&quot; is already installed
alter extension
grant
grant
grant
create schema
grant
grant
set
create function
revoke
grant
grant
create view
revoke
grant
grant
create function
revoke
grant
grant
create view
revoke
grant
grant
create function
revoke
grant
grant
create view
revoke
grant
grant
reset
2022-09-18 10:19:05,009 warning: could not activate linux watchdog device: &quot;can't open watchdog device: [errno 2] no such file or directory: '/dev/watchdog'&quot;
2022-09-18 10:19:10,054 info: initialized a new cluster
2022-09-18 10:19:15,087 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:19:25,582 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:19:35,601 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:19:45,588 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:19:47.662 - /scripts/postgres_backup.sh - i was called as: /scripts/postgres_backup.sh /home/postgres/pgdata/pgroot/data
2022-09-18 10:19:48.397 45 log starting pgqd 3.3
2022-09-18 10:19:48.397 45 log auto-detecting dbs ...
2022-09-18 10:19:48.941 - /scripts/postgres_backup.sh - producing a new backup
info: 2022/09/18 10:19:49.036810 selecting the latest backup as the base for the current delta backup...
info: 2022/09/18 10:19:49.091402 calling pg_start_backup()
info: 2022/09/18 10:19:49.203073 starting a new tar bundle
info: 2022/09/18 10:19:49.203129 walking ...
info: 2022/09/18 10:19:49.203471 starting part 1 ...
info: 2022/09/18 10:19:50.107584 packing ...
info: 2022/09/18 10:19:50.109248 finished writing part 1.
info: 2022/09/18 10:19:50.428312 starting part 2 ...
info: 2022/09/18 10:19:50.428359 /global/pg_control
info: 2022/09/18 10:19:50.437376 finished writing part 2.
info: 2022/09/18 10:19:50.439403 calling pg_stop_backup()
info: 2022/09/18 10:19:51.470246 starting part 3 ...
info: 2022/09/18 10:19:51.496912 backup_label
info: 2022/09/18 10:19:51.497397 tablespace_map
info: 2022/09/18 10:19:51.497645 finished writing part 3.
info: 2022/09/18 10:19:51.632504 wrote backup with name base_000000010000000000000002
2022-09-18 10:19:55,586 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:20:05,587 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:20:15,579 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:20:18.427 45 log {ticks: 0, maint: 0, retry: 0}
2022-09-18 10:20:25,586 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:20:35,578 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:20:45,722 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:20:48.469 45 log {ticks: 0, maint: 0, retry: 0}
2022-09-18 10:20:55,583 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:21:05,587 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:21:15,586 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:21:18.470 45 log {ticks: 0, maint: 0, retry: 0}
2022-09-18 10:21:25,586 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:21:35,590 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:21:45,587 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:21:48.501 45 log {ticks: 0, maint: 0, retry: 0}
2022-09-18 10:21:55,588 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:22:05,589 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:22:15,589 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:22:18.532 45 log {ticks: 0, maint: 0, retry: 0}
2022-09-18 10:22:25,585 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:22:35,589 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:22:45,584 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:22:48.580 45 log {ticks: 0, maint: 0, retry: 0}
2022-09-18 10:22:55,583 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:23:05,600 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:23:15,586 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:23:18.572 45 log {ticks: 0, maint: 0, retry: 0}
2022-09-18 10:23:25,584 info: no action. i am (acid-abc-db-0), the leader with the lock
2022-09-18 10:23:35,591 info: no action. i am (acid-abc-db-0), the leader with the lock

operator logs
# too long and almost all of the logs are operator
# creating stuff that are mostly debug and info
# except the error of pod label updating
# ... more omits

... level=error msg=&quot;failed to create cluster: pod labels error: still failing after 200 retries&quot; cluster-name=default/acid-abc-db pkg=cluster worker=1
... level=error msg=&quot;could not create cluster: pod labels error: still failing after 200 retries&quot; cluster-name=default/acid-abc-db pkg=controller worker=1

# ... more omits

# /home/postgres/.config/patroni/patronictl.yaml

bootstrap:
  clone_with_wale:
    command: envdir &quot;/run/etc/wal-e.d/env-clone-acid-abc-db&quot; python3 /scripts/clone_with_wale.py --recovery-target-time=&quot;&quot;
    recovery_conf:
      recovery_target_action: promote
      recovery_target_timeline: latest
      restore_command: envdir &quot;/run/etc/wal-e.d/env-clone-acid-abc-db&quot; timeout &quot;0&quot; /scripts/restore_command.sh &quot;%f&quot; &quot;%p&quot;
  dcs:
    loop_wait: 10
    maximum_lag_on_failover: 33554432
    postgresql:
      parameters:
        archive_mode: 'on'
        archive_timeout: 1800s
        autovacuum_analyze_scale_factor: 0.02
        autovacuum_max_workers: 5
        autovacuum_vacuum_scale_factor: 0.05
        checkpoint_completion_target: '0.9'
        default_statistics_target: '100'
        effective_io_concurrency: '200'
        hot_standby: 'on'
        log_autovacuum_min_duration: 0
        log_checkpoints: 'on'
        log_connections: 'on'
        log_disconnections: 'on'
        log_line_prefix: '%t [%p]: [%l-1] %c %x %d %u %a %h '
        log_lock_waits: 'on'
        log_min_duration_statement: 500
        log_statement: all
        log_temp_files: 0
        max_connections: '512'
        max_parallel_maintenance_workers: '2'
        max_parallel_workers: '32'
        max_parallel_workers_per_gather: '8'
        max_replication_slots: 10
        max_slot_wal_keep_size: 16gb
        max_standby_archive_delay: 0s
        max_standby_streaming_delay: 0s
        max_wal_senders: '16'
        max_wal_size: 4gb
        max_worker_processes: '256'
        min_wal_size: 1gb
        tcp_keepalives_idle: 900
        tcp_keepalives_interval: 100
        track_functions: all
        wal_compression: 'on'
        wal_level: hot_standby
        wal_log_hints: 'on'
      use_pg_rewind: true
      use_slots: true
    retry_timeout: 10
    synchronous_node_count: 1
    ttl: 30
  initdb:
  - auth-host: md5
  - auth-local: trust
  - data-checksums
  - encoding: utf8
  - locale: en_us.utf-8
  method: clone_with_wale
  post_init: /scripts/post_init.sh &quot;zalandos&quot;
  users:
    zalandos:
      options:
      - createdb
      - nologin
      password: ''
kubernetes:
  bypass_api_service: true
  labels:
    application: spilo
  port: tcp://10.56.0.1:443
  port_443_tcp: tcp://10.56.0.1:443
  port_443_tcp_addr: 10.56.0.1
  port_443_tcp_port: '443'
  port_443_tcp_proto: tcp
  ports:
  - name: postgresql
    port: 5432
  role_label: spilo-role
  scope_label: cluster-name
  service_host: 10.56.0.1
  service_port: '443'
  service_port_https: '443'
  use_endpoints: true
postgresql:
  authentication:
    replication:
      password: xxx
      username: standby
    superuser:
      password: xxx
      username: postgres
  basebackup_fast_xlog:
    command: /scripts/basebackup.sh
    retries: 2
  bin_dir: /usr/lib/postgresql/14/bin
  callbacks:
    on_role_change: /scripts/on_role_change.sh zalandos true
  connect_address: 10.52.5.55:5432
  create_replica_method:
  - wal_e
  - basebackup_fast_xlog
  data_dir: /home/postgres/pgdata/pgroot/data
  listen: '*:5432'
  name: acid-abc-db-0
  parameters:
    archive_command: envdir &quot;/run/etc/wal-e.d/env&quot; wal-g wal-push &quot;%p&quot;
    bg_mon.history_buckets: 120
    bg_mon.listen_address: 0.0.0.0
    extwlist.custom_path: /scripts
    extwlist.extensions: btree_gin,btree_gist,citext,extra_window_functions,first_last_agg,hll,hstore,hypopg,intarray,ltree,pgcrypto,pgq,pgq_node,pg_trgm,postgres_fdw,tablefunc,uuid-ossp,timescaledb,pg_partman
    log_destination: csvlog
    log_directory: ../pg_log
    log_file_mode: '0644'
    log_filename: postgresql-%u.log
    log_rotation_age: 1d
    log_truncate_on_rotation: 'on'
    logging_collector: 'on'
    pg_stat_statements.track_utility: 'off'
    shared_buffers: 256mb
    shared_preload_libraries: bg_mon,pg_stat_statements,pgextwlist,pg_auth_mon,set_user,timescaledb,pg_cron,pg_stat_kcache
    ssl: 'on'
    ssl_cert_file: /run/certs/server.crt
    ssl_key_file: /run/certs/server.key
  pg_hba:
  - local   all             all                                   trust
  - hostssl all             +zalandos    127.0.0.1/32       pam
  - host    all             all                127.0.0.1/32       md5
  - hostssl all             +zalandos    ::1/128            pam
  - host    all             all                ::1/128            md5
  - local   replication     standby                    trust
  - hostssl replication     standby all                md5
  - hostnossl all           all                all                reject
  - hostssl all             +zalandos    all                pam
  - hostssl all             all                all                md5
  pgpass: /run/postgresql/pgpass
  recovery_conf:
    restore_command: envdir &quot;/run/etc/wal-e.d/env&quot; timeout &quot;0&quot; /scripts/restore_command.sh &quot;%f&quot; &quot;%p&quot;
  use_unix_socket: true
  use_unix_socket_repl: true
  wal_e:
    command: envdir /run/etc/wal-e.d/env bash /scripts/wale_restore.sh
    no_master: 1
    retries: 2
    threshold_backup_size_percentage: 30
    threshold_megabytes: 102400
restapi:
  connect_address: 10.52.5.55:8008
  listen: :8008
scope: acid-abc-db

operator configuration
# mostly defaults
# only change common pod secret
# for backups credential
---
apiversion: acid.zalan.do/v1
configuration:
  aws_or_gcp:
    additional_secret_mount_path: /meta/credentials
    aws_region: ap-southeast-1
    enable_ebs_gp3_migration: false
    enable_ebs_gp3_migration_max_size: 1000
  connection_pooler:
    connection_pooler_default_cpu_limit: &quot;1&quot;
    connection_pooler_default_cpu_request: 500m
    connection_pooler_default_memory_limit: 100mi
    connection_pooler_default_memory_request: 512mi
    connection_pooler_image: registry.opensource.zalan.do/acid/pgbouncer:master-22
    connection_pooler_max_db_connections: 512
    connection_pooler_mode: transaction
    connection_pooler_number_of_instances: 2
    connection_pooler_schema: pooler
    connection_pooler_user: pooler
  debug:
    debug_logging: true
    enable_database_access: true
  docker_image: registry.opensource.zalan.do/acid/spilo-14:2.1-p5
  enable_crd_registration: true
  enable_crd_validation: true
  enable_lazy_spilo_upgrade: false
  enable_pgversion_env_var: true
  enable_shm_volume: true
  enable_spilo_wal_path_compat: false
  enable_team_id_clustername_prefix: false
  etcd_host: &quot;&quot;
  kubernetes:
    cluster_domain: cluster.local
    cluster_labels:
      application: spilo
    cluster_name_label: cluster-name
    enable_cross_namespace_secret: false
    enable_init_containers: true
    enable_pod_antiaffinity: true
    enable_pod_disruption_budget: true
    enable_sidecars: true
    master_pod_move_timeout: 20m
    oauth_token_secret_name: postgresql-operator
    pdb_name_format: postgres-{cluster}-pdb
    pod_antiaffinity_topology_key: kubernetes.io/hostname
    pod_environment_secret: postgres-common-secret
    pod_management_policy: ordered_ready
    pod_role_label: spilo-role
    pod_service_account_definition: &quot;&quot;
    pod_service_account_name: postgres-pod
    pod_service_account_role_binding_definition: &quot;&quot;
    pod_terminate_grace_period: 5m
    secret_name_template: '{username}.{cluster}.credentials.{tprkind}.{tprgroup}'
    spilo_allow_privilege_escalation: true
    spilo_privileged: false
    storage_resize_mode: pvc
  kubernetes_use_configmaps: false
  load_balancer:
    db_hosted_zone: db.example.com
    enable_master_load_balancer: false
    enable_master_pooler_load_balancer: false
    enable_replica_load_balancer: false
    enable_replica_pooler_load_balancer: false
    external_traffic_policy: cluster
    master_dns_name_format: '{cluster}.{team}.{hostedzone}'
    replica_dns_name_format: '{cluster}-repl.{team}.{hostedzone}'
  logging_rest_api:
    api_port: 8080
    cluster_history_entries: 1000
    ring_log_lines: 100
  logical_backup:
    logical_backup_docker_image: registry.opensource.zalan.do/acid/logical-backup:v1.8.1
    logical_backup_job_prefix: logical-backup-
    logical_backup_provider: s3
    logical_backup_s3_bucket: my-bucket-url
    logical_backup_s3_sse: aes256
    logical_backup_schedule: 30 00 * * *
  major_version_upgrade:
    major_version_upgrade_mode: &quot;off&quot;
    minimal_major_version: &quot;9.6&quot;
    target_major_version: &quot;14&quot;
  max_instances: -1
  min_instances: -1
  postgres_pod_resources:
    default_cpu_limit: &quot;1&quot;
    default_cpu_request: 100m
    default_memory_limit: 500mi
    default_memory_request: 100mi
    min_cpu_limit: 250m
    min_memory_limit: 250mi
  repair_period: 5m
  resync_period: 30m
  set_memory_request_to_limit: false
  teams_api:
    enable_admin_role_for_users: true
    enable_postgres_team_crd: true
    enable_postgres_team_crd_superusers: false
    enable_team_member_deprecation: false
    enable_team_superuser: false
    enable_teams_api: false
    pam_configuration: https://info.example.com/oauth2/tokeninfo?access_token= uid
      realm=/employees
    pam_role_name: zalandos
    protected_role_names:
    - admin
    - cron_admin
    role_deletion_suffix: _deleted
    team_admin_role: admin
    team_api_role_configuration:
      log_statement: all
    teams_api_url: https://teams.example.com/api/
  timeouts:
    patroni_api_check_interval: 1s
    patroni_api_check_timeout: 5s
    pod_deletion_wait_timeout: 10m
    pod_label_wait_timeout: 10m
    ready_wait_interval: 4s
    ready_wait_timeout: 30s
    resource_check_interval: 3s
    resource_check_timeout: 10m
  users:
    enable_password_rotation: false
    password_rotation_interval: 90
    password_rotation_user_retention: 180
    replication_username: standby
    super_username: postgres
  workers: 8
kind: operatorconfiguration
metadata:
  name: postgresql-operator-default-configuration
  namespace: default

thanks and appreciate for your time on reading this and thanks in advance for guiding to debug this and helping.
update, 0
so i tried manually calling pod patch with curl within one of the pod, and it works as expected
curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt   --header &quot;authorization: bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)&quot; https://${kubernetes_service_host}:${kubernetes_service_port}/api/v1/namespaces/default/pods/acid-abc-db-1 -x patch -h 'content-type: application/merge-patch+json' -d '{&quot;metadata&quot;: {&quot;labels&quot;: {&quot;spilo-role&quot;: &quot;replica&quot;}}}'
and then endpoints of replica became available and connection pooler was able to connect to replica, so why is patroni calling patch to pod result in gateway error, and also patronictl list still did not show in an expected correct result(host is still empty and state not show running)
update, 1
so patronictl list depends on kubernetes's annotations for that, so i copy the request from google cloud log explorer that has the pod patch error, and replace the curl command with this
curl \
  --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\
  --header &quot;authorization: bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)&quot;\
  https://${kubernetes_service_host}:${kubernetes_service_port}/api/v1/namespaces/default/pods/acid-abc-db-0\
  -x patch\
  -h 'content-type: application/merge-patch+json'\
  -d '{&quot;metadata&quot;: {&quot;name&quot;: &quot;acid-abc-db-1&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;annotations&quot;: {&quot;status&quot;: &quot;{\&quot;conn_url\&quot;:\&quot;postgres://10.52.3.48:5432/postgres\&quot;,\&quot;api_url\&quot;:\&quot;http://10.52.3.48:8008/patroni\&quot;,\&quot;state\&quot;:\&quot;running\&quot;,\&quot;role\&quot;:\&quot;replica\&quot;,\&quot;version\&quot;:\&quot;2.1.4\&quot;,\&quot;xlog_location\&quot;:77275856896,\&quot;timeline\&quot;:37}&quot;}, &quot;labels&quot;: {&quot;spilo-role&quot;: &quot;replica&quot;}}}'

now patronictl list showing this
+ cluster: acid-abc-db (7109013759578136647) -----+----+-----------+
| member         | host       | role    | state   | tl | lag in mb |
+----------------+------------+---------+---------+----+-----------+
| acid-abc-db-0  |            | leader  |         |    |           |
| acid-abc-db-1  | 10.52.3.48 | replica | running | 37 |        16 |
+----------------+------------+---------+---------+----+-----------+

",<kubernetes><google-kubernetes-engine><patroni><postgres-operator>,74028419,1,"so the solution turns out to be quite simple, so i explore patroni doc and realizes that there are options to configure dcs related settings and as i tested with normal curl request which working fine so permission should not be an issue and 504 error might be related to timeout so i explore the doc to find if i could configure the request timeout on this, thats what led me to trying out some of the option from patroni doc. so i update postgresql k8s api object like below
apiversion: &quot;acid.zalan.do/v1&quot;
kind: postgresql
metadata:
  name: acid-abc-db
  namespace: default
spec:
  # more omitted
  patroni:
    retry_timeout: 128 # default only 10, change to 128
  * more omitted

now the cluster is working fine again, thanks for everyone that took your time to read about my problem and sorry for which silly mistake
"
73647344,deploying multi-container pod in kubernetes,"newbie here, i am trying to run two containers in a pod. i need them in the &quot;dev&quot; namespace.
here is my yaml:
kind: deployment
apiversion: apps/v1
metadata:
  name: test-deployment
  namespace: dev
spec:
  selector:
    matchlabels:
      deploy: example
  template:
    metadata:
      labels:
        deploy: example
    spec:
      containers:
        - name: nginx
          image: nginx:latest
        - name: busybox
          image: busybox:1.34

i got this error:

crashloopbackoff

kubectl describe pod gives me this result:
events:
  type     reason     age                   from               message
  ----     ------     ----                  ----               -------
  normal   scheduled  8m14s                 default-scheduler  successfully assigned dev/nginx-deployment-55fd86946-hgjlk to minikube
  normal   pulling    8m13s                 kubelet            pulling image &quot;nginx:latest&quot;
  normal   pulled     8m8s                  kubelet            successfully pulled image &quot;nginx:latest&quot; in 4.9201206s
  normal   created    8m8s                  kubelet            created container nginx
  normal   started    8m8s                  kubelet            started container nginx
  normal   pulling    8m8s                  kubelet            pulling image &quot;busybox:1.34&quot;
  normal   pulled     8m3s                  kubelet            successfully pulled image &quot;busybox:1.34&quot; in 5.2652786s
  normal   created    7m16s (x4 over 8m3s)  kubelet            created container busybox
  normal   started    7m16s (x4 over 8m3s)  kubelet            started container busybox
  normal   pulled     7m16s (x3 over 8m2s)  kubelet            container image &quot;busybox:1.34&quot; already present on machine
  warning  backoff    3m3s (x23 over 8m1s)  kubelet            back-off restarting failed container

so what am i doing wrong? tried to run different images, i got the same error. is it even possible to run two containers in a pod through kind:deployment or i should use kind:pod and specify them that way?
",<kubernetes><kubernetes-pod>,73647711,1,"your busybox container is crashing because it needs a process to run for indefinite period. as long as a process is running  inside, the container won't crash (unless there's some un-handled exception). busybox image doesn't come with a default process.
just add command attribute with sleep in your busybox container.
kind: deployment
apiversion: apps/v1
metadata:
  name: test-deployment
  namespace: dev
spec:
  selector:
    matchlabels:
      deploy: example
  template:
    metadata:
      labels:
        deploy: example
    spec:
      containers:
        - name: nginx
          image: nginx:latest
        - name: busybox
          image: busybox:1.34
          command: [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep 3600&quot;] # run sleep command for 3600 seconds / 1 hr

"
73656303,many different hosts in ingress configuration file,"i am trying to automate the hosts in ingress controller and i'm facing the problem of generating many hosts into one file. what i mean is, i have this ingress.yaml:
---
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-host
  namespace: default
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/default-backend: a
spec:
  rules:
  - host: a.example.com
    http:
      paths:
      - pathtype: prefix
        path: /
        backend:
          service:
            name: a
            port:
              number: 80
  - host: b.example.com
    http:
      paths:
      - pathtype: prefix
        path: /
        backend:
          service:
            name: b
            port:
              number: 80
  ...
  - host: x.example.com
    http:
      paths:
      - pathtype: prefix
        path: /
        backend:
          service:
            name: x
            port:
              number: 80
...


in this example i have multiple instances: a, b, all the way to x and i anticipate a lot more. right now i am programmatically regenerating the whole ingress.yaml to add/remove certain hosts. this is prone to errors and hard to maintain, as i must constantly be aware about ingress.yaml to be broken for one reason or another.
what would really help me is to put every host into a separate file and (maybe) just tell ingress.yaml to scan the whole directory where those files are to be stored. this way, i can just add/remove a single file and reload ingress
is there an option for that? i found somewhere that ingressspec could be somehow defined, but i do not see any usefull link with a valid example. maybe someone found a solution to that already and can point me to the right direction?
",<docker><kubernetes><kubernetes-ingress><nginx-ingress><ingress-controller>,73676247,1,"as @jordanm suggested in the comment, i went with multiple ingress objects on one ingresscontroller, being sure i get rid of nginx.ingress.kubernetes.io/default-backend annotation:
---
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-x-host
  namespace: default
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: x.example.com
    http:
      paths:
      - pathtype: prefix
        path: /
        backend:
          service:
            name: x
            port:
              number: 80

i generate a unique file for each of hosts, replacing x with my unique name.
i also have to make sure that metadata.name is unique. if metadata.name is the same for every object, then it just gets replaced as i apply the new configuration. this works perfectly.
"
73674536,application not accessible using ingress but works with loadbalancer gke,"i am trying to configure a hello world application using ingress in gke. i have been referring a gcp official documentation to deploy an application using ingress.
deploying an app using ingress
but this does not work i have tried to refer several documents but none of those work. i have installed the ingress controller in my kubernetes cluster.
kubectl get svc -n ingress-nginx  returns below output
name                                 type           cluster-ip       external-ip      port(s)     
age
ingress-nginx-controller             loadbalancer   10.125.177.232   35.232.139.102   80:31835/tcp,443:31583/tcp   7h24m

kubectl get pods-n ingress-nginx returns
name                                        ready   status      restarts   age
ingress-nginx-admission-create-jj72r        0/1     completed   0          7h24m
ingress-nginx-admission-patch-pktz6         0/1     completed   0          7h24m
ingress-nginx-controller-5cb8d9c6dd-vptkh   1/1     running     0          7h24m

kubectl get ingress returns below output
name               class    hosts                   address       ports   age
ingress-resource   &lt;none&gt;   35.232.139.102.nip.io   34.69.2.173   80      7h48m

kubectl get pods returns below output
name                         ready   status    restarts   age
hello-app-6d7bb985fd-x5qpn   1/1     running   0          43m

kubect get svc returns below output
name         type        cluster-ip       external-ip   port(s)             age
hello-app    clusterip   10.125.187.239   &lt;none&gt;        8080/tcp            43m

ingress resource yml file used
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-resource
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
spec:
  rules:
  - host: 35.232.139.102.nip.io
    http:
      paths:
      - pathtype: prefix
        path: &quot;/hello&quot;
        backend:
          service:
            name: hello-app
            port:
              number: 8080

can someone tell me what i am doing wrong ? when i try to reach the application its not working.
",<kubernetes><google-cloud-platform><google-kubernetes-engine><kubernetes-ingress>,73684226,1,"so i have installed ingress-controller and used ingress controller ip as the host in my ingress file.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-resource
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
spec:
  rules:
  - host: &quot;35.232.139.102.nip.io&quot;
    http:
      paths:
      - pathtype: prefix
        path: &quot;/hello&quot;
        backend:
          service:
            name: hello-app
            port:
              number: 8080


issue here was i forgot to add the ip from which i was accessing the application. when you create a gke cluster there will be a firewall with the cluster-name-all in this firewall you will need to add your ip address of the machine from which you are trying to access the application. also ensure that the port number is also exposed in my case both were not provided hence it was failing.
"
73608398,mongodb on kubernetes community operator. statefulset pods on unique nodes,"i'm deploying a mongodb cluster with the mongodb community operator. replicaset configuration.
i followed this example for my configuration. the difference is that i want a pod on each node with his pv on the node. at every deploy the same node location pod/pv.
i have deployed a mongodb statefulset with two replicas. i want that pods of the statefulset pod-0, pod-1 lays on specific nodes: node-0, node-1.
for the two pods i deployed two persistent volumes of type hostpath. one for each node: pv-0 and pv-1.
all seems fine but there is the problem:
sometimes the pvc of pod-0(forced on node-0) is bounded to pv-1 (foce on node-1) or vice-versa. so the pod can't start because there is a node conflict
is there a way to force pod-0 on the same node of pv-0?
maybe with the mongodbcommunity.spec.statefulset.volumeclametemplates, but i can't figure out how.
i read here, but i can't figure out how to apply to statefulset.
follows my yamls. satefulset:
apiversion: mongodbcommunity.mongodb.com/v1
kind: mongodbcommunity
metadata:
  name: my-mongo
  labels:
    app: my-mongo
  namespace: mongo-system
spec:
  members: 2
  statefulset:
    spec:
      template:
      volumeclaimtemplates:
      - metadata:
          name: data-volume
        spec:
          storageclassname: hostpath
          accessmodes: [ &quot;readwriteonce&quot; ]
          resources:
            requests:
              storage: 11gi
          selector:
            matchlabels:
              type: data

pv
apiversion: v1
kind: persistentvolume
metadata:
  name: data-volume-db-0
  labels:
    type: data
spec:
  storageclassname: hostpath
  persistentvolumereclaimpolicy: retain
  capacity:
    storage: 11gi
  accessmodes:
    - readwriteonce
  hostpath:
    path: /data/volumes/db-data
    type: &quot;&quot;
  nodeaffinity:
    required:
      # this is just an example for matchexpression
      # this field is required depends on the specific
      # of the environment the resource is deployed in
      nodeselectorterms:
      - matchexpressions:
        - key: kubernetes.io/hostname
          operator: in
          values:
          - node-0

---
apiversion: v1
kind: persistentvolume
metadata:
  name: data-volume-db-1
  labels:
    type: data
spec:
  storageclassname: hostpath
  persistentvolumereclaimpolicy: retain
  capacity:
    storage: 11gi
  accessmodes:
    - readwriteonce
  hostpath:
    path: /data/volumes/db-data
    type: &quot;&quot;
  nodeaffinity:
    required:
      # this is just an example for matchexpression
      # this field is required depends on the specific
      # of the environment the resource is deployed in
      nodeselectorterms:
      - matchexpressions:
        - key: kubernetes.io/hostname
          operator: in
          values:
          - node-1

",<mongodb><kubernetes><persistent-volumes><kubernetes-statefulset>,73701741,1,"i resolved using the claimref on the pv yaml
apiversion: v1
kind: persistentvolume
metadata:
  name: data-volume-db-1
  labels:
    type: data
spec:
  storageclassname: hostpath
  persistentvolumereclaimpolicy: retain
  capacity:
    storage: 11gi
  claimref:
    namespace: system-mongo
    name: data-volume-db-0
  accessmodes:
    - readwriteonce
  hostpath:
    path: /data/volumes/db-data
    type: &quot;&quot;
  nodeaffinity:
    required:
      # this is just an example for matchexpression
      # this field is required depends on the specific
      # of the environment the resource is deployed in
      nodeselectorterms:
      - matchexpressions:
        - key: kubernetes.io/hostname
          operator: in
          values:
          - node-1

"
73600821,configure ingress traffic for kubernetes image,"i have a standalone kubernetes cluster:
plane node - hostname kubernetes1 - 192.168.1.126
work node - hostname kubernetes2 - 192.168.1.138

i deployed this private repository:
apiversion: v1
kind: persistentvolume
metadata:
  name: pv1
spec:
  capacity:
    storage: 5gi # specify your own size
  volumemode: filesystem
  persistentvolumereclaimpolicy: retain
  local:
    path: /opt/registry # can be any path
  nodeaffinity:
    required:
      nodeselectorterms:
      - matchexpressions:
        - key: kubernetes.io/hostname
          operator: in
          values:
          - kubernetes2
  accessmodes:
    - readwritemany # only 1 node will read/write on the path.
---
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: pv1-claim
spec: # should match specs added in the persistenvolume
  accessmodes:
    - readwritemany
  volumemode: filesystem
  resources:
    requests:
      storage: 5gi
---
apiversion: apps/v1
kind: deployment
metadata:
  name: private-repository-k8s
  labels:
    app: private-repository-k8s
spec:
  replicas: 1
  selector:
    matchlabels:
      app: private-repository-k8s
  template:
    metadata:
      labels:
        app: private-repository-k8s
    spec:
      volumes:
       - name: certs-vol
         hostpath:
          path: /opt/certs
          type: directory
       - name: task-pv-storage
         persistentvolumeclaim:
           claimname: pv1-claim # specify the pvc that you've created. pvc and deployment must be in same namespace.
      containers:
        - image: registry:2
          name: private-repository-k8s
          imagepullpolicy: ifnotpresent
          env:
          - name: registry_http_tls_certificate
            value: &quot;/opt/certs/registry.crt&quot;
          - name: registry_http_tls_key
            value: &quot;/opt/certs/registry.key&quot;
          ports:
            - containerport: 5000
          volumemounts:
          - name: certs-vol
            mountpath: /opt/certs
          - name: task-pv-storage
            mountpath: /opt/registry

service is deployed on the work node:
kubernetes@kubernetes1:/opt/registry$ kubectl get pods
name                                      ready   status    restarts   age
private-repository-k8s-6d5d954b4f-ldwd6   1/1     running   0          153m
kubernetes@kubernetes1:/opt/registry$

i tried to create a ingress access because i want to access this pod from the outside:
kubectl create namespace test

service controller:
apiversion: networking.k8s.io/v1   
kind: ingress   
metadata:
  namespace: test   
  name: private-repository-service-ingress   
  annotations:   
    nginx.ingress.kubernetes.io/rewrite-target: /$1   
spec:   
  rules:   
    - host: kubernetes2   
      http:   
        paths:   
          - path: /   
            pathtype: prefix   
            backend:   
              service:   
                name: private-repository-service   
                port:   
                  number: 5000

service-load-balancer:
apiversion: v1
kind: service
metadata:
  namespace: test
  name: private-repository-service
spec:
  #type: nodeport
  selector:
    app: private-repository-k8s
  ports:
      # by default and for convenience, the `targetport` is set to the same value as the `port` field.
    - port: 5000
      targetport: 5000
      # optional field
      # by default and for convenience, the kubernetes control plane will allocate a port from a range (default: 30000-32767)
      #nodeport: 30007

when i run curl 192.168.1.138:5000 there is no response.
do you know where i might be wrong?
edit:
kubernetes@kubernetes1:~$ kubectl get svc
name                       type        cluster-ip       external-ip   port(s)    age
kubernetes                 clusterip   10.96.0.1        &lt;none&gt;        443/tcp    3d20h
private-registry-service   clusterip   10.103.148.234   &lt;none&gt;        5000/tcp   6h34m
kubernetes@kubernetes1:~$

",<kubernetes><kubernetes-ingress>,73636988,1,"looks like you don't have an ingress-controller of any kind and just want to access the registry directly. in that case you need to create a service of type nodeport.
apiversion: v1
kind: service
metadata:
  namespace: test
  name: private-repository-service
spec:
  type: nodeport # added
  selector:
    app: private-repository-k8s
  ports:
    - port: 5000
      targetport: 5000
      nodeport: 30123 # added

this will bind the service port 5000 to the host's port 30123.
if you run kubectl get svc this will than give you a slightly different output.
kubernetes@kubernetes1:~$ kubectl get svc
name                       type        cluster-ip       external-ip   port(s)    age
kubernetes                 clusterip   10.96.0.1        &lt;none&gt;        443/tcp    3d20h
private-registry-service   clusterip   10.103.148.234   &lt;none&gt;        5000:30123/tcp   6h34m

notice the mapping 30312:5000. now you can send a request to the registry on that port: curl 192.168.1.138:30312. you can also omit the nodeport field, kubernetes will then choose a random one in the range between 3000 and 32767 for you. it will be displayed in the kubectl get svc command as shown above. the ingress is not needed and can be removed.
if you want to use an ingress as you provided you need to use an ingress-controller, like nginx or traefik, see also kubernetes docs on that topic.

[...] an ingress controller is responsible for fulfilling the ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic.

edit
there are a lot of ingress-controllers out there, they all have their advantages and disadvantages. for a beginner, nginx might be a good choise, see docs.
to install it, run these commands (from the install docs page)
$ helm repo add nginx-stable https://helm.nginx.com/stable
$ helm repo update
$ helm install my-release nginx-stable/nginx-ingress

where my-release is a random name, you can choose what ever you want. this will create an nginx pod in the namespace you installed the chart. it will also create an nginx-ingress service of type loadbalancer, like this:
kubectl get svc
name                       type           cluster-ip     external-ip          
my-release-nginx-ingress   loadbalancer   10.109.27.49   &lt;pending&gt;     80:30536/tcp,443:31694/tcp   111s

as you can see the external-ip is in &lt;pending&gt; state. in a public cloud environment like aws a load-balancer resource like elb is created and its public ip will be assigned to the service as external-ip. in your on-premise setup it will stay in &lt;pending&gt; status. but as you can see, two random node ports are mapped to the http/https ports, like with the nodeport setup above. here it's 80 -&gt; 30536 and 433 -&gt; 31694, for you it will be something similar.
now you can apply your manifests as above. you'll get a service of type clusterip. also create an ingress like this:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: private-repository-service 
spec:
  ingressclassname: nginx
  rules:
    - host: example.com
      http:
        paths:
        - path: /
          pathtype: prefix
          backend:
            service:
              name: private-repository-service 
              port:
                number: 5000

now you can run curl against that host curl -hhost:example.com &quot;http://192.168.1.138:30536/&quot; (for me it's port 30536, will be a different one for you) and you will get an answer from the registry. works the same with https using the other port.
note that i installed everything in the same namespace. in reality you should have a dedicated ingress namespace.
i would also highly recommand to learn the basics of kubernetes, e.g. by a udemy course or a youtube tutorail series. if you want to know more about ingress-controller in an on-premise setup, check out my other answer on that topic.
"
66423005,hpa with different namespaces,"i have a kubernetes setup with following
sn.  type          service  namespace
1.   statefulset   rabbitmq rabbitmq
2.   deployment    pods     default
3.   hpa           hpa      default

the metrics is exported on `/apis/custom.metrics.k8s.io/v1beta1 as follows
{
  &quot;kind&quot;: &quot;apiresourcelist&quot;,
  &quot;apiversion&quot;: &quot;v1&quot;,
  &quot;groupversion&quot;: &quot;custom.metrics.k8s.io/v1beta1&quot;,
  &quot;resources&quot;: [
    {
      &quot;name&quot;: &quot;services/rabbitmq_queue_messages_ready&quot;,
      &quot;singularname&quot;: &quot;&quot;,
      &quot;namespaced&quot;: true,
      &quot;kind&quot;: &quot;metricvaluelist&quot;,
      &quot;verbs&quot;: [
        &quot;get&quot;
      ]
    },
    {
      &quot;name&quot;: &quot;namespaces/rabbitmq_queue_messages_ready&quot;,
      &quot;singularname&quot;: &quot;&quot;,
      &quot;namespaced&quot;: false,
      &quot;kind&quot;: &quot;metricvaluelist&quot;,
      &quot;verbs&quot;: [
        &quot;get&quot;
      ]
    },
    {
      &quot;name&quot;: &quot;pods/rabbitmq_queue_messages_ready&quot;,
      &quot;singularname&quot;: &quot;&quot;,
      &quot;namespaced&quot;: true,
      &quot;kind&quot;: &quot;metricvaluelist&quot;,
      &quot;verbs&quot;: [
        &quot;get&quot;
      ]
    }
  ]
}

configuration of hpa
apiversion: autoscaling/v2beta2
kind: horizontalpodautoscaler
metadata:
  name: demo-hpa
  namespace: default
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: test-pod2
  minreplicas: 1
  maxreplicas: 2
  metrics:
  - type: object
    object:
      metric:
        name: &quot;rabbitmq_queue_messages_ready&quot;
      describedobject:
        apiversion: custom.metrics.k8s.io/v1/beta1
        kind: service
        name: rabbitmq
      target:
        type: value
        value: 50

whenever i try to deploy the hpa in default namespace i am getting the following error.
 scalingactive  false   failedgetobjectmetric  the hpa was unable to compute the replica count: unable to get metric rabbitmq_queue_messages_ready: service on default rabbitmq/unable to fetch metrics from custom metrics api: the server could not find the metric rabbitmq_queue_messages_ready for services rabbitmq

but when the same hpa is as set do namespace rabbitmq with hpa
apiversion: autoscaling/v2beta2
kind: horizontalpodautoscaler
metadata:
  name: demo-hpa
  namespace: rabbitmq
spec:
  scaletargetref:
    apiversion: apps/v1
    kind: deployment
    name: test-pod2
  minreplicas: 1
  maxreplicas: 2
  metrics:
  - type: object
    object:
      metric:
        name: &quot;rabbitmq_queue_messages_ready&quot;
      describedobject:
        apiversion: custom.metrics.k8s.io/v1/beta1
        kind: service
        name: rabbitmq
      target:
        type: value
        value: 50

and deployment.yaml as
apiversion: apps/v1
kind: deployment
metadata:
  name: test-pod2
  labels:
    app: test-pod2
  namespace: rabbimtq
spec:
  replicas: 1
  selector:
    matchlabels:
      app: test-pod2
  template:
    metadata:
      labels:
        app: test-pod2
    spec:
      containers:
        - name: test-pod2
          image: golang:1.16
          command: [&quot;sh&quot;, &quot;-c&quot;, &quot;tail -f /etc/hosts&quot;]

the deployment works perfect.
is there a way, i can export the metrics for service rabbitmq from rabbitmq namespace to any other namespaces which i can then use for scaling???
",<kubernetes><rabbitmq><kubernetes-helm><hpa>,66437465,1,"hpa is a namespaced resource. it means that it can only scale deployments which are in the same namespace as the hpa itself. that's why it is only working when both hpa and deployment are in the namespace: rabbitmq. you can check it within your cluster by running:
    kubectl api-resources --namespaced=true | grep hpa
    name                        shortnames   apigroup                    namespaced   kind
    horizontalpodautoscalers    hpa          autoscaling                 true         horizontalpodautoscaler

the easiest way to make it work would be to simply set the namespace value of the hpa to the same namespace of deployment which you want to scale. for example, if your deployment is set like below:
apiversion: apps/v1
kind: deployment
metadata:
  namespace: rabbimtq

the hpa must also be in the same namespace:
apiversion: autoscaling/v2beta2
kind: horizontalpodautoscaler
metadata:
  namespace: rabbitmq

in other words: set the metadata.namespace value of your hpa to the metadata.namespace value of your deployment as the hpa is not able to scale deployments outside of its namespace.
namespaces are like separate &quot;boxes&quot; in your cluster. namespaced resources cannot work outside of the namespace they are into. they can't see resources that are not in their &quot;box&quot;.
by doing it that way there would be no need to reconfigure the custom metrics.
"
75825083,"""forbidden"" error when trying to delete kubernetes pods","i'm trying to delete kubernetes pod via go-client library using the following code:
err := ks.clientset.corev1().pods(kubedata.podnamespace).delete(context.background(), kubedata.podname, metav1.deleteoptions{})
if err != nil {
  log.fatal(err)
}

however receiving an error:

pods &quot;app-name&quot; is forbidden: user &quot;system:serviceaccount:default:app-name&quot; cannot delete resource &quot;pods&quot; in api group &quot;&quot; in the namespace &quot;default&quot;&quot;

here is the serviceaccount.yaml:
{{- $sa := print .release.name &quot;-&quot; .values.serviceaccount -}}
---
apiversion: v1
kind: serviceaccount
metadata:
  name: {{ $sa }}
  namespace: {{ .release.namespace }}

---
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  name: {{ $sa }}
rules:
  - apigroups: [&quot;apps&quot;]
    verbs: [&quot;patch&quot;, &quot;get&quot;, &quot;list&quot;]
    resources:
      - deployments
---
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  name: {{ $sa }}
rules:
  - apigroups: [&quot;apps&quot;]
    verbs: [&quot;delete&quot;, &quot;get&quot;, &quot;list&quot;]
    resources:
      - pods
---
apiversion: rbac.authorization.k8s.io/v1
kind: rolebinding
metadata:
  name: {{ $sa }}
roleref:
  apigroup: rbac.authorization.k8s.io
  kind: role
  name: {{ $sa }}
subjects:
  - kind: serviceaccount
    name: {{ $sa }}

looks like something related to user permissions, however not sure how to properly configure it.
thanks.
",<kubernetes><kubernetes-go-client>,75826996,1,"as you can see from the error:
pods &quot;app-name&quot; is forbidden: user &quot;system:serviceaccount:default:app-name&quot; cannot delete resource &quot;pods&quot; in api group &quot;&quot; in the namespace &quot;default&quot;&quot;
the important part is: in api group &quot;&quot;
take a look on your manifest:
apiversion: rbac.authorization.k8s.io/v1
kind: role
metadata:
  name: {{ $sa }}
rules:
  # - apigroups: [&quot;apps&quot;] # &lt;-- bad!
  - apigroups: [&quot;&quot;] # &lt;-- good!
    verbs: [&quot;delete&quot;, &quot;get&quot;, &quot;list&quot;]
    resources:
      - pods

this definition is specifying that a resource pod is in the apigroup: apps which is not correct and &quot;&quot; should be used instead.
more on that you can read here:

kubernetes.io: docs: reference: using api: api groups

"
75802235,getting '502 bad gateway' while deploying springboot app in eks,"i deployed a spring boot app on aws elastic kubernetes service. i am facing a 502 bad gateway error. i cannot find anything useful from the logs, there is no event to check, it works fine locally and the docker image is also running without any issue.
right now its just a simple hello world app,
here are the yaml files files or reference.
deployment.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: backend-deployment
  namespace: my-namespace
  labels:
    app: backend-java
spec:
  replicas: 1
  selector:
    matchlabels:
      app: backend-java
  template:
    metadata:
      labels:
        app: backend-java
    spec:
      containers:
        - name: backend-java
          image: &lt;docker-image-location&gt;
          ports:
            - containerport: 81
          resources:
            limits:
              cpu: &quot;4000m&quot;
              memory: &quot;2048mi&quot;
            requests:
              cpu: &quot;100m&quot;
              memory: &quot;1024mi&quot;

service.yaml
apiversion: v1
kind: service
metadata:
  namespace: my-namespace
  name: backend-service
spec:
  type: nodeport
  selector:
    app: backend-java
  ports:
    - port: 81
      targetport: 8080
      nodeport: 30019

ingress.yaml
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: &quot;my-app-ingress&quot;
  namespace: &quot;my-namespace&quot;
  annotations:
    alb.ingress.kubernetes.io/scheme: internal
    alb.ingress.kubernetes.io/backend-protocol: http
    alb.ingress.kubernetes.io/listen-ports: '[{&quot;http&quot;: 80}]'
  spec:
   ingressclassname: alb
   rules:
     - host: myapp.aws.com
       http:
         paths:
           - path: /
             pathtype: prefix
             backend:
               service:
                 name: &quot;backend-service&quot;
                 port:
                   number: 81

similar configuration has worked for deploying a react app, which works as expected. only while deploying backend it give '502 bad gateway'
",<spring-boot><docker><kubernetes><amazon-eks>,75809467,1,"your targetport in the service and the containerport in the deployment do not match. you can fix it by changing the targetport in the service
apiversion: v1
kind: service
metadata:
  namespace: my-namespace
  name: backend-service
spec:
  type: nodeport
  selector:
    app: backend-java
  ports:
    - port: 81
      targetport: 81
      nodeport: 30019

read more about the difference between port and targetport here.
"
66199574,ingress .yml file isn't being applied to gke but works fine in minikube,"i've been using minikube and this yml file:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-service
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
    - http:
        paths:
          - path: /?(.*)
            pathtype: prefix
            backend:
              service:
                name: client-cluster-ip
                port:
                  number: 3000
          - path: /api/?(.*)
            pathtype: prefix
            backend:
              service:
                name: server-cluster-ip
                port:
                  number: 5000

i've installed helm on my gke cluster and installed ingress-nginx via helm following their directions here.
i kubectl apply my k8s and they all spin up besides the ingress-service from the file above.
any help is much appreciated.
i've tried this:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-service
  namespace: my-ingress-nginx
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
    - http:
        paths:
          - path: /*
            backend:
              servicename: client-cluster-ip
              serviceport: 3000
          - path: /api/*
            backend:
              servicename: server-cluster-ip
              serviceport: 5000

i'm really stuck here. not seeing ingress-service show up like i would in minikube and i have no idea why.
server-cluster-ip:
apiversion: v1
kind: service
metadata:
  name: server-cluster-ip
spec:
  type: clusterip
  selector:
    component: server
  ports:
    - port: 5000
      targetport: 5000

client-cluster-ip:
apiversion: v1
kind: service
metadata:
  name: client-cluster-ip
spec:
  type: clusterip
  selector:
    component: web
  ports:
    - port: 3000
      targetport: 3000

the deployments and the clusterip services above are being applied to the cluster but the ingress-service to direct traffic to them is not.
services:
name                                    type           
client-cluster-ip              clusterip      
kubernetes                              clusterip      
my-ingress-nginx-controller             loadbalancer   
my-ingress-nginx-controller-admission   clusterip      
postgres-cluster-ip            clusterip      
redis-cluster-ip               clusterip      
server-cluster-ip              clusterip  

the my-ingress-nginx-controller and my-ingress-nginx-controller-admission was created when i did helm install my-ingress-nginx ingress-nginx/ingress-nginx
why can't i create an ingress service?
",<kubernetes><google-kubernetes-engine><kubernetes-ingress><nginx-ingress>,66208251,1,"i realized i needed to open port 8443 from the documentation.
so i went to the firewall list in google cloud. found the rules that had tcp:80,443 in the protocols / ports. clicked it, clicked edit and added 8443 to it.
i had an error after but this fixed it:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: ingress-resource
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
    - http:
        paths:
          - path: /?(.*)
            backend:
              servicename: client-cluster-ip
              serviceport: 3000
          - path: /api/?(.*)
            backend:
              servicename: server-cluster-ip
              serviceport: 5000

notice i changed * for ?(.*)
"
75729884,eks service account annotation,"i have a service account, and this needs access to multiple aws services. is there any way we could specify multiple role an annotations, or do we expect to create a generic role and give access to all the required services?
the below not supported?

chatgpt says,


apiversion: v1
kind: serviceaccount
metadata:
  name: my-service-account
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::111111111111:role/my-role-1
    eks.amazonaws.com/role-arn: arn:aws:iam::111111111111:role/my-role-2



",<kubernetes><amazon-eks><k8s-serviceaccount>,75743819,1,"yes its not supported like what you shown.
example supported:
apiversion: v1
kind: serviceaccount
metadata:
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/xxxx

your options is to use 1 iam role arn like above, with multiple iam policy attached. i don't think its hard to add multiple policy to a role.
"
75610695,kubernetes informer fails with unauthorized,"i'm trying to construct a kubernetes informer outside of the eks cluster that it's watching. i'm using aws-iam-authenticator plugin to provide the exec-based credentials to the eks cluster. for the plugin to work, i'm assuming an iam role and passing the aws iam credentials as environment variables.
the problem is that these credentials expire after an hour and cause the informer to fail with

e0301 23:34:22.167817     582 runtime.go:79] observed a panic: &amp;errors.statuserror{errstatus:v1.status{typemeta:v1.typemeta{kind:&quot;&quot;, apiversion:&quot;&quot;}, listmeta:v1.listmeta{selflink:&quot;&quot;, resourceversion:&quot;&quot;, continue:&quot;&quot;, remainingitemcount:(*int64)(nil)}, status:&quot;failure&quot;, message:&quot;the server has asked for the client to provide credentials (get pods)&quot;, reason:&quot;unauthorized&quot;, details:(*v1.statusdetails)(0xc0005b0300), code:401}} (the server has asked for the client to provide credentials (get pods))

is there a better way of getting clientconfig and aws-iam-authenticator to refresh the credentials?
here's a rough skeleton of my code:
credentialsprovider := aws.newcredentialscache(stscreds.newwebidentityroleprovider(...))
creds, err := credentialsprovider.retrieve(ctx)

config := clientcmdapi.newconfig()
// ...
config.authinfos[&quot;eks&quot;] = &amp;clientcmdapi.authinfo{
    exec: &amp;clientcmdapi.execconfig{
        command: &quot;aws-iam-authenticator&quot;,
        args: []string{
            &quot;token&quot;,
            &quot;-i&quot;,
            clustername,
        },
        // these env vars are static! :(
        env: []clientcmdapi.execenvvar{
            {
                name:  &quot;aws_access_key_id&quot;,
                value: creds.accesskeyid,
            },
            {
                name:  &quot;aws_secret_access_key&quot;,
                value: creds.secretaccesskey,
            },
            {
                name:  &quot;aws_session_token&quot;,
                value: creds.sessiontoken,
            },
        },
        apiversion:      &quot;client.authentication.k8s.io/v1beta1&quot;,
        interactivemode: clientcmdapi.neverexecinteractivemode,
    },
}

restconfig, err := config.clientconfig()
clientset, err = kubernetes.newforconfig(restconfig)

informerfactory := informers.newsharedinformerfactory(clientset, time.second*30)
podinformer := cw.informerfactory.core().v1().pods().informer()

here are a couple similar threads i found:

kubernetes client-go informers getting &quot;unauthorized&quot; error after 15 mins
https://github.com/kubernetes/client-go/issues/1189

",<kubernetes><amazon-eks><client-go><aws-iam-authenticator>,75621398,1,"my solution was to create write the credentials to a file and create a background thread to refresh that file. i can then pass tell aws-iam-authenticator to read the credentials from the file via the aws_shared_credentials_file environment variable.
this might also be possible using aws_web_identity_token_file to save some steps, but i didn't look further.
the updated code looks like this
func updatecredentials(ctx context.context) {
    creds, err := c.credentialsprovider.retrieve(ctx)
    s := fmt.sprintf(`[default]
aws_access_key_id=%s
aws_secret_access_key=%s
aws_session_token=%s`, creds.accesskeyid, creds.secretaccesskey, creds.sessiontoken)
    err = os.writefile(credentialsfile.name(), []byte(s), 0666)
    return nil
}

func updatecredentialsloop(ctx context.context) {
    for {
        err := updatecredentials(ctx)
        time.sleep(5*time.minute)
    }
}

credentialsprovider := aws.newcredentialscache(stscreds.newwebidentityroleprovider(...))

credentialsfile, err := os.createtemp(&quot;&quot;, &quot;credentials&quot;)
updatecredentials(ctx)
go updatecredentialsloop(ctx)

config := clientcmdapi.newconfig()
// ...
config.authinfos[&quot;eks&quot;] = &amp;clientcmdapi.authinfo{
    exec: &amp;clientcmdapi.execconfig{
        command: &quot;aws-iam-authenticator&quot;,
        args: []string{
            &quot;token&quot;,
            &quot;-i&quot;,
            clustername,
        },
        env: []clientcmdapi.execenvvar{
            {
                name:  &quot;aws_shared_credentials_file&quot;,
                value: credentialsfile.name(),
            },
        },
        apiversion:      &quot;client.authentication.k8s.io/v1beta1&quot;,
        interactivemode: clientcmdapi.neverexecinteractivemode,
    },
}

restconfig, err := config.clientconfig()
clientset, err = kubernetes.newforconfig(restconfig)

informerfactory := informers.newsharedinformerfactory(clientset, time.second*30)
podinformer := cw.informerfactory.core().v1().pods().informer()

"
75587750,503 service temporary unavailable ingress eks,"getting a 503 error for the ingress, did the basic trouble shooting with labels and stuff looks good though. i see the pods are running and can be listed when ran with the service label.
the readiness probe has a warning but it did not fail
what else can be checked tor resolve this issue. any ideas appreciated


kubectl get service -n staging
name                    type        cluster-ip       external-ip   port(s)    age
app-staging   clusterip   172.20.174.146   &lt;none&gt;        8000/tcp   242d


kubectl describe service app-staging -n staging
name:              app-staging
namespace:         staging
labels:            &lt;none&gt;
annotations:       &lt;none&gt;
selector:          app=app-staging
type:              clusterip
ip family policy:  singlestack
ip families:       ipv4
ip:                172.20.174.146
ips:               172.20.174.146
port:              app-staging  8000/tcp
targetport:        8000/tcp
endpoints:         10.200.32.6:8000,10.200.64.2:8000
session affinity:  none
events:            &lt;none&gt;


kubectl get pods -n staging -l app=app-staging                     
name                                     ready   status    restarts   age
app-staging-5677656dc8-djp8l   1/1     running   0          4d7h
app-staging-5677656dc8-dln5v   1/1     running   0          4d7h



this is the readiness probe


 kubectl describe pod app-staging-5677656dc8-djp8l -n staging|grep -i readiness
    readiness:      http-get http://:8000/ delay=30s timeout=1s period=30s #success=1 #failure=6
  warning  probewarning  40s (x12469 over 4d7h)  kubelet  readiness probe warning:



here is the manifest file for the pod, service and ingress


# this deployment is setup to use ecr for now, but should switch to  artifactory in the future.
apiversion: apps/v1
kind: deployment
metadata:
  name: app-staging
  namespace: staging
spec:
  replicas: 2
  selector:
    matchlabels:
      app: app-staging
  template:
    metadata:
      labels:
        app: app-staging
    spec:
      containers:
        - name: app-staging
          image: ""${docker_registry}/:${image_tag}""
          readinessprobe:
            failurethreshold: 6
            httpget:
              path: /
              port: 8000
            initialdelayseconds: 30
            periodseconds: 30
            successthreshold: 1
            timeoutseconds: 1
          imagepullpolicy: always
         # setting autodynatrace_forkable environment variable will cause an ominous looking error message similar to the one below:
         #
         #  `warning autodynatrace - init: could not initialize the oneagent sdk, agentstate: 1`
         #
         # this error message is expected when ""forkable"" mode is enabled. see the link below for more information:
         # https://github.com/dynatrace/oneagent-sdk-for-python/blob/fa4dd209b6a21407abca09a6fb8da1b85755ab0a/src/oneagent/__init__.py#l205-l217
          command: [""/bin/sh""]
          args:
            - -c
            - &gt;-
                /bin/sed -i -e ""s/# 'autodynatrace.wrappers.django'/'autodynatrace.wrappers.django'/"" /app//on_/on_/settings.py &amp;&amp;
                /usr/local/bin/python manage.py collectstatic --noinput &amp;&amp;
                autowrapt_bootstrap=autodynatrace autodynatrace_forkable=true /usr/local/bin/gunicorn --workers 8 --preload --timeout 120 --config gunicorn.conf.py --bind 0.0.0.0:8000
          env:
            - name: autodynatrace_pod_name
              valuefrom:
                fieldref:
                  apiversion: v1
                  fieldpath: metadata.name
            - name: autodynatrace_application_id
              value: django ($(autodynatrace_pod_name):8000)
          ports:
            - containerport: 8000
          volumemounts:
          # mount config in both locations while we migrate to running container as non-root user.
          - name: secrets
            readonly: true
            mountpath: /root/fhirengine/conf
          - name: secrets
            readonly: true
            mountpath: /home//fhirengine/conf
      imagepullsecrets:
        - name: jfrogcred
      volumes:
       - name: secrets
         secret:
            secretname: config
            defaultmode: 420
      restartpolicy: always
---
apiversion: v1
kind: service
metadata:
  name: app-staging
  namespace: staging
spec:
  ports:
    - name: app-staging
      port: 8000
      targetport: 8000
  selector:
    app: app-staging
---
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: app-staging
  namespace: staging
  annotations:
    external-dns.alpha.kubernetes.io/hostname: staging.tv-pd.sh.io
    external-dns.alpha.kubernetes.io/type: internal
    kubernetes.io/ingress.class: nginx-internal
spec:
  rules:
    - host: staging.tv-pd.sh.io
      http:
        paths:
          - path: /
            backend:
              service:
                name: app
                port:
                  number: 8000
            pathtype: implementationspecific
            #pathtype is now required for each specified path. options are prefix, exact, and implementationspecific. to match the undefined v1beta1 behavior, use implementationspecific
---



",<kubernetes><manifest><amazon-eks><nginx-ingress>,75589893,1,"i see that your service is named &quot;app-staging&quot;
apiversion: v1
kind: service
metadata:
  name: app-staging

but in the ingress the path mapping to service is incorrectly identifying the service name as &quot;app&quot;
spec:
  rules:
    - host: staging.tv-pd.sh.io
      http:
        paths:
          - path: /
            backend:
              service:
                name: app
                port:
                  number: 8000

please change the backend service name in ingress to &quot;app-staging&quot; instead of &quot;app&quot;.
please accept the answer if this resolves your issue.
"
76119763,how to connect - correctly authenticated - to eks with loadfromoptions using @kubernetes/client-node?,"i am trying to use @kubernetes/client-node to access a kubernetes api running on aws eks.
i have setup a new iam user which is allowed to access the kubernetes api (eks:accesskubernetesapi).
this here is an excerpt from my code, i am mostly confused on how to provide the user credentials (since in the kube config they would be provided by exec, and i'm unsure what that resolves to).
const kubeconfigdata = await getekskubeconfigdata(); // this gives me all clusters with relevant eks data included

const clusters = kubeconfigdata.map((cluster) =&gt; ({
  name: cluster.arn as string,
  server: cluster.endpoint as string,
  cadata: cluster.certificateauthority as string,
  skiptlsverify: false,
}));

const contexts = kubeconfigdata.map((cluster) =&gt; ({
  name: cluster.arn as string,
  cluster: cluster.arn as string,
  user: cluster.arn as string,
}));

/** 
as far as i understand here lies the problem.
i am unsure how to correctly authenticate against the api, can i provide the token here?
the access id and secret? 
i can't read a kube config from the filesystem, so i need to provide it either via sts token or through env variables, as far as i understand?
*/
const users = kubeconfigdata.map((cluster) =&gt; ({
  name: cluster.arn as string,
  password: cluster.token as string,
}));

const currentcontext = contexts[0].name;

kubeconfig.loadfromoptions({
  clusters,
  contexts,
  users,
  currentcontext,
});

trying to listnamespace() with this config results in the following response body:
body: {
    kind: 'status',
    apiversion: 'v1',
    metadata: {},
    status: 'failure',
    message: 'namespaces is forbidden: user &quot;system:anonymous&quot; cannot list resource &quot;namespaces&quot; in api group &quot;&quot; at the cluster scope',
    reason: 'forbidden',
    details: { kind: 'namespaces' },
    code: 403
  }

please tell me what i'm doing wrong.
",<javascript><amazon-web-services><kubernetes><kubectl><amazon-eks>,76131866,1,"ok, so my main problem and the reason why i thought i needed to use loadfromoptions was that i had missing rbac permissions and didn't have a connection between my iam user and rbac.
here is what i did if anyone finds this and wants to know:
first things first: create a rbac role by applying the following yaml, using the original account that created the eks cluster. here i gave it the name rbac-add-reader.yaml, while giving read acces to all resources to the new role reader:
kubectl apply -f rbac-add-reader.yaml
---  
apiversion: rbac.authorization.k8s.io/v1  
kind: clusterrole  
metadata:  
name: reader  
rules:  
- apigroups: [&quot;*&quot;]  
resources: [&quot;*&quot;]  
verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]  
---  
apiversion: rbac.authorization.k8s.io/v1  
kind: clusterrolebinding  
metadata:  
name: reader  
subjects:  
- kind: group  
name: reader  
apigroup: rbac.authorization.k8s.io  
roleref:  
kind: clusterrole  
name: reader  
apigroup: rbac.authorization.k8s.io

create a new iam user.
create an iam policy (read access to all eks properties, as well as the ability to get a session token):
{
    &quot;version&quot;: &quot;2012-10-17&quot;,
    &quot;statement&quot;: [
        {
            &quot;effect&quot;: &quot;allow&quot;,
            &quot;action&quot;: [
                &quot;eks:describenodegroup&quot;,
                &quot;eks:listnodegroups&quot;,
                &quot;eks:describecluster&quot;,
                &quot;eks:listclusters&quot;,
                &quot;eks:accesskubernetesapi&quot;,
                &quot;eks:listupdates&quot;,
                &quot;eks:listfargateprofiles&quot;,
                &quot;sts:getsessiontoken&quot;,
                &quot;ssm:getparameter&quot;,
                &quot;eks:describefargateprofile&quot;,
                &quot;eks:describeaddonconfiguration&quot;,
                &quot;eks:listtagsforresource&quot;,
                &quot;eks:listaddons&quot;,
                &quot;eks:describeaddon&quot;,
                &quot;eks:describeidentityproviderconfig&quot;,
                &quot;eks:describeupdate&quot;,
                &quot;eks:describeaddonversions&quot;,
                &quot;eks:listidentityproviderconfigs&quot;
            ],
            &quot;resource&quot;: &quot;*&quot;
        }
    ]
}

create a user group, add the policy to the group and then add the newly created user to the group too.
now we need to connect the iam user to the rbac role (if you don't want to use vim prepend kube_editor=&quot;nano&quot;):
kubectl edit -n kube-system configmap/aws-auth
add the iam user to mapusers:
apiversion: v1
data:
  mapaccounts: |
    []
  maproles: |
    []
  mapusers: |
    - &quot;userarn&quot;: &quot;arn:aws:iam::xxxxxxxxxxx:user/&lt;iam-user-name&gt;&quot;
      &quot;username&quot;: &quot;&lt;iam-user-name&gt;&quot;
      &quot;groups&quot;:
      - &quot;reader&quot;
kind: configmap
.
.
.


switch over to your new user. (if you have not, configure a new profile with aws configure --profile &lt;your-new-profile-name&gt;)
update your kubeconfig: aws eks update-kubeconfig --name &lt;clustername&gt; --profile &lt;your-new-profile-name&gt;
you can now check if your permissions were set correctly:
kubectl auth can-i get pods should return yes, kubectl auth can-i create pods should return no.
now you are able to loadfromdefault().
"
66550263,update json file placeholder with regex from configmap,"assuming the following json:
{
   &quot;a&quot;:{
      &quot;a_key&quot;:&quot;%placeholder_1%&quot;,
      &quot;b_key&quot;:&quot;%placeholder_2%&quot;
   }
}

and, the following values.yaml:
placeholders:
  placeholder_1: hello
  placeholder_2: world

i would like to load this json using configmap into my pod. but, replace the placeholders with the values under values.yaml automatically based on the key.
thought about writing a simple regex which will search for the words between two % and use this word with .values.placeholders.$1.
so far, i managed to replace single value using:
apiversion: v1
kind: configmap
metadata:
  name: config
data:
  config.json: |-
    {{- regexreplaceall &quot;%placeholder_1%&quot; (  .files.get &quot;config.json&quot;) .values.placeholders.placeholder_1 | nindent 4 }}

the final goal is to replace both placeholder_1 and placeholder_2 by single regex.
desired json:
{
   &quot;a&quot;:{
      &quot;a_key&quot;:&quot;hello&quot;,
      &quot;b_key&quot;:&quot;world&quot;
   }
}

any help will be much appriciated.
",<kubernetes><kubernetes-helm>,66562016,1,"here is what i have come up with:
apiversion: v1
kind: configmap
metadata:
  name: config
data:
  config.json: |-
    {{- $file := .files.get &quot;config.json&quot; }}
    {{- range $k, $v := .values.placeholders }}
      {{- $file = regexreplaceall (printf &quot;%%%s%%&quot; $k) $file  $v }}
    {{- end }}
    {{- print $file | nindent 4 }}

i load a config file content into a $file variable and then iterate over all placeholers.keys from values.yaml file and replace them one by one, saving the output back to the same variable. at the end, $file variable has all fields substituted so i just print it.
"
66631343,pod crashes when applying attaching persistent volume to yaml file,"here is my persistent volume definition
apiversion: v1
kind: persistentvolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageclassname: manual
  capacity:
    storage: 10gi
  accessmodes:
    - readwriteonce
  hostpath:
    path: &quot;/mnt/data&quot;

here is my persistent volume claim
apiversion: v1
kind: persistentvolumeclaim
metadata:
  name: task-pv-claim
spec:
  storageclassname: manual
  accessmodes:
    - readwriteonce
  resources:
    requests:
      storage: 3gi

here is the pod i'm trying to deploy
apiversion: v1
kind: pod
metadata:
  name: task-pv-pod
spec:
  volumes:
    - name: task-pv-storage
      persistentvolumeclaim:
        claimname: task-pv-claim
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerport: 80
          name: &quot;http-server&quot;
      volumemounts:
        - mountpath: &quot;/usr/share/nginx/html&quot;
          name: task-pv-storage

when i try to deploy the pod using kubectl create -f task-pv-pod.yaml, i get this error

failed to start container &quot;52c5f707bb90d87b4178e8508d710ae0912d8ee7bdd7c4b9b802bd6b35f266de&quot;: error response from daemon: error while creating mount source path '/mnt/data': mkdir /mnt/data: read-only file system: runcontainererror

i am following this guide https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/, i have another pod running with a different application, and wanted to apply persistent storage to that pod once i had this one up and running.
",<kubernetes><google-cloud-platform><kubernetes-pod><persistent-volumes>,66635320,1,"while defining  persistent volume  you are using  type: local . this means that you want to create directory in  /mnt. local  do not support  dynamic volume provisioning. for example when you will ssh to any of your nodes you will find that this folder is  readonly file system.
/mnt $ mkdir something mkdir: cannot create directory something: read-only file system

you just could change in your pv yaml
apiversion: v1
kind: persistentvolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageclassname: manual
  capacity:
    storage: 10gi
  accessmodes:
    - readwritemany
  hostpath:
    path: &quot;/var/lib/data&quot;

notice changes in accessmode and path.
also in your pvc definition change:
  accessmodes:
    - readwritemany

remember that  must delete old pv and pvc (if they wont vanish you will probably need redeploy nginx pod also) as in some resources you cannot change values after creation.
take a look: read-only-fs.
read: gke-dynamics-provisioning.
"
66740340,kubernetes deployment podname setting,"apiversion: apps/v1
kind: deployment
metadata:
  name: test-deployment
  labels:
    app: test
spec:
  replicas: 1
  selector:
    matchlabels:
      app: test
  template:
    metadata:
      name: test
      labels:
        app: test
    spec:
      containers:
        - name: server
          image: test_ml_server:2.3
          ports:
            - containerport: 8080
          volumemounts:
            - name: hostpath-vol-testserver
              mountpath: /app/test/api
#          env:
#            - name: pod_name
#              valuefrom:
#                fieldref:
#                  fieldpath: template.metadata.name
        - name: testdb
          image: test_db:1.4
          ports:
            - name: testdb
              containerport: 1433
          volumemounts:
            - name: hostpath-vol-testdb
              mountpath: /var/opt/mssql/data
#          env:
#            - name: pod_name
#              valuefrom:
#                fieldref:
#                  fieldpath: template.metadata.name
      volumes:
        - name: hostpath-vol-testserver
          hostpath:
            path: /usr/testhostpath/testserver
        - name: hostpath-vol-testdb
          hostpath:
            path: /usr/testhostpath/testdb

i want to set the name of the pod because it communicates internally based on the name of the pod
but when a pod is created, it cannot be used because the variable name is appended to the end.
how can i set the pod name?
",<kubernetes><rancher><kubernetes-deployment>,66740371,1,"it's better if you use, statefulset instead of deployment. statefulset's pod name will be like &lt;statefulsetname-0&gt;,&lt;statefulsetname-1&gt;... and you will need a clusterip service. with which you can bound your pods. see the doc for more details. ref
apiversion: v1
kind: service
metadata:
  name: test-svc
  labels:
    app: test
spec:
  ports:
  - port: 8080
    name: web
  clusterip: none
  selector:
    app: test

apiversion: apps/v1
kind: statefulset
metadata:
  name: test-statefulset
  labels:
    app: test
spec:
  replicas: 1
  servicename: test-svc
  selector:
    matchlabels:
      app: test
  template:
    metadata:
      name: test
      labels:
        app: test
    spec:
      containers:
        - name: server
          image: test_ml_server:2.3
          ports:
            - containerport: 8080
          volumemounts:
            - name: hostpath-vol-testserver
              mountpath: /app/test/api
        - name: testdb
          image: test_db:1.4
          ports:
            - name: testdb
              containerport: 1433
          volumemounts:
            - name: hostpath-vol-testdb
              mountpath: /var/opt/mssql/data
      volumes:
        - name: hostpath-vol-testserver
          hostpath:
            path: /usr/testhostpath/testserver
        - name: hostpath-vol-testdb
          hostpath:
            path: /usr/testhostpath/testdb

here, the pod name will be like this test-statefulset-0.
"
66741179,is there any method for detecting yaml issue for aws deployment by kubectl?,"i think that there are lots of devops engineer realized this issue. because i am from a software background. explanations for syntax not enough for me. below yaml is working for the azure environment but not working for eks and aws.
error:
 error validating data: validationerror(deployment.spec): unknown field &quot;spec&quot; in io.k8s.api.apps.v1.deploymentspec; if you choose to ignore these errors, turn validation off with --validate=false


my deployment yaml :
apiversion: apps/v1
kind: deployment
metadata:
  name:  my-flask
spec:
  selector:
    matchlabels:
      app: my-flask
  replicas: 2
  template:
    metadata:
      labels:
        app: my-flask
  spec:
   containers:
     - name: my-flask
       image: yusufkaratoprak/awsflaskeks:latest
       ports:
         - containerport: 5000
       


",<amazon-web-services><kubernetes><amazon-ec2><kubernetes-ingress><amazon-eks>,66741351,1,"there is some indentation problem with your yamls.
the field secondspec is under the template.
will also encourage you to see the official docs of kubernetes_deployment
apiversion: apps/v1
kind: deployment
metadata:
  name:  my-flask
spec:
  selector:
    matchlabels:
      app: my-flask
  replicas: 2
  template:
    metadata:
      labels:
        app: my-flask
    spec:
    containers:
        - name: my-flask
          image: yusufkaratoprak/awsflaskeks:latest
          ports:
            - containerport: 5000

"
74718284,how to execute a binary and use sleep pod after some time together in kubernetes,"i have a use case that my  &quot;./main&quot; binary should run inside the pod and stop after some time (90 seconds) before launching a new pod by the cronjob object.
but i am not confused about how to add both sleep and run my binary in the background together. please suggest a good approach to this and excuse me for any wrong syntax.
dockerfile
from golang:alpine
workdir /app
copy main /app
run  apk update &amp;&amp; apk add bash
cmd  [&quot;./main &amp;&quot;]

---
cronjob.yaml

apiversion: batch/v1
kind: cronjob
metadata:
  name: cron
  namespace: test-cron
spec:
  schedule: &quot;*/2 * * * *&quot;
  concurrencypolicy: replace
  successfuljobshistorylimit: 0
  failedjobshistorylimit: 0
  jobtemplate:
    spec:
      template:
        spec:
          restartpolicy: onfailure
          volumes:
          - name: log
            hostpath:
                path: /data/log/test-cron/
          containers:
            - name: test-cron
              image:  test-kafka-0.5
              command: [&quot;sleep&quot;, &quot;90&quot;] // by adding this, the sleep command is working but my binary is not running inside my container.

",<docker><kubernetes><dockerfile><kubernetes-cronjob>,74734764,1,"kubernetes has built-in support for killing a pod after a deadline; you do not need to try to implement this manually.
in your dockerfile, set up your image to run the program normally.  do not try to include any sort of &quot;sleep&quot; or &quot;kill&quot; option, and do not try to run the program in the background.
cmd [&quot;./main&quot;]

in the job spec, you need to set an activedeadlineseconds: field.
apiversion: batch/v1
kind: cronjob
spec:
  jobtemplate:
    spec:
      activedeadlineseconds: 90  # &lt;-- add
      template:
        spec:
          containers:
            - name: test-cron
              image:  test-kafka-0.5
              # do not override `command:`

an identical option exists at the pod level.  for your use case this doesn't especially matter, but if the job launches multiple pods then there's a difference of whether each individual pod has the deadline or whether the job as a whole does.
it looks from the question like you're trying to run this job every two minutes and not have two concurrent copies of it.  assuming the pod is created and starts promptly,this should accomplish that.  if you had a reason to believe the job would run faster the second time and just want to restart it, the cronjob might not be the setup you're after.
"
74769252,pass an entire yaml from values.yaml to templates in helm,"i am trying to pass entire set of yamls from values.yaml in helm to the templates, so that whatever yaml inputs i pass in the values.yaml section goes in the templates yaml as it is :
for example :
values.yaml
...
...
metallbconfig: |-
  apiversion: metallb.io/v1beta2
  kind: bgppeer
  metadata:
    creationtimestamp: null
    name: peer1
    namespace: metallb-system
  spec:
    holdtime: 3s
    keepalivetime: 0s
    myasn: 64026
    passwordsecret: {}
    peerasn: 65227
    peeraddress: 10.252.254.194
  status: {}

templates/resources.yaml :
{{ toyaml .values.metallbconfig }}

essentially what i want to achieve is whole bgppeer section to be present in the resources.yaml when i deploy the chart.
currently i am getting this error :
# helm template metallbcnf . --output-dir outputs --debug
...
...
error: yaml parse error on metallb/templates/resources.yaml: error unmarshaling json: while decoding json: json: cannot unmarshal string into go value of type releaseutil.simplehead
helm.go:84: [debug] error unmarshaling json: while decoding json: json: cannot unmarshal string into go value of type releaseutil.simplehead

kindly help me resolve the same.
",<kubernetes><kubernetes-helm>,74769409,1,"if you want to embed the yaml entirely, you don't need the |-
for example, i have this in values.yaml
...
probes:
  livenessprobe:
    httpget:
      path: /ping
      port: 80
    initialdelayseconds: 15
    periodseconds: 60
    successthreshold: 1
    timeoutseconds: 5
    failurethreshold: 3
  readinessprobe:
    httpget:
      path: /ping
      port: 80
    initialdelayseconds: 15
    periodseconds: 60
    successthreshold: 1
    timeoutseconds: 5
    failurethreshold: 3
...

then use this in my helm deployment:
apiversion: apps/v1
kind: deployment
...
spec:
  ...
  template:
    ...
    spec:
      ...
      containers:
        - name: {{ .chart.name }}
          image: &quot;{{ .values.image.repository }}:{{ .values.image.tag}}&quot;
          imagepullpolicy: {{ .values.image.pullpolicy }}

          {{- toyaml .values.probes | nindent 10 }}
           ...
      ...

you'll notice i need to be explicit about the indenting using nindent otherwise helm just pastes in the yaml as in the values.yaml which breaks the parsing
"
74749097,google cloud manifest unknown during deploy to kubernetes cluster,"i'm trying to deploy my code placed in bitbucket repo to kubernetes clusters available in google cloud. i've created a trigger which react when i push new changes to bitbucket repo.
i mean:

based on guidance: https://cloud.google.com/build/docs/deploying-builds/deploy-gke#automating_deployments i've created: cloudbuild.yaml file with content:
steps:
  - name: &quot;gcr.io/cloud-builders/gke-deploy&quot;
    args:
    - run
    - --filename=${_tech_radar_kub_config_location}
    - --image=gcr.io/java-kubernetes-clusters-test/bitbucket.org/intivetechradar/technology-radar-be:${short_sha}
    - --location=${_tech_radar_gke_location}
    - --cluster=${_tech_radar_cluster_name}
    env:
      - 'short_sha=$short_sha'
options:
  logging: cloud_logging_only

in my repo i additionally i have additionally deployment.yaml file with kuberneties config:
apiversion: &quot;apps/v1&quot;
kind: &quot;deployment&quot;
metadata:
  name: &quot;java-kubernetes-clusters-test&quot;
  namespace: &quot;default&quot;
  labels:
    app: &quot;java-kubernetes-clusters-test&quot;
spec:
  replicas: 3
  selector:
    matchlabels:
      app: &quot;java-kubernetes-clusters-test&quot;
  template:
    metadata:
      labels:
        app: &quot;java-kubernetes-clusters-test&quot;
    spec:
      containers:
        - name: &quot;technology-radar-be-1&quot;
          image: &quot;gcr.io/java-kubernetes-clusters-test/bitbucket.org/intivetechradar/technology-radar-be:short_sha&quot;

my build is starting but got stack on issue manifest unknown like below:

how can i solve this problem? can i somehow let kubernetes engine know that such manifest exits in additional pre step in cloudbuild.yaml file?
i would be grateful for help.
thanks!
update:
i'm trying to use cloudbuild.yaml like below:
substitutions:
  _cloudsdk_compute_zone: us-central1-c  # default value
  _cloudsdk_container_cluster: kubernetes-cluster-test      # default value

steps:
  - id: 'build test image'
    name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-t', 'gcr.io/${_tech_radar_project_id}/technology-radar-be/master:$short_sha', '.']
  - id: 'push test core image'
    name: 'gcr.io/cloud-builders/docker'
    args: ['push', 'gcr.io/${_tech_radar_project_id}/technology-radar-be/master:$short_sha:$short_sha']
  - id: 'set test core image in yamls'
    name: 'ubuntu'
    args: ['bash','-c','sed -i &quot;s,${_tech_container_image},gcr.io/$project_id/technology-radar-be/$master:$short_sha,&quot; deployment.yaml']
  - name: 'gcr.io/cloud-builders/kubectl'
    args: ['apply', '-f', 'deployment.yaml']
    env:
      - 'cloudsdk_compute_zone=${_cloudsdk_compute_zone}'
      - 'cloudsdk_container_cluster=${_cloudsdk_container_cluster}'
options:
  logging: cloud_logging_only

the last error message is:

is there something wrong with my dockerfile definition? :
from maven:3.8.2-jdk-11 as builder
arg jar_file=target/docker-demo.jar
copy ${jar_file} app.jar
entrypoint [&quot;java&quot;, &quot;-jar&quot;, &quot;/app.jar&quot;]

pom.xml:
&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/pom/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/xmlschema-instance&quot;
         xsi:schemalocation=&quot;http://maven.apache.org/pom/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelversion&gt;4.0.0&lt;/modelversion&gt;
    &lt;parent&gt;
        &lt;groupid&gt;org.springframework.boot&lt;/groupid&gt;
        &lt;artifactid&gt;spring-boot-starter-parent&lt;/artifactid&gt;
        &lt;version&gt;2.7.6&lt;/version&gt;
        &lt;relativepath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;groupid&gt;com.example&lt;/groupid&gt;
    &lt;artifactid&gt;docker-kubernetes&lt;/artifactid&gt;
    &lt;version&gt;0.0.1&lt;/version&gt;
    &lt;name&gt;docker-kubernetes&lt;/name&gt;
    &lt;description&gt;demo project for spring boot&lt;/description&gt;
    &lt;properties&gt;
        &lt;java.version&gt;11&lt;/java.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupid&gt;org.springframework.boot&lt;/groupid&gt;
            &lt;artifactid&gt;spring-boot-starter-actuator&lt;/artifactid&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupid&gt;org.springframework.boot&lt;/groupid&gt;
            &lt;artifactid&gt;spring-boot-starter-web&lt;/artifactid&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupid&gt;org.springframework.boot&lt;/groupid&gt;
            &lt;artifactid&gt;spring-boot-devtools&lt;/artifactid&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupid&gt;org.springframework.boot&lt;/groupid&gt;
            &lt;artifactid&gt;spring-boot-starter-test&lt;/artifactid&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupid&gt;org.springframework.boot&lt;/groupid&gt;
                &lt;artifactid&gt;spring-boot-maven-plugin&lt;/artifactid&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
        &lt;finalname&gt;docker-demo&lt;/finalname&gt;
    &lt;/build&gt;
&lt;/project&gt;

screenshot from project tree:

local deployment works:

",<kubernetes><google-kubernetes-engine><google-cloud-build><kubernetes-deployment><cloudbuild.yaml>,74750822,1,"not sure but if you look at the error it's trying to fetch the
gcr.io/java-kubernetes-clusters-test/bitbucket.org/intivetechradar/technology-radar-be/short_sha not gcr.io/java-kubernetes-clusters-test/bitbucket.org/intivetechradar/technology-radar-be:short_sha it's not using : before the tag are you building images ?
update :
just tested this file with cloud build and gke working for me : https://github.com/harsh4870/basic-ci-cd-cloudbuild/blob/main/cloudbuild-gke.yaml
however you can use the cloudbuild.yaml
substitutions:
    _cloudsdk_compute_zone: us-central1-c  # default value
    _cloudsdk_container_cluster: standard-cluster-1      # default value
steps:
- id: 'build test image'
  name: 'gcr.io/cloud-builders/docker'
  args: ['build', '-t', 'gcr.io/$project_id/$repo_name/$branch_name:$short_sha', '.']
- id: 'push test core image'
  name: 'gcr.io/cloud-builders/docker'
  args: ['push', 'gcr.io/$project_id/$repo_name/$branch_name:$short_sha']
- id: 'set test core image in yamls'
  name: 'ubuntu'
  args: ['bash','-c','sed -i &quot;s,test_image_name,gcr.io/$project_id/$repo_name/$branch_name:$short_sha,&quot; deployment.yaml']
- name: 'gcr.io/cloud-builders/kubectl'
  args: ['apply', '-f', 'deployment.yaml']
  env:
  - 'cloudsdk_compute_zone=${_cloudsdk_compute_zone}'
  - 'cloudsdk_container_cluster=${_cloudsdk_container_cluster}'

deployment.yaml
apiversion: &quot;apps/v1&quot;
kind: &quot;deployment&quot;
metadata:
  name: &quot;java-kubernetes-clusters-test&quot;
  namespace: &quot;default&quot;
  labels:
    app: &quot;java-kubernetes-clusters-test&quot;
spec:
  replicas: 3
  selector:
    matchlabels:
      app: &quot;java-kubernetes-clusters-test&quot;
  template:
    metadata:
      labels:
        app: &quot;java-kubernetes-clusters-test&quot;
    spec:
      containers:
        - name: &quot;technology-radar-be-1&quot;
          image: test_image_name

"
74706037,kubernetes: prevent pods from communicating with the node-ips,"i have a kubernetes cluster running behind a nat. now i want to forbid the pods to communicate with the network in which my kubernetes nodes / servers are. the network has the cird: 10.12.12.0/27.
i've already tried the kubernetes networkpolicy, but i haven't figured out how to prohibit communication with certain ips. instead, i have limited the konnunikation to these ip's.
here is my previous networkpolicy:
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: block-net-kubernetes
  namespace: default
spec:
  podselector:
    matchlabels:
      namespace: default
  policytypes:
    - egress
  egress:
    - to:
        - ipblock:
            cidr: 10.12.12.0/27

many thanks in advance!
kind regards niclas
",<kubernetes><firewall><k3s><kubernetes-networkpolicy>,74706225,1,"you can use the expect block to filter out some ips.  using that, the below example is allowing all egress but blocking traffic to 10.12.12.0/27
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: block-net-kubernetes
  namespace: default
spec:
  podselector:
    matchlabels:
      namespace: default
  policytypes:
    - egress
  egress:
    - to:
        - ipblock:
            cidr: 0.0.0.0/0
            except:
              - 10.12.12.0/27

"
74492858,kubernetes cronjob with a sidecar container,"i am having some issues with a kubernetes cronjob running two containers inside of a gke cluster.
one of the two containers is actually executing the job that must be done by the cronjob.
this works perfectly fine. it is started when it is supposed to be started, does the job and then terminates. all fine up until this point.
what seems to be causing some issues is the second container, which is a sidecar container used to access a database instance. this won't terminate and seems to be leading to the problem that the cronjob itself won't terminate. which is an issue, since i see an accumulation of running job instances over time.
is there a way to configure a kubernetes batch cronjob to be terminating when one of the container is successfully exe
apiversion: batch/v1
kind: cronjob
metadata:
  name: chron-job-with-a-sidecar
  namespace: my-namespace
spec:
#             minute (0 - 59)
#              hour (0 - 23)
#                day of the month (1 - 31)
#                 month (1 - 12)
#                  day of the week (0 - 6) (sunday to saturday;
#                                                    7 is also sunday on some systems)
#                                                    or sun, mon, tue, wed, thu, fri, sat
#                 
  schedule: &quot;0 8 * * *&quot; # -&gt; every day at 8am
  jobtemplate:
    metadata:
      labels:
        app: my-label
    spec:
      template:
          containers:
          # --- job container -----------------------------------------------
          - image: my-job-image:latest
            imagepullpolicy: always
            name: my-job
            command:
            - /bin/sh
            - -c
            - /some-script.sh; exit 0;
          # --- sidecar container ----------------------------------------------
          - command:
            - &quot;/cloud_sql_proxy&quot;
            - &quot;-instances=my-instance:antarctica-south-3:user=tcp:1234&quot;
            # ... some other settings ...
            image: gcr.io/cloudsql-docker/gce-proxy:1.30.0
            imagepullpolicy: always
            name: cloudsql-proxy
            # ... some other values ...

",<kubernetes><scheduled-tasks><google-kubernetes-engine><sidecar>,78973678,1,"if you are running kubernetes 1.29 the &quot;sidecar&quot; containers feature is now included by default.
as per https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/#sidecar-example the sidecar containers should go into the initcontainers space. the key variable restartpolicy would seem to be what differentiates this &quot;init&quot; container from one that the main container needs to wait for.
example flagrantly copied and pasted from above link:
apiversion: apps/v1
kind: deployment
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  replicas: 1
  selector:
    matchlabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp
          image: alpine:latest
          command: ['sh', '-c', 'while true; do echo &quot;logging&quot; &gt;&gt; /opt/logs.txt; sleep 1; done']
          volumemounts:
            - name: data
              mountpath: /opt
      initcontainers:
        - name: logshipper
          image: alpine:latest
          restartpolicy: always
          command: ['sh', '-c', 'tail -f /opt/logs.txt']
          volumemounts:
            - name: data
              mountpath: /opt
      volumes:
        - name: data
          emptydir: {}

"
67068889,expose spark-ui with zeppelin on kubernetes,"first of all i'm pretty new on all this (kubernetes, ingress, spark/zeppelin ...) so my apologies if this is obvious. i tried searching here, documentations etc but couldn't find anything.
i am trying to make the spark interpreter ui accessible from my zeppelin notebook running on kubernetes.
following what i understood from here: http://zeppelin.apache.org/docs/0.9.0-preview1/quickstart/kubernetes.html, my ingress yaml looks something like this:
ingress.yaml
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-zeppelin-server-http
http
spec:
  rules:
  - host: my-zeppelin.my-domain
    http:
      paths:
      - backend:
          servicename: zeppelin-server
          serviceport: 8080
  - host: '*.my-zeppelin.my-domain'
    http:
      paths:
      - backend:
          servicename: spark-guovyx
          serviceport: 4040
status:
  loadbalancer: {}

my issue here is that i need to rely on the service-name (in this case spark-guovyx) being set to the interpreter pod name in order to have the ui show up. however since this name is bound to change / have different ones (i.e. i have one interpreter per user + interpreters are frequently restarted) obviously i cannot rely on setting it manually. my initial thought was to use some kind of wildcard naming for the servicename but turns out ingress/kubernetes don't support that. any ideas please ?
thanks.
",<apache-spark><kubernetes><kubernetes-ingress><apache-zeppelin>,67097266,1,"you can create a new service and leverage the interpretersettingname label of the spark master pod. when zeppelin creates a master spark pod it adds this label and its value is spark. i am not sure if it will work for more than one pods in a per user per interpreter setting. below is the code for service, do let me know how it behaves for per user per interpreter.
kind: service
apiversion: v1
metadata:
  name: sparkui
spec:
  ports:
    - name: spark-ui
      protocol: tcp
      port: 4040
      targetport: 4040
  selector:
    interpretersettingname: spark
  clusterip: none
  type: clusterip

and then you can have your ingress as:
apiversion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-zeppelin-server-http
http
spec:
  rules:
  - host: my-zeppelin.my-domain
    http:
      paths:
      - backend:
          servicename: zeppelin-server
          serviceport: 8080
  - host: '*.my-zeppelin.my-domain'
    http:
      paths:
      - backend:
          servicename: sparkui
          serviceport: 4040
status:
  loadbalancer: {}

also do checkout this repo https://github.com/cuebook/cuelake, it is still in early stage of development but would love to hear your feedback.
"
67246746,tls nginx ingress in aws eks cluster results in 404 not found,"i am trying to use kubernetes ingress nginx controller and running a simple nginx server in aws eks.
browser (https) --&gt; route 53 (dns) --&gt; clb --&gt; nginx ingress (terminate tls) --&gt; service --&gt; pod
but i am receiving 404 error in browser (url used: https://example.com/my-nginx):
&lt;html&gt;
&lt;head&gt;&lt;title&gt;404 not found&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;404 not found&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx/1.19.10&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;

and in ingress logs (kubectl logs -n nginx-ingress nginx-ingress-nginx-controller-6db6f85bc4-mfpwx), i can see below:
192.168.134.181 - - [24/apr/2021:19:02:01 +0000] &quot;get /my-nginx http/2.0&quot; 404 154 &quot;-&quot; &quot;mozilla/5.0 (windows nt 10.0; win64; x64; rv:88.0) gecko/20100101 firefox/88.0&quot; 219 0.002 [eshop-dev-my-nginx-9443] [] 192.168.168.105:80 154 0.000 404 42fbe692a032bb40bf193954526369cd
here is my deployment yaml:
apiversion: apps/v1
kind: deployment
metadata:
  name: my-nginx
  namespace: eshop-dev
spec:
  selector:
    matchlabels:
      run: my-nginx
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx
        ports:
        - containerport: 80

service yaml:
apiversion: v1
kind: service
metadata:
  namespace: eshop-dev
  name: my-nginx
spec:
  selector:
    run: my-nginx
  ports:
    - name: server
      port: 9443
      targetport: 80
      protocol: tcp

and ingress yaml:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: test-ingress
  namespace: eshop-dev
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: /my-nginx
        pathtype: implementationspecific
        backend:
          service:
            name: my-nginx
            port:
                number: 9443
  tls:
  - hosts:
    - example.com
    secretname: externaluicerts

i have verified that service returns the desired output, when used with port forwarding:
kubectl -n eshop-dev port-forward service/my-nginx 9443:9443

i'm not sure if the ingress is incorrectly configured or if it is another problem.thanks in advance for the help!
nginx-port-forward
here is the output of kubectl get ingress -n eshop-dev test-ingress -o yaml
kubectl get ingress -n eshop-dev test-ingress -o yaml
warning: extensions/v1beta1 ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 ingress
apiversion: extensions/v1beta1
kind: ingress
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {&quot;apiversion&quot;:&quot;networking.k8s.io/v1&quot;,&quot;kind&quot;:&quot;ingress&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubernetes.io/ingress.class&quot;:&quot;nginx&quot;},&quot;name&quot;:&quot;test-ingress&quot;,&quot;namespace&quot;:&quot;eshop-dev&quot;},&quot;spec&quot;:{&quot;rules&quot;:[{&quot;host&quot;:&quot;example.com&quot;,&quot;http&quot;:{&quot;paths&quot;:[{&quot;backend&quot;:{&quot;service&quot;:{&quot;name&quot;:&quot;my-nginx&quot;,&quot;port&quot;:{&quot;number&quot;:9443}}},&quot;path&quot;:&quot;/my-nginx&quot;,&quot;pathtype&quot;:&quot;implementationspecific&quot;}]}}],&quot;tls&quot;:[{&quot;hosts&quot;:[&quot;example.com&quot;],&quot;secretname&quot;:&quot;externaluicerts&quot;}]}}
    kubernetes.io/ingress.class: nginx
  creationtimestamp: &quot;2021-04-24t13:16:21z&quot;
  generation: 13
  managedfields:
  - apiversion: networking.k8s.io/v1beta1
    fieldstype: fieldsv1
    fieldsv1:
      f:status:
        f:loadbalancer:
          f:ingress: {}
    manager: nginx-ingress-controller
    operation: update
    time: &quot;2021-04-24t13:16:40z&quot;
  - apiversion: extensions/v1beta1
    fieldstype: fieldsv1
    fieldsv1:
      f:metadata:
        f:annotations: {}
    manager: kubectl-client-side-apply
    operation: update
    time: &quot;2021-04-24t13:18:36z&quot;
  - apiversion: networking.k8s.io/v1
    fieldstype: fieldsv1
    fieldsv1:
      f:metadata:
        f:annotations:
          f:kubectl.kubernetes.io/last-applied-configuration: {}
          f:kubernetes.io/ingress.class: {}
      f:spec:
        f:rules: {}
        f:tls: {}
    manager: kubectl-client-side-apply
    operation: update
    time: &quot;2021-04-24t16:33:47z&quot;
  name: test-ingress
  namespace: eshop-dev
  resourceversion: &quot;7555944&quot;
  selflink: /apis/extensions/v1beta1/namespaces/eshop-dev/ingresses/test-ingress
  uid: a7694655-20c6-48c7-8adc-cf3a53cf2ffe
spec:
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          servicename: my-nginx
          serviceport: 9443
        path: /my-nginx
        pathtype: implementationspecific
  tls:
  - hosts:
    - example.com
    secretname: externaluicerts
status:
  loadbalancer:
    ingress:
    - hostname: xxxxxxxxxxxxxxxxdc75878b2-433872486.eu-west-1.elb.amazonaws.com

",<kubernetes><https><kubernetes-ingress><amazon-eks><nginx-ingress>,67250785,1,"from the image you posted of the nginx-port-forward, i see you went on localhost:9443 directly, which means the nginx server you are trying to access serve its content under /
but in the ingress definition, you define that the service will be served with path: /my-nginx. this could be the problem, as you are requesting https://example.com/my-nginx which will basically go to my-nginx:9443/my-nginx and, depending on the pod behind this service, it could return a 404 if there's nothing at that path.
to test if the problem is what i specified above, you have a few options:

easiest one, remove path: /my-nginx an, instead, go with path: /. you could also specify pathtype: prefix which means that everything matching the subpath specified will be served by the service.
add a rewrite target, which is necessary if you want to serve a service at a different path from the one expected by the application.

add an annotation similar to the following:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: test-ingress
  namespace: eshop-dev
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    # this will rewrite request under / + second capture group
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  rules:
  - host: example.com
    http:
      paths:
        # this will serve all paths under /my-nginx and capture groups for regex annotations
      - path: /my-nginx(/|$)(.*)
        pathtype: implementationspecific
        backend:
          service:
            name: my-nginx
            port:
              number: 9443
  tls:
  - hosts:
    - example.com
    secretname: externaluicerts


configure your application to know that it will be served under the path you desire. this is often the better approach, as frontend applications should almost always be served under the path that they expect to be.

from the info you posted, i think this is the problem an once fixed, your setup should work.

if you are curious about rewrite targets or how paths work in an ingress, here is some documentation:
rewrites ( https://kubernetes.github.io/ingress-nginx/examples/rewrite/#rewrite )
path types ( https://kubernetes.io/docs/concepts/services-networking/ingress/#path-types )

update
about why configuring the application to directly serve its content at the path specified in the ingress (basically to know at which path it will served) is the best solution:
let's say you serve a complex application in your pod which will serve its content under /. the main page will try to load several other resources like css, js code and so on, everything from the root directory. basically, if i open /, the app will load also:
/example.js
/my-beautiful.css

now, if i serve this app behind an ingress at another path, let's say under /test/ with a rewrite target, the main page will work, because:
/test/ --&gt; /  # this is my rewrite rule

but then, the page will request /example.js, and the rewrite works in one direction only, so the browser will request a resource which will go in 404, because the request should have been /test/example.js (as that would rewrite to remove the /test part of the path)
so, with frontend applications, rewrite targets may not be enough, mostly if the applications request resources with absolute paths. with just rest api or single requests instead, rewrites usually works great.
"
67432005,kubernetes - how do i extract a secret tar file?,"i have mounted two tar files as secrets. i would like to mount them to my container and then unpack the contents. the commands that created the secrets are as follows:
kubectl create secret generic orderer-genesis-block --from-file=./channel-artifacts/genesis.block

kubectl create secret generic crypto-config --from-file=crypto-config.tar

kubectl create secret generic channel-artifacts --from-file=channel-artifacts.tar

the following is what i kubectl apply:
apiversion: apps/v1
kind: deployment
metadata:
  name: fabric-orderer-01
spec:
  selector:
    matchlabels:
      app: fabric-orderer-01
  replicas: 1
  template:
    metadata:
      labels:
        app: fabric-orderer-01
    spec:
      initcontainers:
      - name: init-channel-artifacts
        image: busybox
        volumemounts:
        - name: channel-artifacts
          mountpath: /hlf/channel-artifacts
        command: ['sh', '-c', 'tar -xf /hlf/channel-artifacts/channel-artifacts.tar']
      containers:
      - name: fabric-orderer-01
        image: hyperledger/fabric-orderer:1.4.9
        env:
        - name: orderer_cfg_path
          value: /hlf/
        - name: configtx_orderer_addresses
          value: &quot;orderer.example.com:7050&quot;
        - name: orderer_general_listenaddress
          value: 0.0.0.0
        - name: orderer_general_listenport
          value: &quot;7050&quot;
        - name: orderer_general_loglevel
          value: debug
        - name: orderer_general_localmspid
          value: orderermsp
        - name: orderer_general_genesismethod
          value: file
        - name: orderer_general_genesisfile
          value: /hlf/genesis.block
        imagepullpolicy: always
        ports:
        - containerport: 8080
        volumemounts:
        - name: fabricfiles-01
          mountpath: /fabric
        - name: orderer-genesis-block
          mountpath: /hlf/
          readonly: true
        - name: crypto-config
          mountpath: /hlf/crypto-config
          readonly: true
        - name: channel-artifacts
          mountpath: /hlf/channel-artifacts
          readonly: true
      volumes:
      - name: orderer-genesis-block
        secret:
          secretname: orderer-genesis-block
      - name: crypto-config
        secret:
          secretname: crypto-config
      - name: channel-artifacts
        secret:
          secretname: channel-artifacts
      - name: fabricfiles-01
        persistentvolumeclaim:
          claimname: fabric-pvc-01

my deployment succeeds, but when i bash into my pod, i don't see my tar files being extracted. i only see my tar files /hlf/channel-artifacts/channel-artifacts.tar and /hlf/crypto-config/crypto-config.tar. how should i go about extracting their contents?
",<kubernetes><kubectl><kubernetes-secrets>,67484560,1,"when you create an initcontainer and execute this command:
command: ['sh', '-c', 'tar -xvf /hlf/channel-artifacts/channel-artifacts.tar']
it runs in default for this container path.
i checked this by adding pwd and ls -l commands.
whole line is:
command: ['sh', '-c', 'tar -xvf /hlf/channel-artifacts/channel-artifacts.tar ; pwd ;  ls -l']
from an initcontainer you can get logs by:
kubectl logs fabric-orderer-01-xxxxxx -c init-channel-artifacts
output was:
channel-artifacts.txt # first line for -v option so tar was untared indeed
/ # working directory
total 44
drwxr-xr-x    2 root     root         12288 may  3 21:57 bin
-rw-rw-r--    1 1001     1002            32 may 10 14:15 channel-artifacts.txt # file which was in tar
drwxr-xr-x    5 root     root           360 may 11 08:41 dev
drwxr-xr-x    1 root     root          4096 may 11 08:41 etc
drwxr-xr-x    4 root     root          4096 may 11 08:41 hlf
drwxr-xr-x    2 nobody   nobody        4096 may  3 21:57 home
dr-xr-xr-x  225 root     root             0 may 11 08:41 proc
drwx------    2 root     root          4096 may  3 21:57 root
dr-xr-xr-x   13 root     root             0 may 11 08:41 sys
drwxrwxrwt    2 root     root          4096 may  3 21:57 tmp
drwxr-xr-x    3 root     root          4096 may  3 21:57 usr
drwxr-xr-x    1 root     root          4096 may 11 08:41 var

as you can see your file is stored in / path of the container which means when this container is terminated, its filesystem is terminated as well and your file is gone.
once we know what happened, it's time to workaround it.
first and impotant thing is secrets are read-only and should be used in prepared form, you can't write file to secret like you wanted to do in your example.
instead one of the options is you can untar your secrets to a persistent volume:
command: ['sh', '-c', 'tar -xvf /hlf/channel-artifacts/channel-artifacts.tar -c /hlf/fabric']
and then use poststart hook for the main container where you can e.g. copy your files to desired locations or create simlinks and you won't need to mount your secrets to the main container.
simple example of poststart hook (reference):
apiversion: v1
kind: pod
metadata:
  name: lifecycle-demo
spec:
  containers:
  - name: lifecycle-demo-container
    image: nginx
    lifecycle:
      poststart:
        exec:
          command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo hello from the poststart handler &gt; /usr/share/message&quot;]

small notice is

kubernetes sends the poststart event immediately after the container
is created. there is no guarantee, however, that the poststart handler
is called before the container's entrypoint is called.

to workaround it you can add sleep 5 in your main container before entrypoint. here's an example of a beginning of container section with nginx image (for your image it'll be different):
containers:
      - name: main-container
        image: nginx
        command: [&quot;bash&quot;, &quot;-c&quot;, 'sleep 5 ; echo &quot;daemon off;&quot; &gt;&gt; /etc/nginx/nginx.conf ; nginx']

this will fix your issue. also you can use this approach for untar your files and you won't even need an initcontainer
it's not clear why you want to use tar for this purpose as you can store small files in secrets or configmaps and mount them directly using subpath where they are needed without additional steps (you can read about it and find an example here)
to use secrets securely, you should consider e.g. hashicorp vault (vault with kubernetes)
"
67411996,terraform how to pass a file as value for helm_release to create config map,"i have a helm chart that is creating a config map for which i am passing content as a value from terraform using helm_release.
values.yml: default is empty
sql_queries_file: &quot;&quot;

helm template for configmap:
apiversion: v1
kind: configmap
metadata:
  name: sql-queries
data:
{{ .values.sql_queries_file }}

terraform file:
resource &quot;helm_release&quot; &quot;example&quot; {
............
..................
  set {
    name  = &quot;sql_queries_file&quot;
    value = file(./sql_queries.sql)
  }
}

i have a sql_queris.sql fine inside terraform folder with sample data below.
-- from http://docs.confluent.io/current/ksql/docs/tutorials/basics-docker.html#create-a-stream-and-table

-- create a stream pageviews_original from the kafka topic pageviews, specifying the value_format of delimited
create stream pageviews_original (viewtime bigint, userid varchar, pageid varchar) with (kafka_topic='pageviews', value_format='delimited');


error:
failed parsing key sql_queries_file with value &lt;entire content here&gt;

is this the right way? or is there a better way?
",<kubernetes><terraform><kubernetes-helm>,67420285,1,"i would use filebase64 to get the file with terraform to avoid templating issues. you can unmarshal it in helm like this: {{ b64dec .values.sql_queries_file }}. by the way you should use data field in configmaps like this:
apiversion: v1
kind: configmap
metadata:
  name: sql-queries
data: 
  sql_queries.sql: |-
    {{ .values.sql_queries_file | nindent 4 }}
#   {{ b64dec .values.sql_queries_file | nindent 4 }} if you want to unmarshal

edit: fixed typo in answer.
"
74074955,pod not able to read configmap despite role and rolebinding being in place,"i would like permit a kubernetes pod in namespace my-namespace to access configmap/config in the same namespace. for this purpose i have defined the following role and rolebinding:
apiversion: v1
kind: list
items:
- kind: role
  apiversion: rbac.authorization.k8s.io/v1
  metadata:
    name: config
    namespace: my-namespace
  rules:
  - apigroups: [&quot;&quot;]
    resources: [&quot;configmaps&quot;]
    resourcenames: [&quot;config&quot;]
    verbs: [&quot;get&quot;] 
- kind: rolebinding
  apiversion: rbac.authorization.k8s.io/v1
  metadata:
    name: config
    namespace: my-namespace
  subjects:
  - kind: serviceaccount
    name: default
    namespace: my-namespace
  roleref:
    kind: role
    name: config
    apigroup: rbac.authorization.k8s.io

yet still, the pod runs into the following error:
configmaps \&quot;config\&quot; is forbidden: user \&quot;system:serviceaccount:my-namespace:default\&quot; 
cannot get resource \&quot;configmaps\&quot; in api group \&quot;\&quot; in the namespace \&quot;my-namespace\&quot;

what am i missing? i guess it must be a simple thing, which a second pair of eyes may spot immediately.
update here is a relevant fragment of my client code, which uses go-client:
cfg, err := rest.inclusterconfig()
if err != nil {
        logger.fatalf(&quot;cannot obtain kubernetes config: %v&quot;, err)
}
k8sclient, err := k8s.newforconfig(cfg)
if err != nil {
        logger.fatalf(&quot;cannot create clientset&quot;)
}       
configmapclient := k8sclient.corev1().configmaps(namespace)

configmap, err := configmapclient.get(ctx, &quot;config&quot;, metav1.getoptions{})
if err != nil {
        logger.fatalf(&quot;cannot obtain configmap: %v&quot;, err) // error occurs here
}

",<kubernetes><kubernetes-go-client><kubernetes-rbac>,74075805,1,"i don't see anything in particular wrong with your role or
rolebinding, and in fact when i deploy them into my environment they
seem to work as intended. you haven't provided a complete reproducer in your question, so here's how i'm testing things out:

i started by creating a namespace my-namespace

i have the following in kustomization.yaml:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization
namespace: my-namespace

commonlabels:
  app: rbactest

resources:
- rbac.yaml
- deployment.yaml

generatoroptions:
  disablenamesuffixhash: true

configmapgenerator:
  - name: config
    literals:
      - foo=bar
      - this=that


in rbac.yaml i have the role and rolebinding from your question (without modification).

in deployment.yaml i have:
apiversion: apps/v1
kind: deployment
metadata:
  name: cli
spec:
  replicas: 1
  template:
    spec:
      containers:
        - name: cli
          image: quay.io/openshift/origin-cli
          command:
            - sleep
            - inf



with this in place, i deploy everything by running:
kubectl apply -k .

and then once the pod is up and running, this works:
$ kubectl exec -n my-namespace deploy/cli -- kubectl get cm config
name     data   age
config   2      3m50s

attempts to access other configmaps will not work, as expected:
$ kubectl exec deploy/cli -- kubectl get cm foo
error from server (forbidden): configmaps &quot;foo&quot; is forbidden: user &quot;system:serviceaccount:my-namespace:default&quot; cannot get resource &quot;configmaps&quot; in api group &quot;&quot; in the namespace &quot;my-namespace&quot;
command terminated with exit code 1

if you're seeing different behavior, it would be interesting to figure out where your process differs from what i've done.

your go code looks fine also; i'm able to run this in the &quot;cli&quot; container:
package main

import (
    &quot;context&quot;
    &quot;fmt&quot;
    &quot;log&quot;

    metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
    &quot;k8s.io/client-go/kubernetes&quot;
    &quot;k8s.io/client-go/rest&quot;
)

func main() {
    config, err := rest.inclusterconfig()
    if err != nil {
        panic(err.error())
    }
    clientset, err := kubernetes.newforconfig(config)
    if err != nil {
        panic(err.error())
    }

    namespace := &quot;my-namespace&quot;

    configmapclient := clientset.corev1().configmaps(namespace)

    configmap, err := configmapclient.get(context.todo(), &quot;config&quot;, metav1.getoptions{})
    if err != nil {
        log.fatalf(&quot;cannot obtain configmap: %v&quot;, err)
    }

    fmt.printf(&quot;%+v\n&quot;, configmap)
}

if i compile the above, kubectl cp it into the container and run it, i get as output:
&amp;configmap{objectmeta:{config  my-namespace  2ef6f031-7870-41f1-b091-49ab360b98da 2926 0 2022-10-15 03:22:34 +0000 utc &lt;nil&gt; &lt;nil&gt; map[app:rbactest] map[kubectl.kubernetes.io/last-applied-configuration:{&quot;apiversion&quot;:&quot;v1&quot;,&quot;data&quot;:{&quot;foo&quot;:&quot;bar&quot;,&quot;this&quot;:&quot;that&quot;},&quot;kind&quot;:&quot;configmap&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;labels&quot;:{&quot;app&quot;:&quot;rbactest&quot;},&quot;name&quot;:&quot;config&quot;,&quot;namespace&quot;:&quot;my-namespace&quot;}}
] [] [] [{kubectl-client-side-apply update v1 2022-10-15 03:22:34 +0000 utc fieldsv1 {&quot;f:data&quot;:{&quot;.&quot;:{},&quot;f:foo&quot;:{},&quot;f:this&quot;:{}},&quot;f:metadata&quot;:{&quot;f:annotations&quot;:{&quot;.&quot;:{},&quot;f:kubectl.kubernetes.io/last-applied-configuration&quot;:{}},&quot;f:labels&quot;:{&quot;.&quot;:{},&quot;f:app&quot;:{}}}} }]},data:map[string]string{foo: bar,this: that,},binarydata:map[string][]byte{},immutable:nil,}

"
67533890,"psycopg2.operationalerror: fatal: password authentication failed for user ""username"" in minikube","i am working on a multi-container flask app, which involves a web container(flask app), postgres container(for db services), and a redis container(for caching services).
web app has web_deployment.yaml and web_service.yaml files.
postgres app has postgres_deployment.yaml and postgres_service.yaml files.
redis app has redis_deployment.yaml and redis_service.yaml files.
my web_deployment.yaml file looks like this:
apiversion: apps/v1
kind: deployment
metadata:
  name: web-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        component: my-web-app
    spec:
      containers:
      - name: my-web-app-container
        image: web_app_image:latest
        ports:
        - containerport: 80
        env:
          - name: redis_host
            value: redis-service
          - name: redis_port
            value: '6379'
          - name: postgres_user
            value: username
          - name: postgres_host
            value: postgres-service
          - name: postgres_port
            value: '5432'
          - name: postgres_db
            value: postgres_db
          - name: pgpassword
            valuefrom:
              secretkeyref:
                name: pgpassword
                key: pgpassword
  selector:
    matchlabels:
      component: my-web-app

the postgres_deployment.yaml  file looks like this:
apiversion: apps/v1
kind: deployment 
metadata:
  name: postgres-deployment 
spec: 
  replicas: 1 
  selector:
    matchlabels:
      component: postgres 
  template:
    metadata:
      labels:
        component: postgres
    spec: 
      volumes: 
        - name: postgres-storage
          persistentvolumeclaim:
            claimname: database-persistent-volume-claim 
      containers:
        - name: postgres
          image: postgres:12-alpine
          ports:
            - containerport: 5432
          volumemounts: 
            - name: postgres-storage
              mountpath: /var/lib/postgresql/data 
              subpath: postgres
          env:
            - name: postgres_password
              valuefrom:
                secretkeyref:
                  name: pgpassword
                  key: pgpassword

while trying to establish connection for web container with the postgres container, i got the following issue:
  file &quot;/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py&quot;, line 211, in raise_
    raise exception
  file &quot;/usr/local/lib/python3.8/site-packages/sqlalchemy/pool/base.py&quot;, line 599, in __connect
    connection = pool._invoke_creator(self)
  file &quot;/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/create.py&quot;, line 578, in connect
    return dialect.connect(*cargs, **cparams)
  file &quot;/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py&quot;, line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  file &quot;/usr/local/lib/python3.8/site-packages/psycopg2/__init__.py&quot;, line 127, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)

psycopg2.operationalerror: fatal:  password authentication failed for user &quot;username&quot;

",<postgresql><kubernetes><flask-sqlalchemy><kubernetes-ingress><minikube>,67533891,1,"i successfully fixed it!
the mistake was that, i just mentioned the password in the posgres_deployment.yaml file, but i should also mention the database name and the username, using which the web_deployment.yaml is trying to access this db service.
now the new postgres_deployment.yaml file, after the correction, looks like this:
apiversion: apps/v1
kind: deployment 
metadata:
  name: postgres-deployment 
spec: 
  replicas: 1 
  selector:
    matchlabels:
      component: postgres 
  template:
    metadata:
      labels:
        component: postgres
    spec: 
      volumes: 
        - name: postgres-storage
          persistentvolumeclaim:
            claimname: database-persistent-volume-claim 
      containers:
        - name: postgres
          image: postgres:12-alpine
          ports:
            - containerport: 5432
          volumemounts: 
            - name: postgres-storage
              mountpath: /var/lib/postgresql/data 
              subpath: postgres
          env:
            - name: postgres_user
              value: username
            - name: postgres_db
              value: postgres_db
            - name: postgres_password
              valuefrom:
                secretkeyref:
                  name: pgpassword
                  key: pgpassword

"
67551961,haproxy ingress controller service changed ip on gcp,"i am using haproxy as the ingress-controller in my gke clusters. and exposing haproxy service as loadbalancer service(internal).
recently, i experienced an issue, where the ha-proxy service changed its external-ip, and traffic stopped routing to haproxy. this issue occurred multiple times on different days(now it has stopped). i had to manually add that new external-ip to the frontend of that loadbalancer to allow traffic to haproxy.
there were two pods running for haproxy, and both had been running for days, and there was nothing in their logs. i assume it was something related to service or gcp lb and not haproxy itself.
i am afraid that i don't have any logs related to that.
i still don't know, what caused the service ip to change. as there were no recent changes, and the cluster and all services were running for many days properly, and suddenly this occurred.
has anyone faced a similar issue earlier? or what can i do to avoid such issue in future?
what could have caused the ip to change?
this is how my service is configured:
---
apiversion: v1
kind: service
metadata:
  labels:
    run: haproxy-ingress
  name: haproxy-ingress
  namespace: haproxy-controller
  annotations:
    cloud.google.com/load-balancer-type: &quot;internal&quot;
    networking.gke.io/internal-load-balancer-allow-global-access: &quot;true&quot;
    cloud.google.com/network-tier: &quot;premium&quot;
spec:
  selector:
    run: haproxy-ingress
  type: loadbalancer
  ports:
  - name: http
    port: 80
    protocol: tcp
    targetport: 80
  - name: https
    port: 443
    protocol: tcp
    targetport: 443
  - name: stat
    port: 1024
    protocol: tcp
    targetport: 1024


found some logs:
warning syncloadbalancerfailed 30m (x3570 over 13d) service-controller error syncing load balancer: failed to ensure load balancer: googleapi: error 409: ip_in_use_by_another_resource - ip '10.17.129.17' is already being used by another resource.
normal ensuringloadbalancer 3m33s (x3576 over 13d) service-controller ensuring load balancer

",<kubernetes><google-kubernetes-engine><kubernetes-ingress><kubernetes-service><haproxy-ingress>,67584911,1,"the short answer is: external ip for the service are ephemeral.
because ha-proxy controller pods are recreated the ha-proxy service is created with an ephemeral ip.
to avoid this issue, i would recommend using a static ip that you can reference in the loadbalancerip field.
this can be done by following steps:

reserve a static ip. (link)
use this ip, to create a service (link)

example yaml:
apiversion: v1
kind: service
metadata:
  name: helloweb
  labels:
    app: hello
spec:
  selector:
    app: hello
    tier: web
  ports:
  - port: 80
    targetport: 8080
  type: loadbalancer
  loadbalancerip: &quot;your.ip.address.here&quot;

"
67538229,eks ingress resource,"i'm trying to configure an ingress resource in an eks cluster running v1.18 using the below configuration.  after running kubectl apply -f blah.yaml i'm returned error: unable to recognize &quot;blah.yaml&quot;: no matches for kind &quot;ingress&quot; inversion &quot;networking.k8s.io/v1&quot; i think it's an apiversion mismatch.  what am i missing?
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: foo
spec:
  rules:
  - http:
      paths:
      - path: /boom
        pathtype: prefix
        backend:
          service:
            name: foo
            port:
              number: 80

",<kubernetes><kubernetes-ingress><amazon-eks>,67538501,1,"you can check what is the apiversions are present for networking.k8s.io resource in your system using
kubectl api-versions  | grep networking.k8s.io

check if you have networking.k8s.io/v1 in the output.
"
68917362,gke backendconfig not working with customrequestheaders,"i have a nodejs application running on google kubernetes engine (v1.20.8-gke.900)
i want to add custom header to get client's region and lat long so i refer to this article and this one also and created below kubernetes config file, but when i am printing the header i am not getting any custom header.
#k8s.yaml
apiversion: v1
kind: namespace
metadata:
  name: my-app-ns-prod
---
apiversion: apps/v1
kind: deployment
metadata:
  name: npm-app-deployment
  namespace: my-app-ns-prod
  labels:
    app: npm-app-deployment
    tier: backend

spec:
  template:
    metadata:
      name: npm-app-pod
      namespace: my-app-ns-prod
      labels:
        app: npm-app-pod
        tier: backend

    spec:
      containers:
      - name: my-app-container
        image: us.gcr.io/img/my-app:latest
        ports:
        - containerport: 3000
          protocol: tcp
        envfrom:
          - secretref:
              name: npm-app-secret
          - configmapref:
              name: npm-app-configmap
        imagepullpolicy: always
      imagepullsecrets:
        - name: gcr-regcred

  replicas: 3
  minreadyseconds: 30

  selector:
    matchlabels:
      app: npm-app-pod
      tier: backend
---
apiversion: v1
kind: service
metadata:
  name: npm-app-service
  namespace: my-app-ns-prod
  annotations:
    cloud.google.com/backend-config: '{&quot;ports&quot;: {&quot;80&quot;:&quot;npm-app-backendconfig&quot;}}'
    cloud.google.com/neg: '{&quot;ingress&quot;: true}'
spec:
  selector:
    app: npm-app-pod
    tier: backend
  ports:
    - name: http
      protocol: tcp
      port: 80
      targetport: 3000
    - name: https
      protocol: tcp
      port: 443
      targetport: 3000
  type: loadbalancer
---
apiversion: cloud.google.com/v1
kind: backendconfig
metadata:
  name: npm-app-backendconfig
  namespace: my-app-ns-prod
spec:
  customrequestheaders:
    headers:
    - &quot;http-x-client-citylatlong:{client_city_lat_long}&quot;
    - &quot;http-x-client-region:{client_region}&quot;
    - &quot;http-x-client-region-subdivision:{client_region_subdivision}&quot;
    - &quot;http-x-client-city:{client_city}&quot;
---
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-service
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
spec:
  rules:
    - host: app.example.com
      http:
        paths:
          - path: /api/v1
            pathtype: prefix
            backend:
              service:
                name: npm-app-service
                port:
                  number: 80
---
apiversion: v1
kind: configmap
metadata:
  name: npm-app-configmap
  namespace: my-app-ns-prod
data:
  app_id: &quot;my app&quot;
  port: &quot;3000&quot;
---
apiversion: v1
kind: secret
metadata:
  name: npm-app-secret
  namespace: my-app-ns-prod
type: opaque
data:
  mongo_connection_uri: &quot;&quot;
  session_secret: &quot;&quot;

",<kubernetes><google-kubernetes-engine><kubernetes-ingress><kubernetes-networking>,72028988,1,"actually the issue was with ingress controller, i missed to defined &quot;cloud.google.com/backend-config&quot;. once i had defined that i was able to get the custom header. also i switched to gke ingress controller (gce) from nginx. but the same things works with nginx also
this is how my final ingress looks like.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: ingress-service
  annotations:
    cloud.google.com/backend-config: '{&quot;default&quot;: &quot;npm-app-backendconfig&quot;}'
    kubernetes.io/ingress.class: &quot;gce&quot;
spec:
  ...
  ...

reference: user-defined request headers :
"
68931789,digital ocean kubernetes secrets certificate connect with managed database,"i am trying to connect my kubernetes cluster in digital ocean with a managed database.
i need to add the ca certificate that is a file with extension cer.  is this the right way to add this file/certificate to a secret?
apiversion: v1
kind: secret
metadata:
  name: secret-db-ca
type: kubernetes.io/tls
data:
  .tls.ca: |
        &quot;&lt;base64 encoded ~/.digitalocean-db.cer&gt;&quot;

",<kubernetes><digital-ocean><kubernetes-secrets>,68938640,1,"how to create a secret from certificate

the easiest and fastest way is to create a secret from command line:
kubectl create secret generic secret-db-ca --from-file=.tls.ca=digitalocean-db.cer

please note that type of this secret is generic, not kubernetes.io/tls because tls one requires both keys provided: tls.key and tls.crt
also it's possible to create a key from manifest, however you will need to provide full base64 encoded string to the data field and again use the type opaque in manifest (this is the same as generic from command line).
it will look like:
apiversion: v1
kind: secret
metadata:
  name: secret-db-ca
type: opaque
data:
  .tls.ca: |
     ls0tls1crudjtibdrvj..........

option you tried to use is used for docker config files. please see docker config - secrets

note! i tested the above with cer certificate.
der (distinguished encoding rules) is a binary encoding for x.509 certificates and private keys, they do not contain plain text (extensions .cer and .der). secret was saved in etcd (generally speaking database for kubernetes cluster), however there may be issues with workability of secrets based on this type of secrets.
there is a chance that different type/extension of certificate should be used (digital ocean has a lot of useful and good documentation).

please refer to secrets in kubernetes page.
"
68936694,minikube nginx ingress not finding service endpoint,"i'm having some trouble getting the nginx ingress controller working in my minikube cluster. it's likely to be some faults in ingress configuration but i cannot pick it out.
first, i deployed a service and it worked well without ingress.
kind: service
apiversion: v1
metadata:
  name: online
  labels:
    app: online
spec:
  selector:
    app: online
  ports:
  - protocol: tcp
    port: 8080
    targetport: 5001
  type: loadbalancer

---
apiversion: apps/v1
kind: deployment
metadata:
  name: online
  labels:
    app: online
spec:
  replicas: 1
  selector:
    matchlabels:
      app: online
  template:
    metadata:
      labels:
        app: online
      annotations:
        dapr.io/enabled: &quot;true&quot;
        dapr.io/app-id: &quot;online&quot;
        dapr.io/app-port: &quot;5001&quot;
        dapr.io/log-level: &quot;debug&quot;
        dapr.io/sidecar-liveness-probe-threshold: &quot;300&quot;
        dapr.io/sidecar-readiness-probe-threshold: &quot;300&quot;
    spec:
      containers:
      - name: online
        image: online:latest
        ports:
        - containerport: 5001
        env:
        - name: address
          value: &quot;:5001&quot;
        - name: dapr_http_port
          value: &quot;8080&quot;
        imagepullpolicy: never

then check its url
minikube service online --url
http://192.168.49.2:32323


it looks ok for requests.
curl http://192.168.49.2:32323/useronline
ok

after that i tried to apply nginx ingress offered by minikube.
i installed ingress and run an example by referring to this and it's all ok.
lastly, i configured my ingress.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: online-ingress
  annotations:
spec:
  rules:
    - host: online
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: online
                port:
                  number: 8080

and changed /etc/hosts by adding line
192.168.49.2    online

and test:
curl online/useronline
502 bad gateway

the logs are like this:
192.168.49.1 - - [26/aug/2021:09:45:56 +0000] &quot;get /useronline http/1.1&quot; 502 150 &quot;-&quot; &quot;curl/7.68.0&quot; 80 0.002 [default-online-8080] [] 172.17.0.5:5001, 172.17.0.5:5001, 172.17.0.5:5001 0, 0, 0 0.004, 0.000, 0.000 502, 502, 502 578ea1b1471ac973a2ac45ec4c35d927
2021/08/26 09:45:56 [error] 2514#2514: *426717 upstream prematurely closed connection while reading response header from upstream, client: 192.168.49.1, server: online, request: &quot;get /useronline http/1.1&quot;, upstream: &quot;http://172.17.0.5:5001/useronline&quot;, host: &quot;online&quot;
2021/08/26 09:45:56 [error] 2514#2514: *426717 connect() failed (111: connection refused) while connecting to upstream, client: 192.168.49.1, server: online, request: &quot;get /useronline http/1.1&quot;, upstream: &quot;http://172.17.0.5:5001/useronline&quot;, host: &quot;online&quot;
2021/08/26 09:45:56 [error] 2514#2514: *426717 connect() failed (111: connection refused) while connecting to upstream, client: 192.168.49.1, server: online, request: &quot;get /useronline http/1.1&quot;, upstream: &quot;http://172.17.0.5:5001/useronline&quot;, host: &quot;online&quot;
w0826 09:45:56.918446       7 controller.go:977] service &quot;default/online&quot; does not have any active endpoint.
i0826 09:46:21.345177       7 status.go:281] &quot;updating ingress status&quot; namespace=&quot;default&quot; ingress=&quot;online-ingress&quot; currentvalue=[] newvalue=[{ip:192.168.49.2 hostname: ports:[]}]
i0826 09:46:21.349078       7 event.go:282] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;default&quot;, name:&quot;online-ingress&quot;, uid:&quot;b69e2976-09e9-4cfc-a8e8-7acb51799d6d&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;23100&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'sync' scheduled for sync


i found the error is very about annotations of ingress. if i changed it to:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1

the error would be:
404 page not found

and logs:
i0826 09:59:21.342251       7 status.go:281] &quot;updating ingress status&quot; namespace=&quot;default&quot; ingress=&quot;online-ingress&quot; currentvalue=[] newvalue=[{ip:192.168.49.2 hostname: ports:[]}]
i0826 09:59:21.347860       7 event.go:282] event(v1.objectreference{kind:&quot;ingress&quot;, namespace:&quot;default&quot;, name:&quot;online-ingress&quot;, uid:&quot;8ba6fe97-315d-4f00-82a6-17132095fab4&quot;, apiversion:&quot;networking.k8s.io/v1beta1&quot;, resourceversion:&quot;23760&quot;, fieldpath:&quot;&quot;}): type: 'normal' reason: 'sync' scheduled for sync
192.168.49.1 - - [26/aug/2021:09:59:32 +0000] &quot;get /useronline http/1.1&quot; 404 19 &quot;-&quot; &quot;curl/7.68.0&quot; 80 0.002 [default-online-8080] [] 172.17.0.5:5001 19 0.000 404 856ddd3224bbe2bde9d7144b857168e0

other infos.
name     type           cluster-ip     external-ip   port(s)          age
online   loadbalancer   10.111.34.87   &lt;pending&gt;     8080:32323/tcp   6h54m

the example i mentioned above is a nodeport service and mine is a loadbalancer, that's the biggest difference. but i don't know why it does not work for me.
",<nginx><kubernetes><kubernetes-ingress><minikube>,68980811,1,"moving this out of comments so it will be visible.

ingress
main issue was with path in ingress rule since application serves traffic on online/useronline. if requests go to online then ingress returns 404.
rewrite annotation is not needed in this case as well.
ingress.yaml should look like:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: online-ingress
#  annotations:
spec:
  rules:
    - host: online
      http:
        paths:
          - path: /useronline
            pathtype: prefix
            backend:
              service:
                name: online
                port:
                  number: 8080

more details about ingress

loadbalancer on minikube
since minikube is considered as bare metal installation, to get external ip for service/ingress, it's necessary to use specially designed metallb solution.
metallb is a load-balancer implementation for bare metal kubernetes clusters, using standard routing protocols.
it ships as add-on for minikube and can be enabled with:
minikube addons enable metallb

and it needs to create a configmap with setup. please refer to metallb configuration
"
73970713,kube-prometheus stack with loadbalancer for external access to prometheus (and grafana),"i have installed the kube-prometheus stach from here and want to expose prometheus and grafana with a loadbalancer to get access to them from another cluster.
to acchieve this i have changed the prometheus-service.yaml by adding a type: loadbalancer. when i try to access the exposed ip, the server says that the connection has timed out. what should i do to be able to access the prometheus server?
the altered  prometheus-service.yaml looks like this:

apiversion: v1
kind: service
metadata:
  labels:
    app.kubernetes.io/component: prometheus
    app.kubernetes.io/instance: k8s
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/part-of: kube-prometheus
    app.kubernetes.io/version: 2.38.0
  name: prometheus-k8s
  namespace: monitoring
spec:
  ports:
  - name: web
    port: 9090
    targetport: web
  - name: reloader-web
    port: 8080
    targetport: reloader-web
  type: loadbalancer
  selector:
    app.kubernetes.io/component: prometheus
    app.kubernetes.io/instance: k8s
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/part-of: kube-prometheus
  sessionaffinity: clientip

ideas:

should i alter the networkpolicy to allow for external access? in that case, how?

",<kubernetes><kubernetes-helm><prometheus-operator><kube-prometheus-stack>,73971367,1,"i found a way to allow it to be exposed, it was networkpolicy. chekc link from github.
one has to add a seperate networkpolicy for prometheus to allow external, as shown here:
kind: networkpolicy
apiversion: networking.k8s.io/v1
metadata:
  name: prometheus-allow-external
  namespace: monitoring
spec:
  podselector:
    matchlabels:
      app.kubernetes.io/component: prometheus
      app.kubernetes.io/instance: k8s
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: kube-prometheus
  ingress:
  - ports:
    - port: 9090

the problem is that i tought this was already done under the prometheus-networkpolicy.yaml:
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  labels:
    app.kubernetes.io/component: prometheus
    app.kubernetes.io/instance: k8s
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/part-of: kube-prometheus
    app.kubernetes.io/version: 2.38.0
  name: prometheus-k8s
  namespace: monitoring
spec:
  egress:
  - {}
  ingress:
  - from:
    - podselector:
        matchlabels:
          app.kubernetes.io/name: prometheus
    ports:
    - port: 9090
      protocol: tcp
    - port: 8080
      protocol: tcp
  - from:
    - podselector:
        matchlabels:
          app.kubernetes.io/name: grafana
    ports:
    - port: 9090
      protocol: tcp
  podselector:
    matchlabels:
      app.kubernetes.io/component: prometheus
      app.kubernetes.io/instance: k8s
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: kube-prometheus
  policytypes:
  - egress
  - ingress

can anybody explain what the difference is?
"
68590914,"ingress uses wildcard, although i didn't specify that","i have the following ingress:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: my-ingress
  namespace: apps
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/custom-http-errors: '404'
spec:
  tls:
    - hosts:
        - mywebsite.com
      secretname: my-secret-tls
  rules:
  - host: mywebsite.com
  - http:
      paths:
        - path: /api/events
          pathtype: implementationspecific
          backend:
            service:
              name: my-events-api-svc
              port:
                number: 80

when i kubectl describe this ingress, i see the following
name:             my-ingress
namespace:        apps
address:          52.206.112.10
default backend:  default-http-backend:80 (&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;)
tls:
  my-secret-tls terminates mywebsite.com
rules:
  host        path  backends
  ----        ----  --------
  *           
              /api/events   my-events-api-svc:80 (10.244.4.145:4000,10.244.5.118:4000)
annotations:  kubernetes.io/ingress.class: nginx
              nginx.ingress.kubernetes.io/custom-http-errors: 404
events:
  type    reason  age                from                      message
  ----    ------  ----               ----                      -------
  normal  update  9s (x7 over 153m)  nginx-ingress-controller  ingress apps/my-ingress


the issue is that the specified path doesn't work, i'm getting 404. i noticed that in the output above, there is a * under the host. my other ingresses are configured pretty much the same (only have different paths set), and there is no * in the kubectl describe output. instead, in my other ingresses the proper host - &quot;mywebsite.com&quot; - is shown.
the output above also has some error (&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;), but i see it as well in my other ingresses (that do work).
what could be the problem?
",<kubernetes><kubernetes-ingress><nginx-ingress>,68620763,1,"its your indentation, please check official example
spec:
  rules:
    - host: hello-world.info
      http:
        paths:

try
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: my-ingress
  namespace: apps
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/custom-http-errors: '404'
spec:
  tls:
    - hosts:
        - mywebsite.com
      secretname: my-secret-tls
  rules:
  - host: mywebsite.com
    http:
      paths:
        - path: /api/events
          pathtype: implementationspecific
          backend:
            service:
              name: my-events-api-svc
              port:
                number: 80

"
68642661,generating a redirect with traefik ingress on k3s?,"i'm running prometheus and grafana under k3s, accessible (respectively) at http://monitoring.internal/prometheus and http://monitoring.internal/grafana. the grafana ingress object, for example, looks like:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: grafana
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
  rules:
    - host: monitoring.internal
      http:
        paths:
          - path: /grafana
            pathtype: prefix
            backend:
              service:
                name: grafana
                port:
                  number: 3000

this works fine, except  that if you land at
http://monitoring.internal/, you get a 404 error.  i would like
requests for http://monitoring.internal/ to redirect to
http://monitoring.internal/grafana. i could perhaps create another
service that runs  something like darkhttpd ... --forward-all http://monitoring.internal/grafana, and create  an  ingress object
that would  map / to that service, but it seems like there  ought to
be a way to do this with traefik  itself.
it looks like i'm running traefik 2.4.8 locally:
$ kubectl -n kube-system exec -it deployment/traefik -- traefik version
version:      2.4.8
codename:     livarot
go version:   go1.16.2
built:        2021-03-23t15:48:39z
os/arch:      linux/amd64

i've found this documentation for 1.7 that suggests there is an annotation for exactly this purpose:

traefik.ingress.kubernetes.io/app-root: &quot;/index.html&quot;: redirects
all requests for / to the defined path.

but setting that on the grafana ingress object doesn't appear to have
any impact, and i haven't been able to find similar docs for 2.x
(i've looked around
here, for
example).
what's the right way to set up this sort of redirect?
",<kubernetes><kubernetes-ingress><traefik><k3s>,68643697,1,"since i haven't been able to figure out traefik yet, i thought i'd post my solution here in case anyone else runs into the same situation. i am hoping someone comes along who knows the right way to to do this, and if i figure out i'll update this answer.
i added a new deployment that runs darkhttpd as a simple director:
apiversion: apps/v1
kind: deployment
metadata:
  name: redirector
spec:
  replicas: 1
  template:
    spec:
      containers:
        - name: redirector
          image: docker.io/alpinelinux/darkhttpd
          ports:
            - containerport: 8080
          args:
            - --forward-all
            - http://monitoring.internal/grafana

a corresponding service:
apiversion: v1
kind: service
metadata:
  name: redirector
spec:
  ports:
    - port: 8080
      protocol: tcp
      targetport: 8080

and the  following ingress object:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: redirector
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
  rules:
    - host: monitoring.internal
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: redirector
                port:
                  number: 8080

these are  all deployed with kustomize, which  takes care of
adding labels and selectors in the appropriate places. the
kustomization.yaml look like:
apiversion: kustomize.config.k8s.io/v1beta1
kind: kustomization

resources:
- deployment.yaml
- ingress.yaml
- service.yaml

commonlabels:
  component: redirector

with all this in place, requests to http://monitoring.internal/ hit the redirector pod.
"
68649137,"kubectl - error from server (forbidden): users ""xxx@xxx.it"" is forbidden: user ""system:serviceaccount:gke-connect:connect-agent-sa""","i have this strange situation, how i can solve this problem ?
ubuntu@anth-mgt-wksadmin:~$ kubectl get nodes
error: the server doesn't have a resource type &quot;nodes&quot;

ubuntu@anth-mgt-wksadmin:~$ kubectl cluster-info
to further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
error: the server doesn't have a resource type &quot;services&quot;

ubuntu@anth-mgt-wksadmin:~$ kubectl cluster-info dump
error from server (forbidden): users &quot;xxx@xxx.it&quot; is forbidden: user system:serviceaccount:gke-connect:connect-agent-sa&quot; cannot impersonate resource &quot;users&quot; in api group &quot;&quot; at the cluster scope

i think that the problem has been generated by this following apply searching a way to connect the admin cluster to cloud console but how to rollback ?
user_account=foo@example.com
cat &lt;&lt;eof &gt; /tmp/impersonate.yaml
apiversion: rbac.authorization.k8s.io/v1
kind: clusterrole
metadata:
  name: gateway-impersonate
rules:
- apigroups:
  - &quot;&quot;
  resourcenames:
  - ${user_account}
  resources:
  - users
  verbs:
 - impersonate
 - --
 - apiversion: rbac.authorization.k8s.io/v1
 - kind: clusterrolebinding
metadata:
  name: gateway-impersonate
roleref:
  kind: clusterrole
  name: gateway-impersonate
  apigroup: rbac.authorization.k8s.io
subjects:
- kind: serviceaccount
  name: connect-agent-sa
  namespace: gke-connect
eof
# apply impersonation policy to the cluster.
kubectl apply -f /tmp/impersonate.yaml

",<kubernetes><kubectl><google-anthos>,68707277,1,"i have copied the admin.conf file from one admin cluster node to the admin workstation and renamed to kubeconfig
root@anth-admin-host1:~# cat /etc/kubernetes/admin.conf apiversion: v1 clusters:

"
74170287,kubernetes nginx ingress can't access pod in different namespace,"
i am trying to setup the kuard demo app in the namespace example-ns exposed by nginx ingress. 
exposing it in the default namespace works but when i expose it in the namespace example-ns i get:  ```503 service temporarily unavailable```
these are to service, deployment and ingress yamls i use for kuard:
apiversion: apps/v1
kind: deployment
metadata:
  name: kuard
  namespace: example-ns
spec:
  selector:
    matchlabels:
      app: kuard
  replicas: 1
  template:
    metadata:
      labels:
        app: kuard
    spec:
      containers:
      - image: gcr.io/kuar-demo/kuard-amd64:1
        imagepullpolicy: always
        name: kuard
        ports:
        - containerport: 8080

---

apiversion: v1
kind: service
metadata:
  name: kuard
  namespace: example-ns
spec:
  ports:
  - port: 80
    targetport: 8080
    protocol: tcp
  selector:
    app: kuard

---

apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: kuard
  namespace: example-ns
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    cert-manager.io/cluster-issuer: &quot;letsencrypt-prod&quot;
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: htpasswd
    nginx.ingress.kubernetes.io/auth-realm: &quot;enter your credentials&quot;

spec:
  tls:
  - hosts:
    - example.mydomain.dev
    secretname: quickstart-example-tls
  rules:
  - host: example.mydomain.dev
    http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: kuard
            port:
              number: 80

as you can see everything is in the same namespace and describing the ingress results in:
 kubectl describe ingress kuard -n example-ns
name:             kuard
labels:           &lt;none&gt;
namespace:        example-ns
address:          192.168.69.1
ingress class:    &lt;none&gt;
default backend:  &lt;default&gt;
tls:
  quickstart-example-tls terminates example.mydomain.dev
rules:
  host                    path  backends
  ----                    ----  --------
  example.mydomain.dev
                          /   kuard:80 (10.69.58.226:8080)
annotations:              cert-manager.io/cluster-issuer: letsencrypt-prod
                          kubernetes.io/ingress.class: nginx
                          nginx.ingress.kubernetes.io/auth-realm: enter your credentials
                          nginx.ingress.kubernetes.io/auth-secret: htpasswd
                          nginx.ingress.kubernetes.io/auth-type: basic
events:
  type    reason             age                from                       message
  ----    ------             ----               ----                       -------
  normal  createcertificate  28m                cert-manager-ingress-shim  successfully created certificate &quot;quickstart-example-tls&quot;
  normal  sync               27m (x2 over 28m)  nginx-ingress-controller   scheduled for sync
  normal  sync               27m (x2 over 28m)  nginx-ingress-controller   scheduled for sync

i also read same issues like this but this solution is not working as seen here.
anyone has an idea whats wrong here?
thanks in advance!
solution:
i checked the logs of the ingress controller and saw that the auth secret was in the default namespace. thats why only pods from default namespace were acessible. moving the secret into the proper namespace solved the issue!
",<nginx><kubernetes><kubernetes-ingress><nginx-ingress>,74178891,1,"first of all you should not use the annotation kubernetes.io/ingress.class anymore as it's deprecated. instead use .spec.ingressclassname to refer to your desired ingress controller:
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata:
  name: ingress-myservicea
spec:
  ingressclassname: nginx
  rules:
  ...

it seems like that your ingress in your desired namespace can't seem to sync with the controller, so if there is any netpols in your example-ns and the namespace where your controller resides; back them up and delete them, to make sure the connection isn't being blocked.
next you should check the logs of your ingress controller itself, if the connection reaches it; you will see surely the reason in the logs why the ingress resource doesn't work. also sharing your config for the ingress controller would be helpful.
"
68729966,kubernetes config map data value externalisation,"i'm installing fluent-bit in our k8s cluster.  i have the helm chart for it on our repo, and argo is doing the deployment.
among the resources in the helm chart is a config-map with data value as below:
apiversion: v1
kind: configmap
metadata:
  name: fluent-bit
  labels:
    app: fluent-bit
data:
...
  output-s3.conf: |
    [output]
        name s3
        match *
        bucket bucket/prefix/random123/test
        region ap-southeast-2
...

my question is how can i externalize the value for the bucket so it's not hardcoded (please note  that the bucket value has random numbers)? as the s3 bucket is being created by a separate app that gets ran on the same master node, the randomly generated s3 bucket name is available as environment variable, e.g. doing &quot;echo $s3bucketname&quot; on the node would give the actual value).
i have tried doing below on the config map but it didn't work and is just getting set as it is when inspected on pod:
bucket $(echo $s3bucketname) 

using helm, i know it can be achieved something like below and then can populate using scripting something like helm --set to set the value from environment variable.  but the deployment is happening auto through argocd so it's not like there is a place to do helm --set command or please let me know if otherwise.
bucket {{.values.s3.bucket}}

tia
",<kubernetes><kubernetes-helm><configmap>,68905680,1,"with fluentbit you should be able to use environment variables such as:
  output-s3.conf: |
    [output]
        name s3
        match *
        bucket ${s3_bucket_name}
        region ap-southeast-2

you can then set the environment variable on your helm values. depending on the chart you are using and how values are passed you may have to perform a different setup, but for example using the official fluentbit charts with a values-prod.yml like:
env:
- name: s3_bucket_name
  value: &quot;bucket/prefix/random123/test&quot;

using argocd, you probably have a git repository where helm values files are defined (like values-prod.yml) and/or an argocd application defining values direct. for example, if you have an argocd application defined such as:
apiversion: argoproj.io/v1alpha1
kind: application
metadata:
  # [...]
spec:
  source:
    # ...
    helm:      
      # helm values files for overriding values in the helm chart
      valuefiles:
      # you can update this file
      - values-prod.yaml

      # helm values
      values: |
        # or update values here
        env:
        - name: s3_bucket_name
          value: &quot;bucket/prefix/random123/test&quot;
        # ...

you should be able to update either values-prod.yml on the repository used by argocd or update directly values: with you environment variable
"
68766219,how to allow an assume role connect from ec2 to eks on aws?,"
i created an ec2 instance and an eks cluster in the same aws account.
in order to use the eks cluster from ec2, i have to grant necessary permissions to it.
i added an instance profile role with some eks operation permissions. its role arn is arn:aws:iam::11111111:role/ec2-instance-profile-role(a) on dashboard. but in the ec2 instance, it can be found as arn:aws:sts::11111111:assumed-role/ec2-instance-profile-role/i-00000000(b).
$ aws sts get-caller-identity
{
    &quot;account&quot;: &quot;11111111&quot;,
    &quot;userid&quot;: &quot;aaaaaaaaaaaaaaa:i-000000000000&quot;,
    &quot;arn&quot;: &quot;arn:aws:sts::11111111:assumed-role/ec2-instance-profile-role/i-00000000&quot;
}

i also created an aws-auth config map to set into kubernetes' system config in eks, in order to allow the ec2 instance profile role can be registered and accessible. i tried both a and b to set into the maproles, all of them got the same issue. when i run kubectl command on ec2:
$ aws eks --region aws-region update-kubeconfig --name eks-cluster-name

$ kubectl config view --minify
apiversion: v1
clusters:
- cluster:
    certificate-authority-data: data+omitted
    server: https://xxxxxxxxxxxxxxxxxxxxxxxxxxxx.aw1.aws-region.eks.amazonaws.com
  name: arn:aws:eks:aws-region:11111111:cluster/eks-cluster-name
contexts:
- context:
    cluster: arn:aws:eks:aws-region:11111111:cluster/eks-cluster-name
    user: arn:aws:eks:aws-region:11111111:cluster/eks-cluster-name
  name: arn:aws:eks:aws-region:11111111:cluster/eks-cluster-name
current-context: arn:aws:eks:aws-region:11111111:cluster/eks-cluster-name
kind: config
preferences: {}
users:
- name: arn:aws:eks:aws-region:11111111:cluster/eks-cluster-name
  user:
    exec:
      apiversion: client.authentication.k8s.io/v1alpha1
      args:
      - --region
      - aws-region
      - eks
      - get-token
      - --cluster-name
      - eks-cluster-name
      - --role
      - arn:aws:sts::11111111:assumed-role/ec2-instance-profile-role/i-00000000
      command: aws
      env: null
      provideclusterinfo: false

$kubectl get svc
error: you must be logged in to the server (unauthorized)

i also checked the type of the assumed role. it's service but not aws.
it seems this type is necessary.
{
    &quot;version&quot;: &quot;2012-10-17&quot;,
    &quot;statement&quot;: {
        &quot;effect&quot;: &quot;allow&quot;,
        &quot;principal&quot;: { &quot;aws&quot;: &quot;arn:aws:iam:: 333333333333:root&quot; },
        &quot;action&quot;: &quot;sts:assumerole&quot;
    }
}

terraform aws assume role
but i tried to create a new assume role with aws type and set it to kubernetes' aws-auth config map, still the same issue.
how to use it? do i need to create a new iam user to use?
",<amazon-web-services><kubernetes><amazon-ec2><amazon-iam><amazon-eks>,68767032,1,"- name: external-staging
  user:
    exec:
      apiversion: client.authentication.k8s.io/v1alpha1
      args:
      - exec
      - test-dev
      - --
      - aws
      - eks
      - get-token
      - --cluster-name
      - ekscluster-1234
      - --role-arn
      - arn:aws:iam::3456789002:role/eks-cluster-admin-role-e65f32f
      command: aws-vault
      env: null

this config file working for me. it should be role-arn &amp; command: aws-vault
"
76511914,trigger kubernetes job on secret update,"how do you trigger rerunning a job once one of its dependent secrets get updated?
for example, say i have this simple job:
apiversion: batch/v1
kind: job
metadata:
  name: job-test
spec:
  template:
    metadata:
      labels:
        app: databricks
    spec:
      containers:
      - name: job-test
        image: alpine
        command:
          - echo
          - hello $(dan_test)
        env:
          - name: dan_test
            valuefrom:
              secretkeyref:
                name: dan-test
                key: dan-test-1
      restartpolicy: never
  backofflimit: 4

adding this job makes it run and print out the secret, but when the secret is changed, the job is not automatically rerun.
is there built-in or 3rd party extension resource that can target the secret and the job and trigger a rerun?
",<kubernetes><kubernetes-secrets><kubernetes-jobs>,77503264,1,"stakater/reloader allows resources to be reloaded based on watched changes in a configmap or secret.
unfortunately, job and cronjob resources are not supported, so to make it work i used a deployment with an initcontainer which executed the job, and a container which kept the pod alive.
apiversion: apps/v1
kind: deployment
metadata:
  name: job-test
  labels:
    app: job-test
  annotations:
    reloader.stakater.com/auto: &quot;true&quot;
spec:
  replicas: 1
  selector:
    matchlabels:
      app: job-test
  template:
    metadata:
      labels:
        app: job-test
    spec:
      initcontainers:
        - name: job-test
          image: alpine
          imagepullpolicy: always
          command:
            - echo
            - hello $(dan_test)
          env:
            - name: dan_test
              valuefrom:
                secretkeyref:
                  name: dan-test
                  key: dan-test-1
      containers:
        - name: job-test-wait
          image: alpine
          command:
            - tail
            - -f
            - /dev/null

i forgot exactly why the pod needed to be kept alive, but from trial and error this is what i got working a few months ago and am still using today.
"
76516201,nginx incorrectly getting web files locally instead of where it is proxying the request to,"background:
i have deployed various applications into aws/kubernetes. some of these applications will asp.net core mvc web applications, with  kestrel as their web server. each of them will be running in their own pod.
all the applications are correctly running in their respective pods with communications to them and between them working fine.
in addition to these applications, i have also deployed an nginx ingress controller that will handle traffic being sent to some of these web applications. this is where the problem is.
issue:
the kubernetes ingress resource i am deploying is causing problems for me. when connecting to the web application through nginx proxying it, the browser has not loaded any javascript, css, images, etc. it's just the unformatted login page of my web app.
note that i am not deploying the built-in kubernetes ingress resource. i am deploying the nginx custom resource called &quot;virtualserver&quot;, but i can only imagine the same would happen with the built-in kubernetes ingress resource. below is an example of the resource i am deploying:
kind: virtualserver
apiversion: k8s.nginx.org/v1
metadata: 
  name: some-name-nginx-dmz-vs
  namespace: dmz
spec: 
  host: some-host-name-that-directs-to-nginx-pod.com
  upstreams: 
  - name: com-dmz-webinterfaceclient-service-1-us
    service: com-dmz-webinterfaceclient-service
    port: 80
  - name: com-dmz-webinterfaceclient-service-2-us
    service: com-dmz-webinterfaceclient-service
    port: 443
  routes: 
  - path: /webinterfaceuip
    action: 
      proxy: 
        upstream: com-dmz-webinterfaceclient-service-1-us
        requestheaders: 
            pass: true
        rewritepath: /
  - path: /webinterfaceuip2
    action: 
      proxy: 
        upstream: com-dmz-webinterfaceclient-service-2-us
        requestheaders: 
            pass: true
        rewritepath: /
---

the logs from the ingress controller errors complaining about missing files. it seams to be looking in its own local pod, instead of the remote pod it is actually proxying the request to. here is an example of the errors:
2023/06/20 14:28:46 [error] 114#114: *55067 open() &quot;/etc/nginx/html/js/wtk.api.view.js&quot; failed (2: no such file or directory)

are there any ideas on how i can fix this issue? perhaps there is a way to set the content root that could for the files to be found?
the main goal here is just proxying the request, in the future, more will be added to the nginx config, but i am not there just yet.
as a side note:
when i connect directly to the pod (so not going through nginx), it correctly loads the page with javascript, css, etc. only when going through nginx is there an issue with files not being found.
",<kubernetes><nginx><kubernetes-ingress><amazon-eks><ingress-controller>,76521935,1,"i have determined what was missing. adding in an additional empty path gets it to load the files as expected, css, javascript, image, etc:
kind: virtualserver
apiversion: k8s.nginx.org/v1
metadata: 
  name: some-name-nginx-dmz-vs
  namespace: dmz
spec: 
  host: some-host-name-that-directs-to-nginx-pod.com
  upstreams: 
  - name: com-dmz-webinterfaceclient-service-1-us
    service: com-dmz-webinterfaceclient-service
    port: 80
  routes: 
  - path: /
    action: 
      proxy: 
        upstream: com-dmz-webinterfaceclient-service-1-us
        requestheaders: 
            pass: true
        rewritepath: /
  - path: /webinterfaceuip
    action: 
      proxy: 
        upstream: com-dmz-webinterfaceclient-service-1-us
        requestheaders: 
            pass: true
        rewritepath: /
---

"
68853798,nginx-ingress session affinity behavior when ingress maps to 2 different ports on the same service,"let's say i have a service that maps to pod that has 2 containers, 1 expose port 8080, the other one expose port 8081. the service expose both ports. the ingress uses nginx-ingress, and has the cookie based session affinity annotations. it has 2 paths, 1 is / mapping to port 8080, the other one is /static mapping to port 8081 on the same service. will the session affinity work in such way where all the requests from the same client will be sent to the same pod no matter if the path is / or /static?
below are full configs:
ingress
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: test-ingress
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/affinity: &quot;cookie&quot;
    nginx.ingress.kubernetes.io/affinity-mode: &quot;persistent&quot;
    nginx.ingress.kubernetes.io/session-cookie-name: &quot;route&quot;
    nginx.ingress.kubernetes.io/session-cookie-expires: &quot;172800&quot;
    nginx.ingress.kubernetes.io/session-cookie-max-age: &quot;172800&quot;
spec:
  rules:
    - host: test.com
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: test-service
                port:
                  number: 8080
          - path: /static
            pathtype: prefix
            backend:
              service:
                name: test-service
                port:
                  number: 8081

service
apiversion: v1
kind: service
metadata:
  name: test-service
spec:
  type: clusterip
  selector:
    app: test-pod
  ports:
    - name: container1
      port: 8080
      targetport: 8080
    - name: container2
      port: 8081
      targetport: 8081

deployment
apiversion: apps/v1
kind: deployment
...
spec:
  ...
  template:
    metadata:
      labels:
        app: test-pod
    spec:
      containers:
        - name: container1
          image: ...
          ports:
            - containerport: 8080
        - name: container2
          image: ...
          ports:
            - containerport: 8081

",<kubernetes><kubernetes-ingress><nginx-ingress>,68865322,1,"i managed to test your configuration.
actually this affinity annotation will work only for / path - this is how nginx ingress works - to make affinity annotation work for both paths you need to create two ingress definitions:
ingress for path /:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: test-ingress-one
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/affinity: &quot;cookie&quot;
    nginx.ingress.kubernetes.io/affinity-mode: &quot;balanced&quot;
    nginx.ingress.kubernetes.io/session-cookie-name: &quot;route-one&quot;
    nginx.ingress.kubernetes.io/session-cookie-expires: &quot;172800&quot;
    nginx.ingress.kubernetes.io/session-cookie-max-age: &quot;172800&quot;
spec:
  rules:
    - host: &lt;your-domain&gt;
      http:
        paths:
          - path: /
            pathtype: prefix
            backend:
              service:
                name: test-service
                port:
                  number: 8080

ingress for path /static:
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: test-ingress-two
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/affinity: &quot;cookie&quot;
    nginx.ingress.kubernetes.io/affinity-mode: &quot;balanced&quot;
    nginx.ingress.kubernetes.io/session-cookie-name: &quot;route-two&quot;
    nginx.ingress.kubernetes.io/session-cookie-expires: &quot;172800&quot;
    nginx.ingress.kubernetes.io/session-cookie-max-age: &quot;172800&quot;
spec:
  rules:
    - host: &lt;your-domain&gt;
      http:
        paths:
          - path: /static
            pathtype: prefix
            backend:
              service:
                name: test-service
                port:
                  number: 8081

back to your main question - as we are creating two different ingresses, with two different cookies, they are independent of each other.  each of them will choose his &quot;pod&quot; to &quot;stick&quot; regardless of what the other has chosen. i did research i couldn't find any information about setting it a way to make it work you want.
briefly answering your question:

will the session affinity work in such way where all the requests from the same client will be sent to the same pod no matter if the path is / or /static?

no.
"
68869684,visual studio kubernetes project 503 error in azure,"i have created a kubernetes project in visual studio 2019, with the default template. this template creates a weatherforecast controller.
after that i have published it to my arc.
i used this command to create the aks:
az aks create -n $myaks -g $myrg --generate-ssh-keys --z 1 -s standard_b2s --attach-acr /subscriptions/mysubscriptionguid/resourcegroups/$myrg/providers/microsoft.containerregistry/registries/$myacr

and i enabled http application routing via the azure portal.
i have deployed it to azure kubernetes (standard_b2s), with the following deployment.yaml:
# deployment.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: kubernetes1-deployment
  labels:
    app: kubernetes1-deployment
spec:
  replicas: 2
  selector:
    matchlabels:
      app: kubernetes1
  template:
    metadata:
      labels:
        app: kubernetes1
    spec:
      containers:
      - name: kubernetes1
        image: mycontainername.azurecr.io/kubernetes1:latest
        ports:
        - containerport: 80

service.yaml:
#service.yaml
apiversion: v1
kind: service
metadata:
  name: kubernetes1
spec:
  type: clusterip
  selector:
    app: kubernetes1
  ports:
    - port: 80 # service exposed port
      name: http # service port name
      protocol: tcp # the protocol the service will listen to
      targetport: http # port to forward to in the pod

ingress.yaml:
#ingress.yaml
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  name: kubernetes1
  annotations:
    kubernetes.io/ingress.class: addon-http-application-routing
spec:
  rules:
    - host: kubernetes1.&lt;uuid (removed for this post)&gt;.westeurope.aksapp.io # which host is allowed to enter the cluster
      http:
        paths:
          - backend: # how the ingress will handle the requests
              service:
               name: kubernetes1 # which service the request will be forwarded to
               port:
                 name: http # which port in that service
            path: / # which path is this rule referring to
            pathtype: prefix # see more at https://kubernetes.io/docs/concepts/services-networking/ingress/#path-types

but when i go to kubernetes1..westeurope.aksapp.io or kubernetes1..westeurope.aksapp.io/weatherforecast i get the following error:

503 service temporarily unavailable
nginx/1.15.3


",<kubernetes><kubernetes-ingress><azure-aks>,68890092,1,"it's working now. for other people who have the same problem. i have updated my deployment config from:
# deployment.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: kubernetes1-deployment
  labels:
    app: kubernetes1-deployment
spec:
  replicas: 2
  selector:
    matchlabels:
      app: kubernetes1
  template:
    metadata:
      labels:
        app: kubernetes1
    spec:
      containers:
      - name: kubernetes1
        image: mycontainername.azurecr.io/kubernetes1:latest
        ports:
        - containerport: 80

to:
# deployment.yaml
apiversion: apps/v1
kind: deployment
metadata:
  name: kubernetes1
spec:
  selector: # define the wrapping strategy
    matchlabels: # match all pods with the defined labels
      app: kubernetes1 # labels follow the `name: value` template
  template: # this is the template of the pod inside the deployment
    metadata:
      labels:
        app: kubernetes1
    spec:
      nodeselector:
        kubernetes.io/os: linux
      containers:
        - image: mycontainername.azurecr.io/kubernetes1:latest
          name: kubernetes1
          resources:
            requests:
              cpu: 100m
              memory: 128mi
            limits:
              cpu: 250m
              memory: 256mi
          ports:
            - containerport: 80
              name: http

i don't know exactly which line solved the problem. feel free to comment it if you know which line the problem was.
"
76251197,not able to add ipv6 type load balancer ip in kubernetes service,"i'm on gke and i'm trying to expose one application using ipv6 address.
this is my service.yaml
apiversion: v1
kind: service
metadata:
  annotations:
    cloud.google.com/neg: '{&quot;ingress&quot;:true}'
  labels:
    run: ubuntu
  name: ubuntu
  namespace: default
spec:
  loadbalancerip: &quot;&lt;ipv6-address&gt;&quot;
  ipfamilies:
  - ipv6
  ipfamilypolicy: singlestack
  ports:
  - nodeport: 30783
    port: 5000
    protocol: tcp
    targetport: 5001
  selector:
    run: ubuntu
  sessionaffinity: none
  type: loadbalancer

this is my gcloud address list
deletable-pzk-reg-ip2-6           &lt;ipv6-address&gt;/96  external                    us-central1  pzksubnet1  reserved

i'm getting this error
  warning  syncloadbalancerfailed  13s (x6 over 2m49s)  service-controller  error syncing load balancer: failed to ensure load balancer: requested ip &quot;&lt;ipv6-address&gt;&quot; is neither static nor assigned to the lb

please help me in debugging this.
",<kubernetes><service><google-kubernetes-engine><ipv6>,76253055,1,"below troubleshooting steps can help you to resolve your issue:

ipv6 is only for http, ssl proxy and tcp proxy and make sure you are using one of them.

the following documentation describes creation of an ingress resource.



using the following reserve a regional external ipv6 address.
- gcloud compute addresses create &lt;your-ipv6-address-name&gt; --global --ip-version=ipv6

specify the global ip address in the yaml file using the annotation:
kubernetes.io/ingress.global-static-ip-name: &lt;your-ipv6-address-name&gt;



if you want to use load balancer check the load balancer parameters, example: after reserving the static ip use it as loadbalancedip in yaml, the load balancer will be created.

apiversion: v1
kind: servicemetadata:
  name: my-lb-service
spec:
  type: loadbalancer
  loadbalancerip: &lt;ip&gt;

attaching a blog http on load balancer and ipv6 authored by john hanley for your reference.
"
68476741,network policy not working with daemonset pods,"is a network policy applicable to pods of a daemonset? i have a default deny network policy for all ingress and egress for all pods. however, it does not seem to seem to be applied for pods belonging to the daemonset.
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: default-deny
spec:
  podselector: {}
  policytypes:
  - ingress
  - egress

",<kubernetes><kubernetes-pod><kubernetes-networkpolicy><calico><daemonset>,68477429,1,"netpol is applicable for the pods spawned under daemonset. for netpol they are just pods like the one deployed by deployments or rs.
if you do the description of the netpol you provided, it says its applicable for namespace=default.
name:         default-deny
namespace:    default
created on:   2021-07-21 17:59:56 -0500 cdt
labels:       &lt;none&gt;
annotations:  &lt;none&gt;
spec:
  podselector:     &lt;none&gt; (allowing the specific traffic to all pods in this namespace)
  allowing ingress traffic:
    &lt;none&gt; (selected pods are isolated for ingress connectivity)
  allowing egress traffic:
    &lt;none&gt; (selected pods are isolated for egress connectivity)
  policy types: ingress, egress

and netpol is a namespaced resource:
name                              shortnames   apiversion                             namespaced   kind
networkpolicies                   netpol       networking.k8s.io/v1                   true         networkpolicy

this means your daemonset is created under some different namespace.
here is one example:
created a daemon set:
kubectl get pod -n jackops  -o wide
name          ready   status    restarts   age   ip               node    nominated node   readiness gates
curl          1/1     running   0          53m   10.233.75.51     node2   &lt;none&gt;           &lt;none&gt;
dummy-2b9qv   1/1     running   0          50m   10.233.75.4      node2   &lt;none&gt;           &lt;none&gt;
dummy-tx9rl   1/1     running   0          50m   10.233.102.149   node1   &lt;none&gt;           &lt;none&gt;

verified that curl is working without netpol:
k exec -it curl -n jackops   -- curl -m 5  10.233.75.4 -i
http/1.1 200 ok
server: nginx/1.23.3
date: tue, 07 feb 2023 17:13:05 gmt
content-type: text/html
content-length: 615
last-modified: tue, 13 dec 2022 15:53:53 gmt
connection: keep-alive
etag: &quot;6398a011-267&quot;
accept-ranges: bytes

applied the below netpol:
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: default-deny
  namespace: jackops
spec:
  podselector: {}
  policytypes:
  - ingress

now, the connection is not going through:
k exec -it curl -n jackops   -- curl -m 5  10.233.75.4 -i
curl: (28) connection timed out after 5001 milliseconds
command terminated with exit code 28

about, knowing who is the parent of an object:
kubectl get pod -n jackops  dummy-2b9qv -ojsonpath='{.metadata.ownerreferences}'
[{&quot;apiversion&quot;:&quot;apps/v1&quot;,&quot;blockownerdeletion&quot;:true,&quot;controller&quot;:true,&quot;kind&quot;:&quot;daemonset&quot;,&quot;name&quot;:&quot;dummy&quot;,&quot;uid&quot;:&quot;cba5c840-672a-4ad8-830f-03993e32117a&quot;}]

"
68574011,kubernetes ingress-nginx rewrite without changing browser url,"we want to load example.xyz.com in site.abc.com. the best way is to redirect/rewrite all the requests from site.abc.com to example.xyz.com. however, we don't want the browser url to be changed. from this similar so problem we understand that we need an nginx location config as below
server {
    servername site.abc.com;
    listen        80;
    ....
    ....
    ....
    ....
    location / {                  
        proxy_pass  http://example.xyz.com;
        rewrite /(.*)$ /$1 break;
    }
}

however, i'm not sure how to create a similar rule in kubernetes ingress-nginx as it adds proxy_pass for each rule, which prevents us from adding proxy_pass config in nginx.ingress.kubernetes.io/configuration-snippet: annotation.
also providing nginx.ingress.kubernetes.io/rewrite-target: http://example.xyz.com/$1 annotation in ingress as below, redirects to example.xyz.com instead of loading example.xyz.com in site.abc.com.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: http://example.xyz.com/$1
  name: url-rewrite
  namespace: default
spec:
  rules:
  - host: site.abc.com
    http:
      paths:
      - backend:
          service:
            name: service
            port:
              number: 80
        path: /(.*)
        pathtype: implementationspecific

how can we load example.xyz.com in site.abc.com without any change in browser url using ingress-nginx in this case?
",<url><kubernetes><url-rewriting><kubernetes-ingress><nginx-ingress>,68584522,1,"with this solution as a reference, pointed out by @wytrzymaywiktor i was able to make changes and it worked.
here is the updated ingress file.
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/server-snippet: |
      location ~ &quot;^/(.*)&quot; {
        proxy_pass http://example.xyz.com;
        rewrite /(.*)$ /$1 break;
      }
  name: url-rewrite
  namespace: default
spec:
  rules:
  - host: site.abc.com

one problem though here in making ssl redirect work. in some cases the target(http://example.xyz.com) will return 302 /some/other/path in such cases http://site.abc.com gets redirected as http://site.abc.com/some/other/path. not sure how to make it to redirect as https://site.abc.com/some/other/path.
setting nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot; and nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot; doesn't seem to work.
adding this as an answer for documentation, as it will be helpful for people with a similar problem. not a possible duplicate as the referenced solution addresses on adding proxy_pass whereas this, addresses url rewrite without changing browser url.
"
76742300,pod is not getting mapped to service if deployed through yaml configuration,"i am trying to deploy the container using yaml.
the pod is getting created but service is not getting mapped to pod. what i am missing here?
be informed i am using trial subscription on google cloud and kubernetes-sdk on my machine.
deployment file
apiversion: apps/v1
kind: deployment
metadata:
  labels:
    app: udemydocker1
  name: udemydocker
  namespace: default
spec:
  progressdeadlineseconds: 30
  replicas: 1
  selector:
    matchlabels:
      app: udemydocker1
  strategy:
    rollingupdate:
      maxsurge: 25%
      maxunavailable: 25%
    type: rollingupdate
  template:
    metadata:
      creationtimestamp: null
      labels:
        app: udemydocker1
    spec:
      containers:
      - image: lazyav/udemydocker:release.1.0.0
        imagepullpolicy: always
        name: udemydocker
      restartpolicy: always
      
---
apiversion: v1
kind: service
metadata:
  labels:
    app: udemydocker1
  name: udemydocker
  namespace: default
spec:
  ports:
  - port: 8080
    protocol: tcp
    targetport: 8080
  selector:
    app: udemydocker
  sessionaffinity: none
  type: loadbalancer


dashboard screen shot and command line commands


",<kubernetes><google-cloud-platform><google-kubernetes-engine>,76742412,1,"the above issue is due to selector app: udemydocker. the service was unable to find the pod.
changing it to app=udemydocker1 solves the problem
apiversion: v1
kind: service
metadata:
  labels:
    app: udemydocker1
  name: udemydocker
  namespace: default
spec:
  ports:
  - port: 8080
    protocol: tcp
    targetport: 8080
  selector:
    app: udemydocker1
  sessionaffinity: none
  type: loadbalancer

"
76764211,kubernetes deny commnunications between pods,"i have two deployments in kubernetes on azure both with three replicas. both deployments use oauth2 reverse proxy for external users/requests authentication. the manifest file for both deployments looks like the following:

apiversion: apps/v1
kind: deployment
metadata:
  name: myservice1
  labels:
    aadpodidbinding: my-pod-identity-binding
spec:
  replicas: 3
  progressdeadlineseconds: 1800
  selector:
    matchlabels:
      app: myservice1
  template:
    metadata:
      labels:
        app: myservice1
        aadpodidbinding: my-pod-identity-binding
      annotations:
        aadpodidbinding.k8s.io/userassignedmsiclientid: pod-id-client-id
        aadpodidbinding.k8s.io/subscriptionid: my-subscription-id
        aadpodidbinding.k8s.io/resourcegroup: my-resource-group
        aadpodidbinding.k8s.io/usemsi: 'true'
        aadpodidbinding.k8s.io/clientid: pod-id-client-id
    spec:
      securitycontext:
        fsgroup: 2000
      containers:
        - name: myservice1
          image: mycontainerregistry.azurecr.io/myservice1:latest
          imagepullpolicy: always
          ports:
          - containerport: 5000
          securitycontext:
            runasuser: 1000
            allowprivilegeescalation: false
          readinessprobe:
            initialdelayseconds: 1
            periodseconds: 2
            timeoutseconds: 60
            successthreshold: 1
            failurethreshold: 1
            httpget:
              host:
              scheme: http
              path: /healthcheck
              port: 5000
              httpheaders:
              - name: host
                value: http://127.0.0.1
          resources:
            requests:
              memory: &quot;4g&quot;
              cpu: &quot;2&quot;
            limits:
              memory: &quot;8g&quot;
              cpu: &quot;4&quot;
          env:
          - name: message
            value: hello from the external app!!
---
apiversion: v1
kind: service
metadata:
  name: myservice1
spec:
  type: clusterip
  ports:
  - port: 80
    targetport: 5000
  selector:
    app: myservice1
---
apiversion: networking.k8s.io/v1
kind: ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/auth-url: &quot;https://myservice1.com/oauth2/auth&quot;
    nginx.ingress.kubernetes.io/auth-signin: &quot;https://myservice1.com/oauth2/start?rd=https://myservice1.com/oauth2/callback&quot;
    kubernetes.io/ingress.class: nginx-external
    nginx.org/proxy-connect-timeout: 3600s
    nginx.org/proxy-read-timeout: 3600s
    nginx.org/proxy-send-timeout: 3600s  
  name: myservice1-external
spec:
  rules:
  - host: myservice1.com
    http:
      paths:
      - path: /
        pathtype: prefix
        backend:
          service:
            name: myservice1
            port:
              number: 80


now, i want to restrict the communication between the pods in two ways:

intra-deployments: i want to deny any communication between the 3 pods of each deployments internally; meaning that all 3 pods can and must only communicate with their corresponding proxy (ingress part of the manifest)

inter-deployments: i want to deny any communications between any two pods belonging to two deployments; meaning that if for example pod1 from deployment1 tries lets say to ping or send http request to pod2 from deployment2; this will be denied.

allow requests throw proxies: the only requests that are allowed to enter must go through the correspoding deployment's proxy.


how to implement the manifest for the netwrok policy that achieves these requirements?
",<kubernetes><kubernetes-ingress><azure-aks>,76814072,1,"you can make use of networkpolicies and reference the policy in your ingress configuration like below:-
my networkpolicy.yml:-
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: default-deny-all
spec:
  podselector: {}
  policytypes:
  - ingress
  - egress


i applied it in my azure kubernetes like below:-
kubectl apply -f networkpolicy.yml
kubectl get networkpolicies




then use the below yml file to reference the networkpolicy in the ingress settings:-
ingress.yml:-
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: ingress-access
spec:
  podselector:
    matchlabels:
      app.kubernetes.io/name: ingress-nginx
  ingress:
  - from:
    - ipblock:
        cidr: 192.168.1.0/24
---
apiversion: networking.k8s.io/v1
kind: networkpolicy
metadata:
  name: ingress-to-backends
spec:
  podselector:
    matchlabels:
      app: myapp
  ingress:
  - from:
    - namespaceselector:
        matchlabels:
          ingress: &quot;true&quot;
      podselector:
        matchlabels:
          app.kubernetes.io/name: ingress-nginx



"
76814834,logback file with kubernetes,"i have a microservice application based on springboot and deployed on kubernetes with helm chart.
i'd like to continue to use the logback file we used previously to write the application logs in a formatted way.
this is my logback.xml file we used in:
&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;configuration scan=&quot;true&quot; scanperiod=&quot;10 seconds&quot; debug=&quot;true&quot;&gt;
    &lt;include resource=&quot;org/springframework/boot/logging/logback/base.xml&quot;/&gt;
    &lt;logger name=&quot;org.springframework.web&quot; level=&quot;info&quot;/&gt;
    &lt;logger name=&quot;com.netflix&quot; level=&quot;warn&quot;/&gt;
&lt;/configuration&gt;

this is my configmap.xml:
apiversion: v1
kind: configmap
metadata:
  name: {{ .values.app.name }}-configmap
  namespace: {{ .values.namespace }}
data:
  application.yml : |+
    server:
      ssl:
        enabled: false

i don't know how to add it.
",<spring-boot><kubernetes><logging><kubernetes-helm><logback>,76814951,1,"you need to add the logback.xml file to the root folder of the helm chart template.
this is the structure of the folder:
 app-root
    logback.xml
    values
    template
        configmap.xml
        deployment.xml
        ...

then, add in the configmap.yml file the import of that file:
apiversion: v1
kind: configmap
metadata:
  name: {{ .values.app.name }}-configmap
  namespace: {{ .values.namespace }}
data:
  application.yml : |+
    server:
      ssl:
        enabled: false
  logback.xml : |+
{{ .files.get &quot;base-logback.xml&quot; | indent 4 }}

"