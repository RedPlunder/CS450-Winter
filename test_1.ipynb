{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RedPlunder/CS450-Winter/blob/main/test_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG version: 1.0**\n",
        "\n",
        "**Prompt version: 3.0**\n",
        "\n",
        "**gpt version: 4o-mini**\n",
        "\n",
        "**key improvements: new XML prompt**\n",
        "\n",
        "*Last modification: Jinwei 3/12*\n"
      ],
      "metadata": {
        "id": "INOL5Qk_vd4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "0SCLSZFI4wIq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83c1e1ce-65d0-45ea-e9dc-27f22ce91a5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fRlgobfM27CF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import faiss\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import asyncio\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S31hVJ-x27CG"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dotdZMsN27CH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "261c4ad3-c899-4975-df47-b680e41b5145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "llm_choice = \"gpt\"\n",
        "openai.api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "client = openai.AsyncClient(api_key=userdata.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Set environment variable to prevent runtime issues\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
        "\n",
        "# Load tokenizer and model for embedding\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
        "\n",
        "# gpu for embed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARfJU-qb27CI"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### Load knowledge data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2UJL_uZo27CI"
      },
      "outputs": [],
      "source": [
        "# Load knowledge database\n",
        "rag_db = pd.read_csv(\"./dataset/kd.csv\")\n",
        "rag_data = rag_db[['ID', 'Content']].dropna()\n",
        "\n",
        "# Load knowledge database\n",
        "rag_data['Content'] = rag_data['Content'].str.lower()\n",
        "documents = rag_data['Content'].tolist()\n",
        "doc_ids = rag_data['ID'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gYaYgf27CI"
      },
      "source": [
        "### embed knowledge text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QY0cN1J927CI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d27881a-61b7-4de5-f073-e1c24ad73947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embeddings from file...\n",
            "FAISS indexing complete!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def embed(documents, batch_size=20):\n",
        "    \"\"\"Generate embeddings in batches to prevent memory overflow.\"\"\"\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        print(f\"embedding {i} / {len(documents)}\")\n",
        "        batch = documents[i:i+batch_size]\n",
        "        # Tokenize the batch with dynamic padding\n",
        "        inputs = tokenizer(batch, padding=\"longest\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        # move input to GPU\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "        # Disable gradient calculation for inference\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        #\n",
        "        embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
        "        all_embeddings.append(embeddings)\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "# Define the path for saving/loading embeddings\n",
        "embeddings_path = './dataset/doc_embeddings.npy'\n",
        "if os.path.exists(embeddings_path):\n",
        "    print(\"Loading embeddings from file...\")\n",
        "    # Load embeddings using memory mapping to avoid loading the entire file into RAM\n",
        "    doc_embeddings = np.load(embeddings_path, mmap_mode='r')\n",
        "else:\n",
        "    print(\"Generating embeddings in batches...\")\n",
        "    doc_embeddings = embed(documents)\n",
        "    np.save(embeddings_path, doc_embeddings)  # Save embeddings to disk\n",
        "\n",
        "# Build the FAISS index using L2 distance\n",
        "dimension = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(doc_embeddings)\n",
        "print(\"FAISS indexing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaqxNZZB27CJ"
      },
      "source": [
        "### Batch Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "id": "v0J84pui27CJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "220a4202-afd8-4ea4-ec8a-8b9ae71112b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 40 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 80 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 120 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 160 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 200 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 240 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 280 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 320 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 360 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 400 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 440 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 480 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 520 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 560 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 600 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 640 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 680 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 720 / 783 questions\n",
            "embedding 0 / 40\n",
            "embedding 20 / 40\n",
            "Processed 760 / 783 questions\n",
            "embedding 0 / 23\n",
            "embedding 20 / 23\n",
            "Processed 783 / 783 questions\n",
            "All questions processed and saved back to ./dataset/test.csv\n"
          ]
        }
      ],
      "source": [
        "# Retrieve and validate documents\n",
        "def retrieve_documents(queries, k=3):\n",
        "    \"\"\"\n",
        "    Efficiently retrieves relevant documents for a batch of queries.\n",
        "\n",
        "    Args:\n",
        "        queries (list of str): A list of query strings.\n",
        "        k (int): The number of documents to retrieve per query.\n",
        "\n",
        "    Returns:\n",
        "        list of dict: A list where each element corresponds to a query and contains\n",
        "                      the keys 'ids' and 'contents' for the retrieved documents.\n",
        "    \"\"\"\n",
        "    # Embed all queries at once to avoid redundant computations\n",
        "    query_embeddings = embed([q.strip().lower() for q in queries])\n",
        "\n",
        "    # Perform batch search in the index\n",
        "    distances, indices = index.search(np.array(query_embeddings), k)\n",
        "\n",
        "    # Retrieve and return documents and their IDs for each query\n",
        "    results = []\n",
        "    for query_indices in indices:\n",
        "        result = {\n",
        "            \"ids\": [doc_ids[i] for i in query_indices],\n",
        "            \"contents\": [documents[i] for i in query_indices]\n",
        "        }\n",
        "        results.append(result)\n",
        "    return results\n",
        "\n",
        "# Roughly truncates the input text to a maximum number of characters.\n",
        "def truncate_text(text, max_chars=4096):\n",
        "    return text[:max_chars] if len(text) > max_chars else text\n",
        "\n",
        "\n",
        "async def generate_responses(title_queries, body_queries, contexts):\n",
        "    \"\"\"\n",
        "    Efficiently generates responses for a batch of queries asynchronously.\n",
        "\n",
        "    Args:\n",
        "        title_queries (list of str): A list of question titles.\n",
        "        body_queries (list of str): A list of question bodies.\n",
        "        contexts (list of str): A list of retrieved knowledge corresponding to each query.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of generated responses.\n",
        "    \"\"\"\n",
        "\n",
        "    async def call_openai(prompt):\n",
        "        \"\"\"Helper function to call OpenAI API with retries.\"\"\"\n",
        "        for attempt in range(3):  # Retry up to 3 times if needed\n",
        "            try:\n",
        "                response = await client.chat.completions.create(\n",
        "                    model=\"gpt-4o-mini\",\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    max_tokens=4090,\n",
        "                    temperature=0\n",
        "                )\n",
        "                return response.choices[0].message.content  # Successful response\n",
        "            except openai.RateLimitError:\n",
        "                print(f\"Rate limit error, retrying ({attempt+1}/3)...\")\n",
        "                await asyncio.sleep(2**attempt)  # Exponential backoff\n",
        "            except Exception as e:\n",
        "                print(f\"OpenAI API Error: {e}\")\n",
        "                return f\"API Error: {e}\"  # Store the exact error message\n",
        "        return \"API Error: Max retries exceeded.\"\n",
        "\n",
        "    # Prepare prompts using the new structured format\n",
        "    prompts = [\n",
        "        f\"\"\"\n",
        "        <prompt>\n",
        "            <retrieved_knowledge>\n",
        "                <![CDATA[ {context} ]]>\n",
        "            </retrieved_knowledge>\n",
        "\n",
        "            <user_query>\n",
        "                <title>{title_query}</title>\n",
        "                <body>{body_query}</body>\n",
        "            </user_query>\n",
        "\n",
        "            <instructions>\n",
        "                <summary>\n",
        "                    You are a Kubernetes expert and troubleshooting assistant. Your task is to diagnose and resolve Kubernetes-related\n",
        "                    issues using structured reasoning, verification, and best practices. Use the retrieved knowledge to analyze the problem step-by-step.\n",
        "                </summary>\n",
        "\n",
        "                <structured_debugging_approach>\n",
        "                    <step1>Localization: Identify the exact YAML field, CLI flag, or Kubernetes object causing the issue.</step1>\n",
        "                    <step2>Reasoning: Explain the precise root cause based on Kubernetes internals, dependencies, and common misconfigurations.</step2>\n",
        "                    <step3>Remediation: Provide a verified fix (YAML or CLI command) with proper justifications.</step3>\n",
        "                    <step4>Verification: Ensure that the suggested YAML is syntactically correct, schema-compliant, and valid for Kubernetes.</step4>\n",
        "                </structured_debugging_approach>\n",
        "\n",
        "                <problem_solving_strategy>\n",
        "                    <step1>\n",
        "                        <title>Reference Retrieved Knowledge First</title>\n",
        "                        <description>\n",
        "                            - Analyze the retrieved knowledge above before generating an answer.\n",
        "                            - If the retrieved knowledge lacks necessary information, apply the fallback strategy.\n",
        "                        </description>\n",
        "                    </step1>\n",
        "\n",
        "                    <step2>\n",
        "                        <title>Apply Localization → Reasoning → Remediation</title>\n",
        "                        <description>\n",
        "                            - <b>Localization:</b> Identify where the issue is occurring (YAML, CLI, or Kubernetes component).\n",
        "                            - <b>Reasoning:</b> Explain why the issue is occurring using Kubernetes behavior and dependencies.\n",
        "                            - <b>Remediation:</b> Provide a precise, tested, and verified fix.\n",
        "                        </description>\n",
        "                    </step2>\n",
        "\n",
        "                    <step3>\n",
        "                        <title>Fallback Strategy for Missing Knowledge</title>\n",
        "                        <description>\n",
        "                            - If retrieved knowledge does not contain a direct answer, do not refuse to answer.\n",
        "                            - Instead, state that the retrieved knowledge is insufficient, then apply Kubernetes best practices to suggest a fix.\n",
        "                            - Example: \"The retrieved knowledge does not provide a direct solution, but based on Kubernetes best practices, here’s a possible fix...\"\n",
        "                        </description>\n",
        "                    </step3>\n",
        "\n",
        "                    <step4>\n",
        "                        <title>YAML Validation Before Output</title>\n",
        "                        <description>\n",
        "                            - Before returning a YAML fix, verify its correctness by ensuring:\n",
        "                                1. **Valid YAML syntax** (correct indentation, structure, and format).\n",
        "                                2. **Schema compliance** (fields match Kubernetes API structure).\n",
        "                                3. **API compatibility** (uses the correct apiVersion and supported fields).\n",
        "                            - If the YAML is incorrect, automatically correct it before returning output.\n",
        "                            - Example: \"The suggested YAML contained an incorrect apiVersion. It has been corrected to apps/v1 before output.\"\n",
        "                        </description>\n",
        "                    </step4>\n",
        "\n",
        "                    <step5>\n",
        "                        <title>Chain-of-Thought (CoT) Debugging for Edge Cases</title>\n",
        "                        <description>\n",
        "                            - If an explanation is necessary, provide a concise reason first before the YAML/CLI fix.\n",
        "                            - Example: \"This error occurs because apiVersion: apps/v1beta1 is outdated. Use the following YAML instead:\"\n",
        "                        </description>\n",
        "                    </step5>\n",
        "\n",
        "                    <step6>\n",
        "                        <title>Generate a Solution-Driven, Verified Answer</title>\n",
        "                        <description>\n",
        "                            - **Issue Summary:** Clearly define the problem based on user input and retrieved knowledge.\n",
        "                            - **Root Cause Analysis:** Deduce why the issue is occurring step-by-step.\n",
        "                            - **Step-by-Step Fix:** Provide an exact, actionable YAML or CLI command fix.\n",
        "                            - **Verification:** Ensure that the solution is correct and aligns with Kubernetes best practices.\n",
        "                            - **References:** Cite relevant Kubernetes documentation if applicable.\n",
        "                            - **Common Mistakes & Best Practices:** Highlight potential pitfalls and how to avoid them.\n",
        "                        </description>\n",
        "                    </step6>\n",
        "\n",
        "                    <step7>\n",
        "                        <title>Ensure a Precise Output Format (YAML or Command Preferred)</title>\n",
        "                        <description>\n",
        "                            - If the answer involves configurations, output only the corrected YAML.\n",
        "                            - If the answer requires CLI commands, output only the correct kubectl commands.\n",
        "                            - If explanation is needed (edge cases), keep it concise and relevant.\n",
        "                        </description>\n",
        "                    </step7>\n",
        "\n",
        "                    <step8>\n",
        "                        <title>No Speculation Allowed</title>\n",
        "                        <description>\n",
        "                            - If retrieved knowledge is completely lacking, state that and suggest best practices instead.\n",
        "                            - Only rely on retrieved knowledge, structured reasoning, and Kubernetes best practices.\n",
        "                        </description>\n",
        "                    </step8>\n",
        "                </problem_solving_strategy>\n",
        "\n",
        "                <output_format>\n",
        "                    <description>\n",
        "                        If a YAML or CLI fix is applicable, return only that output. If explanation is needed, keep it concise and relevant.\n",
        "                    </description>\n",
        "                </output_format>\n",
        "            </instructions>\n",
        "        </prompt>\n",
        "        \"\"\"\n",
        "        for title_query, body_query, context in zip(title_queries, body_queries, contexts)\n",
        "    ]\n",
        "\n",
        "    # Run all API calls asynchronously\n",
        "    tasks = [call_openai(prompt) for prompt in prompts]\n",
        "    responses = await asyncio.gather(*tasks)\n",
        "\n",
        "    return responses\n",
        "\n",
        "\n",
        "\n",
        "async def process_questions(file_path, batch_size=40):\n",
        "    # Load CSV file into a Pandas DataFrame\n",
        "    df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
        "\n",
        "    # Ensure necessary columns exist\n",
        "    top_1_col = f\"{llm_choice}_Top_1_Context\"\n",
        "    top_2_col = f\"{llm_choice}_Top_2_Context\"\n",
        "    top_3_col = f\"{llm_choice}_Top_3_Context\"\n",
        "    context_merged_col = f\"{llm_choice}_Merged_Contexts\"  # Merged for GPT input\n",
        "    response_col_name = f\"{llm_choice}_Generated_Response\"\n",
        "    context_id_col_name = f\"{llm_choice}_Context_IDs\"\n",
        "\n",
        "    if context_merged_col not in df.columns:\n",
        "        df[context_merged_col] = \"\"  # Initialize with empty strings\n",
        "    if response_col_name not in df.columns:\n",
        "        df[response_col_name] = \"\"  # Initialize with empty strings\n",
        "    if context_id_col_name not in df.columns:\n",
        "        df[context_id_col_name] = \"\"  # Initialize with empty strings\n",
        "\n",
        "    for col in [top_1_col, top_2_col, top_3_col, context_merged_col, response_col_name, context_id_col_name]:\n",
        "        if col not in df.columns:\n",
        "            df[col] = \"\"  # Initialize with empty strings\n",
        "\n",
        "    # Get list of unanswered questions\n",
        "    unanswered_mask = df[f\"{llm_choice}_Generated_Response\"] == \"\"\n",
        "    unanswered_df = df[unanswered_mask]\n",
        "\n",
        "    # Process questions in batches\n",
        "    for start in range(0, len(unanswered_df), batch_size):\n",
        "\n",
        "        batch = unanswered_df.iloc[start : start + batch_size]\n",
        "\n",
        "        # Since test 2, retrieve body and title seperately.\n",
        "        title_queries = batch[\"Question Title\"].tolist()\n",
        "        body_queries = batch[\"Question Body\"].tolist()\n",
        "\n",
        "        # Retrieve context in batch (each result is a dict with 'ids' and 'contents')\n",
        "        results = retrieve_documents(body_queries, k=3)\n",
        "\n",
        "        # Extract top 3 contexts individually\n",
        "\n",
        "        top_1_contexts = [truncate_text(result[\"contents\"][0]) if len(result[\"contents\"]) > 0 else \"\" for result in results]\n",
        "        top_2_contexts = [truncate_text(result[\"contents\"][1]) if len(result[\"contents\"]) > 1 else \"\" for result in results]\n",
        "        top_3_contexts = [truncate_text(result[\"contents\"][2]) if len(result[\"contents\"]) > 2 else \"\" for result in results]\n",
        "\n",
        "        # For each result, truncate each document's content and join them\n",
        "        merged_contexts = [\n",
        "            \" \".join([truncate_text(doc) for doc in result[\"contents\"]])\n",
        "            for result in results\n",
        "        ]\n",
        "        # Create context_id strings (concatenated document IDs)\n",
        "        context_ids = [\", \".join([str(doc_id) for doc_id in result['ids']]) for result in results]\n",
        "\n",
        "        # Generate responses asynchronously\n",
        "        responses = await generate_responses(title_queries, body_queries, merged_contexts)\n",
        "\n",
        "        # Update the DataFrame with responses\n",
        "        df.loc[batch.index, response_col_name] = responses\n",
        "        df.loc[batch.index, context_merged_col] = merged_contexts  # Keep merged for reference\n",
        "        df.loc[batch.index, top_1_col] = top_1_contexts\n",
        "        df.loc[batch.index, top_2_col] = top_2_contexts\n",
        "        df.loc[batch.index, top_3_col] = top_3_contexts\n",
        "        df.loc[batch.index, context_id_col_name] = context_ids\n",
        "        print(f\"Processed {start + len(batch)} / {len(unanswered_df)} questions\")\n",
        "\n",
        "\n",
        "    # Save the updated CSV file with responses\n",
        "    df.to_csv(file_path, index=False, encoding=\"utf-8\")\n",
        "    print(f\"All questions processed and saved back to {file_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"./dataset/test.csv\"\n",
        "    await process_questions(file_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}