{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_choice = \"gpt\"\n",
    "openai.api_key = userdata.get(\"OPENAI_API_KEY\")\n",
    "client = openai.Client(api_key=userdata.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Set environment variable to prevent runtime issues\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Load tokenizer and model for embedding\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load knowledge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load knowledge database\n",
    "rag_db = pd.read_csv(\"./dataset/kd.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embed knowledge text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_data = rag_db[['ID', 'Concept']].dropna()\n",
    "\n",
    "# Convert knowledge content to lowercase for better matching\n",
    "rag_data['Concept'] = rag_data['Concept'].str.lower()\n",
    "\n",
    "# Embed documents from RAG for retrieval\n",
    "documents = rag_data['Concept'].tolist()\n",
    "def embed(documents):\n",
    "    inputs = tokenizer(documents, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "if os.path.exists('./dataset/doc_embeddings.npy'):\n",
    "    print(\"Loading embeddings from file...\")\n",
    "    doc_embeddings = np.load('./dataset/doc_embeddings.npy')\n",
    "else:\n",
    "    print(\"Generating embeddings...\")\n",
    "    doc_embeddings = embed(documents)\n",
    "    np.save('./dataset/doc_embeddings.npy', doc_embeddings)\n",
    "\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and validate documents\n",
    "def retrieve_documents(queries, k=3):\n",
    "    \"\"\"\n",
    "    Efficiently retrieves relevant documents for a batch of queries.\n",
    "    \n",
    "    Args:\n",
    "        queries (list of str): A list of query strings.\n",
    "        k (int): The number of documents to retrieve per query.\n",
    "    \n",
    "    Returns:\n",
    "        list of list: A list containing retrieved documents for each query.\n",
    "    \"\"\"\n",
    "    # Embed all queries at once to avoid redundant computations\n",
    "    query_embeddings = embed([q.strip().lower() for q in queries])\n",
    "    \n",
    "    # Perform batch search in the index\n",
    "    distances, indices = index.search(np.array(query_embeddings), k)\n",
    "\n",
    "    # Retrieve and return documents for each query\n",
    "    return [[documents[i] for i in query_indices] for query_indices in indices]\n",
    "\n",
    "def generate_responses(queries, contexts):\n",
    "    \"\"\"\n",
    "    Efficiently generates responses for a batch of queries.\n",
    "    \n",
    "    Args:\n",
    "        queries (list of str): A list of query strings.\n",
    "        contexts (list of str): A list of context strings corresponding to each query.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of generated responses.\n",
    "    \"\"\"\n",
    "    messages_batch = [\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"You are recognized as a Kubernetes and NGINX ingress expert. Before providing an answer, validate the provided context for \"\n",
    "                \"errors, deprecated features, or potential conflicts. Always adhere to the latest Kubernetes and NGINX standards. \"\n",
    "                \"Identify and clearly explain any assumptions made based on the context, and provide necessary corrections or enhancements.\"\n",
    "            )},\n",
    "            {\"role\": \"user\", \"content\": (\n",
    "                f\"Given the following detailed context and choose what you think fit information for question:\\n{context}\\nCan you provide a validated and comprehensive response to this query:\\n{query}\\n\"\n",
    "                \"Your response should:\\n\"\n",
    "                \"1. Include YAML configurations with accurate and effective annotations tailored to address the query.\\n\"\n",
    "                \"2. Explain the rationale behind each configuration and validate them against the provided context and current best practices.\\n\"\n",
    "                \"3. Highlight and discuss any potential issues or critical assumptions that could affect the implementation.\\n\"\n",
    "                \"4. Offer detailed debugging steps and troubleshooting advice to verify and refine the solution.\"\n",
    "            )}\n",
    "        ] for query, context in zip(queries, contexts)\n",
    "    ]\n",
    "\n",
    "    responses = []\n",
    "    for messages in messages_batch:\n",
    "        try:\n",
    "            #fix for openai 1.0.0+\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=messages,\n",
    "                max_tokens=4090,\n",
    "                temperature=0\n",
    "            )\n",
    "            responses.append(response.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            responses.append(\"Error occurred.\")\n",
    "\n",
    "    return responses\n",
    "\n",
    "# Load CSV, process questions, and update responses\n",
    "def process_questions(file_path, batch_size=10):\n",
    "    # Load CSV file into a Pandas DataFrame\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "\n",
    "    # Ensure necessary columns exist\n",
    "    context_col_name = f\"{llm_choice}_Top_3_Contexts\"\n",
    "    response_col_name = f\"{llm_choice}_Generated_Response\"\n",
    "\n",
    "    if context_col_name not in df.columns:\n",
    "        df[context_col_name] = \"\"  # Initialize with empty strings\n",
    "\n",
    "    if response_col_name not in df.columns:\n",
    "        df[response_col_name] = \"\"  # Initialize with empty strings\n",
    "\n",
    "    # Get list of unanswered questions\n",
    "    unanswered_mask = df[f\"{llm_choice}_Generated_Response\"] == \"\"\n",
    "    unanswered_df = df[unanswered_mask]\n",
    "\n",
    "    # Process questions in batches\n",
    "    for start in range(0, len(unanswered_df), batch_size):\n",
    "        batch = unanswered_df.iloc[start : start + batch_size]\n",
    "\n",
    "        queries = batch[\"Question Body\"].tolist()\n",
    "\n",
    "        # Retrieve context in batch\n",
    "        contexts = [\" \".join(docs) for docs in retrieve_documents(queries, k=3)]\n",
    "\n",
    "        # Generate responses in batch\n",
    "        responses = generate_responses(queries, contexts)\n",
    "\n",
    "        # Update the DataFrame with responses\n",
    "        df.loc[batch.index, response_col_name] = responses\n",
    "        df.loc[batch.index, context_col_name] = contexts\n",
    "        print(f\"Processed {start + len(batch)} / {len(unanswered_df)} questions\")\n",
    "\n",
    "    # Save the updated CSV file with responses\n",
    "    df.to_csv(file_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"All questions processed and saved back to {file_path}\")\n",
    "\n",
    "# Run batch processing\n",
    "file_path = \"./dataset/test.csv\"  # Replace with your CSV file path\n",
    "process_questions(file_path, batch_size=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
