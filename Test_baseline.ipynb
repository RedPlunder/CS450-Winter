{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RedPlunder/CS450-Winter/blob/main/Test_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG version: N/A**\n",
        "\n",
        "**Prompt version: N/A**\n",
        "\n",
        "**gpt version: 4o-mini**\n",
        "\n",
        "*Last Modification: 3/10 Jinwei*\n"
      ],
      "metadata": {
        "id": "NcQVw_PKway4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRlgobfM27CF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve OpenAI API key from Google Colab user data\n",
        "openai.api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Use OpenAI AsyncClient for parallel API calls\n",
        "client = openai.AsyncClient(api_key=openai.api_key)\n",
        "\n",
        "# Allow running asyncio inside Jupyter Notebook (Colab)\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "h-tVrLFAY0HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def generate_responses(queries):\n",
        "    \"\"\"\n",
        "    Generate responses for a batch of queries using OpenAI API.\n",
        "    \"\"\"\n",
        "    messages_batch = [[{\"role\": \"user\", \"content\": query}] for query in queries]\n",
        "\n",
        "    async def fetch_response(messages):\n",
        "        try:\n",
        "            response = await client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=messages,\n",
        "                max_tokens=4090,\n",
        "                temperature=0\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching response: {e}\")\n",
        "            return \"Error: Could not generate response.\"\n",
        "\n",
        "    # Execute API calls in parallel\n",
        "    responses = await asyncio.gather(*(fetch_response(messages) for messages in messages_batch))\n",
        "    return responses"
      ],
      "metadata": {
        "id": "eJhzvQsAY7Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def process_questions(file_path, batch_size=40):\n",
        "    \"\"\"\n",
        "    Process unanswered questions in a CSV file, generate responses, and save the results.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
        "\n",
        "    # Ensure the response column exists\n",
        "    response_col_name = \"gpt_Generated_Response\"\n",
        "    if response_col_name not in df.columns:\n",
        "        df[response_col_name] = \"\"\n",
        "\n",
        "    # Filter unanswered questions\n",
        "    unanswered_mask = df[response_col_name] == \"\"\n",
        "    unanswered_df = df[unanswered_mask]\n",
        "\n",
        "    # Process questions in batches\n",
        "    for start in range(0, len(unanswered_df), batch_size):\n",
        "        batch = unanswered_df.iloc[start : start + batch_size]\n",
        "        queries = batch[\"Question Body\"].tolist()\n",
        "\n",
        "        # Generate responses using OpenAI API\n",
        "        responses = await generate_responses(queries)\n",
        "\n",
        "        # Update DataFrame with responses\n",
        "        df.loc[batch.index, response_col_name] = responses\n",
        "\n",
        "        # Display progress\n",
        "        processed = start + len(batch)\n",
        "        progress_percent = (processed / len(unanswered_df)) * 100\n",
        "        print(f\"Processed {processed} / {len(unanswered_df)} questions ({progress_percent:.1f}%)\")\n",
        "\n",
        "    # Save updated CSV file\n",
        "    df.to_csv(file_path, index=False, encoding=\"utf-8\")\n",
        "    print(f\"Processing complete. Results saved to {file_path}\")"
      ],
      "metadata": {
        "id": "RV2c-KCAY7tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    file_path = \"./dataset/test.csv\"\n",
        "    await process_questions(file_path)"
      ],
      "metadata": {
        "id": "uSobwyofY95i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}