{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "29YgiCREWKq8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28\n",
      "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/anaconda3/envs/bench/lib/python3.9/site-packages (from openai==0.28) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/bench/lib/python3.9/site-packages (from openai==0.28) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/bench/lib/python3.9/site-packages (from openai==0.28) (3.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/bench/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/bench/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/bench/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/bench/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/bench/lib/python3.9/site-packages (from aiohttp->openai==0.28) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/bench/lib/python3.9/site-packages (from aiohttp->openai==0.28) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/bench/lib/python3.9/site-packages (from aiohttp->openai==0.28) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/bench/lib/python3.9/site-packages (from aiohttp->openai==0.28) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/bench/lib/python3.9/site-packages (from aiohttp->openai==0.28) (1.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/anaconda3/envs/bench/lib/python3.9/site-packages (from aiohttp->openai==0.28) (4.0.3)\n",
      "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.14.2\n",
      "    Uninstalling openai-1.14.2:\n",
      "      Successfully uninstalled openai-1.14.2\n",
      "Successfully installed openai-0.28.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0zvhaTm5WU4m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/anaconda3/envs/bench/lib/python3.9/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/bench/lib/python3.9/site-packages (from faiss-cpu) (23.2)\n",
      "Downloading faiss_cpu-1.10.0-cp39-cp39-macosx_11_0_arm64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pnVWHzd4WRF2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bench/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set environment variable to prevent runtime issues\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = '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'\n",
    "\n",
    "# Load tokenizer and model for embedding\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "\n",
    "# Load RAG database\n",
    "rag_db = pd.read_csv(\"/Users/zhengnan/Documents/CS450/kd/kd.csv\")\n",
    "rag_data = rag_db[['ID', 'Concept']].dropna()\n",
    "\n",
    "# Convert RAG content to lowercase for better matching\n",
    "rag_data['Concept'] = rag_data['Concept'].str.lower()\n",
    "\n",
    "# Embed documents from RAG for retrieval\n",
    "documents = rag_data['Concept'].tolist()\n",
    "\n",
    "def embed(documents):\n",
    "    inputs = tokenizer(documents, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "if os.path.exists('doc_embeddings.npy'):\n",
    "    print(\"Loading embeddings from file...\")\n",
    "    doc_embeddings = np.load('doc_embeddings.npy')\n",
    "else:\n",
    "    print(\"Generating embeddings...\")\n",
    "    doc_embeddings = embed(documents)\n",
    "    np.save('doc_embeddings.npy', doc_embeddings)\n",
    "\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# Retrieve and validate documents\n",
    "def retrieve_documents(query, k=3):\n",
    "    query_embedding = embed([query.strip().lower()])[0]\n",
    "    distances, indices = index.search(np.array([query_embedding]), k)\n",
    "    return [documents[i] for i in indices[0]]\n",
    "\n",
    "# Function to generate a response using OpenAI API\n",
    "def generate_response(query, context):\n",
    "    print(\"\\n\\nContext passed: \")\n",
    "    print(context)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": (\n",
    "            \"You are recognized as a Kubernetes and NGINX ingress expert. Before providing an answer, validate the provided context for \"\n",
    "            \"errors, deprecated features, or potential conflicts. Always adhere to the latest Kubernetes and NGINX standards. \"\n",
    "            \"Identify and clearly explain any assumptions made based on the context, and provide necessary corrections or enhancements.\"\n",
    "        )},\n",
    "        {\"role\": \"user\", \"content\": (\n",
    "            f\"Given the following detailed context and choose what you think fit information for question:\\n{context}\\nCan you provide a validated and comprehensive response to this query:\\n{query}\\n\"\n",
    "            \"Your response should:\\n\"\n",
    "            \"1. Include YAML configurations with accurate and effective annotations tailored to address the query.\\n\"\n",
    "            \"2. Explain the rationale behind each configuration and validate them against the provided context and current best practices.\\n\"\n",
    "            \"3. Highlight and discuss any potential issues or critical assumptions that could affect the implementation.\\n\"\n",
    "            \"4. Offer detailed debugging steps and troubleshooting advice to verify and refine the solution.\"\n",
    "        )}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            max_tokens=4090,\n",
    "            temperature=0\n",
    "        )\n",
    "        return response.choices[0].message['content']\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Have Error.\"\n",
    "\n",
    "# User query\n",
    "query = input(\"Question: \")\n",
    "retrieved_docs = retrieve_documents(query, k=3)\n",
    "context = \" \".join(retrieved_docs)\n",
    "\n",
    "# GPT Response\n",
    "response = generate_response(query, context)\n",
    "print(\"GPT Answer:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
